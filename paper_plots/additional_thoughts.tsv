response_id	text	vagrant_themes	marius_themes	marius_themes (new)
26	"Mechanistic interpretability" is a marketing term, overhyped and is more tea-leaf reading than it is providing actual benefits for recipients of explanations. That subfield used to be called "component analysis", but then it got flooded with weak research that does the opposite of giving us actionable insights into models. Still, it is a good gateway to bring new people into the field.	hype/marketing, actionability, gateway for new people	hype/marketing, actionability, gateway for new people	
1	A main concern in this form is that it biases me by asking the question if it is useful for X. It is always helpful to know a bit more. But, perhaps the amount of work would have just be better spent with less superficial understanding and more trying, or with more math, or many other things that can drive the field. So the fact some action has value doesn't mean one should do it.	opportunity cost, survey bias	opportunity cost, survey bias	
75	Analysis and interpretability are necessary for ensuring advanced AI systems are safe. We should avoid creating highly advanced models (e.g. human-level or superhuman at most cognitive tasks) until we have methods that allow us to confidently understand their behavior, including on edge cases and out of distribution.	safety, superhuman models	safety, superhuman models	
113	For Q11, I interpret "how important is model analysis and interpretability research to work in the areas" to mean how important interpretability *should be*, not how important/influential it *currently is*. Currently, interpretability research has virtually no impact on any work improving models for NLP tasks or scientific work trying to better understand linguistic phenomena or the nature of intelligence. More specifically, interpretability methods are used widely in NLP applications and downstream scientific research, but we have only extremely limited evidence of widespread utility of explanation methods. For 99% of use cases I see, people are just fooling themselves with these methods. 	no impact, no rigour, no utility	no impact, no rigour, no utility	
41	I believe model analysis and interpretability is a bit like theoretical physics. Engineers dont need to be able to derive physical formulas from first principles to build things (masons in medieval times did not even now how to do math and built cathedrals for example), but it is useful to understand why it works. This has implications on safety and on limitations of models, but above all I believe it is a pursuit of knowledge in its own right.	valuable for its own sake, why it works	valuable for its own sake, why it works	
48	I don't get why don't we have more tools that use practically findings from interpretability research - research of practical use cases from interpretability would force more understanding and recognition in now mostly untouched regions 	tooling, utility	tooling, utility	
93	I encourage more researchers to work on the easiness of visualization toolbox of interpretability methods. And make visualization to a wild range of data instances, instead of a few. How to make interpretability results e.g. visualizations of circuits with statistical significance is very important for actual motivating downstream innovations.	visualization	visualization	
120	I like it when model analysis and interpretability papers have empirical results on large datasets. 	large datasets	large datasets	
36	I think that its a very broad area where many people think its just LIME or Shapely values for attribution or maybe now it getting conflated with mechanistic interpretability (which i feel falls under the umbrella of interpretability reseach), but it really touches on many areas ( data influence, learned concepts, neuron/layer importance, fairness, etc ).  I think its use for guiding/improving models/LLMs is hugely important in order for the area of work to be considered more useful and not something that is only used for linguistic or semantic capability studies.	modelling improvements, actionability, overuse in linguistics	modelling improvements, actionability, overuse in linguistics	
24	I think the impact and "usefulness" of interpretability has been determined more by extrinsic sociological factors in the broader field of AI, rather than by the actual value or findings of the interpretability work. Interpretability asks hard questions about what models actually do, why they do it - but ML practitioners may simply not be interested in these questions.	lack of interest from ML practitioners, who defines impact, why it works	lack of interest from ML practitioners, who defines impact, why it works	
40	I view interpretability research as science - it primarily seeks to *understand* how AI systems are working, and perhaps more broadly how intelligence is working.  I don't think it should be judged by measuring *how useful* it has been for building new things in the world, especially in its current nascent state.  The downstream effects of fundamental understanding are hard to predict. I think it's reasonable to bet that a better understanding of models will lead to concrete applications down the line. But even if there turn out not to be downstream applications, interpretability is a noble academic pursuit in itself.	valuable for its own sake, why it works, who defines impact, new field	valuable for its own sake, why it works, who defines impact, new field	
62	Interpretability for NLP is very different from interpretability for tabular data, where it actually did achieve success. The success in those fields created a false impression for what it can do in NLP, so it would be best to differentiate the two cases. 	who defines impact	who defines impact	
52	It's cool! That's enough for me.	valuable for its own sake	valuable for its own sake	
53	My new paper (NAACL24) touches on various aspects including utility of CoT. https://arxiv.org/abs/2402.11863	plug	plug	
127	One could also argue that this is all valuable for curiosityâ€™s sake, and this is true for a lot of research after all. But tying it to an actual insight on language or good lm training would be most interesting	valuable for its own sake, actionability, modeling improvements	valuable for its own sake, actionability, modeling improvements	
59	One issue of the field is that it is often unreliable in a way that is hard to detect. The techniques give a reasonable sounding explanation for a phenomenon we observed, but it is very difficult to tell if that explanation is actually correct. This can lead to problems as researchers, managers and decision makers can cherry-pick their preferred interpretation. This is not much of an issue in the technical areas (improving model capabilities) where wrong interpretations simply will result in failure. But it is a large problem in non-technical areas, if people base social decisions (what to tell the user, compliance with laws) on this data, since there is no way to verify it. It would be like accepting a study in psychology without even reading what the sample size or the P-value was. Making this more reliable strikes me as one of the most important things to do in the field.	reliable methods, trust, faithfulness	reliable methods, trust, faithfulness	
18	The field has become extremely Balkanized in ways that are not advantages	divisions	divisions	
44	This definition is too broad for my taste! Model analysis seems to encompass scaling laws, singular learning theory and inductive bias research, while interpretability focuses (as far as I have experienced) on explanations of model behaviour that arise from studying model internals, which coincides with the broader model analysis.	survey bias	survey bias	
97	This field of study needs to be opened to the community in a clear and well-defined way. Some researchers consider interpretability and explainability as the same concept, while others treat them differently. Such discrepancies could impede scientific collaboration.	big picture, openness	big picture, openness	
121	When we talk about "progress in NLP", there are so many aspects to consider: e.g.,  improved fluency, assistants that actually "assist" their user and follow their instructions, etc. etc. But better control or mitigating biases more successfully are also progress, and shouldn't be downplayed, though they won't be counted as "new capabilities". In 2016, Tay bot got shut down less than a day after its launch, because it ended up posting horrifying content. The fact that we have safe guards now that don't let that happen so easily is progress. Only celebrating the goal scorer is myopic, we should also value the goal keeper.	who defines impact	who defines impact	
63	Without it, we're not doing science.	valuable for its own sake	valuable for its own sake	
27	You defined model analysis and interpretability, but you didn't define what you meant by "progress in NLP". I view model analysis and interpretability as progress in it's own right.	valuable for its own sake, survey bias	valuable for its own sake, survey bias	