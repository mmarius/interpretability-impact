	citation_count	semantic_scholar_id	url	title	venue	year	track	interp_citation_prop	abstract	marius_topic	vagrant_topic	marius_chat	classifier_wrong	predicted_track_new_classifier
0	8257	395de0bd3837fdf4b4b5e5f04835bcc69c279481	doi.org/10.18653/v1/2020.acl-main.703	BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension	ACL	2020	Generation	14.285714285714285	We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.	novel model, seq-2-seq, pre-training, denoising	novel model, pre-training, denoising	added seq-2-seq		Generation
1	7827	93d63ec754f29fa22572615320afe0521f7ec66d	doi.org/10.18653/v1/D19-1410	Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks	EMNLP	2019	Machine Learning	15.789473684210526	BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.	novel model, embeddings, STS, Siamese	novel model, embeddings, STS, Siamese			Machine Learning
2	4939	6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6	doi.org/10.18653/v1/2020.acl-main.747	Unsupervised Cross-lingual Representation Learning at Scale	ACL	2020	Semantics: Sentence Level	9.523809523809524	This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.	novel model, multilingual, representation learning, cross-lingual transfer 	novel model, multilingual, representation learning, cross-lingual transfer 			Machine Translation and Multilinguality
3	3217	1e077413b25c4d34945cc2707e17e46ed4fe784a	doi.org/10.18653/v1/P18-1031	Universal Language Model Fine-tuning for Text Classification	ACL	2018	Machine Learning	27.586206896551722	Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.	novel method, fine-tuning, classification, transfer learning	novel method, fine-tuning, classification, transfer learning			Machine Learning
4	3120	c4744a7c2bb298e4a52289a1e085c71cc3d37bc6	doi.org/10.18653/v1/P19-1285	Transformer-XL: Attentive Language Models beyond a Fixed-Length Context	ACL	2019	Machine Learning	42.1875	Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.	novel model, architecture, long context	novel model, long context	added architecture, long context		Machine Learning
5	2622	53d8b356551a2361020a948f64454a6d599af69f	doi.org/10.18653/v1/2021.acl-long.353	Prefix-Tuning: Optimizing Continuous Prompts for Generation	ACL	2021	Language Generation	19.607843137254903	Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.	novel method, prompting, fine-tuning, prefix-tuning	novel method, prompting, fine-tuning, prefix-tuning	yes 		Generation
6	2426	ffdbd7f0b03b85747b001b4734d5ee31b5229aa4	doi.org/10.18653/v1/2021.emnlp-main.243	The Power of Scale for Parameter-Efficient Prompt Tuning	EMNLP	2021	Machine Learning for NLP	16.363636363636363	In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.	novel method, parameter-efficient methods, prompting, soft prompts, scaling, prefix-tuning	novel method, parameter-efficient methods, prompting, soft prompts, scaling, prefix-tuning			Machine Learning
7	2392	b26f2037f769d5ffc5f7bdcec2de8da28ec14bee	doi.org/10.18653/v1/2020.emnlp-main.550	Dense Passage Retrieval for Open-Domain Question Answering	EMNLP	2020	Question Answering	1.8518518518518516	Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.	novel model, novel method, retrieval, embeddings, representation learning, QA	novel model, novel method, retrieval, embeddings, representation learning, QA			Question Answering
8	2328	4d1c856275744c0284312a3a50efb6ca9dc4cd4c	doi.org/10.18653/v1/P18-2124	Know What You Don’t Know: Unanswerable Questions for SQuAD	ACL	2018	Best Paper Session	8.0	Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.	novel dataset, evaluation, novel research problem, QA, unanswerability	novel dataset, evaluation, novel research problem, QA, unanswerability			Question Answering
9	2308	156d217b0a911af97fa1b5a71dc909ccef7a8028	doi.org/10.18653/v1/D19-1371	SciBERT: A Pretrained Language Model for Scientific Text	EMNLP	2019	Information Extraction, Text Mining and NLP Applications, Social Media and Computational Social Science, Sentiment Analysis and Argument Mining	6.451612903225806	Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.	novel model, scientific text, pre-training	novel model, scientific text, pre-training			Inconclusive
10	2274	c26759e6c701201af2f62f7ee4eb68742b5bf085	doi.org/10.18653/v1/2021.emnlp-main.552	SimCSE: Simple Contrastive Learning of Sentence Embeddings	EMNLP	2021	Semantics: Lexical, Sentence level, Textual Inference and Other areas	14.285714285714285	This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using “entailment” pairs as positives and “contradiction” pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman’s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show—both theoretically and empirically—that contrastive learning objective regularizes pre-trained embeddings’ anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.	novel method, contrastive learning, embeddings, representation learning, pre-training	novel method, contrastive learning, embeddings, representation learning, pre-training			Inconclusive
11	2163	d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea	doi.org/10.18653/v1/P19-1355	Energy and Policy Considerations for Deep Learning in NLP	ACL	2019	Multidisciplinary	20.0	Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.	ethics, policy, environment, efficiency 	ethics, policy, environment, efficiency 			Machine Learning
12	2113	17dbd7b72029181327732e4d11b52a08ed4630d0	doi.org/10.1162/tacl_a_00276	Natural Questions: a Benchmark for Question Answering Research	ACL	2019	Question Answering	6.451612903225806	We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.	novel dataset, evaluation, QA	novel dataset, evaluation, QA			Question Answering
13	2066	79c93274429d6355959f1e4374c2147bb81ea649	doi.org/10.18653/v1/D19-1514	LXMERT: Learning Cross-Modality Encoder Representations from Transformers	EMNLP	2019	Speech, Vision, Robotics, Multimodal and Grounding	9.090909090909092	Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert	novel model, multimodal, representation learning, vision-language models, pre-training	novel model, multimodal, representation learning, vision-language models, pre-training			Multimodality, Speech and Grounding
14	2032	d0086b86103a620a86bc918746df0aa642e2a8a3	doi.org/10.18653/v1/D19-1250	Language Models as Knowledge Bases?	EMNLP	2019	Question Answering, Textual Inference and Other Areas of Semantics	23.684210526315788	Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.	analysis, evaluation, knowledge bases, factual knowledge, QA	analysis, evaluation, knowledge bases, factual knowledge, QA		1	Question Answering
15	1859	e816f788767eec6a8ef0ea9eddd0e902435d4271	doi.org/10.18653/v1/2020.acl-main.740	Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks	ACL	2020	Semantics: Sentence Level	13.23529411764706	Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.	novel method, fine-tuning, adaptation	novel method, pre-training	they fine-tuning, not pre-train		Machine Learning
16	1849	b4df354db88a70183a64dbc9e56cf14e7669a6c0	doi.org/10.18653/v1/P18-1238	Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning	ACL	2018	Vision, Multimodal, Grounding, Speech	6.0606060606060606	We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.	novel dataset, image captioning, vision, multimodal, vision-languags models	novel dataset, image captioning, vision, multimodal, vision-languags models			Multimodality, Speech and Grounding
17	1685	81f5810fbbab9b7203b9556f4ce3c741875407bc	doi.org/10.1162/tacl_a_00300	SpanBERT: Improving Pre-training by Representing and Predicting Spans	ACL	2020	Machine Learning for NLP	14.545454545454545	We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-Large, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.	novel method, novel model, pre-training, denoising	novel method, novel model, pre-training, denoising			Information Extraction/Retrieval
18	1577	22655979df781d222eaf812b0d325fa9adf11594	doi.org/10.18653/v1/D18-1259	HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering	EMNLP	2018	Area A (Information Extraction and Question Answering) [LONG]	0.0	Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.	novel dataset, QA, explainability	novel dataset, QA, explainability			Question Answering
19	1570	162cad5df347bdac469331df540440b320b5aa21	doi.org/10.18653/v1/D19-1670	EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks	EMNLP	2019	Information Extraction, Text Mining and NLP Applications, Social Media and Computational Social Science, Sentiment Analysis and Argument Mining	14.705882352941178	We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use.	novel method, data augmentation, classification	novel method, data augmentation, classification			Machine Learning
20	1462	85e7d63f75c0916bd350a229e040c5fbb1472e7a	doi.org/10.18653/v1/2021.acl-long.295	Making Pre-trained Language Models Better Few-shot Learners	ACL	2021	Machine Learning for NLP	17.307692307692307	The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.	novel method, few-shot, fine-tuning, prompting	novel method, few-shot, fine-tuning, prompting			Machine Learning
21	1414	495da6f19baa09c6db3697d839e10432cdc25934	doi.org/10.1162/tacl_a_00343	Multilingual Denoising Pre-training for Neural Machine Translation	EMNLP	2020	Machine Translation and Multilinguality	4.918032786885246	This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.	novel model, multilingual, denoising, pre-training, MT	novel model, multilingual, denoising, pre-training, MT			Machine Translation and Multilinguality
22	1248	29de7c0fb3c09eaf55b20619bceaeafe72fd87a6	doi.org/10.18653/v1/P18-1082	Hierarchical Neural Story Generation	ACL	2018	Generation	17.24137931034483	We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.	novel dataset, novel method, story generation	novel dataset, novel method, story generation			Summarization
23	1242	63748e59f4e106cbda6b65939b77589f40e48fcb	doi.org/10.18653/v1/D19-1387	Text Summarization with Pretrained Encoders	EMNLP	2019	Summarization	5.555555555555555	Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings.	application, summarization, novel method, fine-tuning, representation learning, long context	application, summarization, novel method, fine-tuning, representation learning, long context			Summarization
24	1232	97906df07855b029b7aae7c2a1c6c5e8df1d531c	doi.org/10.18653/v1/P19-1452	BERT Rediscovers the Classical NLP Pipeline	ACL	2019	Sentence-level semantics	30.0	Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.	analysis, linguistics, tagging, parsing, NER, coreference	analysis, linguistics, tagging, parsing, NER, coreference		1	Interpretability and Analysis
25	1228	6c7046195f64cccac1ed3275d88d77655534b5a4	doi.org/10.18653/v1/P18-1205	Personalizing Dialogue Agents: I have a dog, do you have pets too?	ACL	2018	Dialog System	3.4482758620689653	Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.	novel dataset, dialogue agents, personalization	novel dataset, dialogue agents, personalization			Dialogue
26	1218	305b2cf37e5dece81e95c92883d5a6e28ac93b22		Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization	EMNLP	2018	Area F (Discourse, Dialog, Summarization, Generation, Multimodal NLP) [LONG]	12.5	We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.	novel dataset, novel model, summarization, novel task, novel model	novel dataset, novel model, summarization, novel task, novel model			Summarization
27	1186	809cc93921e4698bde891475254ad6dfba33d03b	doi.org/10.18653/v1/P19-1493	How Multilingual is Multilingual BERT?	ACL	2019	Multilinguality	6.25	In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.	analysis, multilingual, cross-lingual transfer, zero-shot, probing	analysis, multilingual, cross-lingual transfer, zero-shot, probing		1	Machine Translation and Multilinguality
28	1174	5f994dc8cae24ca9d1ed629e517fcc652660ddde	doi.org/10.18653/v1/P19-1139	ERNIE: Enhanced Language Representation with Informative Entities	ACL	2019	Information Extraction and Text Mining	5.172413793103448	Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.	novel model, knowledge graphs, representation learning, pre-training, fusion	novel model, knowledge graphs, representation learning, pre-training	fusion?		Inconclusive
29	1129	f4a5503783487eba5c5e34b1d02c09016b244b1d	doi.org/10.18653/v1/D18-1547	MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling	EMNLP	2018	Area F (Discourse, Dialog, Summarization, Generation, Multimodal NLP) [LONG]	0.0	Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.	novel dataset, dialogue	novel dataset, dialogue			Dialogue
30	1128	658721bc13b0fa97366d38c05a96bf0a9f4bb0ac	doi.org/10.18653/v1/P19-1441	Multi-Task Deep Neural Networks for Natural Language Understanding	ACL	2019	Sentence-level semantics	5.714285714285714	In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.	novel model, NLU, pre-training, multi-task learning, representation learning	novel model, NLU, pre-training, multi-task learning, representation learning			Machine Learning
31	1114	cb0f3ee1e98faf92429d601cdcd76c69c1e484eb	doi.org/10.1162/tacl_a_00290	Neural Network Acceptability Judgments	EMNLP	2019	Linguistic Theories, Cognitive Modeling and Psycholinguistics	15.384615384615385	Abstract This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on specific grammatical phenomena reveals that both Lau et al.’s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.	novel dataset, acceptability, linguistics 	novel dataset, acceptability, linguistics 			Inconclusive
32	1081	335613303ebc5eac98de757ed02a56377d99e03a	doi.org/10.18653/v1/P19-1356	What Does BERT Learn about the Structure of Language?	ACL	2019	Multidisciplinary	56.52173913043478	BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.	analysis, linguistics, meaning, syntax	analysis, linguistics, meaning, syntax		1	Interpretability and Analysis
33	1067	42ed4a9994e6121a9f325f5b901c5b3d7ce104f5	doi.org/10.18653/v1/P19-1334	Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference	ACL	2019	Semantics	40.816326530612244	A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.	analysis, novel dataset, evaluation, linguistics, syntax, NLI, heuristics, reasoning, generalization	analysis, novel dataset, evaluation, linguistics, syntax, NLI, heuristics, reasoning, generalization		1	Inconclusive
34	1056	e65b346d442e9962a4276dc1c1af2956d9d5f1eb	doi.org/10.18653/v1/2023.acl-long.754	Self-Instruct: Aligning Language Models with Self-Generated Instructions	ACL	2023	Large Language Models	23.076923076923077	Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.	novel method, alignment, instruction-following, generation, instruction-tuning	novel method, alignment, instruction-following, generation, instruction-tuning			Generation
35	1047	a75649771901a4881b44c0ceafa469fcc6e6f968		How Can We Know What Language Models Know	EMNLP	2020	Language Generation	24.561403508771928	Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a _ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a _" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.	analysis, evaluation, factual knowledge, prompting, novel method	analysis, evaluation, factual knowledge, prompting, novel method		1	Inconclusive
36	1046	4ae52766028e69186052ea8f33a137fbbbdb986a	doi.org/10.18653/v1/2020.acl-main.704	BLEURT: Learning Robust Metrics for Text Generation	ACL	2020	Generation	6.122448979591836	Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.	novel metric, generation, generalization, pre-training	novel metric, generation, generalization, pre-training			Generation
37	983	e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e	doi.org/10.18653/v1/P18-1007	Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates	ACL	2018	Machine Translation	22.857142857142858	Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.	novel method, regularization, tokenization, MT	novel method, regularization, tokenization, MT			Machine Translation and Multilinguality
38	975	8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad	doi.org/10.18653/v1/P19-1472	HellaSwag: Can a Machine Really Finish Your Sentence?	ACL	2019	Textual Inference and Other Areas of Semantics	21.052631578947366	Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.	novel dataset, commonsense, NLI, reasoning, adversarial evaluation 	novel dataset, commonsense, NLI, reasoning, adversarial evaluation 			Inconclusive
39	912	a30f912f8c5e2a2bfb06351d4578e1ba3fa37896	doi.org/10.18653/v1/2021.emnlp-main.685	CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation	EMNLP	2021	NLP Applications	5.714285714285714	Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.	novel model, code, pre-training, generation, seq-2-seq	novel model, code, pre-training, generation	added seq-2-seq		Inconclusive
40	902	41d49ec6f73ab5621ab8e8cb5ddb677a886ccc76	doi.org/10.18653/v1/D19-1018	Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects	EMNLP	2019	Sentiment Analysis and Argument Mining	11.76470588235294	Several recent works have considered the problem of generating reviews (or ‘tips’) as a form of explanation as to why a recommendation might match a customer’s interests. While promising, we demonstrate that existing approaches struggle (in terms of both quality and content) to generate justifications that are relevant to users’ decision-making process. We seek to introduce new datasets and methods to address the recommendation justification task. In terms of data, we first propose an ‘extractive’ approach to identify review segments which justify users’ intentions; this approach is then used to distantly label massive review corpora and construct large-scale personalized recommendation justification datasets. In terms of generation, we are able to design two personalized generation models with this data: (1) a reference-based Seq2Seq model with aspect-planning which can generate justifications covering different aspects, and (2) an aspect-conditional masked language model which can generate diverse justifications based on templates extracted from justification histories. We conduct experiments on two real-world datasets which show that our model is capable of generating convincing and diverse justifications.	novel model, recommender systems, explanations, explainability, evaluation 	novel model, recommender systems, explanations, explainability, evaluation 			Inconclusive
41	898	33ec7eb2168e37e3007d1059aa96b9a63254b4da	doi.org/10.18653/v1/2020.acl-main.442	Beyond Accuracy: Behavioral Testing of NLP Models with CheckList	ACL	2020	Resources and Evaluation	34.48275862068966	Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.	evaluation, novel framework, behavioural testing, generalization 	evaluation, novel framework, behavioural testing, generalization 			Inconclusive
42	888	d47a682723f710395454687319bb55635e653105	doi.org/10.18653/v1/2020.acl-main.485	Language (Technology) is Power: A Critical Survey of "Bias" in NLP	ACL	2020	Ethics and NLP	13.513513513513514	We survey 146 papers analyzing "bias" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing "bias" is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating "bias" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing "bias" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of "bias"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements---and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.	survey, social bias, critique, bias detection, bias mitigation, harms	survey, social bias, critique, bias detection, bias mitigation, harms			Ethics
43	883	949fef650da4c41afe6049a183b504b3cc91f4bd	doi.org/10.18653/v1/P19-1656	Multimodal Transformer for Unaligned Multimodal Language Sequences	ACL	2019	Vision, Robotics, Multimodal, Grounding and Speech	5.128205128205128	Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise crossmodal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art methods by a large margin. In addition, empirical analysis suggests that correlated crossmodal signals are able to be captured by the proposed crossmodal attention mechanism in MulT.	novel model, multimodal, time series, architecture, attention	novel model, multimodal, time series, attention	architecture		Multimodality, Speech and Grounding
44	858	514e7fb769950dbe96eb519c88ca17e04dc829f6	doi.org/10.18653/v1/P18-2006	HotFlip: White-Box Adversarial Examples for Text Classification	ACL	2018	Machine Learning	48.0	We propose an efficient method to generate white-box adversarial examples to trick a character-level neural classifier. We find that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic flip operation, which swaps one token for another, based on the gradients of the one-hot input vectors. Due to efficiency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classifier as well.	novel method, adversarial robustness, classification	novel method, adversarial robustness, classification			Machine Learning
45	855	07a64686ce8e43ac475a8d820a8a9f1d87989583	doi.org/10.18653/v1/P19-1580	Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned	ACL	2019	Machine Translation	37.142857142857146	Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.	analysis, novel method, attention analysis, pruning, interpretability, MT	analysis, novel method, attention analysis, pruning, interpretability, MT		1	Machine Translation and Multilinguality
46	846	160563abbd75265b19afc8b4169bab9e1eb33d97	doi.org/10.1162/tacl_a_00288	Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond	EMNLP	2019	Sentence-level Semantics	0.0	Abstract We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.	novel model, novel dataset, embeddings, zero-shot, cross-lingual transfer, multilingual, representation learning	novel model, novel dataset, embeddings, zero-shot, cross-lingual transfer, multilingual, representation learning			Machine Translation and Multilinguality
47	820	a81874b4a651a740fffbfc47ef96515e8c7f782f	doi.org/10.18653/v1/P19-1612	Latent Retrieval for Weakly Supervised Open Domain Question Answering	ACL	2019	Question Answering	9.375	Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.	novel method, retrieval, QA, weak supervision, multi-task learning	novel method, retrieval, QA, weak supervision, multi-task learning			Question Answering
48	815	87e02a265606f31e65986f3c1c448a3e3a3a066e	doi.org/10.18653/v1/2022.emnlp-main.759	Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?	EMNLP	2022	Language Modeling and Analysis of Language Models	24.59016393442623	Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.	demonstrations, in-context learning, analysis, understanding	demonstrations, in-context learning, analysis, understanding		1	Interpretability and Analysis
49	814	b61c6405f4de381758e8b52a20313554d68a9d85	doi.org/10.18653/v1/2020.acl-main.645	CamemBERT: a Tasty French Language Model	ACL	2020	Machine Learning for NLP	7.894736842105263	Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models --in all languages except English-- very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.	novel model, pre-training	novel model, pre-training			Machine Translation and Multilinguality