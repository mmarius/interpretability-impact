	id	title	area	source	year	doi	abstract	semantic_scholar_id	track	citation_count	marius_themes	vagrant_themes
3937	4382	Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?	Language Modeling and Analysis of Language Models	EMNLP	2022	10.18653/v1/2022.emnlp-main.759	Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.	87e02a265606f31e65986f3c1c448a3e3a3a066e	Interpretability and Analysis	817	in-context learning, analysis, demonstrations, understanding	demonstrations, in-context learning, analysis, understanding
2974		StereoSet: Measuring stereotypical bias in pretrained language models	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.416	A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.	babeda48b10a4d638252118f2238d05a06f4ec55	Interpretability and Analysis	648	stereotype detection, bias detection, novel dataset, analysis, data analysis	stereotype detection, bias detection, novel dataset, evaluation
1399	main.421	On the Cross-lingual Transferability of Monolingual Representations	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.421	State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.	9e9d919c1de684ca42c8b581ec62c7aa685f431e	Interpretability and Analysis	631	cross-lingual transfer, multilingual, novel method, novel benchmark, representation learning, analysis	cross-lingual transfer, multilingual, novel method, novel benchmark, representation learning, analysis
1589	tacl.1852	What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models	Interpretability and Analysis of Models for NLP	ACL	2020	10.1162/tacl_a_00298	Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction — and, in particular, it shows clear insensitivity to the contextual impacts of negation.	a0e49f65b6847437f262c59d0d399255101d0b75	Interpretability and Analysis	537	psycholinguistics, evaluation, linguistics, diagnostics, negation, novel dataset	psycholinguistics, evaluation, linguistics, diagnostics, negation, novel dataset
1440	main.385	Quantifying Attention Flow in Transformers	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.385	In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.	76a9f336481b39515d6cea2920696f11fb686451	Interpretability and Analysis	516	attention analysis, explanation, information flow	attention analysis, explanation, information flow
1448	main.408	ERASER: A Benchmark to Evaluate Rationalized NLP Models	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.408	State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the `reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER\, a benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of ``rationales'' (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/	087dd95e13efd47aef2a6582e6801b39fc0f83d8	Interpretability and Analysis	489	interpretability, rationales, novel benchmark, faithfulness, evidence, reasoning, explanations	interpretability, rationales, novel benchmark, faithfulness, evidence, reasoning, explanations
1387	main.386	Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.386	With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is "defined" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.	579476d19566efc842929ea6bdd18ab760c8cfa2	Interpretability and Analysis	419	faithfulness, novel guidelines, explainability, interpretability	faithfulness, novel guidelines, explainability, interpretability
367	main.2644	CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.154	Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.	645bd6eadc247989abc5e0b0aa0be79ec8b11ea6	Interpretability and Analysis	418	social bias, novel dataset, bias detection, evaluation	social bias, novel dataset, bias detection, evaluation
1895		Transformer Feed-Forward Layers Are Key-Value Memories	Interpretability and Analysis of Models for NLP	EMNLP	2021	10.18653/v1/2021.emnlp-main.446	Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys’ input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model’s layers via residual connections to produce the final output distribution.	4a54d58a4b20e4f3af25cea3c188a12082a95e02	Interpretability and Analysis	362	FFN analysis, novel metaphor, representational analysis, MLP analysis, key-value memories	FFN analysis, novel metaphor, representational analysis, MLP analysis, key-value memories
3403	475	Red Teaming Language Models with Language Models	Language Modeling and Analysis of Language Models	EMNLP	2022	10.18653/v1/2022.emnlp-main.225	Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.	5d49c7401c5f2337c4cc88d243ae39ed659afe64	Interpretability and Analysis	343	red teaming, adversarial evaluation, privacy, bias evaluation, harms	red teaming, adversarial evaluation, privacy, bias evaluation, harms
378	main.2705	Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.746	Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments on four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.	ee5fff85d3ec62698eddba162f054b7e73670b2a	Interpretability and Analysis	331	training dynamics, data maps, data analysis, model analysis, novel tool, analysis, evaluation	training dynamics, data maps, data analysis, model analysis, novel tool, analysis, evaluation
3175		Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.568	Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.	e54ffc76d805c48660bb0fd20019ca82ac94ba0d	Interpretability and Analysis	288	effectiveness, fine-tuning, dimensionality, understanding, analysis	effectiveness, fine-tuning, dimensionality, understanding, analysis
391	main.2763	AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.346	The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.	b68b2e81ae2de647394ec05ee62ecf108bf2b50a	Interpretability and Analysis	263	prompting, knowledge, novel method, parameter-efficient methods, evaluation	prompting, knowledge, novel method, parameter-efficient methods, evaluation
737	main.947	Information-Theoretic Probing with Minimum Description Length	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.14	To measure how well  pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier  trained to predict the  property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect  differences in representations.  For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.	f4b585c9a79dfce0807b445a09036ea0f9cbcdce	Interpretability and Analysis	241	information theory, probing, novel method, linguistics	information theory, probing, novel method, linguistics
4671	1845	Knowledge Neurons in Pretrained Transformers	Interpretability and Analysis of Models for NLP	ACL	2022	10.18653/v1/2022.acl-long.581	Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.	2c871df72c52b58f05447fcb3afc838168d94505	Interpretability and Analysis	238	factual knowledge, neuron analysis, analysis, novel method, attribution analysis, model editing	factual knowledge, neuron analysis, analysis, novel method, attribution analysis, model editing
1283	main.491	Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.491	Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods.	cffd8f947ba03644f62baea31c64c8920b06288e	Interpretability and Analysis	234	explainability, evaluation, humans, effectiveness, analysis, methods evaluation, human evaluation	explainability, evaluation, humans, effectiveness, analysis, methods evaluation, human evaluation
4774	144	FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation	Language Modeling and Analysis of Language Models	EMNLP	2023	10.18653/v1/2023.emnlp-main.741	Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.	bd5deadc58ee45b5e004378ba1d54a96bc947b4a	Interpretability and Analysis	213	evaluation, novel metric, factual knowledge, generation	evaluation, novel metric, factual knowledge, generation
261	main.2179	COGS: A Compositional Generalization Challenge Based on Semantic Interpretation	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.731	Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99%), but generalization accuracy was substantially lower (16--35%) and showed high sensitivity to random seed (+-6--8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.	b20ddcbd239f3fa9acc603736ac2e4416302d074	Interpretability and Analysis	207	novel dataset, parsing, compositional generalization	novel dataset, compositional generalization, parsing
1893		Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little	Interpretability and Analysis of Models for NLP	EMNLP	2021	10.18653/v1/2021.emnlp-main.230	A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.	4e00843bc5f60d2b9116abc4320af6d184422291	Interpretability and Analysis	197	distributional hypothesis, training dynamics, word order, probing, linguistics, analysis, pre-training, fine-tuning	distributional hypothesis, training dynamics, word order, probing, linguistics, analysis, pre-training, fine-tuning
4898	851	CodeT5+: Open Code Large Language Models for Code Understanding and Generation	Language Modeling and Analysis of Language Models	EMNLP	2023	10.18653/v1/2023.emnlp-main.68	Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.	9ada8fa11b1cdece31f253acae50b62df8d5f823	Interpretability and Analysis	193	generation, code, novel model, evaluation	generation, code, novel model, evaluation
1444	main.420	Information-Theoretic Probing for Linguistic Structure	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.420	The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually ``know'' about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task. A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic operationalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inherent in the representation. The experimental portion of our paper focuses on empirically estimating the mutual information between a linguistic property and BERT, comparing these estimates to several baselines. We evaluate on a set of ten typologically diverse languages often underrepresented in NLP research---plus English---totalling eleven languages. Our implementation is available in https://github.com/rycolab/info-theoretic-probing.	738c6d664aa6c3854e1aa894957bd595f621fc42	Interpretability and Analysis	185	probing, linguistics, information theory, novel method	probing, linguistics, information theory, novel method
1558	main.432	Learning to Deceive with Attention-Based Explanations	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.432	Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.	cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a	Interpretability and Analysis	172	attention analysis, explanation, analysis, adversarial evaluation, reliability, novel method	attention analysis, explanation, analysis, adversarial evaluation, reliability, novel method
741	main.958	A Diagnostic Study of Explainability Techniques for Text Classification	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.263	Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.	ae06bc1e8e67c27b89329ebcfe61b71625d853f6	Interpretability and Analysis	170	diagnostics, explainability, classification, evaluation, saliency scores, explanations, methods evaluation	diagnostics, explainability, classification, evaluation, saliency scores, explanations, methods evaluation
2979		Amnesic Probing: Behavioral Explanation With Amnesic Counterfactuals	Interpretability and Analysis of Models for NLP	ACL	2021	10.1162/tacl_a_00359	Abstract A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.1	1d7f3297924a9dd90cfc0df522ebe9138c28b46f	Interpretability and Analysis	170	counterfactuals, novel method, probing, causality, representation analysis	counterfactuals, novel method, probing, causality, representation analysis
267	main.2215	When BERT Plays the Lottery, All Tickets Are Winning	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.259	{L}arge {T}ransformer-based models were shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning. For fine-tuned {BERT}, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained {BERT} weights are potentially useful. We also study the ``good" subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.	91ac65431b2dc46919e1673fde67671c29446812	Interpretability and Analysis	167	lottery ticket hypothesis, pruning, subnetworks, analysis	lottery ticket hypothesis, pruning, subnetworks, analysis
662	main.616	Attention is Not Only a Weight: Analyzing Transformers with Vector Norms	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.574	Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and speciﬁc linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The ﬁndings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These ﬁndings provide insights into the inner workings of Transformers.	9a21740d87976bf76f4a9668a9da631035302fb2	Interpretability and Analysis	162	attention analysis, norm-based analysis, analysis, understanding, novel method	attention analysis, norm-based analysis, analysis, understanding, novel method
3181	164	Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space	Interpretability, Interactivity and Analysis of Models for NLP	EMNLP	2022	10.18653/v1/2022.emnlp-main.3	Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.	cf36236015c9f93f15bfafbf282f69e08bdc9c16	Interpretability and Analysis	160	novel metaphor, concepts, FFN analysis, analysis, predictions, interpretability, vocabulary projection, reverse engineering	novel metaphor, concepts, FFN analysis, analysis, predictions, interpretability, vocabulary projection, reverse engineering
1548	main.383	Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.383	By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.	3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f	Interpretability and Analysis	154	probing, novel method, linguistics, parameter-free method, sentiment	probing, novel method, linguistics, parameter-free method, sentiment
1049	main.489	A Re-evaluation of Knowledge Graph Completion Methods	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.489	Knowledge Graph Completion (KGC) aims at automatically predicting missing links for large-scale knowledge graphs. A vast number of state-of-the-art KGC techniques have got published at top conferences in several research fields, including data mining, machine learning, and natural language processing. However, we notice that several recent papers report very high performance, which largely outperforms previous state-of-the-art methods. In this paper, we find that this can be attributed to the inappropriate evaluation protocol used by them and propose a simple evaluation protocol to address this problem. The proposed protocol is robust to handle bias in the model, which can substantially affect the final results. We conduct extensive experiments and report performance of several existing methods using our protocol. The reproducible code has been made publicly available.	1fb3fa2a6a8c0d7a58b1d5dee8b676104d1a5da6	Interpretability and Analysis	142	knowledge graph, evaluation analysis, knowledge graph completion, novel metric	knowledge graph, evaluation analysis, knowledge graph completion, novel metric
1498	main.431	Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.431	Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings --- while more diverse and mature than those available for their dynamic counterparts --- are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.	d34580c522c79d5cde620331dd9ffb18643a8090	Interpretability and Analysis	140	interpretability, representation analysis, novel method, evaluation, social bias, bias detection, contextualized representations, static embeddings	interpretability, representation analysis, novel method, evaluation, social bias, bias detection, contextualized representations, static embeddings
1333	main.492	Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.492	Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which `saliency maps' may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.	0696ad8beb0d765973aa5cdbc6e118889d3583b0	Interpretability and Analysis	139	explainability, influence functions, interpretability, methods evaluation, NLI, novel method	explainability, influence functions, interpretability, methods evaluation, NLI, novel method
1398	main.409	Learning to Faithfully Rationalize by Construction	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.409	In many settings it is important for one to be able to understand why a model made a particular prediction. In NLP this often entails extracting snippets of an input text 'responsible for' corresponding model output; when such a snippet comprises tokens that indeed informed the model's prediction, it is a faithful explanation. In some settings, faithfulness may be critical to ensure transparency. Lei et al. (2016) proposed a model to produce faithful rationales for neural text classification by defining independent snippet extraction and prediction modules. However, the discrete selection over input tokens performed by this method complicates training, leading to high variance and requiring careful hyperparameter tuning. We propose a simpler variant of this approach that provides faithful explanations by construction. In our scheme, named FRESH, arbitrary feature importance scores (e.g., gradients from a trained model) are used to induce binary labels over token inputs, which an extractor can be trained to predict. An independent classifier module is then trained exclusively on snippets provided by the extractor; these snippets thus constitute faithful explanations, even if the classifier is arbitrarily complex. In both automatic and manual evaluations we find that variants of this simple framework yield predictive performance superior to 'end-to-end' approaches, while being more general and easier to train. Code is available at https://github.com/successar/FRESH.	922e6e3bafe38a712597c05d3a907bd10763b427	Interpretability and Analysis	138	explainabaility, interpretability, faithfulness, novel method, rationales	explainabaility, interpretability, faithfulness, novel method, rationales
2480		Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.37	Backdoor attacks are a kind of insidious security threat against machine learning models. After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference. As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently. As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort. In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks. We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses. These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks. All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.	ac6d17a1e4345b6699965fca636590edb91f10a8	Interpretability and Analysis	137	adversarial evaluation, backdoor attacks, evaluation, novel method, evaluation	adversarial evaluation, backdoor attacks, evaluation, novel method, evaluation
524	main.3304	Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.557	Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).	016760dc4a05489ddf5dbb48aecbb49e214e1b71	Interpretability and Analysis	135	probing, commonsense knowledge, numerical knowledge, novel dataset, analysis	probing, commonsense knowledge, numerical knowledge, novel dataset, analysis
3982	4535	Efficient Large Scale Language Modeling with Mixtures of Experts	Language Modeling and Analysis of Language Models	EMNLP	2022	10.18653/v1/2022.emnlp-main.804	Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.	fb01415a0decfa3f3d6339930e95028ae1ff4170	Interpretability and Analysis	133	mixture of experts, evaluation, scaling	mixture of experts, evaluation, scaling
1298	main.493	Finding Universal Grammatical Relations in Multilingual BERT	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.493	Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks' internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.	1376a8e1b06b7a7b7cacd45f52268e427c3b0135	Interpretability and Analysis	130	linguistics, multilingual, cross-lingual transfer, representation analysis, understanding, analysis, novel method	linguistics, multilingual, cross-lingual transfer, representation analysis, understanding, analysis, novel method
1871		Measuring Association Between Labels and Free-Text Rationales	Interpretability and Analysis of Models for NLP	EMNLP	2021		In interpretable NLP, we require faithful rationales that reflect the model’s decision-making process for an explained instance. While prior work focuses on extractive rationales (a subset of the input words), we investigate their less-studied counterpart: free-text natural language rationales. We demonstrate that *pipelines*, models for faithful rationalization on information-extraction style tasks, do not work as well on “reasoning” tasks requiring free-text rationales. We turn to models that *jointly* predict and rationalize, a class of widely used high-performance models for free-text rationalization. We investigate the extent to which the labels and rationales predicted by these models are associated, a necessary property of faithful explanation. Via two tests, *robustness equivalence* and *feature importance agreement*, we find that state-of-the-art T5-based joint models exhibit desirable properties for explaining commonsense question-answering and natural language inference, indicating their potential for producing faithful free-text rationales.	343e06bae852f74a98573e798b501f6003bcb1c0	Interpretability and Analysis	130	rationales, faithfulness, interpretability, explanations, free-text rationales, generation, explainability	rationales, faithfulness, interpretability, explanations, free-text rationales, generation, explainability
3979	4515	ZeroGen: Efficient Zero-shot Learning via Dataset Generation	Language Modeling and Analysis of Language Models	EMNLP	2022	10.18653/v1/2022.emnlp-main.801	There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZeroGen.Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL).Apart from being annotation-free and efficient, we argue that ZeroGen can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of ZeroGen.	2145fcceeb69385e108bf1796d52f974854d4c0b	Interpretability and Analysis	117	zero-shot, synthetic data, generation, novel method, distillation, evaluation	zero-shot, synthetic data, generation, novel method, distillation, evaluation
2978		CausaLM: Causal Model Explanation Through Counterfactual Language Models	Interpretability and Analysis of Models for NLP	ACL	2021	10.1162/coli_a_00404	Abstract Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning–based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1	4f0318290bc75294338fdeb450e4365929b3aa0c	Interpretability and Analysis	116	causality, explanations, novel framework, evaluation, fine-tuning	causality, explanations, novel framework, evaluation, fine-tuning
2551		When Do You Need Billions of Words of Pretraining Data?	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.90	NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.	31392ad8722d9c66181b621936e2013199e02edc	Interpretability and Analysis	112	analysis, pre-training data, data analysis, training dynamics, understanding, probing	analysis, pre-training data, data analysis, training dynamics, understanding, probing
2623		Implicit Representations of Meaning in Neural Language Models	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.143	Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity’s current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.	ac879df2cc36f3f824fa24149517622b6bc7bd09	Interpretability and Analysis	111	probing, steering, representation analysis	linguistics, meaning, representational analysis, understanding, analysis, probing, steering
967	main.407	Compositionality and Generalization In Emergent Languages	Interpretability and Analysis of Models for NLP	ACL	2020	10.18653/v1/2020.acl-main.407	Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive.	d70af4990cba2574c41b1235030f7a5b702e2d70	Interpretability and Analysis	105	compositional generalization, linguistics, disentangled representations, analysis	compositional generalization, linguistics, disentangled representations, analysis
4789	211	Automatic Prompt Optimization with "Gradient Descent" and Beam Search	Language Modeling and Analysis of Language Models	EMNLP	2023		Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language"gradients"that criticize the current prompt. The gradients are then"propagated"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.	c76dd4a70361c3afd2e19d046343e2dedd16ecc3	Interpretability and Analysis	104	prompting, novel method	prompting, novel method
3800	3260	Active Example Selection for In-Context Learning	Language Modeling and Analysis of Language Models	EMNLP	2022	10.18653/v1/2022.emnlp-main.622	With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.	b8bd29a6104d26a16687400049a4e7e026ae6258	Interpretability and Analysis	103	active learning, in-context learning, demonstration selection, novel method, generalization	active learning, in-context learning, demonstration selection, novel method, generalization
5605	4756	Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback	Language Modeling and Analysis of Language Models	EMNLP	2023	10.18653/v1/2023.emnlp-main.330	A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.	ab4ce5dda7ad4d9032995c9c049a89d65723c6aa	Interpretability and Analysis	101	RLHF, calibration, evaluation, confidence scores, methods evaluation	RLHF, calibration, evaluation, confidence scores, methods evaluation
2626		Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases	Interpretability and Analysis of Models for NLP	ACL	2021	10.18653/v1/2021.acl-long.146	Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases.	e337ed6543c2e6e7e51c312c7d998798fc79fdde	Interpretability and Analysis	99	factual knowledge, artifacts, evaluation analysis, data analysis, prompting, understanding, analysis	factual knowledge, artifacts, evaluation analysis, data analysis, prompting, understanding, analysis
1886		Mind the Style of Text! Adversarial and Backdoor Attacks Based on Text Style Transfer	Interpretability and Analysis of Models for NLP	EMNLP	2021	10.18653/v1/2021.emnlp-main.374	Adversarial attacks and backdoor attacks are two common security threats that hang over deep learning. Both of them harness task-irrelevant features of data in their implementation. Text style is a feature that is naturally irrelevant to most NLP tasks, and thus suitable for adversarial and backdoor attacks. In this paper, we make the first attempt to conduct adversarial and backdoor attacks based on text style transfer, which is aimed at altering the style of a sentence while preserving its meaning. We design an adversarial attack method and a backdoor attack method, and conduct extensive experiments to evaluate them. Experimental results show that popular NLP models are vulnerable to both adversarial and backdoor attacks based on text style transfer—the attack success rates can exceed 90% without much effort. It reflects the limited ability of NLP models to handle the feature of text style that has not been widely realized. In addition, the style transfer-based adversarial and backdoor attack methods show superiority to baselines in many aspects. All the code and data of this paper can be obtained at https://github.com/thunlp/StyleAttack.	342f6ae2ebfc0ddddf15e8ba910eab6f2e06bf83	Interpretability and Analysis	97	adversarial evaluation, backdoor attacks, style transfer, evaluation	adversarial evaluation, backdoor attacks, style transfer, evaluation
4758	60	Dissecting Recall of Factual Associations in Auto-Regressive Language Models	Interpretability, Interactivity, and Analysis of Models for NLP	EMNLP	2023	10.18653/v1/2023.emnlp-main.751	Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation"queries"the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.	133b97e40017a9bbbadd10bcd7f13088a97ca3cc	Interpretability and Analysis	96	factual knowledge, information flow, model editing, attention analysis, representation analysis, interventions, analysis	factual knowledge, information flow, model editing, attention analysis, representation analysis, MLP analysis, interventions, analysis
273	main.2238	With Little Power Comes Great Responsibility	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.745	Despite its importance to experimental design, statistical power (the probability that, given a real effect, an experiment will reject the null hypothesis) has largely been ignored by the NLP community. Underpowered experiments make it more difficult to discern the difference between statistical noise and meaningful model improvements, and increase the chances of exaggerated findings.  By meta-analyzing a set of existing NLP papers and datasets, we characterize typical power for a variety of settings and conclude that underpowered experiments are common in the NLP literature. In particular, for several tasks in the popular GLUE benchmark, small test sets mean that most attempted comparisons to state of the art models will not be adequately powered. Similarly, based on reasonable assumptions, we find that the most typical experimental design for human rating studies will be  underpowered to detect small model differences, of the sort that are frequently studied. For machine translation, we find that typical test sets of 2000 sentences have approximately 75% power to detect differences of 1 BLEU point. To improve the situation going forward, we give an overview of best practices for power analysis in NLP and release a series of notebooks to assist with future power analyses.	186d26390779f7c54930e05812cfe85e6973961f	Interpretability and Analysis	94	statistics, statistical power analysis, data analysis, evaluation analysis, meta-analysis, novel metric	statistics, statistical power analysis, data analysis, evaluation analysis, meta-analysis, novel metric
376	main.2696	On the Ability and Limitations of Transformers to Recognize Formal Languages	Interpretability and Analysis of Models for NLP	EMNLP	2020	10.18653/v1/2020.emnlp-main.576	Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.	10c86505de83647c7b4157595ab10f64e97c94ef	Interpretability and Analysis	92	formal languages, analysis, learnability, attention analysis	formal languages, analysis, learnability, attention analysis