Title	Link	Venue	Year	abstract	Count	marius_notes	vagrant_notes	Year	Paper?	Notes	marius_chat	vagrant_chat
"Why Should I Trust You?": Explaining the Predictions of Any Classifier	https://dl.acm.org/doi/pdf/10.1145/2939672.2939778				1			2016				
200 Concrete Open Problems in Mechanistic Interpretability	https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability				1			2022	No	Not a PDF		
A Mathematical Framework for Transformer Circuits	https://transformer-circuits.pub/2021/framework/index.html	blogpost	2021	This paper explores how to reverse engineer transformer language models by conceptualizing them as circuits of attention heads. It studies toy models with one or two layers and finds that they can perform in-context learning using different algorithms.	6	new metaphor, toy models, mechanistic interpretability, attention analysis, residual stream	new metaphor, toy models, mechanistic interpretability, attention analysis, residual stream	2021		Not a PDF; also, I assume this is what they meant by "Anthropic's paper has been big"		
A mathematical theory of semantic development in deep neural networks	https://www.pnas.org/doi/epdf/10.1073/pnas.1820226116				1			2019				
A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity	https://arxiv.org/pdf/2401.01967				1			2024				
A Multiscale Visualization of Attention in the Transformer Model	https://aclanthology.org/P19-3007.pdf				1			2019				
A Primer in BERTology: What We Know About How BERT Works	https://aclanthology.org/2020.tacl-1.54.pdf	TACL	2020	Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.	2	survey	survey	2020				
A Structural Probe for Finding Syntax in Word Representations	https://aclanthology.org/N19-1419.pdf	NAACL	2019	Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.	2	linguistics, probing, representation space geometry, representation analysis	linguistics, probing, representation space geometry, representation analysis	2019				
A Unified Approach to Interpreting Model Predictions	https://arxiv.org/pdf/1705.07874v2				1			2017				
Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts	https://arxiv.org/pdf/2305.13300				1			2023				
Adversarial Examples are not Bugs, they are Features	https://proceedings.neurips.cc/paper/2019/file/e2c420d928d4bf8ce0ff2ec19b371514-Paper.pdf				1			2019				
Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals	https://aclanthology.org/2021.tacl-1.10.pdf	TACL	2021	A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.	2	counterfactuals, novel method, probing, causality, representation analysis	counterfactuals, novel method, probing, causality, representation analysis	2021				
Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned	https://aclanthology.org/P19-1580.pdf				1			2019				
Analyzing Transformers in Embedding Space	https://aclanthology.org/2023.acl-long.893.pdf				1			2023				
Attention is not not Explanation	https://aclanthology.org/D19-1002.pdf	EMNLP	2019	Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.	3	attention analysis, robustness, attribution, explanation, novel guidelines	attention analysis, robustness, attribution, explanation, novel guidelines	2019				
Attention is Not Only a Weight: Analyzing Transformers with Vector Norms	https://aclanthology.org/2020.emnlp-main.574.pdf				1			2020				
Axiomatic attribution for deep networks	https://dl.acm.org/doi/pdf/10.5555/3305890.3306024	ICML	2017	We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.	2	attribution, novel method, steering, extract rules	attribution, novel method, steering, rule extraction	2017				
Backpack Language Models	https://aclanthology.org/2023.acl-long.506.pdf				1			2023				
Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data	https://arxiv.org/pdf/2310.02541				1			2023				
BERT Rediscovers the Classical NLP Pipeline	https://aclanthology.org/P19-1452.pdf	ACL	2019	Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.	3	probing, linguistics, representation analysis	probing, linguistics, representation analysis	2019				
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList	https://aclanthology.org/2020.acl-main.442.pdf				1			2020				
Causal Abstractions of Neural Networks	https://proceedings.neurips.cc/paper_files/paper/2021/file/4f5c422f4d49a5a807eda27434231040-Paper.pdf				1			2021				
Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias	https://arxiv.org/pdf/2004.12265	NeurIPS	2020	Common methods for interpreting neural models in natural language processing typically examine either their structure or their behavior, but not both. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. It enables us to analyze the mechanisms by which information flows from input to output through various model components, known as mediators. We apply this methodology to analyze gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are (i) sparse, concentrated in a small part of the network; (ii) synergistic, amplified or repressed by different components; and (iii) decomposable into effects flowing directly from the input and indirectly through the mediators.	3	causality, gender bias, novel method, neuron analysis, attention analysis, causal mediation analysis	causality, gender bias, novel method, neuron analysis, attention analysis, causal mediation analysis	2020				
Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction	https://aclanthology.org/2021.conll-1.15.pdf				1			2021				
Deep learning for case-based reasoning through prototypes: a neural network that explains its predictions	https://dl.acm.org/doi/abs/10.5555/3504035.3504467				1			2018				
Discovering Latent Knowledge in Language Models Without Supervision	https://openreview.net/pdf?id=ETKGuby0hcs				1			2023				
Dissecting Recall of Factual Associations in Auto-Regressive Language Models	https://aclanthology.org/2023.emnlp-main.751.pdf	EMNLP	2023	Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation "queries" the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.	2	factuality, interventions, information flow, attention analysis, MLP analysis	factuality, interventions, information flow, attention analysis, MLP analysis	2023				
Do Llamas Work in English? On the Latent Language of Multilingual Transformers	https://arxiv.org/pdf/2402.10588				1			2024				
Do Machine Learning Models Memorize or Generalize?	https://pair.withgoogle.com/explorables/grokking/				1			2023	No	Not a PDF		
Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models	https://openreview.net/pdf?id=EldbUlZtbd				1			2023				
Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks	https://aclanthology.org/2020.acl-main.740.pdf				1			2020				
Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task	https://openreview.net/pdf?id=DeG07_TcZvT	ICLR	2023	Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.	2	emergent behaviour, world model, representation analysis, memorization, games	emergent behaviour, world model, representation analysis, memorization, games	2023				
Estimating training data influence by tracing gradient descent	https://dl.acm.org/doi/pdf/10.5555/3495724.3497396				1			2020				
Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?	https://aclanthology.org/2020.acl-main.491.pdf				1			2020				
Evaluating Large Language Models on Controlled Generation Tasks	https://aclanthology.org/2023.emnlp-main.190.pdf				1			2023				
EXACTA: Explainable Column Annotation	https://dl.acm.org/doi/pdf/10.1145/3447548.3467211				1			2021				
Examining Modularity in Multilingual LMs via Language-Specialized Subnetworks	https://arxiv.org/pdf/2311.08273				1			2023				
Explainability in Graph Neural Networks: A Taxonomic Survey	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9875989				1			2023				
Explainability Techniques for Graph Convolutional Networks	https://arxiv.org/pdf/1905.13686				1			2019				
Explanation in artificial intelligence: Insights from the social sciences	https://www.sciencedirect.com/science/article/pii/S0004370218305988?via%3Dihub	Artificial Inteligence	2019	There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.	3	interdisciplinarity, novel guidelines, explainability	interdisciplinarity, novel guidelines, explainability	2019		Not a PDF		
Few-shot Fine-tuning vs. In-context Learning: A Fair Comparison and Evaluation	https://aclanthology.org/2023.findings-acl.779.pdf				1			2023				
Finding Neurons in a Haystack: Case Studies with Sparse Probing	https://openreview.net/pdf?id=JYs1R9IMJr				1			2023				
Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods	https://aclanthology.org/N18-2003.pdf				1			2018				
Gender Bias in Masked Language Models for Multiple Languages	https://aclanthology.org/2022.naacl-main.197.pdf				1			2022				
Hierarchy and interpretability in neural models of language processing	https://eprints.illc.uva.nl/id/eprint/2175/1/DS-2020-06.text.pdf				1			2020				
How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings	https://aclanthology.org/D19-1006.pdf				1			2019				
How do Language Models Bind Entities in Context?	https://arxiv.org/pdf/2310.17191				1			2024				
How Multilingual is Multilingual BERT?	https://aclanthology.org/P19-1493.pdf	ACL	2019	In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.	2	multilingual, probing, transfer, representation analysis	multilingual, probing, transfer, representation analysis	2019				
In-context Learning and Induction Heads	https://arxiv.org/pdf/2209.11895	blogpost	2022	"Induction heads" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all "in-context learning" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.	7	in-context learning, training dynamics, mechanistic interpretability, toy models, novel metaphor	novel metaphor, in-context learning, toy models, training dynamics, mechanistic interpretability	2022				
In-Context Learning Creates Task Vectors	https://aclanthology.org/2023.findings-emnlp.624.pdf				1			2023				
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model	https://proceedings.neurips.cc/paper_files/paper/2023/file/81b8390039b7302c909cb769f8b6cd93-Paper-Conference.pdf	NeurIPS	2023	We introduce Inference-Time Intervention (ITI), a technique designed to enhance the "truthfulness" of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.	2	interventions, novel method, truthfulness, steering	interventions, novel method, truthfulness, steering	2023				
Interpretability at Scale: Identifying Causal Mechanisms in Alpaca	https://openreview.net/pdf?id=nRfClnMhVX				1			2023				
Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)	https://proceedings.mlr.press/v80/kim18d/kim18d.pdf				1			2018				
Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small	https://openreview.net/pdf?id=NpsVSN6o4ul	ICLR	2023	Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.	6	reverse engineering, causality, interventions, attention analysis, mechanistic interpretability	reverse engineering, causality, interventions, attention analysis, mechanistic interpretability	2023				
Interpretable Entity Representations through Large-Scale Typing	https://aclanthology.org/2020.findings-emnlp.54.pdf				1			2020				
Interpretation of neural networks is fragile	https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.33013681				1			2019				
interpreting GPT: the logit lens	https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens				1			2020	No	Not a PDF		
Interpreting Language Models with Contrastive Explanations	https://aclanthology.org/2022.emnlp-main.14.pdf				1			2022				
Knowledge is a Region in Weight Space for Fine-tuned Language Models	https://aclanthology.org/2023.findings-emnlp.95.pdf				1			2023				
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought	https://openreview.net/pdf?id=qFVVBzXxR2V				1			2023				
Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting	https://proceedings.neurips.cc/paper_files/paper/2023/file/ed3fea9033a80fea1376299fa7863f4a-Paper-Conference.pdf				1			2023				
Language Models Represent Space and Time	https://openreview.net/pdf?id=jE8xbmvFin	ICLR	2024	The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.	2	representation analysis, world models, neuro analysis	world models, neuron analysis, representation analysis	2024				
LEACE: Perfect linear concept erasure in closed form	https://papers.nips.cc/paper_files/paper/2023/file/d066d21c619d0a78c5b557fa3291a8f4-Paper-Conference.pdf				1			2023				
Learning Music Helps You Read: Using Transfer to Study Linguistic Structure in Language Models	https://aclanthology.org/2020.emnlp-main.554.pdf				1			2020				
Learning representations by back-propagating errors	https://www.nature.com/articles/323533a0				1			1986		Not a PDF		
Lexinvariant Language Models	https://openreview.net/pdf?id=BbKg6T3KXY				1			2023				
Linear Connectivity Reveals Generalization Strategies	https://openreview.net/pdf?id=hY6M0JHl3uL				1			2023				
Linearity of Relation Decoding in Transformer Language Models	https://openreview.net/pdf?id=w7LU2s14kE				1			2024				
Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them	https://aclanthology.org/N19-1061.pdf				1			2019				
Locating and Editing Factual Associations in GPT	https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf	NeurIPS	2022	We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at this https URL	11	model editing, novel method, causality, factuality, neuron analysis, activation analysis, interventions	model editing, novel method, causality, factuality, neuron analysis, activation analysis, interventions	2022				
Mass-Editing Memory in a Transformer	https://openreview.net/pdf?id=MkbcAHIYgyS				1			2023				
Massive Activations in Large Language Models	https://arxiv.org/pdf/2402.17762				1			2024				
Measuring Faithfulness in Chain-of-Thought Reasoning	https://arxiv.org/pdf/2307.13702				1			2023				
Measuring Geographic Performance Disparities of Offensive Language Classifiers	https://aclanthology.org/2022.coling-1.574.pdf				1			2022				
Mediators in Determining what Processing BERT Performs First	https://aclanthology.org/2021.naacl-main.8.pdf				1			2021				
Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens	https://aclanthology.org/2022.naacl-main.373.pdf				1			2022				
Neighboring Words Affect Human Interpretation of Saliency Explanations	https://aclanthology.org/2023.findings-acl.750.pdf				1			2023				
Neural Word Embedding as Implicit Matrix Factorization	https://proceedings.neurips.cc/paper_files/paper/2014/file/feab05aa91085b7a8012516bc3533958-Paper.pdf				1			2014				
Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection	https://aclanthology.org/2020.acl-main.647.pdf				1			2020				
On the Ability and Limitations of Transformers to Recognize Formal Languages	https://aclanthology.org/2020.emnlp-main.576.pdf				1			2020				
On the Paradox of Learning to Reason from Data	https://www.ijcai.org/proceedings/2023/0375.pdf				1			2023				
On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines	https://openreview.net/pdf?id=nzpLWnVAyah				1			2021				
Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models	https://arxiv.org/pdf/2401.06102	ICML	2024	Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning.	2	attribution, representation analysis, patching, novel framework, probing, unification	attribution, representation analysis, patching, novel framework, probing, unification	2024				
Pathologies of Neural Models Make Interpretations Difficult	https://aclanthology.org/D18-1407.pdf	EMNLP	2018	One way to interpret neural model predictions is to highlight the most important input features---for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word's importance is determined by either input perturbation---measuring the decrease in model confidence when that word is removed---or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction without accuracy loss on regular examples.	2	limitations, pathological behaviour, attribution	limitations, pathological behaviour, attribution	2018				
Probing Classifiers: Promises, Shortcomings, and Advances	https://direct.mit.edu/coli/article-pdf/48/1/207/2006605/coli_a_00422.pdf				1			2021				
Progress measures for grokking via mechanistic interpretability	https://openreview.net/pdf?id=9XFSbDPmdW	ICLR	2023	Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.	3	grokking, mechanistic interpretability, reverse engineering, weight analysis, activation analysis, training dynamics, toy models, emergent behaviour	grokking, mechanistic interpretability, reverse engineering, weight analysis, activation analysis, training dynamics, toy models, emergent behaviour	2023				
Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models	https://aclanthology.org/2022.emnlp-main.141.pdf				1			2022				
Quality Controlled Paraphrase Generation	https://aclanthology.org/2022.acl-long.45.pdf				1			2022				
Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank	https://aclanthology.org/D13-1170.pdf				1			2013				
Rethinking Interpretability in the Era of Large Language Models	https://arxiv.org/pdf/2402.01761				1			2024		we should read this!		
Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?	https://aclanthology.org/2022.emnlp-main.759.pdf				1			2022				
Risks from Learned Optimization in Advanced Machine Learning Systems	https://arxiv.org/pdf/1906.01820v3				1			2019				
Sanity Checks for Saliency Maps	https://papers.nips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf				1			2018				
Scaling Laws for Neural Language Models	https://arxiv.org/pdf/2001.08361	arXiv	2020	We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.	2	scaling laws, architecture, language modelling	scaling laws, architecture, language modelling	2020				
Simulating Weighted Automata over Sequences and Trees with Transformers	https://proceedings.mlr.press/v238/rizvi-martel24a/rizvi-martel24a.pdf				1			2024				
Steering Llama 2 via Contrastive Activation Addition	https://arxiv.org/pdf/2312.06681v3	arXiv	2023	We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).	2	steering, novel method, interventions, factuality, hallucination, activation analysis	steering, novel method, interventions, factuality, hallucination, activation analysis	2023				
The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives	https://aclanthology.org/D19-1448.pdf				1			2019				
The Building Blocks of Interpretability	https://distill.pub/2018/building-blocks/				1			2018	No	Not a PDF		
The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter	https://proceedings.neurips.cc/paper_files/paper/2023/file/7a69ab48efcbb0153e72d458fb091969-Paper-Conference.pdf				1			2023				
The False Promise of Imitating Proprietary LLMs	https://arxiv.org/pdf/2305.15717				1			2023				
The Hidden Space of Transformer Language Adapters	https://arxiv.org/pdf/2402.13137				1			2024				
The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models	https://aclanthology.org/2020.emnlp-demos.15.pdf				1			2020				
The Linear Representation Hypothesis and the Geometry of Large Language Models	https://openreview.net/pdf?id=T0PoOJg8cK				1			2023				
The MultiBERTs: BERT Reproductions for Robustness Analysis	https://openreview.net/pdf?id=K0E_F0gFDgA				1			2022				
The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery.	https://dl.acm.org/doi/pdf/10.1145/3236386.3241340				1			2018		we should read this too!		
The Principles of Deep Learning Theory	https://arxiv.org/pdf/2106.10165v2				1			2021	No			
The Risk of Racial Bias in Hate Speech Detection	https://aclanthology.org/P19-1163.pdf				1			2019				
Thinking Like Transformers	https://arxiv.org/pdf/2106.06981	ICML	2021	What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.	3	computational model, transformer programs	computational model, transformer programs	2021				
Towards Automated Circuit Discovery for Mechanistic Interpretability	https://proceedings.neurips.cc/paper_files/paper/2023/file/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Paper-Conference.pdf				1			2023				
Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?	https://aclanthology.org/2020.acl-main.386.pdf	ACL	2020	With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is “defined” by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.	2	novel guidelines, faithfulness, explanations	novel guidelines, faithfulness, explanations	2020				
Towards Monosemanticity: Decomposing Language Models With Dictionary Learning	https://transformer-circuits.pub/2023/monosemantic-features				1			2023	No	Not a PDF		
Toy Models of Superposition	https://transformer-circuits.pub/2022/toy_model/index.html	blogpost	2022	Neural networks often pack many unrelated concepts into a single neuron - a puzzling phenomenon known as 'polysemanticity' which makes interpretability much more challenging. This paper provides a toy model where polysemanticity can be fully understood, arising as a result of models storing additional sparse features in "superposition." We demonstrate the existence of a phase change, a surprising connection to the geometry of uniform polytopes, and evidence of a link to adversarial examples. We also discuss potential implications for mechanistic interpretability.	2	neuron analysis, mechanistic interpretability, superposition, concepts	neuron analysis, mechanistic interpretability, superposition, concepts	2022	No	Not a PDF		
Transformer Circuits Thread	https://transformer-circuits.pub/				1			2021-2024	No	Not a PDF		
Transformer Feed-Forward Layers Are Key-Value Memories	https://aclanthology.org/2021.emnlp-main.446.pdf	EMNLP	2021	Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.	11	FFN analysis, novel metaphor, representational analysis, MLP analysis, key-value memories	FFN analysis, novel metaphor, representational analysis, MLP analysis, key-value memories	2021				
Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space	https://aclanthology.org/2022.emnlp-main.3.pdf				1			2022				
Transformers Learn In-Context by Gradient Descent	https://proceedings.mlr.press/v202/von-oswald23a/von-oswald23a.pdf				1			2023				
Transformers Learn Shortcuts to Automata	https://openreview.net/pdf?id=De4FYqjFueZ				1			2023				
Understanding Black-box Predictions via Influence Functions	https://arxiv.org/pdf/1703.04730v3	ICML	2017	How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.	2	influence functions, explainability, attribution	influence functions, explainability, attribution	2017				
Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere	http://proceedings.mlr.press/v119/wang20k/wang20k.pdf				1			2020				
Understanding the difficulty of training deep feedforward neural networks	https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf				1			2010				
Universal Adversarial Triggers for Attacking and Analyzing NLP	https://aclanthology.org/D19-1221.pdf				1			2019				
What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary	https://aclanthology.org/2023.acl-long.140.pdf				1			2023				
What do you learn from context? Probing for sentence structure in contextualized word representations	https://openreview.net/pdf?id=SJzSgnRcKX	ICLR	2019	Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.	2	probing, linguistics, representation analysis	probing, linguistics, representation analysis	2019				
What Does BERT Learn about the Structure of Language?	https://aclanthology.org/P19-1356.pdf				1			2019				
What Does BERT Look at? An Analysis of BERT’s Attention	https://aclanthology.org/W19-4828.pdf	BlackBoxNLP	2019	Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.	3	attention analysis, linguistics, probing, representation analysis	attention analysis, linguistics, probing, representation analysis	2019				
What Formal Languages Can Transformers Express? A Survey	https://arxiv.org/pdf/2311.00208				1			2024				
What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties	https://aclanthology.org/P18-1198.pdf				1			2018				
Why Does Unsupervised Pre-training Help Deep Learning?	https://dl.acm.org/doi/pdf/10.5555/1756006.1756025				1			2010				
Zoom In: An Introduction to Circuits	https://distill.pub/2020/circuits/zoom-in/				1			2020	No	Not a PDF		