response_id	text	vagrant_themes	marius_themes
122	As deep learning models become more ubiquitous in our everyday lives, as well as more expensive and environmentally harmful to train, it is now more important than ever to try to understand how these models work so that we can be more assured of their safety and so that they are more efficient to serve and train.  I think the main benefits of interpretability are still forthcoming because there is more motivation now to figure these challenges out than there has been in the past.  We have already seen examples of interpretability being very useful for applications. in NLP, ROME is a canonical example. There are also examples of tuning free interventions that allow changing model behaviors in non-trivial ways that were spawned from interpretability projects.		
52	Current trends make me believe that we will achieve general intelligence with more compute (i.e. better hardware and more money). To that end, interpretability does not matter. I do interpretability because, like any scientist, I want to understand how things work and not merely build them.	scale is all you need, interpretability for understanding and not building	scale is all you need, interpretability for understanding and not building
62	I assumed the question is whether interpretability research is important to *non-interpretability* progress, in which case my answer is derived from the lack of counter-examples (having deeply examined the citation graph of papers outside of interpretability citing papers in interpretability). Inside the "bubble" of interpretability research, however, they are obviously influential to each other.	little influence outside interpretability literature	little influence outside interpretability literature
61	I tried to use methods, but they failed to be useful or improve models	fail to improve models, fail to be useful	fail to improve models, fail to be useful
64	In order to be useful it needs to be actionable which is rarely the case - data interpretability is an exception and interpretability research in "small worlds" to a limited (by that "smallness") extent. In general, the absence of groud truth sadly makes current explanations plausible at best, but not more. What's even more sadening is that this was already the state almost 10 years ago (think LIME), and that our community hasn't left the state is concerning.	actionability	actionability
12	My impression is that most performant LLM models are not essentially touched by current analysis and interpretability SOTA tools, thus these methods feel like "niche" and not impactful. I'd be relieved to be proven wrong in the future.	lack of impact on most capable models	lack of impact on most capable models
90	Once a model works good enough no user or customer asks for it to be explainable.  Analysing models with billions of parameters won't have any insights due to the inherit complexity	performance is all your need, models too big to be analysed	models too big to be analysed, performance is all your need