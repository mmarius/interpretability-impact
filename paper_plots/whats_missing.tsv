response_id	text	vagrant_themes	marius_themes
14	A complete theoretical description of deep neural networks	theoretical description	theory
64	Actionability and ground truth	actionability, ground truth	actionability, ground truth
106	Although this area is gathering a large following, I feel like there's a lack of community/joint gatherings to discuss this work in academic settings (most of the discussion I see is largely in industry lab events). I've only just started working on these topics (but have been in the NLP community for a while) and it would be nice to have the same forums that the general NLP community has (e.g., conferences, workshops, places to speak about this work).	lack of community / conferences	lack of community
48	Analysis of harder, longer, real world examples Analysis by use - more exploration of models behaviour without ties to safety 	realistic evaluation, use beyond safety	real world examples, use beyond safety
17	Clearer pathways to algorithmic insight, and learning about what is critical to a problem (e.g. what are the critical characteristics of learning language?), more work on using it for debugging applications. 	debugging, clarity, interdisciplinarity	debugging, clarity, interdisciplinarity
122	General truths about what models tend to do or should learn are somewhat lacking. There are some examples of these things, but in general I think the focus should be on climbing the right hill towards a higher level understanding instead of focusing on interesting individual behaviors. That is what the field currently looks like, but I think that is fine; one way to get the higher level understanding is by building up a catalogue of these interesting 'smaller' findings. I think in the near future we will be able to synthesize these into broader lessons about deep learning that have far reaching applications for how we train models and use them.  I am also interested in the future of weight based analyses. E.g., predicting how a model might treat an input without running the model (with respect to targeted, specific behaviors)	weight-based predictions without running, higher-level / holistic / unified understanding, goal-setting	big picture / lack of synthesis, goal-setting, predicting model behaviour
82	Ground truth is often hard to find; falsifiable hypotheses are important too.	ground truth, falsifiable hypotheses	ground truth, clear hypotheses needed
91	holistic understanding & interactive interpretability 	higher-level / holistic / unified understanding, interactivity	big picture / lack of synthesis, interactivity
53	How can we utilize or measure the utility of an explanation, besides just providing trust and accountability. How can we benefit both humans and AI models by drawing insights from explanations/interpretations	use beyond safety, actionability, utility, model improvements, benefit to humans	utility, model improvements, benefit to humans, use beyond safety, actionability
93	How to connect the conceptual/empirical/theoretical interpretability understandings/findings to actions of improving model architecture, training and error analysis. The gap is very big, and very task/situation dependent. Because of the gap, researches are mostly focused on challenging and proposing new attribution, probing, intervention techniques, but reluctant to make real leverage of those interpretability toolbox for other topics in NLP and ML, e.g. improving generalization ability, finetuning and data selection.	actionability, model improvements	model improvements, actionability
3	Human evaluation of explanations	human evaluation / user studies	human evaluation
62	I don't know. I could not find an appropriate direction that can enable interpretability breakthrough that influence non-interpretability work. I think the most pressing area is to work on educating and clarifying to stakeholders (regulators, lawmakers) what interpretability can't provide in the near and medium term.	utility, limits of interpretability, better stakeholder communication	limits of interpretability, educating general public, utility
41	I find this research discipline lacks unification. Many people have good ideas and show interesting results, but very little work is done to unify these different points of view	higher-level / holistic / unified understanding	big picture / lack of synthesis
15	I think that the missing element would be predicting the model behavior during generation, out of the current state (parameters, contextual embeddings). Once we learn something about the “future”, this will mean we actually understand something (sorry for being a bit vague).	weight-based predictions without running	predicting model behaviour
36	I think to some extent tooling/research to allow for incorporation of such techniques for feedback and guidance of ML models both during development and during inference time.	tooling	tooling
18	I would like to see more model analysis and interpretability researchers collaborate with domain experts so that they can leverage models as proxies of Scientific natural phenomena	interdisciplinarity	collaborations with domain experts
45	If we could find more structure! 	structure	structure
99	less speculative claims that might be misleading, instead draw only conclusions where experiments provide proof	less speculation	more rigor 
75	Level of confidence: analysis and interpretability can give insight into trends, but none of our methods can achieve high levels of confidence that an LLM will *never* do something, e.g. generate toxic content, or be deceptive or power seeking. This is of course a very difficult problem, but our inability to achieve any kind of guarantees could lead to catastrophic or existential risk when models become more capable than humans at most cognitive tasks.	guarantees	guarantees 
19	Many works in the field are limited to a particular family of models, or even a single model and are hard to extrapolate to more general language modelling (if at all possible).	higher-level / holistic / unified understanding	higher-level / holistic / unified understanding
72	Model evaluation in real life situations (not limited to curated datasets typically used in competitions)	realistic evaluation	real world data
24	More attention to the holistic process around interpretability: what someone (who is not an interpretability researcher) will learn from this work, or learn by using this method. Is the end goal wonder and discovery? Or scientific understanding that carries predictive power, and thus practical utility?	higher-level / holistic / unified understanding, goal-setting, utility	big picture, goal-setting, utility
26	More emphasis on user studies, HCI, cross-disciplinary work with social sciences	interdisciplinarity, human evaluation / user studies	more interdisciplinary research, human evaluation
125	More interpretability work needs to demonstrate practical impacts and actionable insights. Current work focuses primarily on understanding what fully trained models encode or do not encode, but then they do not act on these insights or show why these are important. Demonstrating the causal importance of encoded phenomena for downstream performance is one way to demonstrate value, and is relatively straightforward to implement. Combing interpretability with training dynamics also has potential to show us when, how, and why certain phenomena arise during pre-training. Naomi Saphra's Sudden Drops in the Loss paper is a great example of this.  There are currently streams of interpretability work which are actively demonstrating useful applications (e.g., the ROME and MEMIT papers, and sparse feature circuits). We need more of this—and especially more of this combined with training dynamics work!	actionability, pre-training, trajectories, utility, causality	actionability, causaility, training dynamics, utility
5	Most of model interpretability research is aimed for researchers or engineers who work with Large Language Models themselves. The stakeholder group need to expand beyond that or at least the papers should recognize who are the targeted audience for the interpretability tools and findings. 	better stakeholder communication	who is the target audience?
42	Most of the research I hear about is about analysing model output behavior rather than the model internal causes for that behavior. We don't currently have good reliable ways to probe internal model relations and latent knowledge.	internal causes rather than output behaviour	causality
55	Perhaps new methods and architectures that are based on interpretability results	modelling improvements	modeling improvements
121	Scaling up interpretability research with automation and making the most of the current capabilities of (language) models to do so. Currently there are a lot of manual steps that can become more efficient.	scaling, automating interpretability	scaling up, automating interpretability
97	Several methods used for explaining learning models require too much time and resources. This may be a limit for new researchers and practitioners to take part in the field.	barriers to participation, efficiency (cost / compute / etc.)	barriers to participation, efficiency (cost / compute / etc.)
81	The community seems to be "going in circles" a bit in the last years. We need to start shaping consensus after many years of hearing everybody out and testing hypotheses.	consensus, big picture	lack of synethesis, consensus
29	The field is becoming increasingly divided between people working on the impact of explainability for end-users (Human-Centered XAI, HCXAI), who are closer to AI Ethics work and very critical of "technical solutionism", and people using LMs as subjects to investigate their inner workings, often using techniques borrowed from physics or neuroscience (interpretability, and its mechanistic flavor). The growing gap among these communities makes it so that the general public and HCXAI researchers are mostly disconnected from novel insights in the inner workings of LMs, making interpretability research appear as pointless. On the other hand, interpretability researchers feel like making their findings usable by downstream users (e.g. with convenient interface and thorough documentation) does not compel them. An ideal future would have these communities working together to ensure the latest insights are accessible to end users, rather than pitted against each other in claiming the superiority of their work.	divisions, interdisciplinarity	more interdisciplinary research
79	The main point that's missing for me is very reliable methods, and proven methods, to gain insights into LLMs. A lot of interpretability work seems a bit hand-wavy to me. There should be much more work on good tools, and consent on certain methods that are well understood. I think it would be great to have work that explores interpretability starting from small, rather easily understood models and can scale up to multi hundred billion parameters. 	reliable methods, scaling, consensus, trust, toy models, guarantees	reliable methods, scaling, consensus, trust, toy models, guarantees
113	The main promise of interpretability research is to give us insight into model reasoning beyond what is available from distribution-level input/output testing. This should help us better understand what models are capable of, when to trust their outputs to be correct, etc. There are very few direct tests of this hypothesis in interpretability research, because (1) a lot of interpretability research is still young and focused on building basic understanding of how models do toy tasks, (2) human studies are hard and expensive and there are not strong incentives in the field to run these when you can publish a paper doing something but easier/quicker/cheaper. The community needs to keep its eyes on the prize. When are we going to learn something about how a model works that we couldn't have learned from traditional testing, in a way that is actually useful to someone wondering whether they should use the model or not. 	utility, internal causes rather than output behaviour, toy models	utility, internal causes rather than output behaviour, toy models
112	There's a lack of high-quality open-source tools for this work, of the level of quality and sophistication of Huggingface Transformers or Pytorch. For instance, SAEs seem promising but the top labs just publish blog posts and don't provide pretrained SAEs or analysis tools for SAEs that are high quality. I'd also like to see insights about what's discovered in various models shared in a more centralized way than just as text in research papers. It would be great as well if top labs open-sourced a tuned-lens or set of SAEs along with their models. to make interpretability easier and give everyone a baseline to work off of.	centralized_hub, tooling, higher-level / holistic / unified understanding	high quality tools, synthesis of results
103	Tools to make it more accessible for downstream NLP application research	tooling	tools
1	Trajectories, pretraining, role of data on learning	role of data, pre-training, trajectories	role of data, pre-training, trajectories
86	True guarantees for out-of-distribution test time examples, and direct connections from interpretability results to model improvement.	guarantees, modelling improvements, generalization	guarantees, modeling improvements, generalization
31	Two main things (1) benchmark tasks for interpretability (2) a more concerted search for higher-level abstractions and units of organization.	benchmarks, higher-level / holistic / unified understanding	benchmarks, high-level abstractions
22	Unifying different approaches/perspectives of model analysis and interpretability, including formal language theory, ML/DL theory, (linguistic/visual/categorical) structure of the data/task, mechanistic interpretability. One aspect could be analyzed in another perspective. Towards building a unified theory, which will be more useful for applications.	higher-level / holistic / unified understanding	higher-level / holistic / unified understanding
127	We need to show a use case, an applicable result - showing how this theory is useful, either for better language modeling or for better language understanding (by applying it to good models and getting real insight, whatever that might be, on real languages)	utility, modeling improvements	real world usecase, modeling improvement
52	We should direct interpretability work towards improving data efficiency and easy adaptation of models to new domains. I think that is the best bet for interpretability having any importance in applied research. But mostly, I think we will get better models inevitably and so that shouldn't be the only reason one studies interpretability.	actionability, utility, data efficiency, domain adaptation, wrong focus, modeling improvements	modeling improvements, wrong focus, data efficiency, domain adaptation, utility, actionability
111	We should figure out how to train LMs to be interpretable a priori, instead of being satisfied with post-hoc approaches	a priori	interpretability by design