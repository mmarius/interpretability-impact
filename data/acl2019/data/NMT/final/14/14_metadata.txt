SubmissionNumber#=%=#14
FinalPaperTitle#=%=#Modeling Latent Sentence Structure in Neural Machine Translation
ShortPaperTitle#=%=#
NumberOfPages#=%=#
CopyrightSigned#=%=#
JobTitle#==#
Organization#==#
Abstract#==#Recently it was shown that linguistic structure predicted by a supervised parser can be beneficial for neural machine translation (NMT). In this work we investigate a more challenging setup: we incorporate sentence structure as a latent variable in a standard NMT encoder-decoder and induce it in such a way as to benefit the translation task. We consider German-English and Japanese-English translation benchmarks and observe that when using RNN encoders the model makes no or very limited use of the structure induction apparatus. In contrast, CNN and word-embedding-based encoders rely on latent graphs and force them to encode useful, potentially long-distance, dependencies.

This is a submission to the extended abstract track.
Author{1}{Firstname}#=%=#Joost
Author{1}{Lastname}#=%=#Bastings
Author{1}{Email}#=%=#joost.bastings@gmail.com
Author{1}{Affiliation}#=%=#University of Amsterdam
Author{2}{Firstname}#=%=#Wilker
Author{2}{Lastname}#=%=#Aziz
Author{2}{Email}#=%=#will.aziz@gmail.com
Author{2}{Affiliation}#=%=#University of Amsterdam
Author{3}{Firstname}#=%=#Ivan
Author{3}{Lastname}#=%=#Titov
Author{3}{Email}#=%=#titov@uva.nl
Author{3}{Affiliation}#=%=#University of Edinburgh / University of Amsterdam
Author{4}{Firstname}#=%=#Khalil
Author{4}{Lastname}#=%=#Simaan
Author{4}{Email}#=%=#k.simaan@uva.nl
Author{4}{Affiliation}#=%=#ILLC, University of Amsterdam

==========