
% Default to the notebook output style




% Inherit from the specified cell style.





\documentclass[11pt,a4paper]{article}

\usepackage[hyperref]{acl2018}

    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{times}
    \usepackage{latexsym}
\makeatletter


    \title{The Annotated Transformer}
    \author{Alexander M. Rush \\ srush@seas.harvard.edu \\ Harvard University}
\usepackage{etoolbox}
\aclfinalcopy

\usepackage[indentfirst=true,leftmargin=0pt, rightmargin=0pt]{quoting}
\AtBeginEnvironment{quoting}{\fontfamily{phv}\selectfont}



    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines




    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{The Annotated Transformer}




    % Pygments definitions

\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}




    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults

    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}



    \begin{document}


    \maketitle


\begin{abstract}
  A major aim of open-source NLP is to quickly and accurately
  reproduce the results of new work, in a manner that the community
  can easily use and modify. While most papers publish enough detail
  for replication, it still may be difficult to achieve good results
  in practice. This paper is an experiment. In it, I consider a worked
  exercise with the goal of implementing the results of the recent
  paper. The replication exercise aims at simple code structure that
  follows closely with the original work, while achieving an efficient
  usable system. An implicit premise of this exercise is to encourage
  researchers to consider this method for new results.
\end{abstract}

\section{Introduction}


Replication of published results remains a challenging issue
in open-source NLP. When a new paper is
published with major improvements, it is common for many
members of the community to independently  reproduce the
numbers experimentally, which is often a struggle. Practically this makes it difficult to improve
scores, but also importantly it is a pedagogical issue if students
cannot reproduce results from scientific publications.

The recent turn towards deep learning has exerbated this issue. New
models require extensive hyperparameter tuning and long training
times. Small mistakes can cause major issues.  Fortunately though, new
toolsets have made it possible to write simpler more mathematically
declarative code.

In this experimental paper, I propose an exercise in open-source
NLP. The goal is to transcribe a recent paper into a simple and
understandable form. The document itself is presented as
an annotated paper. That is the main document (in different font) is an excerpt of the recent paper ``Attention is All You Need''
\cite{DBLP:journals/corr/VaswaniSPUJGKP17}. I add annotation in the
form of italicized comments and include code in PyTorch directly in
the paper itself.

Note this document itself is presented as a blog post
\footnote{Presented at \url{http://nlp.seas.harvard.edu/2018/04/03/attention.html} with source code at \url{https://github.com/harvardnlp/annotated-transformer} } and is
completely executable as a notebook. In the spirit of reproducibility
this work itself is distilled from the same source with images
inline.






    \noindent
    \begin{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_0_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \end{tiny}\begin{quoting}
\section{Background}\label{background}
     \end{quoting}\begin{quoting}
The goal of reducing sequential computation also forms the foundation of
the Extended Neural GPU, ByteNet and ConvS2S, all of which use
convolutional neural networks as basic building block, computing hidden
representations in parallel for all input and output positions. In these
models, the number of operations required to relate signals from two
arbitrary input or output positions grows in the distance between
positions, linearly for ConvS2S and logarithmically for ByteNet. This
makes it more difficult to learn dependencies between distant positions.
In the Transformer this is reduced to a constant number of operations,
albeit at the cost of reduced effective resolution due to averaging
attention-weighted positions, an effect we counteract with Multi-Head
Attention.

Self-attention, sometimes called intra-attention is an attention
mechanism relating different positions of a single sequence in order to
compute a representation of the sequence. Self-attention has been used
successfully in a variety of tasks including reading comprehension,
abstractive summarization, textual entailment and learning
task-independent sentence representations. End-to-end memory networks
are based on a recurrent attention mechanism instead of sequencealigned
recurrence and have been shown to perform well on simple-language
question answering and language modeling tasks.

To the best of our knowledge, however, the Transformer is the first
transduction model relying entirely on self-attention to compute
representations of its input and output without using sequence aligned
RNNs or convolution.
     \end{quoting}\begin{quoting}
\section{Model Architecture}\label{model-architecture}
     \end{quoting}\begin{quoting}
Most competitive neural sequence transduction models have an
encoder-decoder structure
\cite{DBLP:journals/corr/BahdanauCB14}. Here, the encoder maps
an input sequence of symbol representations \((x_1, ..., x_n)\) to a
sequence of continuous representations \(\mathbf{z} = (z_1, ..., z_n)\).
Given \(\mathbf{z}\), the decoder then generates an output sequence
\((y_1,...,y_m)\) of symbols one element at a time. At each step the
model is auto-regressive \cite{DBLP:journals/corr/Graves13},
consuming the previously generated symbols as additional input when
generating the next.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{EncoderDecoder}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    A standard Encoder\PYZhy{}Decoder architecture.}
\PY{l+s+sd}{    Base for this and many other models.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{encoder}\PY{p}{,} \PY{n}{decoder}\PY{p}{,} \PY{n}{src\PYZus{}embed}\PY{p}{,}
                 \PY{n}{tgt\PYZus{}embed}\PY{p}{,} \PY{n}{generator}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{EncoderDecoder}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder} \PY{o}{=} \PY{n}{encoder}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder} \PY{o}{=} \PY{n}{decoder}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src\PYZus{}embed} \PY{o}{=} \PY{n}{src\PYZus{}embed}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tgt\PYZus{}embed} \PY{o}{=} \PY{n}{tgt\PYZus{}embed}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{generator} \PY{o}{=} \PY{n}{generator}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{src}\PY{p}{,} \PY{n}{tgt}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Take in and process masked src and target sequences.}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{src}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{)}\PY{p}{,}
                           \PY{n}{src\PYZus{}mask}\PY{p}{,}
                           \PY{n}{tgt}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{encode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{src}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{encoder}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src\PYZus{}embed}\PY{p}{(}\PY{n}{src}\PY{p}{)}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{decode}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{memory}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{tgt}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{decoder}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tgt\PYZus{}embed}\PY{p}{(}\PY{n}{tgt}\PY{p}{)}\PY{p}{,} \PY{n}{memory}\PY{p}{,}
                            \PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Generator}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Define standard linear + softmax generation step.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{vocab}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{Generator}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{proj} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{vocab}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n}{F}\PY{o}{.}\PY{n}{log\PYZus{}softmax}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{proj}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
The Transformer follows this overall architecture using stacked
self-attention and point-wise, fully connected layers for both the
encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
     \end{quoting}\noindent
    \begin{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_14_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \end{tiny}\begin{quoting}
\subsection{Encoder and Decoder
Stacks}\label{encoder-and-decoder-stacks}

\subsubsection{Encoder}\label{encoder}

The encoder is composed of a stack of \(N=6\) identical layers.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{clones}\PY{p}{(}\PY{n}{module}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Produce N identical layers.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{return} \PY{n}{nn}\PY{o}{.}\PY{n}{ModuleList}\PY{p}{(}\PY{p}{[}\PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}\PY{p}{(}\PY{n}{module}\PY{p}{)}
                          \PY{k}{for} \PY{n}{\PYZus{}} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{N}\PY{p}{)}\PY{p}{]}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Encoder}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Core encoder is a stack of N layers}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{layer}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{Encoder}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers} \PY{o}{=} \PY{n}{clones}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{N}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm} \PY{o}{=} \PY{n}{LayerNorm}\PY{p}{(}\PY{n}{layer}\PY{o}{.}\PY{n}{size}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mask}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Pass the input/mask through each layer in turn.}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{layer}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{mask}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
We employ a residual connection
\cite{he2016deep} around each of the two
sub-layers, followed by layer normalization
\cite{ba2016layer}.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{LayerNorm}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Construct a layernorm module (See citation for details).}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{features}\PY{p}{,} \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{LayerNorm}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a\PYZus{}2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{features}\PY{p}{)}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eps} \PY{o}{=} \PY{n}{eps}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{mean} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdim}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{n}{std} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{keepdim}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
        \PY{k}{return} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{a\PYZus{}2} \PY{o}{*} \PY{p}{(}\PY{n}{x} \PY{o}{\PYZhy{}} \PY{n}{mean}\PY{p}{)} \PY{o}{/}
                \PY{p}{(}\PY{n}{std} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{eps}\PY{p}{)} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{b\PYZus{}2}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
That is, the output of each sub-layer is
\(\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))\), where
\(\mathrm{Sublayer}(x)\) is the function implemented by the sub-layer
itself. We apply dropout
\cite{srivastava2014dropout} to the
output of each sub-layer, before it is added to the sub-layer input and
normalized.

To facilitate these residual connections, all sub-layers in the model,
as well as the embedding layers, produce outputs of dimension
\(d_{\text{model}}=512\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{SublayerConnection}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    A layer norm followed by a residual connection.}
\PY{l+s+sd}{    Note norm is not applied to residual x.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{SublayerConnection}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm} \PY{o}{=} \PY{n}{LayerNorm}\PY{p}{(}\PY{n}{size}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{dropout}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{sublayer}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Apply residual connection to sublayer fn.}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{return} \PY{n}{x} \PY{o}{+} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{sublayer}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
Each layer has two sub-layers. The first is a multi-head self-attention
mechanism, and the second is a simple, position-wise fully connected
feed-forward network.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{EncoderLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Encoder calls self\PYZhy{}attn and feed forward.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{self\PYZus{}attn}\PY{p}{,}
                 \PY{n}{feed\PYZus{}forward}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{EncoderLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{self\PYZus{}attn} \PY{o}{=} \PY{n}{self\PYZus{}attn}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward} \PY{o}{=} \PY{n}{feed\PYZus{}forward}
        \PY{n}{sublayer} \PY{o}{=} \PY{n}{SublayerConnection}\PY{p}{(}\PY{n}{size}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer} \PY{o}{=} \PY{n}{clones}\PY{p}{(}\PY{n}{sublayer}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size} \PY{o}{=} \PY{n}{size}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mask}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Follow Figure 1 (left) for connections.}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{k}{lambda} \PY{n}{x}\PY{p}{:}
                             \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{self\PYZus{}attn}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{mask}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsubsection{Decoder}\label{decoder}

The decoder is also composed of a stack of \(N=6\) identical layers.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Decoder}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Generic N layer decoder with masking.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{layer}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{Decoder}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers} \PY{o}{=} \PY{n}{clones}\PY{p}{(}\PY{n}{layer}\PY{p}{,} \PY{n}{N}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm} \PY{o}{=} \PY{n}{LayerNorm}\PY{p}{(}\PY{n}{layer}\PY{o}{.}\PY{n}{size}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{memory}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{layer} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{layers}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{layer}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{memory}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{tgt\PYZus{}mask}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
In addition to the two sub-layers in each encoder layer, the decoder
inserts a third sub-layer, which performs multi-head attention over the
output of the encoder stack. Similar to the encoder, we employ residual
connections around each of the sub-layers, followed by layer
normalization.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{DecoderLayer}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Decoder calls self\PYZhy{}attn, src\PYZhy{}attn, and feed forward.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{self\PYZus{}attn}\PY{p}{,}
                 \PY{n}{src\PYZus{}attn}\PY{p}{,} \PY{n}{feed\PYZus{}forward}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{DecoderLayer}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{self\PYZus{}attn} \PY{o}{=} \PY{n}{self\PYZus{}attn}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src\PYZus{}attn} \PY{o}{=} \PY{n}{src\PYZus{}attn}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward} \PY{o}{=} \PY{n}{feed\PYZus{}forward}
        \PY{n}{sublayer} \PY{o}{=} \PY{n}{SublayerConnection}\PY{p}{(}\PY{n}{size}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer} \PY{o}{=} \PY{n}{clones}\PY{p}{(}\PY{n}{sublayer}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size} \PY{o}{=} \PY{n}{size}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{memory}\PY{p}{,} \PY{n}{s\PYZus{}mask}\PY{p}{,} \PY{n}{t\PYZus{}mask}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Follow Figure 1 (right) for connections.}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{m} \PY{o}{=} \PY{n}{memory}
        \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{k}{lambda} \PY{n}{x}\PY{p}{:}
                             \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{self\PYZus{}attn}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{t\PYZus{}mask}\PY{p}{)}\PY{p}{)}
        \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{k}{lambda} \PY{n}{x}\PY{p}{:}
                             \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src\PYZus{}attn}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{m}\PY{p}{,} \PY{n}{m}\PY{p}{,} \PY{n}{s\PYZus{}mask}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{sublayer}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{feed\PYZus{}forward}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
We also modify the self-attention sub-layer in the decoder stack to
prevent positions from attending to subsequent positions. This masking,
combined with fact that the output embeddings are offset by one
position, ensures that the predictions for position \(i\) can depend
only on the known outputs at positions less than \(i\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{subsequent\PYZus{}mask}\PY{p}{(}\PY{n}{size}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Mask out subsequent positions.}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{attn\PYZus{}shape} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{size}\PY{p}{)}
    \PY{n}{subsequent\PYZus{}mask} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{triu}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{attn\PYZus{}shape}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{from\PYZus{}numpy}\PY{p}{(}
        \PY{n}{subsequent\PYZus{}mask}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uint8}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{0}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsubsection{Attention}\label{attention}

An attention function can be described as mapping a query and a set of
key-value pairs to an output, where the query, keys, values, and output
are all vectors. The output is computed as a weighted sum of the values,
where the weight assigned to each value is computed by a compatibility
function of the query with the corresponding key.

We call our particular attention "Scaled Dot-Product Attention". The
input consists of queries and keys of dimension \(d_k\), and values of
dimension \(d_v\). We compute the dot products of the query with all
keys, divide each by \(\sqrt{d_k}\), and apply a softmax function to
obtain the weights on the values.
     \end{quoting}\noindent
    \begin{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_33_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \end{tiny}\begin{quoting}
In practice, we compute the attention function on a set of queries
simultaneously, packed together into a matrix \(Q\). The keys and values
are also packed together into matrices \(K\) and \(V\). We compute the
matrix of outputs as:

\[
   \mathrm{Attention}(Q, K, V) = \mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\]
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{attention}\PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{,} \PY{n}{mask}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Compute }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{Scaled Dot Product Attention}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{d\PYZus{}k} \PY{o}{=} \PY{n}{query}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{key\PYZus{}t} \PY{o}{=} \PY{n}{key}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{key\PYZus{}t}\PY{p}{)} \PY{o}{/} \PY{n}{math}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n}{d\PYZus{}k}\PY{p}{)}
    \PY{k}{if} \PY{n}{mask} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
        \PY{n}{scores} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{masked\PYZus{}fill}\PY{p}{(}\PY{n}{mask} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mf}{1e9}\PY{p}{)}
    \PY{n}{p\PYZus{}attn} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{softmax}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{if} \PY{n}{dropout} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
        \PY{n}{p\PYZus{}attn} \PY{o}{=} \PY{n}{dropout}\PY{p}{(}\PY{n}{p\PYZus{}attn}\PY{p}{)}
    \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{matmul}\PY{p}{(}\PY{n}{p\PYZus{}attn}\PY{p}{,} \PY{n}{value}\PY{p}{)}\PY{p}{,} \PY{n}{p\PYZus{}attn}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
The two most commonly used attention functions are additive attention
\cite{DBLP:journals/corr/BahdanauCB14}, and dot-product
(multiplicative) attention. Dot-product attention is identical to our
algorithm, except for the scaling factor of \(\frac{1}{\sqrt{d_k}}\).
Additive attention computes the compatibility function using a
feed-forward network with a single hidden layer. While the two are
similar in theoretical complexity, dot-product attention is much faster
and more space-efficient in practice, since it can be implemented using
highly optimized matrix multiplication code.

While for small values of \(d_k\) the two mechanisms perform similarly,
additive attention outperforms dot product attention without scaling for
larger values of \(d_k\)
\cite{DBLP:journals/corr/BritzGLL17}. We suspect that for
large values of \(d_k\), the dot products grow large in magnitude,
pushing the softmax function into regions where it has extremely small
gradients (To illustrate why the dot products get large, assume that the
components of \(q\) and \(k\) are independent random variables with mean
\(0\) and variance \(1\). Then their dot product,
\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\), has mean \(0\) and variance
\(d_k\).). To counteract this effect, we scale the dot products by
\(\frac{1}{\sqrt{d_k}}\).
     \end{quoting}\begin{quoting}

     \end{quoting}\noindent
    \begin{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_38_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \end{tiny}\begin{quoting}
Multi-head attention allows the model to jointly attend to information
from different representation subspaces at different positions. With a
single attention head, averaging inhibits this.\\
\[
\mathrm{MultiHead}(Q, K, V) = \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \]
\[ \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)
\]

Where the projections are parameter matrices
\(W^Q_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\),
\(W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}\),
\(W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}\) and
\(W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\). In this work we
employ \(h=8\) parallel attention layers, or heads. For each of these we
use \(d_k=d_v=d_{\text{model}}/h=64\). Due to the reduced dimension of
each head, the total computational cost is similar to that of
single-head attention with full dimensionality.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{MultiHeadedAttention}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{h}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Take in model size and number of heads.}\PY{l+s+s2}{\PYZdq{}}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{MultiHeadedAttention}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{k}{assert} \PY{n}{d\PYZus{}model} \PY{o}{\PYZpc{}} \PY{n}{h} \PY{o}{==} \PY{l+m+mi}{0}
        \PY{c+c1}{\PYZsh{} We assume d\PYZus{}v always equals d\PYZus{}k}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}k} \PY{o}{=} \PY{n}{d\PYZus{}model} \PY{o}{/}\PY{o}{/} \PY{n}{h}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h} \PY{o}{=} \PY{n}{h}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linears} \PY{o}{=} \PY{n}{clones}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{attn} \PY{o}{=} \PY{k+kc}{None}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{n}{dropout}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{,} \PY{n}{mask}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Implements Figure 2}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{if} \PY{n}{mask} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Same mask applied to all h heads.}
            \PY{n}{mask} \PY{o}{=} \PY{n}{mask}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{nb} \PY{o}{=} \PY{n}{query}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} 1) Do all the linear projections in batch from d\PYZus{}model =\PYZgt{} h x d\PYZus{}k }
        \PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value} \PY{o}{=} \PY{p}{[}
            \PY{n}{l}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{nb}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}k}\PY{p}{)}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
            \PY{k}{for} \PY{n}{l}\PY{p}{,} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linears}\PY{p}{,} \PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{)}\PY{p}{)}\PY{p}{]}

        \PY{c+c1}{\PYZsh{} 2) Apply attention on all the projected vectors in batch. }
        \PY{n}{x}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{attn} \PY{o}{=} \PY{n}{attention}\PY{p}{(}\PY{n}{query}\PY{p}{,} \PY{n}{key}\PY{p}{,} \PY{n}{value}\PY{p}{,} \PY{n}{mask}\PY{o}{=}\PY{n}{mask}\PY{p}{,}
                                 \PY{n}{dropout}\PY{o}{=}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} 3) \PYZdq{}Concat\PYZdq{} using a view and apply a final linear. }
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{transpose}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{contiguous}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}
            \PY{n}{nb}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{h} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}k}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linears}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Position-wise Feed-Forward
Networks}\label{position-wise-feed-forward-networks}

In addition to attention sub-layers, each of the layers in our encoder
and decoder contains a fully connected feed-forward network, which is
applied to each position separately and identically. This consists of
two linear transformations with a ReLU activation in between.
\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2 + b_2\] While the linear
transformations are the same across different positions, they use
different parameters from layer to layer. Another way of describing this
is as two convolutions with kernel size 1. The dimensionality of input
and output is \(d_{\text{model}}=512\), and the inner-layer has
dimensionality \(d_{ff}=2048\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{PositionwiseFeedForward}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Implements FFN equation.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{PositionwiseFeedForward}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{d\PYZus{}ff}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{dropout}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}2}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{w\PYZus{}1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Embeddings and Softmax}\label{embeddings-and-softmax}

Similarly to other sequence transduction models, we use learned
embeddings to convert the input tokens and output tokens to vectors of
dimension \(d_{\text{model}}\). We also use the usual learned linear
transformation and softmax function to convert the decoder output to
predicted next-token probabilities. In our model, we share the same
weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to
\cite{DBLP:journals/corr/PressW16}. In the embedding
layers, we multiply those weights by \(\sqrt{d_{\text{model}}}\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Embeddings}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{vocab}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{Embeddings}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lut} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Embedding}\PY{p}{(}\PY{n}{vocab}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}model} \PY{o}{=} \PY{n}{d\PYZus{}model}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lut}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{o}{*} \PY{n}{math}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{d\PYZus{}model}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Positional Encoding}\label{positional-encoding}

Since our model contains no recurrence and no convolution, in order for
the model to make use of the order of the sequence, we must inject some
information about the relative or absolute position of the tokens in the
sequence. To this end, we add "positional encodings" to the input
embeddings at the bottoms of the encoder and decoder stacks. The
positional encodings have the same dimension \(d_{\text{model}}\) as the
embeddings, so that the two can be summed. There are many choices of
positional encodings, learned and fixed
\cite{DBLP:journals/corr/GehringAGYD17}.

In this work, we use sine and cosine functions of different frequencies:
\[PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\text{model}}})\]
\[PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\text{model}}})\]\\
where \(pos\) is the position and \(i\) is the dimension. That is, each
dimension of the positional encoding corresponds to a sinusoid. The
wavelengths form a geometric progression from \(2\pi\) to
\(10000 \cdot 2\pi\). We chose this function because we hypothesized it
would allow the model to easily learn to attend by relative positions,
since for any fixed offset \(k\), \(PE_{pos+k}\) can be represented as a
linear function of \(PE_{pos}\).

In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the
base model, we use a rate of \(P_{drop}=0.1\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{PositionalEncoding}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Implement the PE function.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{dropout}\PY{p}{,} \PY{n}{max\PYZus{}len}\PY{o}{=}\PY{l+m+mi}{5000}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{PositionalEncoding}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{n}{p}\PY{o}{=}\PY{n}{dropout}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Compute the positional encodings once in log space.}
        \PY{n}{pe} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{max\PYZus{}len}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}
        \PY{n}{position} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{max\PYZus{}len}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{div\PYZus{}term} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)} \PY{o}{*}
                             \PY{o}{\PYZhy{}}\PY{p}{(}\PY{n}{math}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mf}{10000.0}\PY{p}{)} \PY{o}{/} \PY{n}{d\PYZus{}model}\PY{p}{)}\PY{p}{)}
        \PY{n}{pe}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{position} \PY{o}{*} \PY{n}{div\PYZus{}term}\PY{p}{)}
        \PY{n}{pe}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{position} \PY{o}{*} \PY{n}{div\PYZus{}term}\PY{p}{)}
        \PY{n}{pe} \PY{o}{=} \PY{n}{pe}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{register\PYZus{}buffer}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{pe}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{pe}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n}{x} \PY{o}{+} \PY{n}{Variable}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{pe}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                         \PY{n}{requires\PYZus{}grad}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{dropout}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{15}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{pe} \PY{o}{=} \PY{n}{PositionalEncoding}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{pe}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{Variable}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{,} \PY{l+m+mi}{20}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{p}{:}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{:}\PY{l+m+mi}{8}\PY{p}{]}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{numpy}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{dim }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{p} \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{p}{[}\PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{k+kc}{None}
    \end{Verbatim}
     \end{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_49_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \begin{quoting}
We also experimented with using learned positional embeddings
\cite{DBLP:journals/corr/GehringAGYD17} instead, and found
that the two versions produced nearly identical results. We chose the
sinusoidal version because it may allow the model to extrapolate to
sequence lengths longer than the ones encountered during training.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{make\PYZus{}model}\PY{p}{(}\PY{n}{src\PYZus{}vocab}\PY{p}{,} \PY{n}{tgt\PYZus{}vocab}\PY{p}{,} \PY{n}{N}\PY{o}{=}\PY{l+m+mi}{6}\PY{p}{,}
               \PY{n}{d\PYZus{}model}\PY{o}{=}\PY{l+m+mi}{512}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{o}{=}\PY{l+m+mi}{2048}\PY{p}{,} \PY{n}{h}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{dropout}\PY{o}{=}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Helper: Construct a model from hyperparameters.}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{c} \PY{o}{=} \PY{n}{copy}\PY{o}{.}\PY{n}{deepcopy}
    \PY{n}{attn} \PY{o}{=} \PY{n}{MultiHeadedAttention}\PY{p}{(}\PY{n}{h}\PY{p}{,} \PY{n}{d\PYZus{}model}\PY{p}{)}
    \PY{n}{ff} \PY{o}{=} \PY{n}{PositionwiseFeedForward}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{d\PYZus{}ff}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}
    \PY{n}{position} \PY{o}{=} \PY{n}{PositionalEncoding}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}
    \PY{n}{d} \PY{o}{=} \PY{n}{d\PYZus{}model}
    \PY{n}{model} \PY{o}{=} \PY{n}{EncoderDecoder}\PY{p}{(}
        \PY{n}{Encoder}\PY{p}{(}\PY{n}{EncoderLayer}\PY{p}{(}\PY{n}{d}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{attn}\PY{p}{)}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{ff}\PY{p}{)}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{,}
        \PY{n}{Decoder}\PY{p}{(}\PY{n}{DecoderLayer}\PY{p}{(}\PY{n}{d}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{attn}\PY{p}{)}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{attn}\PY{p}{)}\PY{p}{,}
                             \PY{n}{c}\PY{p}{(}\PY{n}{ff}\PY{p}{)}\PY{p}{,} \PY{n}{dropout}\PY{p}{)}\PY{p}{,} \PY{n}{N}\PY{p}{)}\PY{p}{,}
        \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{n}{Embeddings}\PY{p}{(}\PY{n}{d}\PY{p}{,} \PY{n}{src\PYZus{}vocab}\PY{p}{)}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{position}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}\PY{n}{Embeddings}\PY{p}{(}\PY{n}{d}\PY{p}{,} \PY{n}{tgt\PYZus{}vocab}\PY{p}{)}\PY{p}{,} \PY{n}{c}\PY{p}{(}\PY{n}{position}\PY{p}{)}\PY{p}{)}\PY{p}{,}
        \PY{n}{Generator}\PY{p}{(}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{n}{tgt\PYZus{}vocab}\PY{p}{)}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} This was important from their code. }
    \PY{c+c1}{\PYZsh{} Initialize parameters with Glorot / fan\PYZus{}avg.}
    \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{if} \PY{n}{p}\PY{o}{.}\PY{n}{dim}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{xavier\PYZus{}uniform}\PY{p}{(}\PY{n}{p}\PY{p}{)}
    \PY{k}{return} \PY{n}{model}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\section{Training}\label{training}

This section describes the training regime for our models.
     \end{quoting}\begin{quoting}
\subsection{Batches and Masking}\label{batches-and-masking}
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{Batch}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Batch of data with mask for training.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{src}\PY{p}{,} \PY{n}{trg}\PY{o}{=}\PY{k+kc}{None}\PY{p}{,} \PY{n}{pad}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src} \PY{o}{=} \PY{n}{src}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{src\PYZus{}mask} \PY{o}{=} \PY{p}{(}\PY{n}{src} \PY{o}{!=} \PY{n}{pad}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{k}{if} \PY{n}{trg} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trg} \PY{o}{=} \PY{n}{trg}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trg\PYZus{}y} \PY{o}{=} \PY{n}{trg}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trg\PYZus{}mask} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{make\PYZus{}std\PYZus{}mask}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trg}\PY{p}{,} \PY{n}{pad}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{ntokens} \PY{o}{=} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{trg\PYZus{}y} \PY{o}{!=} \PY{n}{pad}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}

    \PY{n+nd}{@staticmethod}
    \PY{k}{def} \PY{n+nf}{make\PYZus{}std\PYZus{}mask}\PY{p}{(}\PY{n}{tgt}\PY{p}{,} \PY{n}{pad}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Create a mask to hide padding and future words.}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{tgt\PYZus{}mask} \PY{o}{=} \PY{p}{(}\PY{n}{tgt} \PY{o}{!=} \PY{n}{pad}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{tgt\PYZus{}mask} \PY{o}{=} \PY{n}{tgt\PYZus{}mask} \PY{o}{\PYZam{}} \PY{n}{Variable}\PY{p}{(}
            \PY{n}{subsequent\PYZus{}mask}\PY{p}{(}\PY{n}{tgt}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
            \PY{o}{.}\PY{n}{type\PYZus{}as}\PY{p}{(}\PY{n}{tgt\PYZus{}mask}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{tgt\PYZus{}mask}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Training Loop}\label{training-loop}
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{run\PYZus{}epoch}\PY{p}{(}\PY{n}{data\PYZus{}iter}\PY{p}{,} \PY{n}{model}\PY{p}{,} \PY{n}{loss\PYZus{}compute}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Standard Training and Logging Function}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{start} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{n}{total\PYZus{}tokens} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{total\PYZus{}loss} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{tokens} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{i}\PY{p}{,} \PY{n}{batch} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{data\PYZus{}iter}\PY{p}{)}\PY{p}{:}
        \PY{n}{out} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{forward}\PY{p}{(}\PY{n}{batch}\PY{o}{.}\PY{n}{src}\PY{p}{,} \PY{n}{batch}\PY{o}{.}\PY{n}{trg}\PY{p}{,}
                            \PY{n}{batch}\PY{o}{.}\PY{n}{src\PYZus{}mask}\PY{p}{,} \PY{n}{batch}\PY{o}{.}\PY{n}{trg\PYZus{}mask}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{loss\PYZus{}compute}\PY{p}{(}\PY{n}{out}\PY{p}{,} \PY{n}{batch}\PY{o}{.}\PY{n}{trg\PYZus{}y}\PY{p}{,} \PY{n}{batch}\PY{o}{.}\PY{n}{ntokens}\PY{p}{)}
        \PY{n}{total\PYZus{}loss} \PY{o}{+}\PY{o}{=} \PY{n}{loss}
        \PY{n}{total\PYZus{}tokens} \PY{o}{+}\PY{o}{=} \PY{n}{batch}\PY{o}{.}\PY{n}{ntokens}
        \PY{n}{tokens} \PY{o}{+}\PY{o}{=} \PY{n}{batch}\PY{o}{.}\PY{n}{ntokens}
        \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{50} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
            \PY{n}{elapsed} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{start}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Epoch Step: }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{ Loss: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{ Tokens / Sec: }\PY{l+s+si}{\PYZpc{}f}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}}
                  \PY{p}{(}\PY{n}{i}\PY{p}{,} \PY{n}{loss} \PY{o}{/} \PY{n}{batch}\PY{o}{.}\PY{n}{ntokens}\PY{p}{,} \PY{n}{tokens} \PY{o}{/} \PY{n}{elapsed}\PY{p}{)}\PY{p}{)}
            \PY{n}{start} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
            \PY{n}{tokens} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{return} \PY{n}{total\PYZus{}loss} \PY{o}{/} \PY{n}{total\PYZus{}tokens}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Training Data and
Batching}\label{training-data-and-batching}

We trained on the standard WMT 2014 English-German dataset consisting of
about 4.5 million sentence pairs. Sentences were encoded using byte-pair
encoding, which has a shared source-target vocabulary of about 37000
tokens. For English-French, we used the significantly larger WMT 2014
English-French dataset consisting of 36M sentences and split tokens into
a 32000 word-piece vocabulary.

Sentence pairs were batched together by approximate sequence length.
Each training batch contained a set of sentence pairs containing
approximately 25000 source tokens and 25000 target tokens.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{global} \PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch}\PY{p}{,} \PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch}
\PY{k}{def} \PY{n+nf}{batch\PYZus{}size\PYZus{}fn}\PY{p}{(}\PY{n}{new}\PY{p}{,} \PY{n}{count}\PY{p}{,} \PY{n}{sofar}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Calculate total number of tokens + padding.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{global} \PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch}\PY{p}{,} \PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch}
    \PY{k}{if} \PY{n}{count} \PY{o}{==} \PY{l+m+mi}{1}\PY{p}{:}
        \PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch}\PY{p}{,}
                           \PY{n+nb}{len}\PY{p}{(}\PY{n}{new}\PY{o}{.}\PY{n}{src}\PY{p}{)}\PY{p}{)}
    \PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch}\PY{p}{,}
                           \PY{n+nb}{len}\PY{p}{(}\PY{n}{new}\PY{o}{.}\PY{n}{trg}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n}{src\PYZus{}elements} \PY{o}{=} \PY{n}{count} \PY{o}{*} \PY{n}{max\PYZus{}src\PYZus{}in\PYZus{}batch}
    \PY{n}{tgt\PYZus{}elements} \PY{o}{=} \PY{n}{count} \PY{o}{*} \PY{n}{max\PYZus{}tgt\PYZus{}in\PYZus{}batch}
    \PY{k}{return} \PY{n+nb}{max}\PY{p}{(}\PY{n}{src\PYZus{}elements}\PY{p}{,} \PY{n}{tgt\PYZus{}elements}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Hardware and Schedule}\label{hardware-and-schedule}

We trained our models on one machine with 8 NVIDIA P100 GPUs. For our
base models using the hyperparameters described throughout the paper,
each training step took about 0.4 seconds. We trained the base models
for a total of 100,000 steps or 12 hours. For our big models, step time
was 1.0 seconds. The big models were trained for 300,000 steps (3.5
days).
     \end{quoting}\begin{quoting}
\subsection{Optimizer}\label{optimizer}

We used the Adam optimizer
\cite{DBLP:journals/corr/KingmaB14} with \(\beta_1=0.9\),
\(\beta_2=0.98\) and \(\epsilon=10^{-9}\). We varied the learning rate
over the course of training, according to the formula:
\[lrate = d_{\text{model}}^{-0.5} \cdot\]
\[\min({step\_num}^{-0.5}, {step\_num} \cdot {warmup\_steps}^{-1.5})\]
This corresponds to increasing the learning rate linearly for the first
\(warmup\_steps\) training steps, and decreasing it thereafter
proportionally to the inverse square root of the step number. We used
\(warmup\_steps=4000\).
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{NoamOpt}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Optim wrapper that implements rate.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{model\PYZus{}size}\PY{p}{,} \PY{n}{factor}\PY{p}{,}
                 \PY{n}{warmup}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer} \PY{o}{=} \PY{n}{optimizer}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}step} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{warmup} \PY{o}{=} \PY{n}{warmup}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{factor} \PY{o}{=} \PY{n}{factor}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model\PYZus{}size} \PY{o}{=} \PY{n}{model\PYZus{}size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}rate} \PY{o}{=} \PY{l+m+mi}{0}

    \PY{k}{def} \PY{n+nf}{step}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Update parameters and rate}\PY{l+s+s2}{\PYZdq{}}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}step} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
        \PY{n}{rate} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rate}\PY{p}{(}\PY{p}{)}
        \PY{k}{for} \PY{n}{p} \PY{o+ow}{in} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{param\PYZus{}groups}\PY{p}{:}
            \PY{n}{p}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{rate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}rate} \PY{o}{=} \PY{n}{rate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

    \PY{k}{def} \PY{n+nf}{rate}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{step}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Implement `lrate` above}\PY{l+s+s2}{\PYZdq{}}
        \PY{k}{if} \PY{n}{step} \PY{o+ow}{is} \PY{k+kc}{None}\PY{p}{:}
            \PY{n}{step} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{\PYZus{}step}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{factor} \PY{o}{*} \PY{p}{(}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{model\PYZus{}size} \PY{o}{*}\PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{)} \PY{o}{*}
            \PY{n+nb}{min}\PY{p}{(}\PY{n}{step} \PY{o}{*}\PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,} \PY{n}{step} \PY{o}{*} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{warmup} \PY{o}{*}\PY{o}{*} \PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{1.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{k}{def} \PY{n+nf}{get\PYZus{}std\PYZus{}opt}\PY{p}{(}\PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{NoamOpt}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{src\PYZus{}embed}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{d\PYZus{}model}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{,}
                   \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                                    \PY{n}{lr}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,}
                                    \PY{n}{betas}\PY{o}{=}\PY{p}{(}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{l+m+mf}{0.98}\PY{p}{)}\PY{p}{,}
                                    \PY{n}{eps}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}9}\PY{p}{)}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Three settings of the lrate hyperparameters.}
\PY{n}{opts} \PY{o}{=} \PY{p}{[}\PY{n}{NoamOpt}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}\PY{p}{,}
        \PY{n}{NoamOpt}\PY{p}{(}\PY{l+m+mi}{512}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{8000}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}\PY{p}{,}
        \PY{n}{NoamOpt}\PY{p}{(}\PY{l+m+mi}{256}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4000}\PY{p}{,} \PY{k+kc}{None}\PY{p}{)}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{20000}\PY{p}{)}\PY{p}{,} \PY{p}{[}\PY{p}{[}\PY{n}{opt}\PY{o}{.}\PY{n}{rate}\PY{p}{(}\PY{n}{i}\PY{p}{)} \PY{k}{for} \PY{n}{opt} \PY{o+ow}{in} \PY{n}{opts}\PY{p}{]}
                               \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{20000}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{512:4000}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{512:8000}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{256:4000}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{k+kc}{None}
    \end{Verbatim}
     \end{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_69_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \begin{quoting}
\subsection{Regularization}\label{regularization}

\subsubsection{Label Smoothing}\label{label-smoothing}

During training, we employed label smoothing of value
\(\epsilon_{ls}=0.1\) \cite{DBLP:journals/corr/SzegedyVISW15}.
This hurts perplexity, as the model learns to be more unsure, but
improves accuracy and BLEU score.
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{LabelSmoothing}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Implement label smoothing.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{size}\PY{p}{,} \PY{n}{padding\PYZus{}idx}\PY{p}{,} \PY{n}{smoothing}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{LabelSmoothing}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{KLDivLoss}\PY{p}{(}\PY{n}{size\PYZus{}average}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{padding\PYZus{}idx} \PY{o}{=} \PY{n}{padding\PYZus{}idx}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{confidence} \PY{o}{=} \PY{l+m+mf}{1.0} \PY{o}{\PYZhy{}} \PY{n}{smoothing}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{smoothing} \PY{o}{=} \PY{n}{smoothing}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size} \PY{o}{=} \PY{n}{size}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{true\PYZus{}dist} \PY{o}{=} \PY{k+kc}{None}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{target}\PY{p}{)}\PY{p}{:}
        \PY{k}{assert} \PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size}
        \PY{n}{true\PYZus{}dist} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{clone}\PY{p}{(}\PY{p}{)}
        \PY{n}{true\PYZus{}dist}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{smoothing} \PY{o}{/} \PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{size} \PY{o}{\PYZhy{}} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
        \PY{n}{true\PYZus{}dist}\PY{o}{.}\PY{n}{scatter\PYZus{}}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{target}\PY{o}{.}\PY{n}{data}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
                           \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{confidence}\PY{p}{)}
        \PY{n}{true\PYZus{}dist}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{padding\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n}{mask} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nonzero}\PY{p}{(}\PY{n}{target}\PY{o}{.}\PY{n}{data} \PY{o}{==} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{padding\PYZus{}idx}\PY{p}{)}
        \PY{k}{if} \PY{n}{mask}\PY{o}{.}\PY{n}{dim}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZgt{}} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{true\PYZus{}dist}\PY{o}{.}\PY{n}{index\PYZus{}fill\PYZus{}}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{mask}\PY{o}{.}\PY{n}{squeeze}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{l+m+mf}{0.0}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{true\PYZus{}dist} \PY{o}{=} \PY{n}{true\PYZus{}dist}
        \PY{k}{return} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion}\PY{p}{(}\PY{n}{x}\PY{p}{,}
                              \PY{n}{Variable}\PY{p}{(}\PY{n}{true\PYZus{}dist}\PY{p}{,}
                                       \PY{n}{requires\PYZus{}grad}\PY{o}{=}\PY{k+kc}{False}\PY{p}{)}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}Example of label smoothing.}
\PY{n}{crit} \PY{o}{=} \PY{n}{LabelSmoothing}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.4}\PY{p}{)}
\PY{n}{predict} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}
    \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}
     \PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.2}\PY{p}{,} \PY{l+m+mf}{0.7}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{v} \PY{o}{=} \PY{n}{crit}\PY{p}{(}\PY{n}{Variable}\PY{p}{(}\PY{n}{predict}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
         \PY{n}{Variable}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Show the target distributions expected by the system.}
\PY{n}{plt}\PY{o}{.}\PY{n}{imshow}\PY{p}{(}\PY{n}{crit}\PY{o}{.}\PY{n}{true\PYZus{}dist}\PY{p}{)}
\PY{k+kc}{None}
    \end{Verbatim}
     \end{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_74_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{crit} \PY{o}{=} \PY{n}{LabelSmoothing}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mf}{0.1}\PY{p}{)}
\PY{k}{def} \PY{n+nf}{loss}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{d} \PY{o}{=} \PY{n}{x} \PY{o}{+} \PY{l+m+mi}{3} \PY{o}{*} \PY{l+m+mi}{1}
    \PY{n}{predict} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{FloatTensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{x} \PY{o}{/} \PY{n}{d}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{d}\PY{p}{,}
                                  \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{d}\PY{p}{,} \PY{l+m+mi}{1} \PY{o}{/} \PY{n}{d}\PY{p}{]}\PY{p}{]}\PY{p}{)}
    \PY{k}{return} \PY{n}{crit}\PY{p}{(}\PY{n}{Variable}\PY{p}{(}\PY{n}{predict}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}
                \PY{n}{Variable}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{,}
         \PY{p}{[}\PY{n}{loss}\PY{p}{(}\PY{n}{x}\PY{p}{)} \PY{k}{for} \PY{n}{x} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\PY{k+kc}{None}
    \end{Verbatim}
     \end{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_76_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \begin{quoting}
\subsection{Loss Computation}\label{loss-computation}
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{SimpleLossCompute}\PY{p}{:}
    \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{A simple loss compute and train function.}\PY{l+s+s2}{\PYZdq{}}
    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{generator}\PY{p}{,}
                 \PY{n}{criterion}\PY{p}{,} \PY{n}{opt}\PY{o}{=}\PY{k+kc}{None}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{generator} \PY{o}{=} \PY{n}{generator}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion} \PY{o}{=} \PY{n}{criterion}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{opt} \PY{o}{=} \PY{n}{opt}

    \PY{k}{def} \PY{n+nf}{\PYZus{}\PYZus{}call\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{norm}\PY{p}{)}\PY{p}{:}
        \PY{n}{x} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{generator}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{criterion}\PY{p}{(}
            \PY{n}{x}\PY{o}{.}\PY{n}{contiguous}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{x}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,}
            \PY{n}{y}\PY{o}{.}\PY{n}{contiguous}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)} \PY{o}{/} \PY{n}{norm}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{opt} \PY{o+ow}{is} \PY{o+ow}{not} \PY{k+kc}{None}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{opt}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{opt}\PY{o}{.}\PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{k}{return} \PY{n}{loss}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{*} \PY{n}{norm}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\section{Decoding and Visualization}\label{decoding-and-visualization}

\subsection{Greedy Decoding}\label{greedy-decoding}
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{greedy\PYZus{}decode}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{src}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,}
                  \PY{n}{max\PYZus{}len}\PY{p}{,} \PY{n}{start\PYZus{}sym}\PY{p}{)}\PY{p}{:}
    \PY{n}{memory} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encode}\PY{p}{(}\PY{n}{src}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{)}
    \PY{n}{ys} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n}{start\PYZus{}sym}\PY{p}{)}\PY{o}{.}\PY{n}{type\PYZus{}as}\PY{p}{(}\PY{n}{src}\PY{o}{.}\PY{n}{data}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{max\PYZus{}len} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
        \PY{n}{out} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{decode}\PY{p}{(}\PY{n}{memory}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,}
                           \PY{n}{Variable}\PY{p}{(}\PY{n}{ys}\PY{p}{)}\PY{p}{,}
                           \PY{n}{Variable}\PY{p}{(}
                               \PY{n}{subsequent\PYZus{}mask}\PY{p}{(}\PY{n}{ys}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
                               \PY{o}{.}\PY{n}{type\PYZus{}as}\PY{p}{(}\PY{n}{src}\PY{o}{.}\PY{n}{data}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{n}{prob} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{generator}\PY{p}{(}\PY{n}{out}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
        \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{next\PYZus{}word} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{n}{prob}\PY{p}{,} \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n}{next\PYZus{}word} \PY{o}{=} \PY{n}{next\PYZus{}word}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{ys} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{cat}\PY{p}{(}\PY{p}{[}\PY{n}{ys}\PY{p}{,}
                        \PY{n}{torch}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
                        \PY{o}{.}\PY{n}{type\PYZus{}as}\PY{p}{(}\PY{n}{src}\PY{o}{.}\PY{n}{data}\PY{p}{)}
                        \PY{o}{.}\PY{n}{fill\PYZus{}}\PY{p}{(}\PY{n}{next\PYZus{}word}\PY{p}{)}\PY{p}{]}\PY{p}{,}
                       \PY{n}{dim}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{k}{return} \PY{n}{ys}
    \end{Verbatim}
     \end{tiny}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}
\PY{n}{sent} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{l+s+s2}{@@@The @@@log @@@file @@@can @@@be @@@sent @@@secret ly}
\PY{l+s+s2}{         @@@with @@@email @@@or @@@FTP @@@to @@@a @@@specified}
\PY{l+s+s2}{         @@@receiver}\PY{l+s+s2}{\PYZdq{}\PYZdq{}\PYZdq{}}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
\PY{n}{src} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{LongTensor}\PY{p}{(}\PY{p}{[}\PY{p}{[}\PY{n}{SRC}\PY{o}{.}\PY{n}{stoi}\PY{p}{[}\PY{n}{w}\PY{p}{]} \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{sent}\PY{p}{]}\PY{p}{]}\PY{p}{)}
\PY{n}{src} \PY{o}{=} \PY{n}{Variable}\PY{p}{(}\PY{n}{src}\PY{p}{)}
\PY{n}{src\PYZus{}mask} \PY{o}{=} \PY{p}{(}\PY{n}{src} \PY{o}{!=} \PY{n}{SRC}\PY{o}{.}\PY{n}{stoi}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}blank\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}\PY{o}{.}\PY{n}{unsqueeze}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
\PY{n}{out} \PY{o}{=} \PY{n}{greedy\PYZus{}decode}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{src}\PY{p}{,} \PY{n}{src\PYZus{}mask}\PY{p}{,}
                    \PY{n}{max\PYZus{}len}\PY{o}{=}\PY{l+m+mi}{60}\PY{p}{,}
                    \PY{n}{start\PYZus{}symbol}\PY{o}{=}\PY{n}{TGT}\PY{o}{.}\PY{n}{stoi}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}s\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Translation:}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{end}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{trans} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}s\PYZgt{} }\PY{l+s+s2}{\PYZdq{}}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{out}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{sym} \PY{o}{=} \PY{n}{TGT}\PY{o}{.}\PY{n}{itos}\PY{p}{[}\PY{n}{out}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{]}
    \PY{k}{if} \PY{n}{sym} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZlt{}/s\PYZgt{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
        \PY{k}{break}
    \PY{n}{trans} \PY{o}{+}\PY{o}{=} \PY{n}{sym} \PY{o}{+} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ }\PY{l+s+s2}{\PYZdq{}}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{trans}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\begin{quoting}
\subsection{Attention Visualization}\label{attention-visualization}
     \end{quoting}\begin{tiny}
        \noindent
    \begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tgt\PYZus{}sent} \PY{o}{=} \PY{n}{trans}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{p}{)}
\PY{k}{def} \PY{n+nf}{draw}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{ax}\PY{p}{)}\PY{p}{:}
    \PY{n}{seaborn}\PY{o}{.}\PY{n}{heatmap}\PY{p}{(}\PY{n}{data}\PY{p}{,}
                    \PY{n}{xticklabels}\PY{o}{=}\PY{n}{x}\PY{p}{,} \PY{n}{square}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                    \PY{n}{yticklabels}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{vmin}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{vmax}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,}
                    \PY{n}{cbar}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{ax}\PY{p}{)}

\PY{k}{for} \PY{n}{layer\PYZus{}num} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Encoder Layer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{layer\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{layer} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{encoder}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}num}\PY{p}{]}
    \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
        \PY{n}{draw}\PY{p}{(}\PY{n}{layer}\PY{o}{.}\PY{n}{self\PYZus{}attn}\PY{o}{.}\PY{n}{attn}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{data}\PY{p}{,}
             \PY{n}{sent}\PY{p}{,} \PY{n}{sent} \PY{k}{if} \PY{n}{h} \PY{o}{==} \PY{l+m+mi}{0} \PY{k}{else} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axs}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{k}{for} \PY{n}{layer\PYZus{}num} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Decoder Self Layer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{layer\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{layer} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{decoder}\PY{o}{.}\PY{n}{layers}\PY{p}{[}\PY{n}{layer\PYZus{}num}\PY{p}{]}
    \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
        \PY{n}{draw}\PY{p}{(}\PY{n}{layer}\PY{o}{.}\PY{n}{self\PYZus{}attn}\PY{o}{.}\PY{n}{attn}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{]}
             \PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{tgt\PYZus{}sent}\PY{p}{)}\PY{p}{,} \PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{tgt\PYZus{}sent}\PY{p}{)}\PY{p}{]}\PY{p}{,}
             \PY{n}{tgt\PYZus{}sent}\PY{p}{,} \PY{n}{tgt\PYZus{}sent} \PY{k}{if} \PY{n}{h} \PY{o}{==} \PY{l+m+mi}{0} \PY{k}{else} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axs}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Decoder Src Layer}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{layer\PYZus{}num} \PY{o}{+} \PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{fig}\PY{p}{,} \PY{n}{axs} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{h} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{4}\PY{p}{)}\PY{p}{:}
        \PY{n}{draw}\PY{p}{(}\PY{n}{layer}\PY{o}{.}\PY{n}{src\PYZus{}attn}\PY{o}{.}\PY{n}{attn}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{h}\PY{p}{]}\PY{o}{.}\PY{n}{data}\PY{p}{[}
            \PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{tgt\PYZus{}sent}\PY{p}{)}\PY{p}{,} \PY{p}{:}\PY{n+nb}{len}\PY{p}{(}\PY{n}{sent}\PY{p}{)}\PY{p}{]}\PY{p}{,}
             \PY{n}{sent}\PY{p}{,} \PY{n}{tgt\PYZus{}sent} \PY{k}{if} \PY{n}{h} \PY{o}{==} \PY{l+m+mi}{0} \PY{k}{else} \PY{p}{[}\PY{p}{]}\PY{p}{,} \PY{n}{ax}\PY{o}{=}\PY{n}{axs}\PY{p}{[}\PY{n}{h}\PY{p}{]}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
    \end{Verbatim}
     \end{tiny}\noindent
    \begin{tiny}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{main_files/main_122_0.png}
    \end{center}
    { \hspace*{\fill} \\}

       \end{tiny}

    % Add a bibliography block to the postdoc

\section{Conclusion}


This paper presents a replication exercise of
  the transformer network. Consult the
  full online version for features such as multi-gpu
  training, real experiments on full translation problems, and
  pointers to other extensions such as beam search, sub-word models,
  and model averaging.
  The goal is to explore a literate programming experiment of
  interleaving model replication with formal writing. While not always
  possible, this modality can be useful for transmitting ideas and
  encouraging faster open-source uptake. Additionally this method can
  be an easy way to learn about a model alongside its implementation.

\bibliographystyle{acl}
\bibliography{ref}



    \end{document}
