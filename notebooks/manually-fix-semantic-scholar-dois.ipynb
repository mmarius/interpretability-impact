{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d56396a1-f4f7-4658-a1ce-ae8a544e3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cede388b-6d37-4c37-8507-a460dbe3ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0c9881-74a3-4c21-817a-17cd67c28f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>embedding</th>\n",
       "      <th>classifier_interpretability_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.8</td>\n",
       "      <td>Large Scale Multi-Actor Generative Dialog Mode...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.8</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Non-goal oriented dialog agents (i.e. chatbots...</td>\n",
       "      <td>[-5.58118939e-01 -1.25368834e-01 -6.33979887e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.52</td>\n",
       "      <td>CDL: Curriculum Dual Learning for Emotion-Cont...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.52</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Emotion-controllable response generation is an...</td>\n",
       "      <td>[-1.12784958e+00 -5.22979379e-01  5.63119724e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.46</td>\n",
       "      <td>Emergence of Syntax Needs Minimal Supervision</td>\n",
       "      <td>Theory and Formalism in NLP (Linguistic and Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.46</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>This paper is a theoretical contribution to th...</td>\n",
       "      <td>[ 2.61768043e-01  8.10616314e-01  2.74266422e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.359</td>\n",
       "      <td>Selecting Backtranslated Data from Multiple So...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.359</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Machine translation (MT) has benefited from us...</td>\n",
       "      <td>[-4.39277172e-01  1.06740630e+00  8.58952925e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.417</td>\n",
       "      <td>ParaCrawl: Web-Scale Acquisition of Parallel C...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.417</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>We report on methods to create the largest pub...</td>\n",
       "      <td>[-3.59388590e-01  3.36527884e-01 -2.65376344e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1656</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Human language is often multimodal, which comp...</td>\n",
       "      <td>[-2.53148854e-01 -8.23063105e-02 -3.35774094e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1657</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Chest X-Ray (CXR) images are commonly used for...</td>\n",
       "      <td>[-4.88766015e-01 -3.07189375e-01 -1.30685127e+...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1658</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>We introduce the first dataset for human edits...</td>\n",
       "      <td>[-6.34846449e-01 -2.18937039e-01  9.53872725e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1659</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this paper, we study abstractive summarizat...</td>\n",
       "      <td>[-9.39114809e-01 -1.15553305e-01 -6.91273287e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1660</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this work, we propose a novel approach that...</td>\n",
       "      <td>[-4.75014389e-01  1.98175609e-01 -3.56193632e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0       main.8  Large Scale Multi-Actor Generative Dialog Mode...   \n",
       "1      main.52  CDL: Curriculum Dual Learning for Emotion-Cont...   \n",
       "2      main.46      Emergence of Syntax Needs Minimal Supervision   \n",
       "3     main.359  Selecting Backtranslated Data from Multiple So...   \n",
       "4     main.417  ParaCrawl: Web-Scale Acquisition of Parallel C...   \n",
       "...        ...                                                ...   \n",
       "9277       889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9278      2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9279       384                          Visual Story Post-Editing   \n",
       "9280      1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9281      2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area interpretability  \\\n",
       "0                      Dialogue and Interactive Systems            False   \n",
       "1                      Dialogue and Interactive Systems            False   \n",
       "2     Theory and Formalism in NLP (Linguistic and Ma...            False   \n",
       "3                                   Machine Translation            False   \n",
       "4                              Resources and Evaluation            False   \n",
       "...                                                 ...              ...   \n",
       "9277  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9278  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9279  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9280  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9281  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "\n",
       "                                doi   source  working_doi  \\\n",
       "0       10.18653/v1/2020.acl-main.8  ACL2020         True   \n",
       "1      10.18653/v1/2020.acl-main.52  ACL2020         True   \n",
       "2      10.18653/v1/2020.acl-main.46  ACL2020         True   \n",
       "3     10.18653/v1/2020.acl-main.359  ACL2020         True   \n",
       "4     10.18653/v1/2020.acl-main.417  ACL2020         True   \n",
       "...                             ...      ...          ...   \n",
       "9277           10.18653/v1/P19-1656  ACL2019         True   \n",
       "9278           10.18653/v1/P19-1657  ACL2019         True   \n",
       "9279           10.18653/v1/P19-1658  ACL2019         True   \n",
       "9280           10.18653/v1/P19-1659  ACL2019         True   \n",
       "9281           10.18653/v1/P19-1660  ACL2019         True   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Non-goal oriented dialog agents (i.e. chatbots...   \n",
       "1     Emotion-controllable response generation is an...   \n",
       "2     This paper is a theoretical contribution to th...   \n",
       "3     Machine translation (MT) has benefited from us...   \n",
       "4     We report on methods to create the largest pub...   \n",
       "...                                                 ...   \n",
       "9277  Human language is often multimodal, which comp...   \n",
       "9278  Chest X-Ray (CXR) images are commonly used for...   \n",
       "9279  We introduce the first dataset for human edits...   \n",
       "9280  In this paper, we study abstractive summarizat...   \n",
       "9281  In this work, we propose a novel approach that...   \n",
       "\n",
       "                                              embedding  \\\n",
       "0     [-5.58118939e-01 -1.25368834e-01 -6.33979887e-...   \n",
       "1     [-1.12784958e+00 -5.22979379e-01  5.63119724e-...   \n",
       "2     [ 2.61768043e-01  8.10616314e-01  2.74266422e-...   \n",
       "3     [-4.39277172e-01  1.06740630e+00  8.58952925e-...   \n",
       "4     [-3.59388590e-01  3.36527884e-01 -2.65376344e-...   \n",
       "...                                                 ...   \n",
       "9277  [-2.53148854e-01 -8.23063105e-02 -3.35774094e-...   \n",
       "9278  [-4.88766015e-01 -3.07189375e-01 -1.30685127e+...   \n",
       "9279  [-6.34846449e-01 -2.18937039e-01  9.53872725e-...   \n",
       "9280  [-9.39114809e-01 -1.15553305e-01 -6.91273287e-...   \n",
       "9281  [-4.75014389e-01  1.98175609e-01 -3.56193632e-...   \n",
       "\n",
       "      classifier_interpretability_prediction  \n",
       "0                                      False  \n",
       "1                                      False  \n",
       "2                                      False  \n",
       "3                                      False  \n",
       "4                                      False  \n",
       "...                                      ...  \n",
       "9277                                   False  \n",
       "9278                                   False  \n",
       "9279                                   False  \n",
       "9280                                   False  \n",
       "9281                                   False  \n",
       "\n",
       "[9282 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/clean_data.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66eef3a-56e2-437d-a687-7ff674443d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers: 19it [00:33,  1.77s/it]\n"
     ]
    }
   ],
   "source": [
    "from utils import bulk_get_paper_details\n",
    "\n",
    "\n",
    "semantic_scholar_papers = bulk_get_paper_details(list(df['doi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc8b431-7aee-48e1-b6fb-00c398bf4124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"42e741e0be43954ae684d14333e4074f4d0ae961\" = None,\n",
      "\"f050575e9567478ae6f1dcfe1d0aede50c76293e\" = None,\n",
      "\"10.18653/v1/D19-1454\" = None,\n",
      "\"10.18653/v1/2021.acl-long.148\" = None,\n",
      "\"10.18653/v1/D18-1045\" = None,\n",
      "\"10.18653/v1/2022.emnlp-main.650\" = None,\n",
      "\"10.18653/v1/P19-1116\" = None,\n",
      "\"10.18653/v1/P19-1366\" = None,\n"
     ]
    }
   ],
   "source": [
    "for (index, row), paper in zip(df.iterrows(), semantic_scholar_papers):\n",
    "    if paper is None:\n",
    "        print(f\"\\\"{row['doi']}\\\" = None,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33ae1b0-cddb-43d0-85ba-048b11bc838a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REDFM: a Filtered and Multilingual Relation Extraction Dataset'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_paper_details\n",
    "\n",
    "get_paper_details(\"3c331785f7f5629563fbc4aabfe973d7c0dc57f7\").title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1c386b5-75ba-4cf2-890c-bcfe8a51993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>embedding</th>\n",
       "      <th>classifier_interpretability_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.8</td>\n",
       "      <td>Large Scale Multi-Actor Generative Dialog Mode...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.8</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Non-goal oriented dialog agents (i.e. chatbots...</td>\n",
       "      <td>[-5.58118939e-01 -1.25368834e-01 -6.33979887e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.52</td>\n",
       "      <td>CDL: Curriculum Dual Learning for Emotion-Cont...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.52</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Emotion-controllable response generation is an...</td>\n",
       "      <td>[-1.12784958e+00 -5.22979379e-01  5.63119724e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.46</td>\n",
       "      <td>Emergence of Syntax Needs Minimal Supervision</td>\n",
       "      <td>Theory and Formalism in NLP (Linguistic and Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.46</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>This paper is a theoretical contribution to th...</td>\n",
       "      <td>[ 2.61768043e-01  8.10616314e-01  2.74266422e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.359</td>\n",
       "      <td>Selecting Backtranslated Data from Multiple So...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.359</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Machine translation (MT) has benefited from us...</td>\n",
       "      <td>[-4.39277172e-01  1.06740630e+00  8.58952925e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.417</td>\n",
       "      <td>ParaCrawl: Web-Scale Acquisition of Parallel C...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.417</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>We report on methods to create the largest pub...</td>\n",
       "      <td>[-3.59388590e-01  3.36527884e-01 -2.65376344e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1656</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Human language is often multimodal, which comp...</td>\n",
       "      <td>[-2.53148854e-01 -8.23063105e-02 -3.35774094e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1657</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Chest X-Ray (CXR) images are commonly used for...</td>\n",
       "      <td>[-4.88766015e-01 -3.07189375e-01 -1.30685127e+...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1658</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>We introduce the first dataset for human edits...</td>\n",
       "      <td>[-6.34846449e-01 -2.18937039e-01  9.53872725e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1659</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this paper, we study abstractive summarizat...</td>\n",
       "      <td>[-9.39114809e-01 -1.15553305e-01 -6.91273287e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1660</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this work, we propose a novel approach that...</td>\n",
       "      <td>[-4.75014389e-01  1.98175609e-01 -3.56193632e-...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0       main.8  Large Scale Multi-Actor Generative Dialog Mode...   \n",
       "1      main.52  CDL: Curriculum Dual Learning for Emotion-Cont...   \n",
       "2      main.46      Emergence of Syntax Needs Minimal Supervision   \n",
       "3     main.359  Selecting Backtranslated Data from Multiple So...   \n",
       "4     main.417  ParaCrawl: Web-Scale Acquisition of Parallel C...   \n",
       "...        ...                                                ...   \n",
       "9277       889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9278      2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9279       384                          Visual Story Post-Editing   \n",
       "9280      1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9281      2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area interpretability  \\\n",
       "0                      Dialogue and Interactive Systems            False   \n",
       "1                      Dialogue and Interactive Systems            False   \n",
       "2     Theory and Formalism in NLP (Linguistic and Ma...            False   \n",
       "3                                   Machine Translation            False   \n",
       "4                              Resources and Evaluation            False   \n",
       "...                                                 ...              ...   \n",
       "9277  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9278  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9279  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9280  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9281  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "\n",
       "                                doi   source  working_doi  \\\n",
       "0       10.18653/v1/2020.acl-main.8  ACL2020         True   \n",
       "1      10.18653/v1/2020.acl-main.52  ACL2020         True   \n",
       "2      10.18653/v1/2020.acl-main.46  ACL2020         True   \n",
       "3     10.18653/v1/2020.acl-main.359  ACL2020         True   \n",
       "4     10.18653/v1/2020.acl-main.417  ACL2020         True   \n",
       "...                             ...      ...          ...   \n",
       "9277           10.18653/v1/P19-1656  ACL2019         True   \n",
       "9278           10.18653/v1/P19-1657  ACL2019         True   \n",
       "9279           10.18653/v1/P19-1658  ACL2019         True   \n",
       "9280           10.18653/v1/P19-1659  ACL2019         True   \n",
       "9281           10.18653/v1/P19-1660  ACL2019         True   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Non-goal oriented dialog agents (i.e. chatbots...   \n",
       "1     Emotion-controllable response generation is an...   \n",
       "2     This paper is a theoretical contribution to th...   \n",
       "3     Machine translation (MT) has benefited from us...   \n",
       "4     We report on methods to create the largest pub...   \n",
       "...                                                 ...   \n",
       "9277  Human language is often multimodal, which comp...   \n",
       "9278  Chest X-Ray (CXR) images are commonly used for...   \n",
       "9279  We introduce the first dataset for human edits...   \n",
       "9280  In this paper, we study abstractive summarizat...   \n",
       "9281  In this work, we propose a novel approach that...   \n",
       "\n",
       "                                              embedding  \\\n",
       "0     [-5.58118939e-01 -1.25368834e-01 -6.33979887e-...   \n",
       "1     [-1.12784958e+00 -5.22979379e-01  5.63119724e-...   \n",
       "2     [ 2.61768043e-01  8.10616314e-01  2.74266422e-...   \n",
       "3     [-4.39277172e-01  1.06740630e+00  8.58952925e-...   \n",
       "4     [-3.59388590e-01  3.36527884e-01 -2.65376344e-...   \n",
       "...                                                 ...   \n",
       "9277  [-2.53148854e-01 -8.23063105e-02 -3.35774094e-...   \n",
       "9278  [-4.88766015e-01 -3.07189375e-01 -1.30685127e+...   \n",
       "9279  [-6.34846449e-01 -2.18937039e-01  9.53872725e-...   \n",
       "9280  [-9.39114809e-01 -1.15553305e-01 -6.91273287e-...   \n",
       "9281  [-4.75014389e-01  1.98175609e-01 -3.56193632e-...   \n",
       "\n",
       "      classifier_interpretability_prediction  \n",
       "0                                      False  \n",
       "1                                      False  \n",
       "2                                      False  \n",
       "3                                      False  \n",
       "4                                      False  \n",
       "...                                      ...  \n",
       "9277                                   False  \n",
       "9278                                   False  \n",
       "9279                                   False  \n",
       "9280                                   False  \n",
       "9281                                   False  \n",
       "\n",
       "[9282 rows x 10 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLD_DOI_TO_NEW_DOI = {\n",
    "    \"10.1162/tacl_a_00307\": '949ee3373bcad512ad2dd1bb2829f3c0000e7eae',\n",
    "    \"10.18653/v1/2021.emnlp-main.582\": '4bbafce8de5301222f236784dfa23217636963e4',\n",
    "    \"10.48550/arXiv.2306.09802\": '3c331785f7f5629563fbc4aabfe973d7c0dc57f7',\n",
    "    \"10.18653/v1/2023.acl-long.835\": '434e375add81eaf82a49c6a0b20683ac20dfdcc1',\n",
    "    \"10.18653/v1/2023.acl-long.725\": '4a461210e454066f482f2237a736975e87e0f6a0',\n",
    "    \"10.18653/v1/D18-1092\": '7a8e4fb7e6733441a6df1e8de822618b8c5778e6',\n",
    "    \"10.18653/v1/2022.emnlp-main.674\": 'd2a1008594eb1ab856def3b143c2df407cc87e0e',\n",
    "    \"10.18653/v1/P18-2113\": '5103795aba5885abbf68a916e71c134793da5ff3',\n",
    "    # I cannot find these papers in semantic scholar\n",
    "    \"10.18653/v1/D19-1454\": None,\n",
    "    \"10.18653/v1/2021.acl-long.148\": None,\n",
    "    \"10.18653/v1/P19-1116\": None,\n",
    "    \"10.18653/v1/D18-1045\": None,\n",
    "    \"10.18653/v1/2022.emnlp-main.650\": None,\n",
    "    '10.18653/v1/2023.emnlp-main.385': None,\n",
    "    '10.18653/v1/2023.emnlp-main.616': None\n",
    "}\n",
    "\n",
    "df['doi'] = df['doi'].apply(lambda x: OLD_DOI_TO_NEW_DOI.get(x) if OLD_DOI_TO_NEW_DOI.get(x) is not None else x)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fb8e8ff-83ba-454c-8c19-bab2a3481784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching papers: 19it [00:41,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"10.18653/v1/2023.emnlp-main.385\": None,\n",
      "\"10.18653/v1/2023.emnlp-main.616\": None,\n",
      "\"10.18653/v1/D19-1454\": None,\n",
      "\"10.18653/v1/2021.acl-long.148\": None,\n",
      "\"10.18653/v1/D18-1045\": None,\n",
      "\"10.18653/v1/2022.emnlp-main.650\": None,\n",
      "\"10.18653/v1/P19-1116\": None,\n",
      "\"10.18653/v1/P19-1366\": None,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_semantic_scholar_papers = bulk_get_paper_details(list(df['doi']))\n",
    "for (index, row), paper in zip(df.iterrows(), new_semantic_scholar_papers):\n",
    "    if paper is None:\n",
    "        print(f\"\\\"{row['doi']}\\\": None,\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f00aee7-c233-42e4-bb3b-38a747b376ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/clean_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
