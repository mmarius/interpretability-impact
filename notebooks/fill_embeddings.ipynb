{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21f6d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "173e3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83a91409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.8</td>\n",
       "      <td>Large Scale Multi-Actor Generative Dialog Mode...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.8</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.52</td>\n",
       "      <td>CDL: Curriculum Dual Learning for Emotion-Cont...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.52</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.46</td>\n",
       "      <td>Emergence of Syntax Needs Minimal Supervision</td>\n",
       "      <td>Theory and Formalism in NLP (Linguistic and Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.46</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.359</td>\n",
       "      <td>Selecting Backtranslated Data from Multiple So...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.359</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.417</td>\n",
       "      <td>ParaCrawl: Web-Scale Acquisition of Parallel C...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.417</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1656</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1657</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1658</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1659</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1660</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0       main.8  Large Scale Multi-Actor Generative Dialog Mode...   \n",
       "1      main.52  CDL: Curriculum Dual Learning for Emotion-Cont...   \n",
       "2      main.46      Emergence of Syntax Needs Minimal Supervision   \n",
       "3     main.359  Selecting Backtranslated Data from Multiple So...   \n",
       "4     main.417  ParaCrawl: Web-Scale Acquisition of Parallel C...   \n",
       "...        ...                                                ...   \n",
       "9277       889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9278      2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9279       384                          Visual Story Post-Editing   \n",
       "9280      1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9281      2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area interpretability  \\\n",
       "0                      Dialogue and Interactive Systems            False   \n",
       "1                      Dialogue and Interactive Systems            False   \n",
       "2     Theory and Formalism in NLP (Linguistic and Ma...            False   \n",
       "3                                   Machine Translation            False   \n",
       "4                              Resources and Evaluation            False   \n",
       "...                                                 ...              ...   \n",
       "9277  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9278  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9279  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9280  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9281  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "\n",
       "                                doi   source  working_doi  \n",
       "0       10.18653/v1/2020.acl-main.8  ACL2020         True  \n",
       "1      10.18653/v1/2020.acl-main.52  ACL2020         True  \n",
       "2      10.18653/v1/2020.acl-main.46  ACL2020         True  \n",
       "3     10.18653/v1/2020.acl-main.359  ACL2020         True  \n",
       "4     10.18653/v1/2020.acl-main.417  ACL2020         True  \n",
       "...                             ...      ...          ...  \n",
       "9277           10.18653/v1/P19-1656  ACL2019         True  \n",
       "9278           10.18653/v1/P19-1657  ACL2019         True  \n",
       "9279           10.18653/v1/P19-1658  ACL2019         True  \n",
       "9280           10.18653/v1/P19-1659  ACL2019         True  \n",
       "9281           10.18653/v1/P19-1660  ACL2019         True  \n",
       "\n",
       "[9282 rows x 7 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/clean_data2.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6bdbc72-3267-4276-9980-3fe32b58f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = df.copy()\n",
    "processed_df['abstract'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26d2ca3e-8ebd-4d9d-9008-c9da5f116018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "def get_abstract_with_acl_anthology(doi):\n",
    "    url_path = doi.split('/')[-1]\n",
    "    url = 'https://aclanthology.org/' + url_path\n",
    "    r = requests.get(url)\n",
    "    soup = bs4.BeautifulSoup(r.text, 'html.parser')\n",
    "    css_selector = \"#main > div.row.acl-paper-details > div.col.col-lg-10.order-2 > div > div > span\"\n",
    "    element = soup.select_one(css_selector)\n",
    "    abstract = element.text\n",
    "    return abstract\n",
    "\n",
    "def get_abstract_from_semantic_scholar(paper_id):\n",
    "    query = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}\"\n",
    "    fields = \"abstract\"\n",
    "    response = requests.get(query, headers={\"x-api-key\": API_KEY}, params={\"fields\": fields})\n",
    "    paper_dict = response.json()\n",
    "\n",
    "    assert type(paper_dict['abstract']) == str\n",
    "    return paper_dict['abstract']\n",
    "\n",
    "\n",
    "def get_abstract_from_crossref(doi):\n",
    "    url = 'https://api.crossref.org/works/' + doi\n",
    "    r = requests.get(url)\n",
    "    response = r.json()\n",
    "    soup = bs4.BeautifulSoup(response['message']['abstract'], 'html.parser')\n",
    "    return soup.find('jats:p').text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1b75f6d-158c-45d5-99f8-5589acc2b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstract(doi):\n",
    "    try:\n",
    "        abstract = get_abstract_with_acl_anthology(doi)\n",
    "        return abstract\n",
    "    except Exception as e:\n",
    "        print('failed to abstract from acl anthology for', doi)\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        abstract = get_abstract_from_semantic_scholar(doi)\n",
    "        return abstract\n",
    "    except Exception as e:\n",
    "        print('failed to abstract from semantic scholar for', doi)\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        abstract = get_abstract_from_crossref(doi)\n",
    "        return abstract\n",
    "    except Exception as e:\n",
    "        print('failed to abstract from semantic scholar for', doi)\n",
    "        print(e)\n",
    "\n",
    "    print('no source has the abstract for', doi)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee448a78-2e53-4670-830a-276b1ac79ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9282/9282 [00:01<00:00, 8655.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.8</td>\n",
       "      <td>Large Scale Multi-Actor Generative Dialog Mode...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.8</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Non-goal oriented dialog agents (i.e. chatbots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.52</td>\n",
       "      <td>CDL: Curriculum Dual Learning for Emotion-Cont...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.52</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Emotion-controllable response generation is an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.46</td>\n",
       "      <td>Emergence of Syntax Needs Minimal Supervision</td>\n",
       "      <td>Theory and Formalism in NLP (Linguistic and Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.46</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>This paper is a theoretical contribution to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.359</td>\n",
       "      <td>Selecting Backtranslated Data from Multiple So...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.359</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Machine translation (MT) has benefited from us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.417</td>\n",
       "      <td>ParaCrawl: Web-Scale Acquisition of Parallel C...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.417</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>We report on methods to create the largest pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1656</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Human language is often multimodal, which comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1657</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Chest X-Ray (CXR) images are commonly used for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1658</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>We introduce the first dataset for human edits...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1659</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this paper, we study abstractive summarizat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1660</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this work, we propose a novel approach that...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0       main.8  Large Scale Multi-Actor Generative Dialog Mode...   \n",
       "1      main.52  CDL: Curriculum Dual Learning for Emotion-Cont...   \n",
       "2      main.46      Emergence of Syntax Needs Minimal Supervision   \n",
       "3     main.359  Selecting Backtranslated Data from Multiple So...   \n",
       "4     main.417  ParaCrawl: Web-Scale Acquisition of Parallel C...   \n",
       "...        ...                                                ...   \n",
       "9277       889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9278      2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9279       384                          Visual Story Post-Editing   \n",
       "9280      1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9281      2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area interpretability  \\\n",
       "0                      Dialogue and Interactive Systems            False   \n",
       "1                      Dialogue and Interactive Systems            False   \n",
       "2     Theory and Formalism in NLP (Linguistic and Ma...            False   \n",
       "3                                   Machine Translation            False   \n",
       "4                              Resources and Evaluation            False   \n",
       "...                                                 ...              ...   \n",
       "9277  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9278  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9279  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9280  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9281  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "\n",
       "                                doi   source  working_doi  \\\n",
       "0       10.18653/v1/2020.acl-main.8  ACL2020         True   \n",
       "1      10.18653/v1/2020.acl-main.52  ACL2020         True   \n",
       "2      10.18653/v1/2020.acl-main.46  ACL2020         True   \n",
       "3     10.18653/v1/2020.acl-main.359  ACL2020         True   \n",
       "4     10.18653/v1/2020.acl-main.417  ACL2020         True   \n",
       "...                             ...      ...          ...   \n",
       "9277           10.18653/v1/P19-1656  ACL2019         True   \n",
       "9278           10.18653/v1/P19-1657  ACL2019         True   \n",
       "9279           10.18653/v1/P19-1658  ACL2019         True   \n",
       "9280           10.18653/v1/P19-1659  ACL2019         True   \n",
       "9281           10.18653/v1/P19-1660  ACL2019         True   \n",
       "\n",
       "                                               abstract  \n",
       "0     Non-goal oriented dialog agents (i.e. chatbots...  \n",
       "1     Emotion-controllable response generation is an...  \n",
       "2     This paper is a theoretical contribution to th...  \n",
       "3     Machine translation (MT) has benefited from us...  \n",
       "4     We report on methods to create the largest pub...  \n",
       "...                                                 ...  \n",
       "9277  Human language is often multimodal, which comp...  \n",
       "9278  Chest X-Ray (CXR) images are commonly used for...  \n",
       "9279  We introduce the first dataset for human edits...  \n",
       "9280  In this paper, we study abstractive summarizat...  \n",
       "9281  In this work, we propose a novel approach that...  \n",
       "\n",
       "[9282 rows x 8 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def fill_abstracts(df):\n",
    "    for i in tqdm(range(len(df))):\n",
    "        if df.iloc[i]['doi'] is not None and pd.isna(df.iloc[i]['abstract']):\n",
    "            abstract = get_abstract(df.iloc[i]['doi'])\n",
    "            df.iloc[i, df.columns.get_loc('abstract')] = abstract\n",
    "    return df\n",
    "\n",
    "processed_df = fill_abstracts(processed_df)\n",
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0599a9c3-bcad-46ac-b94e-0f4df9cff7ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, area, interpretability, doi, source, working_doi, abstract]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df[processed_df['abstract'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bf7f13b-8fd2-4500-a533-4462f75769dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.18653/v1/2022.emnlp-main.782\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doi, abstract \u001b[38;5;129;01min\u001b[39;00m doi_to_abstract\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(doi)\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_df[processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m doi]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m     processed_df\u001b[38;5;241m.\u001b[39mloc[processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m doi, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m abstract\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### some papers did not have their abstract by any of the sources\n",
    "### these abstracts where added manually\n",
    "\n",
    "doi_to_abstract = {\n",
    "    '10.18653/v1/2022.emnlp-main.782': 'The development of conversational agents to interact with patients and deliver clinical advice has attracted the interest of many researchers, particularly in light of the COVID-19 pandemic. The training of an end-to-end neural based dialog system, on the other hand, is hampered by a lack of multi-turn medical dialog corpus. We make the very first attempt to release a highquality multi-turn Medical Dialog dataset relating to Covid-19 disease named CDialog, with over 1K conversations collected from the online medical counselling websites. We annotate each utterance of the conversation with seven different categories of medical entities, including diseases, symptoms, medical tests, medical history, remedies, medications and other aspects as additional labels. Finally, we propose a novel neural medical dialog system based on the CDialog dataset to advance future research on developing automated medical dialog systems. We use pre-trained language models for dialogue generation, incorporating annotated medical entities, to generate a virtual doctorâ€™s response that addresses the patientâ€™s query. Experimental results show that the proposed dialog models perform comparably better when supplemented with entity information and hence can improve the response quality.',\n",
    "    '10.18653/v1/2022.emnlp-main.786': 'Tokenisation is the first step in almost all NLP tasks, and state-of-the-art transformer-based language models all use subword tokenisation algorithms to process input text. Existing algorithms have problems, often producing tokenisations of limited linguistic validity and representing equivalent strings differently depending on their position within a word. We hypothesise that these problems hinder the ability of transformer-based models to handle complex words, and suggest that these problems are a result of allowing tokens to include spaces. We thus experiment with an alternative tokenisation approach where spaces are always treated as individual tokens. Specifically, we apply this modification to the BPE and Unigram algorithms. We find that our modified algorithms lead to improved performance on downstream NLP tasks that involve handling complex words, whilst having no detrimental effect on performance in general natural language understanding tasks. Intrinsically, we find that our modified algorithms give more morphologically correct tokenisations, in particular when handling prefixes. Given the results of our experiments, we advocate for always treating spaces as individual tokens as an improved tokenisation method.'\n",
    "}\n",
    "\n",
    "for doi, abstract in doi_to_abstract.items():\n",
    "    print(doi)\n",
    "    assert len(processed_df[processed_df['doi'] == doi]) == 1\n",
    "    processed_df.loc[processed_df['doi'] == doi, 'abstract'] = abstract\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56fe48-62f2-464b-b0de-c4c5389e060e",
   "metadata": {},
   "source": [
    "# Computing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3775373c-71d7-4c8b-90c5-3268ec959a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3dfe5fa72b45db96d9bb8077a2c044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9282/9282 [14:28<00:00, 10.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from adapters import AutoAdapterModel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter2_base')\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"allenai/specter2_base\")\n",
    "adapter_name = model.load_adapter(\"allenai/specter2_classification\", source=\"hf\", set_active=True)\n",
    "\n",
    "def get_embedding(paper_row):\n",
    "    text = paper_row['title'] + tokenizer.sep_token + paper_row['abstract']\n",
    "    inputs = tokenizer(text,\n",
    "                       padding=True,\n",
    "                       truncation=True,\n",
    "                       return_tensors=\"pt\",\n",
    "                       return_token_type_ids=False,\n",
    "                       max_length=2048)\n",
    "    output = model(**inputs)\n",
    "    embeddings = output.last_hidden_state[:, 0, :][0].detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "processed_df['embedding'] = processed_df.progress_apply(get_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb96294a-6ffd-4f6a-80db-ed01dddcfbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>interpretability</th>\n",
       "      <th>doi</th>\n",
       "      <th>source</th>\n",
       "      <th>working_doi</th>\n",
       "      <th>abstract</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.8</td>\n",
       "      <td>Large Scale Multi-Actor Generative Dialog Mode...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.8</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Non-goal oriented dialog agents (i.e. chatbots...</td>\n",
       "      <td>[-0.55811894, -0.12536883, -0.06339799, -1.815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.52</td>\n",
       "      <td>CDL: Curriculum Dual Learning for Emotion-Cont...</td>\n",
       "      <td>Dialogue and Interactive Systems</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.52</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Emotion-controllable response generation is an...</td>\n",
       "      <td>[-1.1278496, -0.5229794, 0.0056311972, -1.3223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.46</td>\n",
       "      <td>Emergence of Syntax Needs Minimal Supervision</td>\n",
       "      <td>Theory and Formalism in NLP (Linguistic and Ma...</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.46</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>This paper is a theoretical contribution to th...</td>\n",
       "      <td>[0.26176804, 0.8106163, 0.27426642, -1.174295,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.359</td>\n",
       "      <td>Selecting Backtranslated Data from Multiple So...</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.359</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>Machine translation (MT) has benefited from us...</td>\n",
       "      <td>[-0.43927717, 1.0674063, 0.08589529, -0.437168...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.417</td>\n",
       "      <td>ParaCrawl: Web-Scale Acquisition of Parallel C...</td>\n",
       "      <td>Resources and Evaluation</td>\n",
       "      <td>False</td>\n",
       "      <td>10.18653/v1/2020.acl-main.417</td>\n",
       "      <td>ACL2020</td>\n",
       "      <td>True</td>\n",
       "      <td>We report on methods to create the largest pub...</td>\n",
       "      <td>[-0.3593886, 0.33652788, -0.026537634, -0.7595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9277</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1656</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Human language is often multimodal, which comp...</td>\n",
       "      <td>[-0.25314885, -0.08230631, -0.3357741, -1.7636...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9278</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1657</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>Chest X-Ray (CXR) images are commonly used for...</td>\n",
       "      <td>[-0.488766, -0.30718938, -1.3068513, -0.397652...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1658</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>We introduce the first dataset for human edits...</td>\n",
       "      <td>[-0.63484645, -0.21893704, 0.09538727, -0.8349...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1659</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this paper, we study abstractive summarizat...</td>\n",
       "      <td>[-0.9391148, -0.115553305, -0.06912733, -1.159...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.18653/v1/P19-1660</td>\n",
       "      <td>ACL2019</td>\n",
       "      <td>True</td>\n",
       "      <td>In this work, we propose a novel approach that...</td>\n",
       "      <td>[-0.4750144, 0.19817561, -0.35619363, -0.61330...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9282 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "0       main.8  Large Scale Multi-Actor Generative Dialog Mode...   \n",
       "1      main.52  CDL: Curriculum Dual Learning for Emotion-Cont...   \n",
       "2      main.46      Emergence of Syntax Needs Minimal Supervision   \n",
       "3     main.359  Selecting Backtranslated Data from Multiple So...   \n",
       "4     main.417  ParaCrawl: Web-Scale Acquisition of Parallel C...   \n",
       "...        ...                                                ...   \n",
       "9277       889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9278      2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9279       384                          Visual Story Post-Editing   \n",
       "9280      1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9281      2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area interpretability  \\\n",
       "0                      Dialogue and Interactive Systems            False   \n",
       "1                      Dialogue and Interactive Systems            False   \n",
       "2     Theory and Formalism in NLP (Linguistic and Ma...            False   \n",
       "3                                   Machine Translation            False   \n",
       "4                              Resources and Evaluation            False   \n",
       "...                                                 ...              ...   \n",
       "9277  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9278  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9279  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9280  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "9281  Vision, Robotics, Multimodal, Grounding and Sp...              NaN   \n",
       "\n",
       "                                doi   source  working_doi  \\\n",
       "0       10.18653/v1/2020.acl-main.8  ACL2020         True   \n",
       "1      10.18653/v1/2020.acl-main.52  ACL2020         True   \n",
       "2      10.18653/v1/2020.acl-main.46  ACL2020         True   \n",
       "3     10.18653/v1/2020.acl-main.359  ACL2020         True   \n",
       "4     10.18653/v1/2020.acl-main.417  ACL2020         True   \n",
       "...                             ...      ...          ...   \n",
       "9277           10.18653/v1/P19-1656  ACL2019         True   \n",
       "9278           10.18653/v1/P19-1657  ACL2019         True   \n",
       "9279           10.18653/v1/P19-1658  ACL2019         True   \n",
       "9280           10.18653/v1/P19-1659  ACL2019         True   \n",
       "9281           10.18653/v1/P19-1660  ACL2019         True   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Non-goal oriented dialog agents (i.e. chatbots...   \n",
       "1     Emotion-controllable response generation is an...   \n",
       "2     This paper is a theoretical contribution to th...   \n",
       "3     Machine translation (MT) has benefited from us...   \n",
       "4     We report on methods to create the largest pub...   \n",
       "...                                                 ...   \n",
       "9277  Human language is often multimodal, which comp...   \n",
       "9278  Chest X-Ray (CXR) images are commonly used for...   \n",
       "9279  We introduce the first dataset for human edits...   \n",
       "9280  In this paper, we study abstractive summarizat...   \n",
       "9281  In this work, we propose a novel approach that...   \n",
       "\n",
       "                                              embedding  \n",
       "0     [-0.55811894, -0.12536883, -0.06339799, -1.815...  \n",
       "1     [-1.1278496, -0.5229794, 0.0056311972, -1.3223...  \n",
       "2     [0.26176804, 0.8106163, 0.27426642, -1.174295,...  \n",
       "3     [-0.43927717, 1.0674063, 0.08589529, -0.437168...  \n",
       "4     [-0.3593886, 0.33652788, -0.026537634, -0.7595...  \n",
       "...                                                 ...  \n",
       "9277  [-0.25314885, -0.08230631, -0.3357741, -1.7636...  \n",
       "9278  [-0.488766, -0.30718938, -1.3068513, -0.397652...  \n",
       "9279  [-0.63484645, -0.21893704, 0.09538727, -0.8349...  \n",
       "9280  [-0.9391148, -0.115553305, -0.06912733, -1.159...  \n",
       "9281  [-0.4750144, 0.19817561, -0.35619363, -0.61330...  \n",
       "\n",
       "[9282 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd1c114c-5560-411a-b87a-c1d346dff6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.to_csv('../data/new_classifier_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
