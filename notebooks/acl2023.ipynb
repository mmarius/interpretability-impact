{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85926829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from utils import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc07bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'events': {'coffee-break-1': {'chairs': [],\n",
       "   'end_time': '2023-07-09T15:00:00+00:00',\n",
       "   'id': 'coffee-break-1',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 1',\n",
       "   'start_time': '2023-07-09T14:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-10': {'chairs': [],\n",
       "   'end_time': '2023-07-12T15:00:00+00:00',\n",
       "   'id': 'coffee-break-10',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 10',\n",
       "   'start_time': '2023-07-12T14:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-11': {'chairs': [],\n",
       "   'end_time': '2023-07-12T18:00:00+00:00',\n",
       "   'id': 'coffee-break-11',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 11',\n",
       "   'start_time': '2023-07-12T16:30:00+00:00',\n",
       "   'track': 'Lunch Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-12': {'chairs': [],\n",
       "   'end_time': '2023-07-12T19:30:00+00:00',\n",
       "   'id': 'coffee-break-12',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 12',\n",
       "   'start_time': '2023-07-12T19:00:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-2': {'chairs': [],\n",
       "   'end_time': '2023-07-09T18:00:00+00:00',\n",
       "   'id': 'coffee-break-2',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 2',\n",
       "   'start_time': '2023-07-09T16:30:00+00:00',\n",
       "   'track': 'Lunch Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-3': {'chairs': [],\n",
       "   'end_time': '2023-07-09T20:00:00+00:00',\n",
       "   'id': 'coffee-break-3',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 3',\n",
       "   'start_time': '2023-07-09T19:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-4': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:00:00+00:00',\n",
       "   'id': 'coffee-break-4',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 4',\n",
       "   'start_time': '2023-07-10T14:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-5': {'chairs': [],\n",
       "   'end_time': '2023-07-10T18:00:00+00:00',\n",
       "   'id': 'coffee-break-5',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 5',\n",
       "   'start_time': '2023-07-10T16:30:00+00:00',\n",
       "   'track': 'Lunch Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-6': {'chairs': [],\n",
       "   'end_time': '2023-07-10T20:00:00+00:00',\n",
       "   'id': 'coffee-break-6',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 6',\n",
       "   'start_time': '2023-07-10T19:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-7': {'chairs': [],\n",
       "   'end_time': '2023-07-11T15:00:00+00:00',\n",
       "   'id': 'coffee-break-7',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 7',\n",
       "   'start_time': '2023-07-11T14:30:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-8': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:00:00+00:00',\n",
       "   'id': 'coffee-break-8',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 8',\n",
       "   'start_time': '2023-07-11T16:30:00+00:00',\n",
       "   'track': 'Lunch Break',\n",
       "   'type': 'Breaks'},\n",
       "  'coffee-break-9': {'chairs': [],\n",
       "   'end_time': '2023-07-11T20:15:00+00:00',\n",
       "   'id': 'coffee-break-9',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'Coffee Break 9',\n",
       "   'start_time': '2023-07-11T19:45:00+00:00',\n",
       "   'track': 'Coffee Break',\n",
       "   'type': 'Breaks'},\n",
       "  'demo-session-1_-generation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'demo-session-1_-generation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D44', 'D63', 'D103', 'D95'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Generation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-1_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'demo-session-1_-large-language-models-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D74'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-1_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'demo-session-1_-speech-and-multimodality-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D96'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-2_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'demo-session-2_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D107'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-2_-generation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'demo-session-2_-generation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D156'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Generation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-2_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'demo-session-2_-information-extraction-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D91', 'D106'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-2_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'demo-session-2_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D133'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-2_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'demo-session-2_-speech-and-multimodality-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D85'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D39'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-information-extraction-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D89'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-linguistic-diversity-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-linguistic-diversity-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D24'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-machine-translation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-machine-translation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D69'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-nlp-applications-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-nlp-applications-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D78', 'D113', 'D134', 'D123'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-resources-and-evaluation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D148'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-3_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'demo-session-3_-speech-and-multimodality-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D27'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-4_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'demo-session-4_-information-extraction-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D94'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-4_-machine-learning-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'demo-session-4_-machine-learning-for-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D130'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-4_-machine-translation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'demo-session-4_-machine-translation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D45', 'D53'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D66', 'D93'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D56', 'D104', 'D110'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-5_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'demo-session-5_-large-language-models-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D77', 'D80'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-5_-question-answering-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'demo-session-5_-question-answering-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D47'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-5_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'demo-session-5_-resources-and-evaluation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D90'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-6_-generation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'demo-session-6_-generation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D31'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Generation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-6_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'demo-session-6_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D71'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-6_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'demo-session-6_-large-language-models-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D141'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-6_-question-answering-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'demo-session-6_-question-answering-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D19', 'D26', 'D144'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-6_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'demo-session-6_-resources-and-evaluation-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D84'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-7_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'demo-session-7_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D49'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-7_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'demo-session-7_-large-language-models-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D18'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'demo-session-7_-machine-learning-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'demo-session-7_-machine-learning-for-nlp-(demo)-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D147'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Demo Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'type': 'Poster'},\n",
       "  'plenary_-acl-lifetime----tot': {'chairs': [],\n",
       "   'end_time': '2023-07-12T21:00:00+00:00',\n",
       "   'id': 'plenary_-acl-lifetime----tot',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'plenary_-acl-lifetime----tot',\n",
       "   'start_time': '2023-07-12T19:30:00+00:00',\n",
       "   'track': 'Plenary: ACL Lifetime -- ToT',\n",
       "   'type': 'Plenary Sessions'},\n",
       "  'plenary_-best-paper-awards': {'chairs': [],\n",
       "   'end_time': '2023-07-10T21:30:00+00:00',\n",
       "   'id': 'plenary_-best-paper-awards',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'plenary_-best-paper-awards',\n",
       "   'start_time': '2023-07-10T20:00:00+00:00',\n",
       "   'track': 'Plenary: Best Paper Awards',\n",
       "   'type': 'Plenary Sessions'},\n",
       "  'plenary_-business-meeting': {'chairs': [],\n",
       "   'end_time': '2023-07-11T18:10:00+00:00',\n",
       "   'id': 'plenary_-business-meeting',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'plenary_-business-meeting',\n",
       "   'start_time': '2023-07-11T17:30:00+00:00',\n",
       "   'track': 'Plenary: Business Meeting',\n",
       "   'type': 'Plenary Sessions'},\n",
       "  'plenary_-closing-session': {'chairs': [],\n",
       "   'end_time': '2023-07-12T21:30:00+00:00',\n",
       "   'id': 'plenary_-closing-session',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'plenary_-closing-session',\n",
       "   'start_time': '2023-07-12T21:00:00+00:00',\n",
       "   'track': 'Plenary: Closing Session',\n",
       "   'type': 'Plenary Sessions'},\n",
       "  'poster-session-1_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3334', 'P5806'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1323', 'P1466', 'P3638', 'P3852'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4396', 'P4729'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3117', 'P4865', 'P2606'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P526',\n",
       "    'P609',\n",
       "    'P1309',\n",
       "    'P3032',\n",
       "    'P3951',\n",
       "    'P4196',\n",
       "    'P4406',\n",
       "    'P5707',\n",
       "    'P5824',\n",
       "    'P1476'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1160'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P676', 'P2441', 'P2511'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P819', 'P3977'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P294', 'P663', 'P4258', 'P4936', 'P4541'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P391'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P211', 'P1370', 'P5597', 'P5748', 'P5796', 'P5857'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P118', 'P1785', 'P3809'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P283', 'P713', 'P1368', 'P2339', 'P3329'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2379', 'P3672'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P57', 'P4357'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2126', 'P3357', 'P3524', 'P4577', 'P5661', 'P5272'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4058', 'P5684', 'P3782'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1101', 'P1695', 'P4522', 'P4858', 'P5640'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4335'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2160'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1133',\n",
       "    'P2068',\n",
       "    'P2164',\n",
       "    'P2201',\n",
       "    'P4026',\n",
       "    'P4521',\n",
       "    'P4745',\n",
       "    'P5840'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4202'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-1_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'poster-session-1_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P282', 'P692', 'P2659', 'P3849', 'P4296'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P959', 'P2226', 'P5673', 'P3441'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2205', 'P4484', 'P2056'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1286', 'P3763'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2283', 'P4056'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1969', 'P4838', 'P3087'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P759', 'P821', 'P2621', 'P4553'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4020', 'P4234', 'P5812'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P286', 'P2353', 'P3065'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P810',\n",
       "    'P1329',\n",
       "    'P1980',\n",
       "    'P2350',\n",
       "    'P2358',\n",
       "    'P2699',\n",
       "    'P3786',\n",
       "    'P4233',\n",
       "    'P4238',\n",
       "    'P5312',\n",
       "    'P5859'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P319', 'P5646'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P181', 'P599', 'P2448', 'P3081', 'P4446'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P505', 'P2985', 'P3174', 'P3241', 'P4471'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1542', 'P2291'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P530',\n",
       "    'P2701',\n",
       "    'P3061',\n",
       "    'P3116',\n",
       "    'P4090',\n",
       "    'P5099',\n",
       "    'P4462',\n",
       "    'P5713'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3413', 'P4089'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2684',\n",
       "    'P2823',\n",
       "    'P3506',\n",
       "    'P4209',\n",
       "    'P5057',\n",
       "    'P5643',\n",
       "    'P5662',\n",
       "    'P5775',\n",
       "    'P1477'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P35',\n",
       "    'P322',\n",
       "    'P1170',\n",
       "    'P1242',\n",
       "    'P2284',\n",
       "    'P3709',\n",
       "    'P4115',\n",
       "    'P4826',\n",
       "    'P5599'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5728'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P835', 'P4495'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P203'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P341'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1516', 'P1820', 'P3367', 'P4192', 'P5650'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4350'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-2_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'poster-session-2_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1328'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2158', 'P2842', 'P3103', 'P3530'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P267', 'P2942'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3155'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3093',\n",
       "    'P3198',\n",
       "    'P3228',\n",
       "    'P3656',\n",
       "    'P3960',\n",
       "    'P4614',\n",
       "    'P4752',\n",
       "    'P4870'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P254', 'P3380', 'P3802', 'P3903', 'P4317', 'P4980', 'P5869'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4334'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P230', 'P1239', 'P1716', 'P2356', 'P3888', 'P4878'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P249', 'P1139'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P384', 'P1072', 'P2531'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P680', 'P1316', 'P1679', 'P1817', 'P3724', 'P5642', 'P5701'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P327', 'P1680'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P823', 'P1719'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1292', 'P2061', 'P2200', 'P2273'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3382'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2093', 'P2119', 'P2863', 'P4424', 'P4501', 'P5833'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P379',\n",
       "    'P550',\n",
       "    'P614',\n",
       "    'P860',\n",
       "    'P1696',\n",
       "    'P1808',\n",
       "    'P3205',\n",
       "    'P4565',\n",
       "    'P5447',\n",
       "    'P5620',\n",
       "    'P5648',\n",
       "    'P5721'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3828'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1518'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P867', 'P2713', 'P2776'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'poster-session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4198'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4463', 'P5766'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1033',\n",
       "    'P2469',\n",
       "    'P2852',\n",
       "    'P3571',\n",
       "    'P4382',\n",
       "    'P5293',\n",
       "    'P2091'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5572'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P942', 'P2534', 'P4582', 'P5583'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P442', 'P4155', 'P4178', 'P4416', 'P5565'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P64', 'P115', 'P496'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4687', 'P5635'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P352', 'P3729'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1345', 'P3973'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P915', 'P1821', 'P2449', 'P2962', 'P3309', 'P4513'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3074', 'P5578', 'P5826'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P138', 'P4159', 'P4254', 'P4263', 'P4415', 'P5561'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2663', 'P4532'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2166', 'P5238'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P63',\n",
       "    'P339',\n",
       "    'P1099',\n",
       "    'P2401',\n",
       "    'P2521',\n",
       "    'P3273',\n",
       "    'P3487',\n",
       "    'P4326'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2307', 'P5743'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P521',\n",
       "    'P536',\n",
       "    'P1569',\n",
       "    'P2079',\n",
       "    'P3085',\n",
       "    'P3495',\n",
       "    'P3963',\n",
       "    'P4593',\n",
       "    'P5632',\n",
       "    'P5638',\n",
       "    'P5850'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2261', 'P4308'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3422', 'P5576'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4505'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P76', 'P2774', 'P4280'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3736'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-4_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'poster-session-4_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2033',\n",
       "    'P2589',\n",
       "    'P3203',\n",
       "    'P3608',\n",
       "    'P4101',\n",
       "    'P5151',\n",
       "    'P5797'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3769', 'P5854', 'P4260'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P766', 'P1183', 'P4289'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4136'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1276', 'P2633', 'P3250', 'P4226', 'P3167'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2412', 'P3911'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2012', 'P4792'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5682'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P467', 'P4018', 'P4694', 'P5724'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2859', 'P5106'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P522', 'P2901', 'P3810', 'P4662', 'P5637', 'P5760', 'P5855'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P451', 'P2970', 'P3518'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2616', 'P3909', 'P5651'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P576'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P468', 'P5832', 'P1886', 'P718'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3958'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-5_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'poster-session-5_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2910'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1555', 'P3607'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1340',\n",
       "    'P3930',\n",
       "    'P4005',\n",
       "    'P4419',\n",
       "    'P4803',\n",
       "    'P5234',\n",
       "    'P5623',\n",
       "    'P5639',\n",
       "    'P1684'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5730'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P679', 'P2209'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P392', 'P2649', 'P3191', 'P3704', 'P4481', 'P4558'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5692'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2410', 'P3959'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P600', 'P3109', 'P4162', 'P4549', 'P4814', 'P4889'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P91', 'P1207', 'P1448', 'P3426', 'P4559'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-linguistic-diversity-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-linguistic-diversity-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4821'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1867',\n",
       "    'P2122',\n",
       "    'P2129',\n",
       "    'P2755',\n",
       "    'P3129',\n",
       "    'P4482',\n",
       "    'P5688',\n",
       "    'P5816'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1150', 'P1706', 'P3135', 'P3784'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1737', 'P4835'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P404',\n",
       "    'P540',\n",
       "    'P683',\n",
       "    'P2140',\n",
       "    'P2530',\n",
       "    'P4409',\n",
       "    'P4570',\n",
       "    'P5580'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2525'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1455', 'P3302'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1205', 'P1381', 'P3799', 'P4581', 'P4891', 'P2991'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4106'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P965', 'P2740', 'P3701', 'P3962'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1132'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1275', 'P2666', 'P5668'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3992', 'P5772', 'P3699'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-6_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'poster-session-6_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3693'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4282', 'P5740', 'P5755', 'P5770', 'P5699'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2689', 'P2928'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P206'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-ethics-and-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P197', 'P2153', 'P2305', 'P4680'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P284', 'P312', 'P4157'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P824', 'P2841', 'P3976', 'P4215', 'P5767'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3549'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P790', 'P927', 'P2058', 'P3760', 'P5848', 'P4975'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1254', 'P4213', 'P544'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-large-language-models-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-large-language-models-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P292', 'P1087', 'P1302', 'P3998', 'P4529', 'P4763', 'P5676'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P957', 'P2296', 'P3324', 'P3627', 'P5706', 'P5742', 'P255'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1349', 'P1572', 'P1806', 'P3069', 'P4250', 'P1200'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1778', 'P5297', 'P5686'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-nlp-applications-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-nlp-applications-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P51',\n",
       "    'P394',\n",
       "    'P605',\n",
       "    'P955',\n",
       "    'P1388',\n",
       "    'P1479',\n",
       "    'P1917',\n",
       "    'P2330',\n",
       "    'P3759',\n",
       "    'P4007',\n",
       "    'P5677',\n",
       "    'P5771',\n",
       "    'P3539',\n",
       "    'P1012'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-question-answering-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-question-answering-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2364', 'P4105', 'P3537', 'P2550'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-resources-and-evaluation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1624', 'P2717', 'P3490', 'P1879', 'P626'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1424', 'P1831', 'P2152'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P651', 'P909'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-speech-and-multimodality-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3721', 'P5600'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1162', 'P2232'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3504', 'P5566'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'poster-session-7_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'poster-session-7_-theme_-reality-check-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3730'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Poster Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2325',\n",
       "    'P2462',\n",
       "    'P4252',\n",
       "    'P5837',\n",
       "    'P3403',\n",
       "    'P1897',\n",
       "    'P4456',\n",
       "    'P2343',\n",
       "    'P3579'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4075', 'T4263'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P355',\n",
       "    'P382',\n",
       "    'P705',\n",
       "    'P773',\n",
       "    'P809',\n",
       "    'P953',\n",
       "    'P1212',\n",
       "    'P1605',\n",
       "    'P1667',\n",
       "    'P1744',\n",
       "    'P2466',\n",
       "    'P2767',\n",
       "    'P2796',\n",
       "    'P3058',\n",
       "    'P3165',\n",
       "    'P3190',\n",
       "    'P3473',\n",
       "    'P4585',\n",
       "    'P4706',\n",
       "    'P4730',\n",
       "    'P5246',\n",
       "    'P5489',\n",
       "    'P3649',\n",
       "    'P1223',\n",
       "    'P2766'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-discourse-and-pragmatics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3308', 'P4069', 'P4086', 'P4295', 'P4772', 'P2383'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-ethics-and-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-ethics-and-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3955', 'P728', 'P2421', 'P3322', 'P4517', 'P5626'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-1_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-ethics-and-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2039',\n",
       "    'P2145',\n",
       "    'P2270',\n",
       "    'P2934',\n",
       "    'P3969',\n",
       "    'P4017',\n",
       "    'P4032'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-generation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-generation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4503'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-generation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-generation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4601',\n",
       "    'P233',\n",
       "    'P1018',\n",
       "    'P1149',\n",
       "    'P1482',\n",
       "    'P2181',\n",
       "    'P2274',\n",
       "    'P2547',\n",
       "    'P3325',\n",
       "    'P3993',\n",
       "    'P5166',\n",
       "    'P5792',\n",
       "    'P1495',\n",
       "    'P4348',\n",
       "    'P720'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-information-extraction-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P176',\n",
       "    'P501',\n",
       "    'P568',\n",
       "    'P799',\n",
       "    'P1178',\n",
       "    'P1541',\n",
       "    'P1780',\n",
       "    'P2824',\n",
       "    'P3243',\n",
       "    'P4049',\n",
       "    'P4210',\n",
       "    'P4490',\n",
       "    'P4915',\n",
       "    'P2730',\n",
       "    'P4451',\n",
       "    'P2489',\n",
       "    'P3138'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P784', 'P986', 'P1857', 'P2125', 'P1520', 'P3066'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P399',\n",
       "    'P1322',\n",
       "    'P1636',\n",
       "    'P1786',\n",
       "    'P3035',\n",
       "    'P3410',\n",
       "    'P3636',\n",
       "    'P3860',\n",
       "    'P3907',\n",
       "    'P4811',\n",
       "    'P4867',\n",
       "    'P5058',\n",
       "    'P5090',\n",
       "    'P5289',\n",
       "    'P5577',\n",
       "    'P664',\n",
       "    'P2562',\n",
       "    'P5660',\n",
       "    'P4051',\n",
       "    'P3563'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P527',\n",
       "    'P608',\n",
       "    'P726',\n",
       "    'P907',\n",
       "    'P2484',\n",
       "    'P2492',\n",
       "    'P2718',\n",
       "    'P2907',\n",
       "    'P5335',\n",
       "    'P4158',\n",
       "    'P2533',\n",
       "    'P345',\n",
       "    'P1977',\n",
       "    'P3948'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-language-grounding-to-vision,-robotics-and-beyond-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-language-grounding-to-vision,-robotics-and-beyond-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D17'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics and Beyond (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-large-language-models-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-large-language-models-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3572', 'P5693', 'P4503', 'P547', 'P4286', 'P2222'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Oral'},\n",
       "  'session-1_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-large-language-models-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P495',\n",
       "    'P584',\n",
       "    'P590',\n",
       "    'P987',\n",
       "    'P1071',\n",
       "    'P1084',\n",
       "    'P1243',\n",
       "    'P1959',\n",
       "    'P2197',\n",
       "    'P2204',\n",
       "    'P2957',\n",
       "    'P3883',\n",
       "    'P3908',\n",
       "    'P4013',\n",
       "    'P5316',\n",
       "    'P5582',\n",
       "    'P4641',\n",
       "    'P4242',\n",
       "    'P795',\n",
       "    'P1583',\n",
       "    'P1714'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4728', 'P5658'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-machine-learning-for-nlp-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-machine-learning-for-nlp-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D118'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4371', 'T4773'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3649',\n",
       "    'P844',\n",
       "    'P958',\n",
       "    'P1258',\n",
       "    'P1277',\n",
       "    'P1452',\n",
       "    'P2592',\n",
       "    'P2749',\n",
       "    'P3281',\n",
       "    'P3340',\n",
       "    'P4956',\n",
       "    'P5243',\n",
       "    'P5575',\n",
       "    'P5618',\n",
       "    'P5619',\n",
       "    'P5657',\n",
       "    'P5777',\n",
       "    'P2243',\n",
       "    'P2289',\n",
       "    'P3859',\n",
       "    'P1645',\n",
       "    'P1297',\n",
       "    'P5464'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-machine-translation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P807',\n",
       "    'P1059',\n",
       "    'P1449',\n",
       "    'P2893',\n",
       "    'P3017',\n",
       "    'P4124',\n",
       "    'P4584',\n",
       "    'P2059',\n",
       "    'P612',\n",
       "    'P5625',\n",
       "    'P2272',\n",
       "    'P3463'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-multilingualism-and-cross-lingual-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-multilingualism-and-cross-lingual-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3533', 'P5798', 'P3991', 'C2208', 'P693', 'P1915'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2746', 'P3047', 'P5095', 'P5822', 'P1078', 'P4065'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-nlp-applications-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-nlp-applications-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D9', 'D25'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-nlp-applications-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-nlp-applications-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2602', 'P2874', 'P3953', 'P3680', 'P2961', 'P5836'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Oral'},\n",
       "  'session-1_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-nlp-applications-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1180',\n",
       "    'P1411',\n",
       "    'P1890',\n",
       "    'P2460',\n",
       "    'P2722',\n",
       "    'P3486',\n",
       "    'P3765',\n",
       "    'P4268',\n",
       "    'P4557',\n",
       "    'P4718',\n",
       "    'P5048',\n",
       "    'P5723',\n",
       "    'P788',\n",
       "    'P5647',\n",
       "    'P44',\n",
       "    'P299',\n",
       "    'P5785',\n",
       "    'P604',\n",
       "    'P3326'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-question-answering-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-question-answering-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5482', 'P3216', 'P3014', 'P5722', 'P1022', 'P326'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Oral'},\n",
       "  'session-1_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-question-answering-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P186',\n",
       "    'P452',\n",
       "    'P985',\n",
       "    'P1916',\n",
       "    'P2630',\n",
       "    'P3353',\n",
       "    'P3698',\n",
       "    'P4435',\n",
       "    'P4681',\n",
       "    'P4827',\n",
       "    'P5097',\n",
       "    'P5790',\n",
       "    'P872'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-resources-and-evaluation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P707',\n",
       "    'P976',\n",
       "    'P1530',\n",
       "    'P2946',\n",
       "    'P3179',\n",
       "    'P3389',\n",
       "    'P3446',\n",
       "    'P3630',\n",
       "    'P4087',\n",
       "    'P4682',\n",
       "    'P5125',\n",
       "    'P5726',\n",
       "    'P3546',\n",
       "    'P2221'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-semantics_-lexical-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-semantics_-lexical-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P487', 'P1876', 'P2112', 'P1436'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T5043'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P116',\n",
       "    'P1168',\n",
       "    'P2240',\n",
       "    'P3633',\n",
       "    'P3847',\n",
       "    'P4899',\n",
       "    'P5112',\n",
       "    'P3705',\n",
       "    'P5579',\n",
       "    'P472',\n",
       "    'P5631',\n",
       "    'P5255',\n",
       "    'P549'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4059', 'T4405', 'T4407'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Poster'},\n",
       "  'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2538', 'P3011', 'P5573', 'P3342'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-speech-and-multimodality-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-speech-and-multimodality-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D55'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-speech-and-multimodality-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P548',\n",
       "    'P1465',\n",
       "    'P3304',\n",
       "    'P5193',\n",
       "    'P5601',\n",
       "    'P2691',\n",
       "    'P3005',\n",
       "    'P1464'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-student-research-workshop-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S2', 'S6', 'S14', 'S95', 'S133'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-summarization-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-summarization-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1772', 'P2582', 'P3313', 'P3664', 'P378'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4152', 'P5140', 'P5362', 'P4613', 'P3091'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-1_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'id': 'session-1_-theme_-reality-check-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P140',\n",
       "    'P1040',\n",
       "    'P2406',\n",
       "    'P2429',\n",
       "    'P2461',\n",
       "    'P2772',\n",
       "    'P3180',\n",
       "    'P4423',\n",
       "    'P2876'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 1',\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-2_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4401'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'session-2_-language-grounding-to-vision,-robotics,-and-beyond-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-language-grounding-to-vision,-robotics,-and-beyond-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P908', 'P2123', 'P5738', 'P786', 'P3738', 'P4186'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Oral'},\n",
       "  'session-2_-machine-learning-for-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-machine-learning-for-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2565', 'P2522', 'P2349', 'P2404', 'P5570', 'P295'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-2_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4497'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'session-2_-machine-translation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-machine-translation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1126', 'T4591', 'P4709', 'P3890', 'P1185', 'P2258'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Oral'},\n",
       "  'session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1080', 'P2247', 'P3576', 'P3327', 'P2768', 'C2265'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Oral'},\n",
       "  'session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5256', 'P1854', 'P3454', 'P989', 'C2281', 'P3614'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Oral'},\n",
       "  'session-2_-theme_-reality-check-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'id': 'session-2_-theme_-reality-check-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2506', 'P2131', 'P3843', 'P5113', 'P3227', 'P834'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 2',\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-computational-social-science-and-cultural-analytics-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:15:00-04:00',\n",
       "   'id': 'session-3_-computational-social-science-and-cultural-analytics-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P293', 'P2244', 'P4188', 'P4923', 'P5071'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-dialogue-and-interactive-systems-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-dialogue-and-interactive-systems-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1148', 'P481', 'P3945', 'P133', 'P703', 'P2587'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4803'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'session-3_-industry-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-industry-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['I16', 'I128', 'I131', 'I173', 'I81', 'I119'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Industry',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-interpretability-and-analysis-of-models-for-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-interpretability-and-analysis-of-models-for-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P895', 'P1245', 'P4055', 'P2622', 'P4340', 'P3999'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-large-language-models-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-large-language-models-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3938', 'P854', 'P868', 'P1627', 'P1924', 'P2336'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-linguistic-diversity-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-linguistic-diversity-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4208', 'P3832', 'P1365', 'P377', 'P4315', 'P921'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'type': 'Oral'},\n",
       "  'session-3_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4139'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Poster'},\n",
       "  'session-3_-student-research-workshop-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-student-research-workshop-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S24',\n",
       "    'S28',\n",
       "    'S29',\n",
       "    'S36',\n",
       "    'S48',\n",
       "    'S56',\n",
       "    'S58',\n",
       "    'S82',\n",
       "    'S113',\n",
       "    'S20'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Poster'},\n",
       "  'session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'id': 'session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4637'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 3',\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1653', 'P4130', 'P4135', 'P4500', 'P4968', 'P5024'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4113'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4647',\n",
       "    'P21',\n",
       "    'P1102',\n",
       "    'P1213',\n",
       "    'P1504',\n",
       "    'P1513',\n",
       "    'P2057',\n",
       "    'P2576',\n",
       "    'P2769',\n",
       "    'P2797',\n",
       "    'P2860',\n",
       "    'P4985',\n",
       "    'P5254',\n",
       "    'P5352',\n",
       "    'P5831',\n",
       "    'P5768',\n",
       "    'P324',\n",
       "    'P3126',\n",
       "    'P3788',\n",
       "    'P1306',\n",
       "    'P5856',\n",
       "    'P896'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-discourse-and-pragmatics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4589'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-discourse-and-pragmatics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3940', 'P4037', 'P4744', 'P4813', 'P886'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-ethics-and-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1674', 'P2024', 'P3390', 'P4914'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-generation-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-generation-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D32', 'D119'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Generation (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-generation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-generation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P432',\n",
       "    'P437',\n",
       "    'P1228',\n",
       "    'P1312',\n",
       "    'P1342',\n",
       "    'P1588',\n",
       "    'P3377',\n",
       "    'P3458',\n",
       "    'P5206',\n",
       "    'P5680',\n",
       "    'P424',\n",
       "    'P1810'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-industry-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-industry-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['I42',\n",
       "    'I148',\n",
       "    'I8',\n",
       "    'I30',\n",
       "    'I54',\n",
       "    'I60',\n",
       "    'I29',\n",
       "    'I13',\n",
       "    'I36',\n",
       "    'I46',\n",
       "    'I92',\n",
       "    'I107',\n",
       "    'I227',\n",
       "    'I15',\n",
       "    'I18',\n",
       "    'I109',\n",
       "    'I191',\n",
       "    'I43',\n",
       "    'I104',\n",
       "    'I120',\n",
       "    'I96',\n",
       "    'I141',\n",
       "    'I75',\n",
       "    'I83',\n",
       "    'I124',\n",
       "    'I78'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Industry',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-information-extraction-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-information-extraction-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3863'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-information-extraction-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P23',\n",
       "    'P471',\n",
       "    'P581',\n",
       "    'P598',\n",
       "    'P2422',\n",
       "    'P3024',\n",
       "    'P3147',\n",
       "    'P3430',\n",
       "    'P3541',\n",
       "    'P3780',\n",
       "    'P4182',\n",
       "    'P4519',\n",
       "    'P5017',\n",
       "    'P5805',\n",
       "    'P3497',\n",
       "    'P5589',\n",
       "    'P2826',\n",
       "    'P5791',\n",
       "    'P1550',\n",
       "    'P2480',\n",
       "    'P982',\n",
       "    'P2596',\n",
       "    'P3750'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4552', 'P1721'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4019'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P650',\n",
       "    'P1195',\n",
       "    'P1509',\n",
       "    'P2127',\n",
       "    'P2866',\n",
       "    'P3512',\n",
       "    'P5065',\n",
       "    'P5653',\n",
       "    'P5666',\n",
       "    'P5744',\n",
       "    'P1284',\n",
       "    'P5667',\n",
       "    'P5382',\n",
       "    'P2700',\n",
       "    'P3939'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T11:45:00-04:00',\n",
       "   'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3022', 'P3421', 'P5784'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Oral'},\n",
       "  'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4447'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P25',\n",
       "    'P236',\n",
       "    'P1067',\n",
       "    'P1575',\n",
       "    'P2391',\n",
       "    'P2715',\n",
       "    'P3040',\n",
       "    'P3659',\n",
       "    'P4140',\n",
       "    'P4466',\n",
       "    'P5758',\n",
       "    'P4689',\n",
       "    'P1593'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-large-language-models-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-large-language-models-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P367', 'P3183', 'P2262', 'P3259', 'P3052', 'P4993'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Oral'},\n",
       "  'session-4_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-large-language-models-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P273',\n",
       "    'P645',\n",
       "    'P1278',\n",
       "    'P1402',\n",
       "    'P2006',\n",
       "    'P2086',\n",
       "    'P2388',\n",
       "    'P2536',\n",
       "    'P3356',\n",
       "    'P3655',\n",
       "    'P4524',\n",
       "    'P3229',\n",
       "    'P1880',\n",
       "    'P1834',\n",
       "    'P415',\n",
       "    'P5141'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-linguistic-diversity-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-linguistic-diversity-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5830', 'P2333', 'P751'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2018'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-machine-learning-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P187',\n",
       "    'P258',\n",
       "    'P331',\n",
       "    'P348',\n",
       "    'P620',\n",
       "    'P813',\n",
       "    'P933',\n",
       "    'P1074',\n",
       "    'P1843',\n",
       "    'P1856',\n",
       "    'P1937',\n",
       "    'P2048',\n",
       "    'P2098',\n",
       "    'P2311',\n",
       "    'P2786',\n",
       "    'P2887',\n",
       "    'P3686',\n",
       "    'P3961',\n",
       "    'P4371',\n",
       "    'P4442',\n",
       "    'P5109',\n",
       "    'P5603',\n",
       "    'P5611',\n",
       "    'P3287'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-machine-translation-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-machine-translation-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D46'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-machine-translation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P832',\n",
       "    'P1348',\n",
       "    'P1829',\n",
       "    'P1934',\n",
       "    'P2848',\n",
       "    'P3043',\n",
       "    'P3152',\n",
       "    'P5811',\n",
       "    'P276',\n",
       "    'P1922',\n",
       "    'P3841',\n",
       "    'P157',\n",
       "    'P2439'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P245',\n",
       "    'P938',\n",
       "    'P1143',\n",
       "    'P1303',\n",
       "    'P2223',\n",
       "    'P2629',\n",
       "    'P2952',\n",
       "    'P4497',\n",
       "    'P5607',\n",
       "    'P2634'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-nlp-applications-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-nlp-applications-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D105'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-nlp-applications-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P998',\n",
       "    'P1500',\n",
       "    'P1623',\n",
       "    'P1752',\n",
       "    'P1798',\n",
       "    'P2178',\n",
       "    'P2664',\n",
       "    'P3114',\n",
       "    'P3242',\n",
       "    'P3388',\n",
       "    'P3944',\n",
       "    'P4277',\n",
       "    'P5041',\n",
       "    'P5815',\n",
       "    'P5818',\n",
       "    'P4746',\n",
       "    'P3625',\n",
       "    'P4408',\n",
       "    'P5188',\n",
       "    'P2884',\n",
       "    'P2868',\n",
       "    'P1697'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-question-answering-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-question-answering-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D132'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-question-answering-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4433',\n",
       "    'P1325',\n",
       "    'P1362',\n",
       "    'P1847',\n",
       "    'P1970',\n",
       "    'P2133',\n",
       "    'P2419',\n",
       "    'P3418',\n",
       "    'P3540',\n",
       "    'P4228',\n",
       "    'P4660',\n",
       "    'P5345',\n",
       "    'P5567',\n",
       "    'P5591',\n",
       "    'P5691',\n",
       "    'P1985',\n",
       "    'P3397',\n",
       "    'P3593'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-resources-and-evaluation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-resources-and-evaluation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P778', 'P1626', 'P2229', 'P2317', 'P2577', 'P2678'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Oral'},\n",
       "  'session-4_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-resources-and-evaluation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P300',\n",
       "    'P563',\n",
       "    'P1027',\n",
       "    'P3059',\n",
       "    'P3371',\n",
       "    'P3880',\n",
       "    'P3954',\n",
       "    'P4021',\n",
       "    'P4336',\n",
       "    'P5079',\n",
       "    'P3615'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4929'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P106',\n",
       "    'P1190',\n",
       "    'P1453',\n",
       "    'P1471',\n",
       "    'P1638',\n",
       "    'P1691',\n",
       "    'P1694',\n",
       "    'P2660',\n",
       "    'P2712',\n",
       "    'P2830',\n",
       "    'P5171',\n",
       "    'P1475',\n",
       "    'P5031',\n",
       "    'P2885',\n",
       "    'P353',\n",
       "    'P228'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-speech-and-multimodality-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P553', 'P2134', 'P3866', 'P3246'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-student-research-workshop-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:00:00-04:00',\n",
       "   'id': 'session-4_-student-research-workshop-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S52', 'S79', 'S123', 'S129'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Oral'},\n",
       "  'session-4_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-student-research-workshop-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S5', 'S70', 'S124', 'S92', 'S99', 'S144'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-summarization-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-summarization-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P328', 'P968', 'P970', 'P1728', 'P3833', 'T4769'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Oral'},\n",
       "  'session-4_-summarization-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-summarization-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P855',\n",
       "    'P2136',\n",
       "    'P2987',\n",
       "    'P4225',\n",
       "    'P4230',\n",
       "    'P4448',\n",
       "    'P4627',\n",
       "    'P85',\n",
       "    'P1191'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2993', 'P4008'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-4_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'id': 'session-4_-theme_-reality-check-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1236',\n",
       "    'P1291',\n",
       "    'P1391',\n",
       "    'P2637',\n",
       "    'P3188',\n",
       "    'P4619',\n",
       "    'P661',\n",
       "    'P3982'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 4',\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-5_-generation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-generation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['C2217', 'T4607', 'P2055', 'P2165', 'P2192', 'P3438'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Oral'},\n",
       "  'session-5_-industry-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-industry-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['I186',\n",
       "    'I27',\n",
       "    'I41',\n",
       "    'I77',\n",
       "    'I135',\n",
       "    'I45',\n",
       "    'I90',\n",
       "    'I134',\n",
       "    'I156',\n",
       "    'I94',\n",
       "    'I188',\n",
       "    'I69',\n",
       "    'I110',\n",
       "    'I201',\n",
       "    'I140',\n",
       "    'I55',\n",
       "    'I3',\n",
       "    'I17',\n",
       "    'I146',\n",
       "    'I187',\n",
       "    'I196',\n",
       "    'I205',\n",
       "    'I222',\n",
       "    'I14',\n",
       "    'I66',\n",
       "    'I125',\n",
       "    'I171',\n",
       "    'I208',\n",
       "    'I215',\n",
       "    'I93',\n",
       "    'I100',\n",
       "    'I102',\n",
       "    'I47',\n",
       "    'I65',\n",
       "    'I197',\n",
       "    'I213',\n",
       "    'I225',\n",
       "    'I32'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Industry',\n",
       "   'type': 'Poster'},\n",
       "  'session-5_-information-extraction-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-information-extraction-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4372', 'P4328', 'P2130', 'P803', 'P677', 'P4207'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Oral'},\n",
       "  'session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:15:00-04:00',\n",
       "   'id': 'session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3332', 'P4690', 'P178', 'P2373'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:30:00-04:00',\n",
       "   'id': 'session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4519', 'P3175', 'P4116', 'P4171', 'P2186'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Oral'},\n",
       "  'session-5_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4305'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'session-5_-semantics_-lexical-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-semantics_-lexical-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2114', 'P2116', 'P2023', 'P2367', 'P3876', 'P3791'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Oral'},\n",
       "  'session-5_-student-research-workshop-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'id': 'session-5_-student-research-workshop-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S3', 'S43', 'S46', 'S66', 'S71'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 5',\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-dialogue-and-interactive-systems-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3887'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-industry-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-industry-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['I189', 'I111', 'I207', 'I112', 'I139', 'I199'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Industry',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-information-retrieval-and-text-mining-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4285'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4777'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-machine-learning-for-nlp-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-machine-learning-for-nlp-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P624', 'P3267', 'P3374', 'P3542', 'P4769'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-machine-translation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-machine-translation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1433', 'P5204', 'P2132', 'P899', 'P3101'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:15:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-machine-translation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-machine-translation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['C2147'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-nlp-applications-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-nlp-applications-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3245', 'P619', 'P1265', 'P5846', 'P3924', 'P2036'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-phonology,-morphology,-and-word-segmentation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T09:45:00-04:00',\n",
       "   'id': 'session-6_-phonology,-morphology,-and-word-segmentation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4139', 'P2635', 'P2943'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4243', 'T4721'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-semantics_-lexical-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3791'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Poster'},\n",
       "  'session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T3895', 'T3967', 'P1940', 'P2953', 'P3197', 'C2092'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Oral'},\n",
       "  'session-6_-student-research-workshop-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'id': 'session-6_-student-research-workshop-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S32',\n",
       "    'S38',\n",
       "    'S40',\n",
       "    'S41',\n",
       "    'S47',\n",
       "    'S57',\n",
       "    'S64',\n",
       "    'S127',\n",
       "    'S139',\n",
       "    'S145'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 6',\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Poster'},\n",
       "  'session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P606',\n",
       "    'P1047',\n",
       "    'P1637',\n",
       "    'P3493',\n",
       "    'P3917',\n",
       "    'P4053',\n",
       "    'P4422',\n",
       "    'P5374',\n",
       "    'P1793',\n",
       "    'P5563',\n",
       "    'P3923',\n",
       "    'P5059'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P642',\n",
       "    'P658',\n",
       "    'P845',\n",
       "    'P1090',\n",
       "    'P1206',\n",
       "    'P2519',\n",
       "    'P2651',\n",
       "    'P3275',\n",
       "    'P3626',\n",
       "    'P3642',\n",
       "    'P5756',\n",
       "    'P5828',\n",
       "    'P2251',\n",
       "    'P5708',\n",
       "    'P3057',\n",
       "    'P427'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-discourse-and-pragmatics-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-discourse-and-pragmatics-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4419', 'T4197', 'P4079', 'P883', 'P3448', 'P792'],\n",
       "   'room': 'Pier 2&3',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-discourse-and-pragmatics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5261'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-ethics-and-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1333',\n",
       "    'P2298',\n",
       "    'P2465',\n",
       "    'P2703',\n",
       "    'P2788',\n",
       "    'P3687',\n",
       "    'P4844',\n",
       "    'P4937',\n",
       "    'P5741',\n",
       "    'P3629',\n",
       "    'P3150',\n",
       "    'P2238',\n",
       "    'P3983'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-generation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-generation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P302', 'P250', 'P5614'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:45:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-generation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-generation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P110',\n",
       "    'P456',\n",
       "    'P511',\n",
       "    'P558',\n",
       "    'P1355',\n",
       "    'P1538',\n",
       "    'P2297',\n",
       "    'P3082',\n",
       "    'P4494',\n",
       "    'P4950',\n",
       "    'P4974',\n",
       "    'P3794'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Generation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-information-extraction-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T11:45:00-04:00',\n",
       "   'id': 'session-7_-information-extraction-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P5043', 'P430', 'P1658'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-information-extraction-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P68',\n",
       "    'P411',\n",
       "    'P474',\n",
       "    'P585',\n",
       "    'P1595',\n",
       "    'P2067',\n",
       "    'P3447',\n",
       "    'P4181',\n",
       "    'P4691',\n",
       "    'P4824',\n",
       "    'P5026',\n",
       "    'P5669',\n",
       "    'P5809',\n",
       "    'P5694',\n",
       "    'P5168'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Extraction',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-information-retrieval-and-text-mining-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-information-retrieval-and-text-mining-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P971', 'P712', 'P4221', 'P1299', 'P4717', 'P923'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P406',\n",
       "    'P2105',\n",
       "    'P2269',\n",
       "    'P3153',\n",
       "    'P4509',\n",
       "    'P5745',\n",
       "    'P5574',\n",
       "    'P2923',\n",
       "    'P1580'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P426',\n",
       "    'P1715',\n",
       "    'P1996',\n",
       "    'P2320',\n",
       "    'P2396',\n",
       "    'P2411',\n",
       "    'P4012',\n",
       "    'P4074',\n",
       "    'P4220',\n",
       "    'P4470',\n",
       "    'P5617',\n",
       "    'P5627',\n",
       "    'P5749',\n",
       "    'P5793',\n",
       "    'P5841',\n",
       "    'P3878'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P514', 'P582', 'P991', 'P2437', 'P2512', 'P4820', 'P2529'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-large-language-models-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P518',\n",
       "    'P571',\n",
       "    'P1430',\n",
       "    'P2706',\n",
       "    'P2877',\n",
       "    'P3042',\n",
       "    'P3475',\n",
       "    'P3684',\n",
       "    'P4132',\n",
       "    'P4384',\n",
       "    'P4715',\n",
       "    'P4872',\n",
       "    'P5135',\n",
       "    'P5616',\n",
       "    'P5847',\n",
       "    'P4916',\n",
       "    'P4543',\n",
       "    'P3294'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Large Language Models',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-linguistic-diversity-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-linguistic-diversity-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D14', 'D30'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-linguistic-diversity-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-linguistic-diversity-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P4395'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P50', 'P657', 'P2021', 'P5698', 'P5739', 'P2891'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-machine-learning-for-nlp-(demo)-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-machine-learning-for-nlp-(demo)-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['D117'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-machine-learning-for-nlp-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4291'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Poster'},\n",
       "  'session-7_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-machine-learning-for-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['C2121',\n",
       "    'P129',\n",
       "    'P274',\n",
       "    'P486',\n",
       "    'P941',\n",
       "    'P1015',\n",
       "    'P1065',\n",
       "    'P1166',\n",
       "    'P1266',\n",
       "    'P1545',\n",
       "    'P1823',\n",
       "    'P1833',\n",
       "    'P1939',\n",
       "    'P2154',\n",
       "    'P2526',\n",
       "    'P2920',\n",
       "    'P5388',\n",
       "    'P5810',\n",
       "    'P1173',\n",
       "    'P1459',\n",
       "    'P882',\n",
       "    'P2609',\n",
       "    'P2080',\n",
       "    'P4536',\n",
       "    'P3457',\n",
       "    'P5624'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-machine-translation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P34',\n",
       "    'P304',\n",
       "    'P1463',\n",
       "    'P2071',\n",
       "    'P2873',\n",
       "    'P4782',\n",
       "    'P5655',\n",
       "    'P1676',\n",
       "    'P2728',\n",
       "    'P2835',\n",
       "    'P279',\n",
       "    'P4966',\n",
       "    'P4153'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Machine Translation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P194',\n",
       "    'P1936',\n",
       "    'P2106',\n",
       "    'P2288',\n",
       "    'P2331',\n",
       "    'P2416',\n",
       "    'P3121',\n",
       "    'P3596',\n",
       "    'P3667',\n",
       "    'P3845',\n",
       "    'P5357',\n",
       "    'P1855',\n",
       "    'P1075',\n",
       "    'P1315'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-nlp-applications-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P541',\n",
       "    'P589',\n",
       "    'P601',\n",
       "    'P1154',\n",
       "    'P1227',\n",
       "    'P1678',\n",
       "    'P2037',\n",
       "    'P2092',\n",
       "    'P2187',\n",
       "    'P2354',\n",
       "    'P2507',\n",
       "    'P3836',\n",
       "    'P4341',\n",
       "    'P4590',\n",
       "    'P5015',\n",
       "    'P5167',\n",
       "    'P5486',\n",
       "    'P5652',\n",
       "    'P420',\n",
       "    'P4674',\n",
       "    'P52'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'NLP Applications',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1621', 'P4724'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-question-answering-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P477',\n",
       "    'P747',\n",
       "    'P1056',\n",
       "    'P1398',\n",
       "    'P1925',\n",
       "    'P2046',\n",
       "    'P4704',\n",
       "    'P5150',\n",
       "    'P5683',\n",
       "    'P1069',\n",
       "    'P1634',\n",
       "    'P3072',\n",
       "    'P4722',\n",
       "    'P1952'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Question Answering',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-resources-and-evaluation-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-resources-and-evaluation-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3145', 'P3209', 'P3601', 'P4526', 'P4776', 'P4908'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-resources-and-evaluation-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P433',\n",
       "    'P596',\n",
       "    'P736',\n",
       "    'P2407',\n",
       "    'P2645',\n",
       "    'P3184',\n",
       "    'P3773',\n",
       "    'P3868',\n",
       "    'P4028',\n",
       "    'P4433',\n",
       "    'P4810',\n",
       "    'P5609',\n",
       "    'P5644',\n",
       "    'P5679',\n",
       "    'P3107',\n",
       "    'P1378',\n",
       "    'P4253',\n",
       "    'P789',\n",
       "    'P3824'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-semantics_-lexical-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-semantics_-lexical-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1662', 'P2095', 'P3970', 'P5670', 'P1142'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4253'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Poster'},\n",
       "  'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4523', 'P910', 'P5325', 'P4387'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P701', 'P1670', 'P1816', 'P739'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-speech-and-multimodality-(oral)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-speech-and-multimodality-(oral)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3291', 'P652', 'P5725', 'P2971', 'P532', 'P3391'],\n",
       "   'room': 'Pier 4&5',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Oral'},\n",
       "  'session-7_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-speech-and-multimodality-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P555', 'P1270', 'P2393', 'P3412', 'P4679', 'P5404'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-student-research-workshop-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-student-research-workshop-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S12', 'S21', 'S25', 'S34', 'S50', 'S98', 'S122'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Poster'},\n",
       "  'session-7_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-student-research-workshop-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['S19', 'S72', 'S94'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Student Research Workshop',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-summarization-(poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-summarization-(poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['T4475'],\n",
       "   'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Poster'},\n",
       "  'session-7_-summarization-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-summarization-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2321',\n",
       "    'P2992',\n",
       "    'P3234',\n",
       "    'P3774',\n",
       "    'P4634',\n",
       "    'P5596',\n",
       "    'P2433'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Summarization',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P2423', 'P3985', 'P202', 'P1498', 'P3482'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'session-7_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'id': 'session-7_-theme_-reality-check-(virtual-poster)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P3225',\n",
       "    'P3472',\n",
       "    'P3557',\n",
       "    'P3926',\n",
       "    'P4064',\n",
       "    'P4544',\n",
       "    'P4675',\n",
       "    'P5383',\n",
       "    'P5397',\n",
       "    'P3734',\n",
       "    'P395'],\n",
       "   'room': 'Pier 7&8',\n",
       "   'session': 'Session 7',\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'type': 'Virtual Poster'},\n",
       "  'spotlight-session_-spotlight---metropolitan-centre-(spotlight)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T21:00:00-04:00',\n",
       "   'id': 'spotlight-session_-spotlight---metropolitan-centre-(spotlight)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1065',\n",
       "    'P1258',\n",
       "    'P1277',\n",
       "    'P1545',\n",
       "    'P187',\n",
       "    'P2048',\n",
       "    'P2243',\n",
       "    'P2289',\n",
       "    'P2311',\n",
       "    'P2526',\n",
       "    'P274',\n",
       "    'P2749',\n",
       "    'P2786',\n",
       "    'P2887',\n",
       "    'P331',\n",
       "    'P3340',\n",
       "    'P3961',\n",
       "    'P4371',\n",
       "    'P4956',\n",
       "    'P941',\n",
       "    'P1084',\n",
       "    'P1278',\n",
       "    'P1402',\n",
       "    'P1801',\n",
       "    'P1959',\n",
       "    'P2197',\n",
       "    'P2204',\n",
       "    'P2706',\n",
       "    'P2957',\n",
       "    'P3356',\n",
       "    'P3655',\n",
       "    'P3796',\n",
       "    'P4132',\n",
       "    'P4384',\n",
       "    'P4524',\n",
       "    'P4543',\n",
       "    'P5135',\n",
       "    'P571',\n",
       "    'P590',\n",
       "    'P1018',\n",
       "    'P1312',\n",
       "    'P1342',\n",
       "    'P1355',\n",
       "    'P1538',\n",
       "    'P2274',\n",
       "    'P2297',\n",
       "    'P3082',\n",
       "    'P3794',\n",
       "    'P3993',\n",
       "    'P1772',\n",
       "    'P2321',\n",
       "    'P3234',\n",
       "    'P4225',\n",
       "    'P4230',\n",
       "    'P4448',\n",
       "    'P4627',\n",
       "    'P4634',\n",
       "    'P1348',\n",
       "    'P1676',\n",
       "    'P1934',\n",
       "    'P2893',\n",
       "    'P3017',\n",
       "    'P4124',\n",
       "    'P5811',\n",
       "    'P23',\n",
       "    'P2422',\n",
       "    'P2824',\n",
       "    'P3447',\n",
       "    'P3780',\n",
       "    'P4519',\n",
       "    'P471',\n",
       "    'P5809',\n",
       "    'P1067',\n",
       "    'P2437',\n",
       "    'P25',\n",
       "    'P2718',\n",
       "    'P2907',\n",
       "    'P3659',\n",
       "    'P3948',\n",
       "    'P4158',\n",
       "    'P4466',\n",
       "    'P726',\n",
       "    'P1270',\n",
       "    'P2393',\n",
       "    'P5193',\n",
       "    'P548',\n",
       "    'P553'],\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 'Spotlight Session',\n",
       "   'start_time': '2023-07-10T19:00:00-04:00',\n",
       "   'track': 'Spotlight - Metropolitan Centre',\n",
       "   'type': 'Spotlight'},\n",
       "  'spotlight-session_-spotlight---metropolitan-east-(spotlight)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T21:00:00-04:00',\n",
       "   'id': 'spotlight-session_-spotlight---metropolitan-east-(spotlight)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1530',\n",
       "    'P300',\n",
       "    'P3059',\n",
       "    'P3371',\n",
       "    'P3615',\n",
       "    'P3630',\n",
       "    'P3868',\n",
       "    'P3880',\n",
       "    'P3954',\n",
       "    'P4021',\n",
       "    'P4028',\n",
       "    'P4087',\n",
       "    'P433',\n",
       "    'P4336',\n",
       "    'P4682',\n",
       "    'P4810',\n",
       "    'P5125',\n",
       "    'P707',\n",
       "    'P736',\n",
       "    'P1520',\n",
       "    'P1857',\n",
       "    'P2105',\n",
       "    'P2269',\n",
       "    'P3153',\n",
       "    'P406',\n",
       "    'P4509',\n",
       "    'P4552',\n",
       "    'P1798',\n",
       "    'P1890',\n",
       "    'P2092',\n",
       "    'P2187',\n",
       "    'P3242',\n",
       "    'P3765',\n",
       "    'P4277',\n",
       "    'P4341',\n",
       "    'P4674',\n",
       "    'P5015',\n",
       "    'P5188',\n",
       "    'P5486',\n",
       "    'P788',\n",
       "    'P2057',\n",
       "    'P2251',\n",
       "    'P2466',\n",
       "    'P2651',\n",
       "    'P2767',\n",
       "    'P2796',\n",
       "    'P2860',\n",
       "    'P3165',\n",
       "    'P3190',\n",
       "    'P3473',\n",
       "    'P382',\n",
       "    'P4706',\n",
       "    'P5756',\n",
       "    'P5831',\n",
       "    'P705',\n",
       "    'P1325',\n",
       "    'P1362',\n",
       "    'P1398',\n",
       "    'P1847',\n",
       "    'P2133',\n",
       "    'P2419',\n",
       "    'P4228',\n",
       "    'P4435',\n",
       "    'P452',\n",
       "    'P4681',\n",
       "    'P4722',\n",
       "    'P5150',\n",
       "    'P5567',\n",
       "    'P985',\n",
       "    'P1670',\n",
       "    'P1691',\n",
       "    'P1694',\n",
       "    'P2712',\n",
       "    'P2830',\n",
       "    'P3011',\n",
       "    'P2462',\n",
       "    'P3493',\n",
       "    'P3917',\n",
       "    'P4053',\n",
       "    'P4135',\n",
       "    'P4252',\n",
       "    'P4422',\n",
       "    'P4500',\n",
       "    'P5374',\n",
       "    'P606'],\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 'Spotlight Session',\n",
       "   'start_time': '2023-07-10T19:00:00-04:00',\n",
       "   'track': 'Spotlight - Metropolitan East',\n",
       "   'type': 'Spotlight'},\n",
       "  'spotlight-session_-spotlight---metropolitan-west-(spotlight)': {'chairs': [],\n",
       "   'end_time': '2023-07-10T21:00:00-04:00',\n",
       "   'id': 'spotlight-session_-spotlight---metropolitan-west-(spotlight)',\n",
       "   'link': None,\n",
       "   'paper_ids': ['P1621',\n",
       "    'P4724',\n",
       "    'P3985',\n",
       "    'P4152',\n",
       "    'P1142',\n",
       "    'P1436',\n",
       "    'P1662',\n",
       "    'P1876',\n",
       "    'P2095',\n",
       "    'P2112',\n",
       "    'P5670',\n",
       "    'P1168',\n",
       "    'P2240',\n",
       "    'P3633',\n",
       "    'P5325',\n",
       "    'P2021',\n",
       "    'P4728',\n",
       "    'P5739',\n",
       "    'P657',\n",
       "    'P2383',\n",
       "    'P2383',\n",
       "    'P3308',\n",
       "    'P3940',\n",
       "    'P4069',\n",
       "    'P4086',\n",
       "    'P4744',\n",
       "    'P4813',\n",
       "    'P2333',\n",
       "    'P4395',\n",
       "    'P5830',\n",
       "    'P1303',\n",
       "    'P1936',\n",
       "    'P194',\n",
       "    'P2288',\n",
       "    'P2416',\n",
       "    'P2629',\n",
       "    'P2746',\n",
       "    'P3121',\n",
       "    'P3596',\n",
       "    'P3667',\n",
       "    'P3845',\n",
       "    'P4065',\n",
       "    'P938',\n",
       "    'P1636',\n",
       "    'P1786',\n",
       "    'P1996',\n",
       "    'P2127',\n",
       "    'P2320',\n",
       "    'P2411',\n",
       "    'P3035',\n",
       "    'P3512',\n",
       "    'P3636',\n",
       "    'P3907',\n",
       "    'P399',\n",
       "    'P4012',\n",
       "    'P4051',\n",
       "    'P4074',\n",
       "    'P4811',\n",
       "    'P5289',\n",
       "    'P5577',\n",
       "    'P5627',\n",
       "    'P5841',\n",
       "    'P2024',\n",
       "    'P2039',\n",
       "    'P2238',\n",
       "    'P2298',\n",
       "    'P2465',\n",
       "    'P2788',\n",
       "    'P4937',\n",
       "    'P1040',\n",
       "    'P1291',\n",
       "    'P1391',\n",
       "    'P2406',\n",
       "    'P2429',\n",
       "    'P2461',\n",
       "    'P2772',\n",
       "    'P3225',\n",
       "    'P3557',\n",
       "    'P3926',\n",
       "    'P4423',\n",
       "    'P4544',\n",
       "    'P4619',\n",
       "    'P4675',\n",
       "    'P661',\n",
       "    'P661'],\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 'Spotlight Session',\n",
       "   'start_time': '2023-07-10T19:00:00-04:00',\n",
       "   'track': 'Spotlight - Metropolitan West',\n",
       "   'type': 'Spotlight'},\n",
       "  'welcome': {'chairs': [],\n",
       "   'end_time': '2023-07-10T13:30:00+00:00',\n",
       "   'id': 'welcome',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': None,\n",
       "   'session': 'welcome',\n",
       "   'start_time': '2023-07-10T13:00:00+00:00',\n",
       "   'track': 'Welcome',\n",
       "   'type': 'Plenary Sessions'}},\n",
       " 'papers': {'ACL-CODI_1': {'abstract': 'This paper presents the results obtained by the MELODI team for the three tasks proposed within the DISRPT 2023 shared task on discourse: segmentation, connective identification, and relation classification. The competition involves corpora in various languages in several underlying frameworks, and proposes two tracks depending on the presence or not of annotations of sentence boundaries and syntactic information. For these three tasks, we rely on a transformer-based architecture, and investigate several optimizations of the models, including hyper-parameter search and layer freezing.For discourse relations, we also explore the use of adapters---a lightweight solution for model fine-tuning---and introduce relation mappings to partially deal with the label set explosion we are facing within the setting of the shared task in a multi-corpus perspective. In the end, we propose one single architecture for segmentation and connectives, based on XLM-RoBERTa large, freezed at lower layers, with new state-of-the-art results for segmentation, and we propose 3 different models for relations, since the task makes it harder to generalize across all corpora.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Eleni Metheniti',\n",
       "    'Chlo Braud',\n",
       "    'Philippe Muller',\n",
       "    'Laura Rivire'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DISRPT'],\n",
       "   'id': 'ACL-CODI_1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'all',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DisCut and DiscReT: MELODI at DISRPT 2023',\n",
       "   'tldr': 'This paper presents the results obtained by the MELODI team for the three tasks proposed within the DISRPT 2023 shared task on discourse: segmentation, connective identification, and relation classification. The competition involves corpora in various languages in several underlying frameworks, and ',\n",
       "   'track': 'The Shared Task on Discourse Relation Parsing and Treebanking',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL-CODI_2': {'abstract': 'HITS participated in the Discourse Segmentation (DS, Task 1) and Connective Detection (CD, Task 2) tasks at the DISRPT 2023. Task 1 focuses on segmenting the text into discourse units, while Task 2 aims to detect the discourse connectives. We deployed a framework based on different pre-trained models according to the target language for these two tasks.HITS also participated in the Relation Classification track (Task 3). The main task was recognizing the discourse relation between text spans from different languages. We designed a joint model for languages with a small corpus while separate models for large corpora. The adversarial training strategy is applied to enhance the robustness of relation classifiers.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wei Liu', 'Yi Fan', 'Michael Strube'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DISRPT'],\n",
       "   'id': 'ACL-CODI_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'all',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HITS at DISRPT 2023: Discourse Segmentation, Connective Detection, and Relation Classification',\n",
       "   'tldr': 'HITS participated in the Discourse Segmentation (DS, Task 1) and Connective Detection (CD, Task 2) tasks at the DISRPT 2023. Task 1 focuses on segmenting the text into discourse units, while Task 2 aims to detect the discourse connectives. We deployed a framework based on different pre-trained model',\n",
       "   'track': 'The Shared Task on Discourse Relation Parsing and Treebanking',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL-CODI_3': {'abstract': 'This paper introduces DiscoFlan, a multilingual discourse relation classifier submitted for DISRPT 2023. Our submission represents the first attempt at building a multilingual discourse relation classifier for the DISRPT 2023 shared task. By our model addresses the issue to mismatches caused by hallucination in a seq2seq model by utilizing the label distribution information for label generation. In contrast to the previous state-of-the-art model, our approach eliminates the need for hand-crafted features in computing the discourse relation classes. Furthermore, we propose a novel label generation mechanism that anchors the labels to a fixed set by selectively enhancing training on the decoder model. Our experimental results demonstrate that our model surpasses the current state-of-the-art performance in 11 out of the 26 datasets considered, however the submitted model compatible with provided evaluation scripts is better in 7 out of 26 considered datasets, while demonstrating competitive results in the rest. Overall, DiscoFlan showcases promising advancements in multilingual discourse relation classification for the DISRPT 2023 shared task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kaveri Anuranjana'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DISRPT'],\n",
       "   'id': 'ACL-CODI_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'relations',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DiscoFlan: Instruction Fine-tuning and Refined Text Generation for Discourse Relation Label Classification',\n",
       "   'tldr': 'This paper introduces DiscoFlan, a multilingual discourse relation classifier submitted for DISRPT 2023. Our submission represents the first attempt at building a multilingual discourse relation classifier for the DISRPT 2023 shared task. By our model addresses the issue to mismatches caused by hall',\n",
       "   'track': 'The Shared Task on Discourse Relation Parsing and Treebanking',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL-CODI_4': {'abstract': 'In 2023, the third iteration of the DISRPT Shared Task (Discourse Relation Parsing and Treebanking) was held, dedicated to the underlying units used in discourse parsing across formalisms. Following the success of the 2019and 2021 tasks on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification, this iteration has added 10 new corpora, including 2 new languages (Thai and Italian) and 3 discourse treebanks annotated in the discourse dependency representation in addition to the previously included frameworks: RST, SDRT, and PDTB. In this paper, we review the data included in the Shared Task, which covers 26 datasets across 13 languages, survey and compare submitted systems, and report on system performance on each task for both annotated and plain-tokenized versions of the data.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chlo Braud',\n",
       "    'Yang Janet Liu',\n",
       "    'Eleni Metheniti',\n",
       "    'Philippe Muller',\n",
       "    'Laura Rivire',\n",
       "    'Attapol Rutherford',\n",
       "    'Amir Zeldes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DISRPT'],\n",
       "   'id': 'ACL-CODI_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'all',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The DISRPT 2023 Shared Task on Elementary Discourse Unit Segmentation, Connective Detection, and Relation Classification',\n",
       "   'tldr': 'In 2023, the third iteration of the DISRPT Shared Task (Discourse Relation Parsing and Treebanking) was held, dedicated to the underlying units used in discourse parsing across formalisms. Following the success of the 2019and 2021 tasks on Elementary Discourse Unit Segmentation, Connective Detection',\n",
       "   'track': 'The Shared Task on Discourse Relation Parsing and Treebanking',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_1': {'abstract': 'Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous work on pretrained Hebrew LMs focused on encoder-only models. While the encoder-only architecture is beneficial for classification tasks, it does not cater well for sub-word prediction tasks, such as Named Entity Recognition, when considering the morphologically rich nature of Hebrew. In this paper we argue that sequence-to-sequence generative architectures are more suitable for large LMs in morphologically rich languages (MRLs) such as Hebrew. We demonstrate this by casting tasks in the Hebrew NLP pipeline as text-to-text tasks, for which we can leverage powerful multilingual, pretrained sequence-to-sequence models as mT5, eliminating the need for a separate, specialized, morpheme-based, decoder. Using this approach, our experiments show substantial improvements over previously published results on all existing Hebrew NLP benchmarks. These results suggest that multilingual sequence-to-sequence models present a promising building block for NLP for MRLs.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Matan Eyal',\n",
       "    'Hila Noga',\n",
       "    'Roee Aharoni',\n",
       "    'Idan Szpektor',\n",
       "    'Reut Tsarfaty'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multilingual Sequence-to-Sequence Models for Hebrew NLP',\n",
       "   'tldr': 'Recent work attributes progress in NLP to large language models (LMs) with increased model size and large quantities of pretraining data. Despite this, current state-of-the-art LMs for Hebrew are both under-parameterized and under-trained compared to LMs in other languages. Additionally, previous wo',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_11': {'abstract': 'Due to the lack of data resources, rule-based or transfer learning is mainly used in the morphological tagging of low-resource languages. However, these methods require expert knowledge, ignore contextual features, and have error propagation. Therefore, we propose a joint morphological tagger for low-resource agglutinative languages to alleviate the above challenges. First, we represent the contextual input with multi-dimensional features of agglutinative words. Second, joint training reduces the direct impact of part-of-speech errors on morphological features and increases the indirect influence between the two types of labels through a fusion mechanism. Finally, our model separately predicts part-of-speech and morphological features. Part-of-speech tagging is regarded as sequence tagging. When predicting morphological features, two-label adjacency graphs are dynamically reconstructed by integrating multilingual global features and monolingual local features. Then, a graph convolution network is used to learn the higher-order intersection of labels. A series of experiments show that the proposed model in this paper is superior to other comparative models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gulinigeer Abudouwaili',\n",
       "    'Kahaerjiang Abiderexiti',\n",
       "    'Nian Yi',\n",
       "    'Aishan Wumaier'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Joint Learning Model for Low-Resource Agglutinative Language Morphological Tagging',\n",
       "   'tldr': 'Due to the lack of data resources, rule-based or transfer learning is mainly used in the morphological tagging of low-resource languages. However, these methods require expert knowledge, ignore contextual features, and have error propagation. Therefore, we propose a joint morphological tagger for lo',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_12': {'abstract': 'Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion-based language models have great potential under low-resource settings.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yiwei Lyu',\n",
       "    'Tiange Luo',\n",
       "    'Jiacheng Shi',\n",
       "    'Todd Hollon',\n",
       "    'Honglak Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Fine-grained Text Style Transfer with Diffusion-Based Language Models',\n",
       "   'tldr': 'Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowl',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_1282': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Lingfeng Shen', 'Haiyun Jiang', 'Lemao Liu', 'Shuming Shi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_1282',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_13': {'abstract': 'Although Question Answering (QA) have advanced to the human-level language skills in NLP tasks, there is still a problem: the QA model gets confused when there are similar sentences or paragraphs. Existing studies focus on enhancing the text understanding of the candidate answers to improve the overall performance of the QA models. However, since these methods focus on re-ranking queries or candidate answers, they fail to resolve the confusion when many generated answers are similar to the expected answer. To address these issues, we propose a novel contrastive learning framework called ContrastiveQA that alleviates the confusion problem in answer extraction. We propose a supervised method where we generate positive and negative samples from the candidate answers and the given answer, respectively. \\nWe thus introduce ContrastiveQA, which uses contrastive learning with sampling data to reduce incorrect answers. Experimental results on four QA benchmarks show the effectiveness of the proposed method.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Seungyeon Lee', 'Minho Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing text comprehension for Question Answering with Contrastive Learning ',\n",
       "   'tldr': 'Although Question Answering (QA) have advanced to the human-level language skills in NLP tasks, there is still a problem: the QA model gets confused when there are similar sentences or paragraphs. Existing studies focus on enhancing the text understanding of the candidate answers to improve the over',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_1380': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cheng-Han Chiang',\n",
       "    'Hung-yi Lee',\n",
       "    'Yung-Sung Chuang',\n",
       "    'James Glass'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_1380',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_14': {'abstract': 'Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called \"delicate text.\" We provide the taxonomy of delicate text and present a detailed annotation scheme. We annotate DeTexD, the first benchmark dataset for delicate text detection. The significance of the difference in the definitions is highlighted by the relative performance deltas between models trained each definitions and corpora and evaluated on the other. We make publicly available the DeTexD Benchmark dataset, annotation guidelines, and baseline model for delicate text detection.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Artem Chernodub',\n",
       "    'Serhii Yavnyi',\n",
       "    'Oleksii Sliusarenko',\n",
       "    'Jade Razzaghi',\n",
       "    'Yichen Mo',\n",
       "    'Knar Hovakimyan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DeTexD: A Benchmark Dataset for Delicate Text Detection',\n",
       "   'tldr': 'Over the past few years, much research has been conducted to identify and regulate toxic language. However, few studies have addressed a broader range of sensitive texts that are not necessarily overtly toxic. In this paper, we introduce and define a new category of sensitive text called \"delicate t',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_1454': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Guangsheng Bao', 'Zhiyang Teng', 'Yue Zhang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_1454',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Token-level Fitting Issues of Seq2seq Models',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_15': {'abstract': 'Cyberbullying is a serious societal issue widespread on various channels and platforms, particularly social networking sites. Such platforms have proven to be exceptionally fertile grounds for such behavior. The dearth of high-quality training data for multilingual and low-resource scenarios, data that can accurately capture the nuances of social media conversations, often poses a roadblock to this task. This paper attempts to tackle cyberbullying, specifically its two most common manifestations - aggression and offensiveness. We present a novel, manually annotated dataset of a total of 10,000 English and Hindi-English code-mixed tweets, manually annotated for aggression detection and offensive language detection tasks. Our annotations are supported by inter-annotator agreement scores of 0.67 and 0.74 for the two tasks, indicating substantial agreement. We perform comprehensive fine-tuning of pre-trained language models (PTLMs) using this dataset to check its efficacy. Our challenging test sets show that the best models achieve macro F1-scores of 67.87 and 65.45 on the two tasks, respectively. Further, we perform cross-dataset transfer learning to benchmark our dataset against existing aggression and offensive language datasets. We also present a detailed quantitative and qualitative analysis of errors in prediction, and with this paper, we publicly release the novel dataset, code, and models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nazia Nafis',\n",
       "    'Diptesh Kanojia',\n",
       "    'Naveen Saini',\n",
       "    'Rudra Murthy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_15',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Safer Communities: Detecting Aggression and Offensive Language in Code-Mixed Tweets to Combat Cyberbullying',\n",
       "   'tldr': 'Cyberbullying is a serious societal issue widespread on various channels and platforms, particularly social networking sites. Such platforms have proven to be exceptionally fertile grounds for such behavior. The dearth of high-quality training data for multilingual and low-resource scenarios, data t',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_16': {'abstract': 'As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performance of the models trained on datasets labeled using different HS taxonomies cannot be compared. To ease this problem, we propose applying extremely weak supervision that only relies on the class name rather than on class samples from the annotated data. We demonstrate the effectiveness of a state-of-the-art weakly-supervised text classification model in various in-dataset and cross-dataset settings. Furthermore, we conduct an in-depth quantitative and qualitative analysis of the source of poor generalizability of HS classification models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yiping Jin',\n",
       "    'Leo Wanner',\n",
       "    'Vishakha Kadam',\n",
       "    'Alexander Shvets'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Weakly-Supervised Hate Speech Classification Across Datasets',\n",
       "   'tldr': 'As pointed out by several scholars, current research on hate speech (HS) recognition is characterized by unsystematic data creation strategies and diverging annotation schemata. Subsequently, supervised-learning models tend to generalize poorly to datasets they were not trained on, and the performan',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_17': {'abstract': 'Linguistic analysis is a core task in the process of documenting, analyzing, and describing endangered and less-studied languages. In addition to providing insight into the properties of the language being studied, having tools to automatically label words in a language for grammatical category and morphological features can support a range of applications useful for language pedagogy and revitalization. At the same time, most modern NLP methods for these tasks require both large amounts of data in the language and compute costs well beyond the capacity of most research groups and language communities.  In this paper, we present a gloss-to-gloss (g2g) model for linguistic analysis (specifically, morphological analysis and part-of-speech tagging) that is lightweight in terms of both data requirements and computational expense. The model is designed for the interlinear glossed text (IGT) format, in which we expect the source text of a sentence in a low-resource language, a translation of that sentence into a language of wider communication, and a detailed glossing of the morphological properties of each word in the sentence. We first produce silver standard parallel glossed data by automatically labeling the high-resource translation. The model then learns to transform source language morphological labels into output labels for the target language, mediated by a structured linguistic representation layer. We test the model on both low-resource and high-resource languages, and find that our simple CNN-based model achieves comparable performance to a state-of-the-art transformer-based model, at a fraction of the computational cost.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Bhargav Shandilya', 'Alexis Palmer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Lightweight morpheme labeling in context: Using structured linguistic representations to support linguistic analysis for the language documentation context',\n",
       "   'tldr': 'Linguistic analysis is a core task in the process of documenting, analyzing, and describing endangered and less-studied languages. In addition to providing insight into the properties of the language being studied, having tools to automatically label words in a language for grammatical category and ',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_18': {'abstract': 'Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is unstable, task-specific, and bias-prone. This paper proposes a principled learning framework---soft prompt construction (SPC)---to facilitate learning domain-adaptable soft prompts. Derived from the SPC framework is a simple loss that can plug into various models and tuning approaches to improve their cross-domain performance. We show SPC can improve upon SOTA for contextual query rewriting, summarization, and paraphrase detection by up to 5\\\\%, 19\\\\%, and 16\\\\%, respectively.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wenbo Zhao', 'Arpit Gupta', 'Tagyoung Chung', 'Jing Huang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SPC: Soft Prompt Construction for Cross Domain Generalization',\n",
       "   'tldr': 'Recent advances in prompt tuning have proven effective as a new language modeling paradigm for various natural language understanding tasks. However, it is challenging to adapt the soft prompt embeddings to different domains or generalize to low-data settings when learning soft prompts itself is uns',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_19': {'abstract': 'The ground truth in classification tasks is often approximated by the fraction of annotators who classified an item as belonging to the positive class. Instances for which this fraction is equal to or above 50\\\\textbackslash{}\\\\% are considered positive, including however ones that receive polarized opinions. This is a problematic encoding convention that disregards the potentially polarized nature of opinions and which is often employed to estimate abusive language. We present the distance from unimodality (DFU), a measure that estimates the extent of polarization on the distribution of opinions and which correlates well with human judgment. By applying DFU to posts crowd-annotated for toxicity, we found that polarized opinions are more likely by annotators originating from different countries. Also, we show that DFU can be exploited as an objective function to train models to predict whether a post will provoke polarized opinions in the future.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['John Pavlopoulos', 'Aristidis Likas'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Distance from Unimodality: Assessing Polarized Opinions in Abusive Language Detection',\n",
       "   'tldr': 'The ground truth in classification tasks is often approximated by the fraction of annotators who classified an item as belonging to the positive class. Instances for which this fraction is equal to or above 50\\\\textbackslash{}\\\\% are considered positive, including however ones that receive polarized o',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_2': {'abstract': 'Active learning is an algorithmic approach that strategically selects a subset of examples for labeling, with the goal of reducing workload and required resources. Previous research has applied active learning to Neural Machine Translation (NMT) for high-resource or well-represented languages, achieving significant reductions in manual labor. In this study, we explore the application of active learning for NMT in the context of Mapudungun, a low-resource language spoken by the Mapuche community in South America. Mapudungun was chosen due to the limited number of fluent speakers and the pressing need to provide access to content predominantly available in widely represented languages. We assess both model-dependent and model-agnostic active learning strategies for NMT between Spanish and Mapudungun in both directions, demonstrating that we can achieve over 40\\\\% reduction in manual translation workload in both cases.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Begoa Pendas', 'Andres Carvallo', 'Carlos Aspillaga'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short Paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Neural Machine Translation through Active Learning on low-resource languages: The case of Spanish to Mapudungun',\n",
       "   'tldr': 'Active learning is an algorithmic approach that strategically selects a subset of examples for labeling, with the goal of reducing workload and required resources. Previous research has applied active learning to Neural Machine Translation (NMT) for high-resource or well-represented languages, achie',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_21': {'abstract': 'This paper evaluates various character alignment methods on the task of sentence-level standardization of dialect transcriptions. We compare alignment methods from different scientific traditions (dialectometry, speech processing, machine translation) and apply them to Finnish, Norwegian and Swiss German dialect datasets. In the absence of gold alignments, we evaluate the methods on a set of characteristics that are deemed undesirable for the task. We find that trained alignment methods only show marginal benefits to simple Levenshtein distance. On this particular task, eflomal outperforms related methods such as GIZA++ or fast_align by a large margin.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yves Scherrer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_21',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Character alignment methods for dialect-to-standard normalization',\n",
       "   'tldr': 'This paper evaluates various character alignment methods on the task of sentence-level standardization of dialect transcriptions. We compare alignment methods from different scientific traditions (dialectometry, speech processing, machine translation) and apply them to Finnish, Norwegian and Swiss G',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_22': {'abstract': 'We investigate native language identification (LangID) for Brazilian Indigenous Languages (BILs), using the Bible as training data. Our research extends from previous work, by presenting two analyses on the generalization of Bible-based LangID in non-biblical data. First, with newly collected non-biblical datasets, we show that such a LangID can still provide quite reasonable accuracy in languages for which there are more established writing standards, such as Guarani Mbya and Kaigang, but there can be a quite drastic drop in accuracy depending on the language. Then, we applied the LangID on a large set of texts, about 13M sentences from the Portuguese Wikipedia, towards understanding the difficulty factors may come out of such task in practice. The main outcome is that the lack of handling other American indigenous languages can affect considerably the precision for BILs, suggesting the need of a joint effort with related languages from the Americas.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Paulo Cavalin',\n",
       "    'Pedro Domingues',\n",
       "    'Julio Nogima',\n",
       "    'Claudio Pinhanez'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short Paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Understanding Native Language Identification for Brazilian Indigenous Languages',\n",
       "   'tldr': 'We investigate native language identification (LangID) for Brazilian Indigenous Languages (BILs), using the Bible as training data. Our research extends from previous work, by presenting two analyses on the generalization of Bible-based LangID in non-biblical data. First, with newly collected non-bi',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_23': {'abstract': 'We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on an ensemble with a knowledge graph embedding model, which itself is excessively large and costly to use. In this short paper, we show empirically that adding contextual information  i.e., information about the direct neighborhood of the query entity  alleviates the need for a separate KGE model to obtain good performance. The resulting KGT5-context model is simple, reduces model size significantly, and obtains state-of-the-art performance in our experimental study.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Adrian Kochsiek',\n",
       "    'Apoorv Saxena',\n",
       "    'Inderjeet Nair',\n",
       "    'Rainer Gemulla'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_23',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction',\n",
       "   'tldr': 'We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on a',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_24': {'abstract': 'This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus and experimental data showed the best performance with the overall accuracy of around 83%. We found that the best model must be trained on both corpus data and the experimental data of one particular participant. Our examination of speaker-variability and speaker-specific information did not explain why a particular participant combined well with the corpus data. We recommend Linear Discriminative Learning models as a future non-neural baseline system, owning to its training speed, accuracy, model interpretability and cognitive plausibility. In order to improve the model performance, we suggest using bigger data and/or performing data augmentation and incorporating speaker- and item-specifics considerably.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cheonkam Jeong',\n",
       "    'Dominic Schmitz',\n",
       "    'Akhilesh Kakolu Ramarao',\n",
       "    'Anna Stein',\n",
       "    'Kevin Tang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Linear Discriminative Learning: a competitive non-neural baseline for morphological inflection',\n",
       "   'tldr': 'This paper presents our submission to the SIGMORPHON 2023 task 2 of Cognitively Plausible Morphophonological Generalization in Korean. We implemented both Linear Discriminative Learning and Transformer models and found that the Linear Discriminative Learning model trained on a combination of corpus ',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_25': {'abstract': 'Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. \\nThis paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Flor Miriam Plaza-del-arco', 'Debora Nozza', 'Dirk Hovy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech',\n",
       "   'tldr': 'Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_2575': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['John Harvill',\n",
       "    'Mark Hasegawa-Johnson',\n",
       "    'Hee Suk Yoon',\n",
       "    'Chang D. Yoo',\n",
       "    'Eunseop Yoon'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_2575',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'One-Shot Exemplification Modeling via Latent Sense Representations',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_26': {'abstract': 'We present an extensive evaluation of different fine-tuned models to detect instances of offensive and abusive language in Dutch across three benchmarks: a standard held-out test, a task- agnostic functional benchmark, and a dynamic test set. We also investigate the use of data cartography to identify high quality training data. Our results show a relatively good quality of the manually annotated data used to train the models while highlighting some critical weakness. We have also found a good portability of trained models along the same language phenomena. As for the data cartography, we have found a positive impact only on the functional benchmark and when selecting data per annotated dimension rather than using the entire training material.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tommaso Caselli', 'Hylke Van Der Veen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Benchmarking Offensive and Abusive Language in Dutch Tweets',\n",
       "   'tldr': 'We present an extensive evaluation of different fine-tuned models to detect instances of offensive and abusive language in Dutch across three benchmarks: a standard held-out test, a task- agnostic functional benchmark, and a dynamic test set. We also investigate the use of data cartography to identi',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_27': {'abstract': 'We draw from the framework of relationality as a pathway for modeling social relations to address gaps in text classification, generally, and offensive language classification, specifically. We use minoritized language, such as queer speech, to motivate a need for understanding and modeling social relationsboth among individuals and among their social communities. We then point to socio-ethical style as a research area for inferring and measuring social relations as well as propose additional questions to structure future research on operationalizing social context.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Razvan Amironesei', 'Mark Diaz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Relationality and Offensive Speech: A Research Agenda',\n",
       "   'tldr': 'We draw from the framework of relationality as a pathway for modeling social relations to address gaps in text classification, generally, and offensive language classification, specifically. We use minoritized language, such as queer speech, to motivate a need for understanding and modeling social r',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_28': {'abstract': \"This paper presents my submission to Track 1 of the 2023 SIGMORPHON shared task on interlinear glossed text (IGT). There are a wide amount of techniques for building and training IGT models (see Moeller and Hulden, 2018; McMillan-Major, 2020; Zhao et al., 2020). I describe my ensembled sequence-to-sequence approach, perform experiments, and share my submission's test-set accuracy. I also discuss future areas of research in low-resource token classification methods for IGT.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Edith Coates'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Ensembled Encoder-Decoder System for Interlinear Glossed Text',\n",
       "   'tldr': 'This paper presents my submission to Track 1 of the 2023 SIGMORPHON shared task on interlinear glossed text (IGT). There are a wide amount of techniques for building and training IGT models (see Moeller and Hulden, 2018; McMillan-Major, 2020; Zhao et al., 2020). I describe my ensembled sequence-to-s',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_29': {'abstract': 'The prevalence of abusive language on different online platforms has been a major concern that raises the need for automated cross-platform abusive language detection. However, prior works focus on concatenating data from multiple platforms, inherently adopting Empirical Risk Minimization (ERM) method. In this work, we address this challenge from the perspective of domain generalization objective. We design SCL-Fish, a supervised contrastive learning integrated meta-learning algorithm to detect abusive language on unseen platforms. Our experimental analysis shows that SCL-Fish achieves better performance over ERM and the existing state-of-the-art models. We also show that SCL-Fish is data-efficient and achieves comparable performance with the large-scale pre-trained models upon finetuning for the abusive language detection task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Md Tawkat Islam Khondaker',\n",
       "    'Muhammad Abdul-mageed',\n",
       "    'Laks Lakshmanan, V.s.'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_29',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Cross-Platform and Cross-Domain Abusive Language Detection with Supervised Contrastive Learning',\n",
       "   'tldr': 'The prevalence of abusive language on different online platforms has been a major concern that raises the need for automated cross-platform abusive language detection. However, prior works focus on concatenating data from multiple platforms, inherently adopting Empirical Risk Minimization (ERM) meth',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_3': {'abstract': 'Clean-label (CL) attack is a form of data poisoning attack where an adversary  modifies only the textual input of the training data, without requiring access to the labeling function. CL attacks are relatively unexplored in NLP, as compared to label flipping (LF) attacks, where the latter additionally requires access to the labeling function as well. While CL attacks are more resilient to data sanitization and manual relabeling methods than LF attacks, they often demand as high as ten times the  poisoning budget than LF attacks.  In this work, we first  introduce an Adversarial Clean Label attack  which can adversarially perturb in-class training examples for poisoning the training set. We then show that an adversary can significantly bring down the data requirements for a CL attack, using the aforementioned approach, to as low as 20 \\\\% of the data otherwise required.  We then systematically benchmark and analyze a number of defense methods, for both LF and CL attacks, some previously employed solely for LF attacks in the textual domain and others adapted from computer vision. We find that text-specific defenses greatly vary in their effectiveness depending on their properties.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ashim Gupta', 'Amrith Krishna'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Adversarial Clean Label Backdoor Attacks and Defenses on Text Classification Systems',\n",
       "   'tldr': 'Clean-label (CL) attack is a form of data poisoning attack where an adversary  modifies only the textual input of the training data, without requiring access to the labeling function. CL attacks are relatively unexplored in NLP, as compared to label flipping (LF) attacks, where the latter additional',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_30': {'abstract': 'Warning: this paper contains content that some may find disturbing.\\n\\nWhile there has been increasing attention paid to the potential harms perpetuated by online platforms, most academic work on the subject centers on one narrow context: Western communities in primarily English language settings. Yet, social media platforms like YouTube support users globally and provide content in several languages, including low-resourced languages. In this study, we investigate this context via a mixed methods approach: collecting and analysing search and recommendation data from YouTube in low-resource language settings and conducting semi-structured interviews with YouTube users who speak low-resourced languages in Ethiopia. Our early findings indicate the failure of current content moderation schemes for low-resource languages and the further infliction and distribution of harm to marginalized communities through recommendation systems.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hellina Hailu Nigatu', 'Inioluwa Raji'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Auditing YouTube Content Moderation in Low Resource Language Settings',\n",
       "   'tldr': 'Warning: this paper contains content that some may find disturbing.\\n\\nWhile there has been increasing attention paid to the potential harms perpetuated by online platforms, most academic work on the subject centers on one narrow context: Western communities in primarily English language settings. Yet',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_3114': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Likang Xiao', 'Richong Zhang', 'Zijie Chen', 'Junfan Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_3114',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Tucker Decomposition with Frequency Attention for Temporal Knowledge Graph Completion',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_32': {'abstract': 'This paper presents the submission by the MeLeL team to the SIGMORPHONUniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology in Polish, Estonian, and Finnish. This task requires us to produce the word form given a lemma and a grammatical case, while trying to produce the same error-rate as in children. We approach this task with a reduced-size character-based transformer model, multilingual training and an upsampling method to introduce bias.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gal Astrach', 'Yuval Pinter'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The BGU-MeLeL System for the SIGMORPHON 2023 Shared Task on Morphological Inflection',\n",
       "   'tldr': 'This paper presents the submission by the MeLeL team to the SIGMORPHONUniMorph Shared Task on Typologically Diverse and Acquisition-Inspired Morphological Inflection Generation Part 3: Models of Acquisition of Inflectional Noun Morphology in Polish, Estonian, and Finnish. This task requires us to p',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_33': {'abstract': 'We introduce ExtremeBB, a textual database of over 53.5M posts made by 38.5k users on 12 extremist bulletin board forums promoting online hate, harassment, the manosphere and other forms of extremism. It enables large-scale analyses of qualitative and quantitative historical trends going back two decades: measuring hate speech and toxicity; tracing the evolution of different strands of extremist ideology; tracking the relationships between online subcultures, extremist behaviours, and real-world violence; and monitoring extremist communities in near real time. This can shed light not only on the spread of problematic ideologies but also the effectiveness of interventions. ExtremeBB comes with a robust ethical data-sharing regime that allows us to share data with academics worldwide. Since 2020, access has been granted to 49 licensees in 16 research groups from 12 institutions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Anh V. Vu',\n",
       "    'Lydia Wilson',\n",
       "    'Yi Ting Chua',\n",
       "    'Ilia Shumailov',\n",
       "    'Ross Anderson'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ExtremeBB: A Database for Large-Scale Research into Online Hate, Harassment, the Manosphere and Extremism',\n",
       "   'tldr': 'We introduce ExtremeBB, a textual database of over 53.5M posts made by 38.5k users on 12 extremist bulletin board forums promoting online hate, harassment, the manosphere and other forms of extremism. It enables large-scale analyses of qualitative and quantitative historical trends going back two de',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_34': {'abstract': 'While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as hindering poverty reduction policy measures. We demonstrate that aporophobic attitudes are indeed present in social media and argue that the existing NLP datasets and models are inadequate to effectively address this problem. Efforts toward designing specialized resources and novel socio-technical mechanisms for confronting aporophobia are needed.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Svetlana Kiritchenko',\n",
       "    'Georgina Curto Rex',\n",
       "    'Isar Nejadgholi',\n",
       "    'Kathleen C. Fraser'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_34',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Aporophobia: An Overlooked Type of Toxic Language Targeting the Poor',\n",
       "   'tldr': 'While many types of hate speech and online toxicity have been the focus of extensive research in NLP, toxic language stigmatizing poor people has been mostly disregarded. Yet, aporophobia, a social bias against the poor, is a common phenomenon online, which can be psychologically damaging as well as',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_35': {'abstract': 'In this paper, we introduce a fine-tuned transformer-based model focused on problematic webpage classification to identify webpages promoting hate and violence of various forms. Due to the unavailability of labelled problematic webpage data, first we propose a novel webpage data collection strategy which leverages well-studied short-text hate speech datasets. We have introduced a custom GPT-4 few-shot prompt annotation scheme taking various webpage features to label the prohibitively expensive webpage annotation task. The resulting annotated data is used to build our problematic webpage classification model. We report the accuracy (87.6\\\\% F1-score) of our webpage classification model and conduct a detailed comparison of it against other state-of-the-art hate speech classification model on problematic webpage identification task. Finally, we have showcased the importance of various webpage features in identifying a problematic webpage.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ojasvin Sood', 'Sandipan Dandapat'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Problematic Webpage Identification: A Trilogy of Hatespeech, Search Engines and GPT',\n",
       "   'tldr': 'In this paper, we introduce a fine-tuned transformer-based model focused on problematic webpage classification to identify webpages promoting hate and violence of various forms. Due to the unavailability of labelled problematic webpage data, first we propose a novel webpage data collection strategy ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_3566': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Romain Bielawski', 'Rufin VanRullen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_3566',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CLIP-based image captioning via unsupervised cycle-consistency in the latent space',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_36': {'abstract': 'In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous Mexican languages. We evaluated the usability of the collected corpus using three different approaches: transformer, transfer learning, and fine-tuning pre-trained multilingual MT models. Fine-tuning the Facebook m2m100-48 model outperformed the other approaches, with BLEU scores of 12.09 and 22.25 for Mazatec-Spanish and Spanish-Mazatec translations, respectively, and 16.75 and 22.15 for Mixtec-Spanish and Spanish-Mixtec translations, respectively. The results indicate that translation performance is influenced by the dataset size (9,799 sentences in Mazatec and 13,235 sentences in Mixtec) and is more effective when indigenous languages are used as target languages. The findings emphasize the importance of creating parallel corpora for indigenous languages and fine-tuning models for low-resource translation tasks. Future research will investigate zero-shot and few-shot learning approaches to further improve translation performance in low-resource settings.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Atnafu Lambebo Tonja',\n",
       "    'Christian Maldonado-sifuentes',\n",
       "    'David Alejandro Mendoza Castillo',\n",
       "    'Olga Kolesnikova',\n",
       "    'No Castro-snchez',\n",
       "    'Grigori Sidorov',\n",
       "    'Alexander Gelbukh'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_36',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long Paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Parallel Corpus for Indigenous Language Translation: Spanish-Mazatec and Spanish-Mixtec',\n",
       "   'tldr': 'In this paper, we present a parallel Spanish- Mazatec and Spanish-Mixtec corpus for machine translation (MT) tasks, where Mazatec and Mixtec are two indigenous Mexican languages. We evaluated the usability of the collected corpus using three different approaches: transformer, transfer learning, and ',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_38': {'abstract': 'Classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. It is imperative to have methods in place that can compare different models and identify over-reliances on specific concepts. We consider three well-known abusive language classifiers trained on large English datasets and focus on the concept of negative emotions, which is an important signal but should not be learned as a sufficient feature for the label of abuse. Motivated by the definition of global sufficiency, we first examine the unwanted dependencies learned by the classifiers by assessing their accuracy on a challenge set across all decision thresholds. Further, recognizing that a challenge set might not always be available, we introduce concept-based explanation metrics to assess the influence of the concept on the labels. These explanations allow us to compare classifiers regarding the degree of false global sufficiency they have learned between a concept and a label.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Isar Nejadgholi',\n",
       "    'Svetlana Kiritchenko',\n",
       "    'Kathleen C. Fraser',\n",
       "    'Esma Balkir'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_38',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers',\n",
       "   'tldr': 'Classifiers tend to learn a false causal relationship between an over-represented concept and a label, which can result in over-reliance on the concept and compromised classification accuracy. It is imperative to have methods in place that can compare different models and identify over-reliances on ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_39': {'abstract': 'A rise in the circulation of memes has led to the spread of a new form of multimodal hateful content. Unfortunately, the degree of hate women receive on the internet is disproportionately skewed against them. This, combined with the fact that multimodal misogyny is more challenging to detect as opposed to traditional text-based misogyny, signifies that the task of identifying misogynistic memes online is one of utmost importance. To this end, the MAMI dataset was released, consisting of 12000 memes annotated for misogyny and four sub-classes of misogyny - shame, objectification, violence and stereotype. While this balanced dataset is widely cited, we find that the task itself remains largely unsolved. Thus, in our work, we investigate the performance of multiple models in an effort to analyse whether domain specific pretraining helps model performance. We also investigate why even state of the art models find this task so challenging, and whether domain-specific pretraining can help. Our results show that pretraining BERT on hateful memes and leveraging an attention based approach with ViT outperforms state of the art models by more than 10\\\\%. Further, we provide insight into why these models may be struggling with this task with an extensive qualitative analysis of random samples from the test set.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Smriti Singh', 'Amritha Haridasan', 'Raymond Mooney'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '\"Female Astronaut: Because sandwiches won\\'t make themselves up there: Towards Multimodal misogyny detection in memes',\n",
       "   'tldr': 'A rise in the circulation of memes has led to the spread of a new form of multimodal hateful content. Unfortunately, the degree of hate women receive on the internet is disproportionately skewed against them. This, combined with the fact that multimodal misogyny is more challenging to detect as oppo',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_4': {'abstract': 'We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shahriar Golchin',\n",
       "    'Mihai Surdeanu',\n",
       "    'Nazgol Tavabi',\n",
       "    'Ata Kiapour'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords',\n",
       "   'tldr': 'We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 202',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_40': {'abstract': 'Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment  predicts signs of derailment in advance enabling proactive moderation of conversations. Current state-of-the-art approaches to address this problem rely on sequence models that treat dialogues as text streams. We propose a novel model based on a graph convolutional neural network that considers dialogue user dynamics and the influence of public perception on conversation utterances. Through empirical evaluation, we show that our model effectively captures conversation dynamics and outperforms the state-of-the-art models on the CGA and CMV benchmark datasets by 1.5\\\\textbackslash{}\\\\% and 1.7\\\\textbackslash{}\\\\%, respectively.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Enas Altarawneh',\n",
       "    'Ameeta Agrawal',\n",
       "    'Michael Jenkin',\n",
       "    'Manos Papagelis'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Conversation Derailment Forecasting with Graph Convolutional Networks',\n",
       "   'tldr': 'Online conversations are particularly susceptible to derailment, which can manifest itself in the form of toxic communication patterns like disrespectful comments or verbal abuse. Forecasting conversation derailment  predicts signs of derailment in advance enabling proactive moderation of conversati',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_41': {'abstract': 'Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation\\\\textasciitilde{}(and symbolic reasoners). However, reasoning with formal language has proved challenging\\\\textasciitilde{}(e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation\\\\textasciitilde{}(and pretrained language models as reasoners), including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, desirable tasks \\\\textbackslash{}\\\\& methods in the future, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zonglin Yang',\n",
       "    'Xinya Du',\n",
       "    'Rui Mao',\n",
       "    'Jinjie Ni',\n",
       "    'Erik Cambria'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_41',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Logical Reasoning over Natural Language as Knowledge Representation: A Survey',\n",
       "   'tldr': 'Logical reasoning is central to human cognition and intelligence. Past research of logical reasoning within AI uses formal language as knowledge representation\\\\textasciitilde{}(and symbolic reasoners). However, reasoning with formal language has proved challenging\\\\textasciitilde{}(e.g., brittleness ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_4158': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xudong Hong',\n",
       "    'Vera Demberg',\n",
       "    'Asad Sayeed',\n",
       "    'Qiankun Zheng',\n",
       "    'Bernt Schiele'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_4158',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Visual Coherence Loss for Coherent and Visually Grounded Story Generation',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_42': {'abstract': 'Online Gender-Based Violence (GBV), such as misogynistic abuse is an increasingly prevalent problem that technological approaches have struggled to address.Through the lens of the GBV framework, which is rooted in social science and policy, we systematically review 63 available resources for automated identification of such language. We find the datasets are limited in a number of important ways, such as their lack of theoretical grounding and stakeholder input, static nature, and focus on certain media platforms. Based on this review, we recommend development of future resources rooted in sociological expertise and\\ncentering stakeholder voices, namely GBV experts and people with lived experience of GBV.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gavin Abercrombie',\n",
       "    'Aiqi Jiang',\n",
       "    'Poppy Gerrard-abbott',\n",
       "    'Ioannis Konstas',\n",
       "    'Verena Rieser'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Resources for Automated Identification of Online Gender-Based Violence: A Systematic Review',\n",
       "   'tldr': 'Online Gender-Based Violence (GBV), such as misogynistic abuse is an increasingly prevalent problem that technological approaches have struggled to address.Through the lens of the GBV framework, which is rooted in social science and policy, we systematically review 63 available resources for automat',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_43': {'abstract': 'The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency approaches geared towards high throughput and performance. Multi-input multi-output (MIMO) algorithms such as data multiplexing, offer a promising solution with a many-fold increase in throughput by performing inference for multiple inputs at the cost of a single input. Yet these approaches are not currently performant enough to be deployed in modern systems. We change that by developing MUX-PLMs, a class of high throughput pre-trained language models (PLMs) trained with data multiplexing, that can be fine-tuned for any downstream task to yield high-throughput high-performance. Our novel multiplexing and demultiplexing modules proficiently entangle and disentangle inputs, and enable high-performance high throughput \\\\muxplms{} that are competitive with vanilla PLMs while achieving 2x/5x inference speedup with only a 14% drop on a broad suite of tasks.\\n',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vishvak Murahari',\n",
       "    'Ameet Deshpande',\n",
       "    'Carlos Jimenez',\n",
       "    'Izhak Shafran',\n",
       "    'Mingqiu Wang',\n",
       "    'Yuan Cao',\n",
       "    'Karthik Narasimhan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_43',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MUX-PLMs: Pre-training Language Models with Data Multiplexing',\n",
       "   'tldr': 'The widespread adoption of large language models such as ChatGPT and Bard has led to unprecedented demand for these technologies. The burgeoning cost of inference for ever-increasing model sizes coupled with hardware shortages has limited affordable access and poses a pressing need for efficiency ap',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_44': {'abstract': 'In this paper, we present a new online dictionary of Akuzipik, an Indigenous language of St. Lawrence Island (Alaska) and Chukotka (Russia).We discuss community desires for strengthening language use in the community and in educational settings, and present specific features of an online dictionary designed to serve these community goals.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Benjamin Hunt',\n",
       "    'Lane Schwartz',\n",
       "    'Sylvia Schreiner',\n",
       "    'Emily Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long Paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Community consultation and the development of an online Akuzipik-English dictionary',\n",
       "   'tldr': 'In this paper, we present a new online dictionary of Akuzipik, an Indigenous language of St. Lawrence Island (Alaska) and Chukotka (Russia).We discuss community desires for strengthening language use in the community and in educational settings, and present specific features of an online dictionary ',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_45': {'abstract': \"What is deemed offensive depends inherently on the socio-cultural contexts within which those assessments are made. However, the subjective nature of this task is often overlooked in traditional NLP studies. While there has been recent work on socio-demographic correlates of offensiveness, the contribution of socio-cultural factors to perceiving offensiveness, when considered as a moral judgement, across the globe is still under-explored. \\nWe summarize the findings from a cross-cultural study of offensiveness annotations by 4295 participants from 21 different countries across 8 geo-cultural regions. Our results show that (1) perceptions of offensiveness vary significantly across geo-cultural regions, despite controlling for gender, age, and socio-economic status, and (2) these differences are significantly mediated by annotators' individual moral concerns that vary across cultures.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aida Mostafazadeh Davani',\n",
       "    'Mark Diaz',\n",
       "    'Dylan Baker',\n",
       "    'Vinodkumar Prabhakaran'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Disentangling Disagreements on Offensiveness: A Cross-Cultural Study',\n",
       "   'tldr': 'What is deemed offensive depends inherently on the socio-cultural contexts within which those assessments are made. However, the subjective nature of this task is often overlooked in traditional NLP studies. While there has been recent work on socio-demographic correlates of offensiveness, the contr',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_4560': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Bin Wang', 'Haizhou Li'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_4560',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Relational Sentence Embedding for Flexible Semantic Matching',\n",
       "   'tldr': '',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_46': {'abstract': 'Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inference (NLI) models which perform well in zero- and few-shot settings can benefit hate speech detection performance in scenarios where only a limited amount of labeled data is available in the target language. Our evaluation on five languages demonstrates large performance improvements of NLI fine-tuning over direct fine-tuning in the target language. However, the effectiveness of previous work that proposed intermediate fine-tuning on English data is hard to match. Only in settings where the English training data does not match the test domain, can our customised NLI-formulation outperform intermediate fine-tuning on English. Based on our extensive experiments, we propose a set of recommendations for hate speech detection in languages where minimal labeled training data is available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Janis Goldzycher',\n",
       "    'Moritz Preisig',\n",
       "    'Chantal Amrhein',\n",
       "    'Gerold Schneider'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_46',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating the Effectiveness of Natural Language Inference for Hate Speech Detection in Languages with Limited Labeled Data',\n",
       "   'tldr': 'Most research on hate speech detection has focused on English where a sizeable amount of labeled training data is available. However, to expand hate speech detection into more languages, approaches that require minimal training data are needed. In this paper, we test whether natural language inferen',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_47': {'abstract': 'Recent work has demonstrated that using parameter efficient tuning techniques such as prefix tuning (or P-tuning) on pretrained language models can yield performance that is comparable or superior to fine-tuning  while dramatically reducing trainable parameters. Nevertheless, the effectiveness of such methods under the context of data augmentation, a common strategy to improve learning under  low data regimes, has not been fully explored. In this paper, we examine the effectiveness of several popular task-agnostic data augmentation techniques, i.e., EDA, Back Translation, and Mixup, when using two general parameter efficient tuning methods, P-tuning v2 and LoRA, under data scarcity.  We  show that  data augmentation can be used to boost the performance of P-tuning and LoRA models, but  the effectiveness of each technique varies and certain methods can lead to a notable degradation in performance, particularly when using larger models and on harder tasks. We further analyze the sentence representations of P-tuning compared to fine-tuning to help understand the above behaviour, and reveal how P-tuning generally presents a more limited ability to separate the sentence embeddings from different classes of augmented data. In addition, it displays poorer performance on heavily altered data. However, we demonstrate that by adding a simple contrastive loss function it can help mitigate such issues for prefix tuning, resulting in sizable improvements to augmented data performance. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Stephen Obadinma', 'Hongyu Guo', 'Xiaodan Zhu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Effectiveness of Data Augmentation for Parameter Efficient Tuning with Limited Data',\n",
       "   'tldr': 'Recent work has demonstrated that using parameter efficient tuning techniques such as prefix tuning (or P-tuning) on pretrained language models can yield performance that is comparable or superior to fine-tuning  while dramatically reducing trainable parameters. Nevertheless, the effectiveness of su',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_48': {'abstract': 'We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine translation shared task. We focus on the Spanish-Quechua language pair and explore the usage of different approaches: (1) Obtain new parallel corpora from the literature and legal domains, (2) Compare a high-resource Spanish-English pre-trained MT model with a Spanish-Finnish pre-trained model (with Finnish being chosen as a target language due to its morphological similarity to Quechua), and (3) Explore additional techniques such as copied corpus and back-translation. Overall, we show that the Spanish-Finnish pre-trained model outperforms other setups, while low-quality synthetic data reduces the performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nouman Ahmed', 'Natalia Flechas Manrique', 'Antonije Petrovi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Shared Task System Description',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Spanish-Quechua Machine Translation with Pre-Trained Models and Diverse Data Sources: LCT-EHU at AmericasNLP Shared Task',\n",
       "   'tldr': 'We present the LCT-EHU submission to the AmericasNLP 2023 low-resource machine translation shared task. We focus on the Spanish-Quechua language pair and explore the usage of different approaches: (1) Obtain new parallel corpora from the literature and legal domains, (2) Compare a high-resource Span',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_49': {'abstract': \"We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments.NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Samuel Cahyawijaya',\n",
       "    'Holy Lovenia',\n",
       "    'Alham Fikri Aji',\n",
       "    'Genta Winata',\n",
       "    'Bryan Wilie',\n",
       "    'Fajri Koto',\n",
       "    'Rahmad Mahendra',\n",
       "    'Christian Wibisono',\n",
       "    'Ade Romadhony',\n",
       "    'Karissa Vincentio'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_49',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Previously Presented Work',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NusaCrowd: Open Source Initiative for Indonesian NLP Resources',\n",
       "   'tldr': 'We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the dataset',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_5': {'abstract': 'Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task developed to test rule-like generalizations, our experiments on detecting subject-verb agreement yield several promising results. First, we show that while the usual sentence representations encoded as one-dimensional arrays do not easily support extraction of rule-like regularities, a two-dimensional  reshaping of these vectors allows various learning architectures to access such information. Next, we show that various architectures can detect patterns in these two-dimensional reshaped sentence embeddings and successfully learn a model based on smaller amounts of simpler training data, which performs well on more complex test data. This indicates that current sentence embeddings contain information that is regularly distributed, and which can be captured when the embeddings are reshaped into higher dimensional arrays. Our results cast light on representations produced by language models and help move towards developing few-shot learning approaches.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vivi Nastase', 'Paola Merlo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Grammatical information in BERT sentence embeddings as two-dimensional arrays',\n",
       "   'tldr': 'Sentence embeddings induced with various transformer architectures encode much semantic and syntactic information in a distributed manner in a one-dimensional array. We investigate whether specific grammatical information can be accessed in these distributed representations. Using data from a task d',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_50': {'abstract': 'In the past few years, the NLP community has actively worked on detecting LGBT+Phobia in online spaces, using textual data publicly available Most of these are for the English language and its variants since it is the most studied language by the NLP community. Nevertheless, efforts towards creating corpora in other languages are active worldwide. Despite this, the Spanish language is an understudied language regarding digital LGBT+Phobia. The only corpus we found in the literature was for the Peninsular Spanish dialects, which use LGBT+phobic terms different than those in the Mexican dialect. For this reason, we present Homo-MEX, a novel corpus for detecting LGBT+Phobia in Mexican Spanish. In this paper, we describe our data-gathering and annotation process. Also, we present a classification benchmark using various traditional machine learning algorithms and two pre-trained deep learning models to showcase our corpus classification potential.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Juan Vsquez',\n",
       "    'Scott Andersen',\n",
       "    'Gemma Bel-enguix',\n",
       "    'Helena Gmez-adorno',\n",
       "    'Sergio-luis Ojeda-trueba'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HOMO-MEX: A Mexican Spanish Annotated Corpus for LGBT+phobia Detection on Twitter',\n",
       "   'tldr': 'In the past few years, the NLP community has actively worked on detecting LGBT+Phobia in online spaces, using textual data publicly available Most of these are for the English language and its variants since it is the most studied language by the NLP community. Nevertheless, efforts towards creating',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_51': {'abstract': 'In this work we propose a novel annotation scheme which factors hate speech into five separate discursive categories. To evaluate our scheme, we construct a corpus of over 2.9M Twitter posts containing hateful expressions directed at Jews, and annotate a sample dataset of 1,050 tweets. We present a statistical analysis of the annotated dataset as well as discuss annotation examples, and conclude by discussing promising directions for future work.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gal Ron', 'Effi Levi', 'Odelia Oshri', 'Shaul Shenhav'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_51',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Factoring Hate Speech: A New Annotation Framework to Study Hate Speech in Social Media',\n",
       "   'tldr': 'In this work we propose a novel annotation scheme which factors hate speech into five separate discursive categories. To evaluate our scheme, we construct a corpus of over 2.9M Twitter posts containing hateful expressions directed at Jews, and annotate a sample dataset of 1,050 tweets. We present a ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_52': {'abstract': \"This paper presents PlayGround's submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M, a multilingual MT model pre-trained on Flores-200, on 10 low-resource language directions and examined the effectiveness of weight averaging and back translation. Our experiments showed that weight averaging, on average, led to a 0.0169 improvement in the ChrF++ score. Additionally, we found that back translation resulted in a 0.008 improvement in the ChrF++ score.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tianrui Gu', 'Kaie Chen', 'Siqi Ouyang', 'Lei Li'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_52',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Shared Task System Description',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'PlayGround Low Resource Machine Translation System for the 2023 AmericasNLP Shared Task',\n",
       "   'tldr': \"This paper presents PlayGround's submission to the AmericasNLP 2023 shared task on machine translation (MT) into indigenous languages. We finetuned NLLB-600M, a multilingual MT model pre-trained on Flores-200, on 10 low-resource language directions and examined the effectiveness of weight averaging \",\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_53': {'abstract': 'The definitions of abusive, offensive, toxic and uncivil comments used for annotating corpora for automated content moderation are highly intersected and researchers call for their disambiguation. We summarize the definitions of these terms as they appear in 23 papers across different fields. \\nWe compare examples given for uncivil, offensive, and toxic comments, attempting to foster more unified scientific resources. Additionally, we stress that the term incivility that frequently appears in social science literature has hardly been mentioned in the literature we analyzed that focuses on computational linguistics and natural language processing.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Pia Pachinger',\n",
       "    'Julia Neidhardt',\n",
       "    'Allan Hanbury',\n",
       "    'Anna Planitzer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_53',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Toward Disambiguating the Definitions of Abusive, Offensive, Toxic, and Uncivil Comments',\n",
       "   'tldr': 'The definitions of abusive, offensive, toxic and uncivil comments used for annotating corpora for automated content moderation are highly intersected and researchers call for their disambiguation. We summarize the definitions of these terms as they appear in 23 papers across different fields. \\nWe co',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_54': {'abstract': 'We analyze homotransphobic content on Twitter across seven different languages, highlighting how this pervasive issue manifests in distinct cultural contexts worldwide. By developing a comprehensive taxonomy to classify homotransphobic speech, our study establishes a crucial foundation for building effective models to identify and counter such harmful speech on online platforms.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Davide Locatelli', 'Greta Damo', 'Debora Nozza'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_54',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Non-Archival',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Cross-Lingual Study of Homotransphobia on Twitter',\n",
       "   'tldr': 'We analyze homotransphobic content on Twitter across seven different languages, highlighting how this pervasive issue manifests in distinct cultural contexts worldwide. By developing a comprehensive taxonomy to classify homotransphobic speech, our study establishes a crucial foundation for building ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_55': {'abstract': 'The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable application, thus calling for more systematic studies. In this paper, we explore re-annotation as a means of examining the robustness of already existing labelled datasets, showing that, despite using alternative definitions, the inter-annotator agreement remains very inconsistent, highlighting the intrinsically subjective and variable nature of the task. In addition, we build automatic toxicity detectors using the existing datasets, with their original labels, and we evaluate them on our multi-definition and multi-source datasets. Surprisingly, while other studies show that hate speech detection models perform better on data that are derived from the same distribution as the training set, our analysis demonstrates this is not necessarily true.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Katerina Korre',\n",
       "    'John Pavlopoulos',\n",
       "    'Jeffrey Sorensen',\n",
       "    'Lo Laugier',\n",
       "    'Ion Androutsopoulos',\n",
       "    'Lucas Dixon',\n",
       "    'Alberto Barrn-cedeo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Harmful Language Datasets: An Assessment of Robustness',\n",
       "   'tldr': 'The automated detection of harmful language has been of great importance for the online world, especially with the growing importance of social media and, consequently, polarisation. There are many open challenges to high quality detection of harmful text, from dataset creation to generalisable appl',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_56': {'abstract': 'In this work, we present the results of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages of the Americas. This edition of the shared task featured eleven language pairs, one of which  Chatino-Spanish  uses a newly collected evaluation dataset, consisting of professionally translated text from the legal domain. Seven teams participated in the shared task, with a total of 181 submissions. Additionally, we conduct a human evaluation of the best system outputs, and compare them to the best submissions from the prior shared task. We find that this analysis agrees with the quantitative measures used to rank submissions, which shows further improvements of 9.64 ChrF on average across all languages, when compared to the prior winning system.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Abteen Ebrahimi',\n",
       "    'Manuel Mager',\n",
       "    'Shruti Rijhwani',\n",
       "    'Enora Rice',\n",
       "    'Arturo Oncevay',\n",
       "    'Claudia Baltazar',\n",
       "    'Mara Corts',\n",
       "    'Cynthia Montao',\n",
       "    'John E. Ortega',\n",
       "    'Rolando Coto-solano',\n",
       "    'Hilaria Cruz',\n",
       "    'Alexis Palmer',\n",
       "    'Katharina Kann'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['AmericasNLP'],\n",
       "   'id': 'ACL_56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long Paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Findings of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages',\n",
       "   'tldr': 'In this work, we present the results of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages of the Americas. This edition of the shared task featured eleven language pairs, one of which  Chatino-Spanish  uses a newly collected evaluation dataset, consisting of profess',\n",
       "   'track': 'Third Workshop on Natural Language Processing for Indigenous Languages of the Americas',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_57': {'abstract': 'The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inherently learn from these dataset-specific biases. In this paper, we perform a large-scale cross-dataset comparison where we fine-tune language models on different hate speech detection datasets. This analysis shows how some datasets are more generalizable than others when used as training data. Crucially, our experiments show how combining hate speech detection datasets can contribute to the development of robust hate speech detection models. This robustness holds even when controlling by data size and compared with the best individual datasets.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dimosthenis Antypas', 'Jose Camacho-Collados'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_57',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation',\n",
       "   'tldr': 'The automatic detection of hate speech online is an active research area in NLP. Most of the studies to date are based on social media datasets that contribute to the creation of hate speech detection models trained on them. However, data creation processes contain their own biases, and models inher',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_58': {'abstract': 'Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic metaphors. This is a challenging task for diffusion-based text-to-image models, such as DALL-E-2, since it requires the ability to model implicit meaning and compositionality. We propose to solve the task through the collaboration between Large Language Models and Diffusion Models. We use GPT-3 with Chain-of-Thought prompting to generate text that represents a visual elaboration of the linguistic metaphor, containing the implicit meaning and relevant objects, which is then used as input to the diffusion-based text-to-image models. Using a human-AI collaboration framework, where humans interact both with the LLM and the top-performing diffusion model, we create a high-quality dataset containing 6,476 visual metaphors. Evaluation by professional illustrators show the promise of LLM-Diffusion Model collaboration for this task. We also perform an intrinsic and an extrinsic evaluation using a downstream task: visual entailment. Fine-tuning a state-of-the-art vision-language model on our dataset leads to 23-point improvement in accuracy compared to its performance when finetuned on SNLI-VE, a large-scale visual entailment dataset.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tuhin Chakrabarty',\n",
       "    'Arkadiy Saakyan',\n",
       "    'Olivia Winn',\n",
       "    'Artemis Panagopoulou',\n",
       "    'Yue Yang',\n",
       "    'Marianna Apidianaki',\n",
       "    'Smaranda Muresan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_58',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors',\n",
       "   'tldr': 'Visual metaphors are powerful rhetorical devices used to persuade or communicate creative ideas through images. Similar to linguistic metaphors, they convey meaning implicitly through symbolism and juxtaposition of the symbols. We propose a new task of generating visual metaphors from linguistic met',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_59': {'abstract': 'Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated\\\\_complementary\\\\_commonsense.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Navid Rezaei', 'Marek Reformat'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_59',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Negated Complementary Commonsense using Large Language Models',\n",
       "   'tldr': 'Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions ad',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_6': {'abstract': 'Adversarial evaluations of language models typically focus on English alone. In this paper, we performed a multilingual evaluation of Named Entity Recognition (NER) in terms of its robustness to small perturbations in the input. Our results showed the NER models we explored across three languages (English, German and Hindi) are not very robust to such changes, as indicated by the fluctuations in the overall F1 score as well as in a more fine-grained evaluation. With that knowledge, we further explored whether it is possible to improve the existing NER models using a part of the generated adversarial data sets as augmented training data to train a new NER model or as fine-tuning data to adapt an existing NER model. Our results showed that both these approaches improve performance on the original as well as adversarial test sets. While there is no significant difference between the two approaches for English, re-training is significantly better than fine-tuning for German and Hindi.  ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Akshay Srinivasan', 'Sowmya Vajjala'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['RepL4NLP'],\n",
       "   'id': 'ACL_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Multilingual Evaluation of NER Robustness to Adversarial Inputs',\n",
       "   'tldr': 'Adversarial evaluations of language models typically focus on English alone. In this paper, we performed a multilingual evaluation of Named Entity Recognition (NER) in terms of its robustness to small perturbations in the input. Our results showed the NER models we explored across three languages (E',\n",
       "   'track': 'The 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_61': {'abstract': 'In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help to reason over the states of involved entities in a procedural text. We consider a deep semantic parser\\\\textasciitilde{}(TRIPS) and semantic role labeling as two sources of semantic parsing knowledge. First, we propose PROPOLIS, a symbolic parsing-based procedural reasoning framework.Second, we integrate semantic parsing information into state-of-the-art neural models to conduct procedural reasoning.Our experiments indicate that explicitly incorporating such semantic knowledge improves procedural understanding. This paper presents new metrics for evaluating procedural reasoning tasks that clarify the challenges and identify differences among neural, symbolic, and integrated models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hossein Rajaby Faghihi',\n",
       "    'Parisa Kordjamshidi',\n",
       "    'Choh Man Teng',\n",
       "    'James Allen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_61',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Role of Semantic Parsing in Understanding Procedural Text',\n",
       "   'tldr': 'In this paper, we investigate whether symbolic semantic representations, extracted from deep semantic parsers, can help to reason over the states of involved entities in a procedural text. We consider a deep semantic parser\\\\textasciitilde{}(TRIPS) and semantic role labeling as two sources of semanti',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_62': {'abstract': 'We study the problem of generating coherent and correct intermediate solution steps for math word problems (MWPs). Solutions to MWPs with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches narrowly focus on obtaining the final correct answer. A few recent approaches leverage intermediate solution steps to improve final answer correctness but often cannot generate coherent steps with a clear solution strategy. Contrary to existing work, we focus on improving the correctness and coherence of the intermediate solutions steps. We propose a step-by-step planning method for intermediate solution generation, which strategically plans the generation of the next solution step based on the MWP and the previous solution steps. Our approach first plans the next step by predicting the necessary math operation needed to proceed given history steps, then generates the next step, token-by-token, by prompting a language model with the predicted math operation. Experiments on the GSM8K dataset demonstrate that our method improves the accuracy and interpretability of the solution by both automatic metrics and human evaluation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mengxue Zhang',\n",
       "    'Zichao Wang',\n",
       "    'Zhichao Yang',\n",
       "    'Weiqi Feng',\n",
       "    'Andrew Lan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_62',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Interpretable Math Word Problem Solution Generation Via Step-by-step Planning',\n",
       "   'tldr': 'We study the problem of generating coherent and correct intermediate solution steps for math word problems (MWPs). Solutions to MWPs with step-by-step explanations are valuable, especially in education, to help students better comprehend problem-solving strategies. Most existing approaches narrowly ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_65': {'abstract': \"Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which prevents the student from ignoring the rationales to make inconsistent predictions. Experiments show that while yielding comparable performance,  our method leads to a more faithful model than baselines. Further analysis shows that such a model respects the rationales more when making decisions; thus, we can improve its performance more by refining its rationales.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Peifeng Wang',\n",
       "    'Zhengyang Wang',\n",
       "    'Zheng Li',\n",
       "    'Yifan Gao',\n",
       "    'Bing Yin',\n",
       "    'Xiang Ren'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_65',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SCOTT: Self-Consistent Chain-of-Thought Distillation',\n",
       "   'tldr': 'Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_66': {'abstract': 'Prompting has been utilized to exploit large language models (LLM) for sequential planning tasks within interactive settings. In this paper, we propose a novel prompting approach, Actor-Summarizer-Hierarchical prompting, for interactive web navigation. Diverging from previous prompting approaches that always put the full state (eg a web page) to the prompt, we propose to first construct an action-aware state which is more condensed and relevant with a dedicated summarizer prompt. The resulting state is concatenated to the summarized history and fed to an actor prompt to predict the next action. This hierarchical mechanism is especially useful since the full state of a step in web navigation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanism with the same LLM by 6.2\\\\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chi-fan Lo',\n",
       "    'Abishek Sridhar',\n",
       "    'Hao Zhu',\n",
       "    'Frank F. Xu',\n",
       "    'Shuyan Zhou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_66',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Hierarchical Prompting Assists Large Language Model on Web Navigation',\n",
       "   'tldr': 'Prompting has been utilized to exploit large language models (LLM) for sequential planning tasks within interactive settings. In this paper, we propose a novel prompting approach, Actor-Summarizer-Hierarchical prompting, for interactive web navigation. Diverging from previous prompting approaches th',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_67': {'abstract': 'Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jie Huang', 'Kevin Chen-chuan Chang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_67',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Reasoning in Large Language Models: Survey, Implication, and Reflection',\n",
       "   'tldr': 'Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_68': {'abstract': \"Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such texts may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework that leverages external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. It then uses this knowledge to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show that FARM obtains state-of-the-art results on the SafeText dataset, showing absolute improvement in safety classification accuracy by 5.9 points.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Alex Mei', 'Sharon Levy', 'William Yang Wang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_68',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI',\n",
       "   'tldr': \"Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such texts may arise from everyday sc\",\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_7': {'abstract': 'Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community produces a wide range of novel identity terms and, while terms for women are most common, mentions of other minoritized identities are increasing. An analysis of the associations made with identity groups suggests an essentialist ideology where physical appearance, as well as gender and racial hierarchies, determine human value. We discuss implications for research into automated misogynist hate speech detection.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Michael Yoder',\n",
       "    'Chloe Perry',\n",
       "    'David Brown',\n",
       "    'Kathleen Carley',\n",
       "    'Meredith Pruden'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Identity Construction in a Misogynist Incels Forum',\n",
       "   'tldr': 'Online communities of involuntary celibates (incels) are a prominent source of misogynist hate speech. In this paper, we use quantitative text and network analysis approaches to examine how identity groups are discussed on incels.is, the largest black-pilled incels forum. We find that this community',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_71': {'abstract': 'Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that self-debugging can teach the large language model to perform rubber duck debugging; i.e., without any feedback on the code correctness or error messages, the model is able to identify its mistakes by explaining the generated code in natural language. Self-debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, self-debugging with code explanation consistently improves the baseline by 2-3\\\\%, and improves the prediction accuracy on problems of the hardest label by 9\\\\%\\\\$. On TransCoder and MBPP where unit tests are available, self-debugging can improve the baseline accuracy by 12\\\\%. Meanwhile, by leveraging feedback messages and reusing failed predictions, self-debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xinyun Chen',\n",
       "    'Maxwell Lin',\n",
       "    'Nathanael Schaerli',\n",
       "    'Denny Zhou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_71',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Teaching Large Language Models to Self-Debug',\n",
       "   'tldr': 'Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program vi',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_72': {'abstract': 'Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers are all in a single paragraph. By contrast, many natural questions, such as \"What players were drafted by the Brooklyn Nets? have a long list of answers extracted from multiple paragraphs. Answering such questions requires retrieving and reading many passages from a large corpus. We introduce QAMPARI, an ODQA benchmark, where answers are lists of entities, spread across many paragraphs. We created QAMPARI by (a) generating questions with multiple answers from Wikipedia\\'s knowledge graph and tables, (b) automatically pairing answers with supporting evidence in Wikipedia paragraphs, and (c) manually paraphrasing questions and validating each answer. Across a wide range of ODQA models, we find that QAMPARI is challenging in terms of both passage retrieval and answer generation, with models reaching an F1 score of 32.8 at best. We view QAMPARI as a valuable resource for ODQA research, which will aid to develop models that handle a broad range of question types, including single and multianswer questions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Samuel Amouyal',\n",
       "    'Tomer Wolfson',\n",
       "    'Ohad Rubin',\n",
       "    'Ori Yoran',\n",
       "    'Jonathan Herzig',\n",
       "    'Jonathan Berant'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_72',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'QAMPARI: A Benchmark for Open-domain Questions with Many Answers',\n",
       "   'tldr': 'Existing benchmarks for open-domain question answering (ODQA) typically focus on questions whose answers are all in a single paragraph. By contrast, many natural questions, such as \"What players were drafted by the Brooklyn Nets? have a long list of answers extracted from multiple paragraphs. Answer',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_73': {'abstract': \"Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the user's question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48\\\\% in average, across multiple LLMs of various sizes.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jinheon Baek', 'Alham Fikri Aji', 'Amir Saffari'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_73',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering',\n",
       "   'tldr': 'Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wro',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_74': {'abstract': \"A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so that it can retrieve different documents at each hop. However, such a bi-encoder approach has limitations in multi-hop settings; (1) the reformulated query gets longer as the number of hops increases, which further tightens the embedding bottleneck of the query vector, and (2) it is prone to error propagation. In this paper, we focus on alleviating these limitations in multi-hop settings by formulating the problem in a fully generative way. We propose an encoder-decoder model that performs multi-hop retrieval by simply generating the entire text sequences of the retrieval targets, which means the query and the documents interact in the language model's parametric space rather than L2 or inner product space as in the bi-encoder approach. Our approach, Generative Multi-hop Retrieval (GMR), consistently achieves comparable or higher performance than bi-encoder models in five datasets while demonstrating superior GPU memory and storage footprint.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hyunji Lee', 'Sohee Yang', 'Hanseok Oh', 'Minjoon Seo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_74',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generative Multi-hop Retrieval',\n",
       "   'tldr': 'A common practice for text retrieval is to use an encoder to map the documents and the query to a common vector space and perform a nearest neighbor search (NNS); multi-hop retrieval also often adopts the same paradigm, usually with a modification of iteratively reformulating the query vector so tha',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_75': {'abstract': 'Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Li Zhang',\n",
       "    'Hainiu Xu',\n",
       "    'Yue Yang',\n",
       "    'Shuyan Zhou',\n",
       "    'Weiqiu You',\n",
       "    'Manni Arora',\n",
       "    'Chris Callison-burch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_75',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Causal Reasoning of Entities and Events in Procedural Texts',\n",
       "   'tldr': 'Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_76': {'abstract': \"Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations.We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Michal Tefnik', 'Marek Kadlcik'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_76',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Can In-context Learners Learn a Reasoning Concept from Demonstrations?',\n",
       "   'tldr': 'Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_77': {'abstract': 'When people answer questions about a specific situation, e.g., \"I cheated on my mid-term exam last week. Was that wrong?, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture that they may answer more accurately if they are also provided with additional details about the question situation, elaborating the \"scene. To test this conjecture, we train a new model, DREAM, to answer questions that elaborate the scenes that situated questions are about, and then provide those elaborations as additional context to a question-answering (QA) model. We find that DREAM is able to create better scene elaborations (more accurate, useful, and consistent) than a representative state-of-the-art, zero-shot model (Macaw). We also find that using the scene elaborations as additional context improves the answer accuracy of a downstream QA system, including beyond that obtainable by simply further fine-tuning the QA system on DREAM\\'s training data. These results suggest that adding focused elaborations about a situation can improve a system\\'s reasoning about it, and may serve as an effective way of injecting new scenario-based knowledge into QA models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuling Gu', 'Bhavana Dalvi Mishra', 'Peter Clark'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_77',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DREAM: Improving Situational QA by First Elaborating the Situation',\n",
       "   'tldr': 'When people answer questions about a specific situation, e.g., \"I cheated on my mid-term exam last week. Was that wrong?, cognitive science suggests that they form a mental picture of that situation before answering. While we do not know how language models (LMs) answer such questions, we conjecture',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_78': {'abstract': 'Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80\\\\% of available data on a benchmark task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cheng-yu Hsieh',\n",
       "    'Chun-liang Li',\n",
       "    'Chih-kuan Yeh',\n",
       "    'Hootan Nakhost',\n",
       "    'Yasuhisa Fujii',\n",
       "    'Alex Ratner',\n",
       "    'Ranjay Krishna',\n",
       "    'Chen-yu Lee',\n",
       "    'Tomas Pfister'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_78',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes',\n",
       "   'tldr': 'Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_79': {'abstract': 'Argumentation is an important means of communication. For describing especially arguments about consequences, the notion of effect relations has been introduced recently. We propose a method to extract effect relations from large text resources and apply it on encyclopedic and argumentative texts. By connecting the extracted relations, we generate a knowledge graph which we call effect graph. For evaluating the effect graph, we perform crowd and expert annotations and create a novel dataset. We demonstrate a possible use case of the effect graph by proposing a method for explaining arguments from consequences.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jonathan Kobbe', 'Ioana Hulpu', 'Heiner Stuckenschmidt'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_79',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Effect Graph: Effect Relation Extraction for Explanation Generation',\n",
       "   'tldr': 'Argumentation is an important means of communication. For describing especially arguments about consequences, the notion of effect relations has been introduced recently. We propose a method to extract effect relations from large text resources and apply it on encyclopedic and argumentative texts. B',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_80': {'abstract': \"In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to the \\\\textless{}problem, solution\\\\textgreater{} pairs. We show that despite poor performance in solving competitive-level programming problems, state-of-the-art LLMs exhibit a strong capacity in describing and explaining their solutions. Our explanation generation methodology can generate a structured solution explanation for the problem while containing the description and analysis. To evaluate the quality of the annotated explanations, we examine their effectiveness in two aspects: 1) satisfying the human programming expert who authored the oracle solution, and 2) aiding LLMs in solving problems more effectively. The experimental results on the CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities in describing the solution are comparable, GPT-4 shows a better understanding of the key idea behind the solution.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jierui Li',\n",
       "    'Szymon Tworkowski',\n",
       "    'Yingying Wu',\n",
       "    'Raymond Mooney'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_80',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Explaining Competitive-Level Programming Solutions using LLMs',\n",
       "   'tldr': 'In this paper, we approach competitive-level programming problem-solving as a composite task of reasoning and code generation. We propose a novel method to automatically annotate natural language explanations to the \\\\textless{}problem, solution\\\\textgreater{} pairs. We show that despite poor performa',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_82': {'abstract': \"We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4\\\\%) and Analogical (+13.9\\\\%) reasoning, as well as skills that exhibit negligible or negative effects.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Badr Alkhamissi',\n",
       "    'Siddharth Verma',\n",
       "    'Ping Yu',\n",
       "    'Zhijing Jin',\n",
       "    'Asli Celikyilmaz',\n",
       "    'Mona Diab'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_82',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models',\n",
       "   'tldr': 'We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning c',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_83': {'abstract': 'Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pretrained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, R ETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO ++, which largely improves the open-domain QA results of the original RETRO and significantly outperforms retrieval-augmented GPT across different model sizes. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Boxin Wang',\n",
       "    'Wei Ping',\n",
       "    'Peng Xu',\n",
       "    'Lawrence Mcafee',\n",
       "    'Zihan Liu',\n",
       "    'Mohammad Shoeybi',\n",
       "    'Yi Dong',\n",
       "    'Oleksii Kuchaiev',\n",
       "    'Bo Li',\n",
       "    'Chaowei Xiao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_83',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study',\n",
       "   'tldr': 'Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To ans',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_87': {'abstract': 'Human language richly invokes our intuitive physical knowledge. We talk about physical objects, scenes, properties, and events; and we can make predictions and draw inferences about physical worlds described entirely in language. Understanding this everyday language requires inherently probabilistic reasoningover possible physical worlds invoked in language and over uncertainty inherent to those physical worlds. In this paper, we propose PiLoT, a neurosymbolic generative model that translates language into probabilistic programs grounded in a physics engine. Our model integrates a large language model to robustly parse language into program expressions and uses a probabilistic physics engine to support inferences over scenes described in language. We construct a linguistic reasoning benchmark based on prior psychophysics experiments that requires reasoning about physical outcomes based on linguistic scene descriptions. We show that PiLoT well predicts human judgments and outperforms baseline large language models across this battery of tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cedegao Zhang',\n",
       "    'Lionel Wong',\n",
       "    'Gabriel Grand',\n",
       "    'Josh Tenenbaum'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_87',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Grounded physical language understanding with probabilistic programs and simulated worlds',\n",
       "   'tldr': 'Human language richly invokes our intuitive physical knowledge. We talk about physical objects, scenes, properties, and events; and we can make predictions and draw inferences about physical worlds described entirely in language. Understanding this everyday language requires inherently probabilistic',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_90': {'abstract': 'Most benchmarks for question answering on knowledge bases (KBQA) operate with the i.i.d. assumption. Recently, the GrailQA dataset was established to evaluate  zero-shot generalization capabilities of KBQA models. Reasonable performance of current KBQA systems on the zero-shot GrailQA split hints that the field might be moving towards more generalizable systems. In this work, we observe a bias in the GrailQA dataset towards simpler one or two-hop questions which results in an inaccurate assessment of the aforementioned prowess. We propose GrailQA++, a challenging zero-shot KBQA test set that contains a larger number of questions relying on complex reasoning.  We leverage the concept of reasoning paths to control the complexity of the questions and to ensure that our proposed test set has a fair distribution of simple and complex questions. Evaluating existing KBQA models on this new test set shows that they suffer a substantial drop in performance as compared to the GrailQA zero-shot split. This highlights the non-generalizability of existing models and the necessity for harder benchmarks. Our analysis reveals how reasoning paths can be used to understand complementary strengths of different KBQA models, and provide a deeper insight into model mispredictions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ritam Dutt',\n",
       "    'Sopan Khosla',\n",
       "    'Vinayshekhar Bannihatti Kumar',\n",
       "    'Rashmi Gangadharaiah'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_90',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Designing harder benchmarks for evaluating zero-shot generalizability in Question Answering over Knowledge Bases',\n",
       "   'tldr': 'Most benchmarks for question answering on knowledge bases (KBQA) operate with the i.i.d. assumption. Recently, the GrailQA dataset was established to evaluate  zero-shot generalization capabilities of KBQA models. Reasonable performance of current KBQA systems on the zero-shot GrailQA split hints th',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_91': {'abstract': 'Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity holds, and extrinsically, using them to assist planning in natural language proof generation. Lastly, we create a dataset, Single-Step Reasoning Contrast (SSRC), to further probe performance on various reasoning types. Our findings suggest that while standard embedding methods frequently embed conclusions near the sums of their premises, they fall short of being effective heuristics and lack the ability to model certain categories of reasoning.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zayne Sprague',\n",
       "    'Kaj Bostrom',\n",
       "    'Swarat Chaudhuri',\n",
       "    'Greg Durrett'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_91',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Deductive Additivity for Planning of Natural Language Proofs',\n",
       "   'tldr': 'Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step ofte',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_92': {'abstract': 'The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhivar Sourati',\n",
       "    'Filip Ilievski',\n",
       "    'Hng-n Sandlin',\n",
       "    'Alain Mermoud'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_92',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Case-Based Reasoning with Language Models for Classification of Logical Fallacies',\n",
       "   'tldr': 'The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classific',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_93': {'abstract': 'Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiao Liu',\n",
       "    'Da Yin',\n",
       "    'Chen Zhang',\n",
       "    'Yansong Feng',\n",
       "    'Dongyan Zhao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_93',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code',\n",
       "   'tldr': 'Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given th',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_95': {'abstract': \"It has been suggested in literature that large pre-trained language models (PLMs) are able to suppress human-level performance for natural language inference (NLI) tasks. However, the failure of learning the underlying generalizations and the inconsistency to small textual perturbations rise doubt about whether models rely on adopting shallow heuristics to guess the correct label. To mitigate this issue, we propose a neural-symbolic contrastive learning framework inspired by Inductive Logic Programming (ILP) to better capture logical relationships from data. Unlike the usual methods for NLI tasks, our approach represents data as logic programs, sets of logic rules. We aim to learn an embedding space in which  the examples share as various as possible textual information with as similar as possible underlying logical meanings that are close together, and vice versa. Experimental results affirm this approach's ability to enhance the model's transferability performance.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mingyue Liu',\n",
       "    'Jialin Yu',\n",
       "    'Hao Cui',\n",
       "    'Sara Uckelman',\n",
       "    'Yang Long'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_95',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Neural-symbolic Contrastive Learning for Cross-domain Inference',\n",
       "   'tldr': 'It has been suggested in literature that large pre-trained language models (PLMs) are able to suppress human-level performance for natural language inference (NLI) tasks. However, the failure of learning the underlying generalizations and the inconsistency to small textual perturbations rise doubt a',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_97': {'abstract': 'We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from the SICK dataset (Marelli et al., 2014). To this end, we modify the original texts using a set of phrases  modifiers that correspond to universal quantifiers, existential quantifiers, negation, and other concept modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to modify the subject, verb, and object parts of the premise and hypothesis. Lastly, we annotate these modified texts with the corresponding entailment labels following NL rules. We conduct a preliminary verification of how well the change in the structural and semantic composition is captured by neural NLI models, in both zero-shot and fine-tuned scenarios. We found that the performance of NLI models under the zero-shot setting is poor, especially for modified sentences with negation and existential quantifiers. After fine-tuning this dataset, we observe that models continue to perform poorly over negation, existential and universal modifiers.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sushma Anand Akoju',\n",
       "    'Robert Vacareanu',\n",
       "    'Eduardo Blanco',\n",
       "    'Haris Riaz',\n",
       "    'Mihai Surdeanu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_97',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Synthetic Dataset for Evaluating Complex Compositional Knowledge for Natural Language Inference',\n",
       "   'tldr': 'We introduce a synthetic dataset called Sentences Involving Complex Compositional Knowledge (SICCK) and a novel analysis that investigates the performance of Natural Language Inference (NLI) models to understand compositionality in logic. We produce 1,304 sentence pairs by modifying 15 examples from',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_98': {'abstract': 'We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the question are used to produce intermediate conclusions that can prove the correctness of a certain answer. We perform extensive evaluation with popular language models such as few-shot prompting GPT-3 and fine-tuned T5. We find that these models still lag behind human performance when producing such structured reasoning steps. We believe this work will provide a way for the community to better train and test systems on multi-step reasoning and explanations in natural language.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Danilo Neves Ribeiro',\n",
       "    'Shen Wang',\n",
       "    'Xiaofei Ma',\n",
       "    'Henghui Zhu',\n",
       "    'Rui Dong',\n",
       "    'Deguang Kong',\n",
       "    'Juliette Burger',\n",
       "    'Anjelica Ramos',\n",
       "    'William Yang Wang',\n",
       "    'Zhiheng Huang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_98',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'STREET: A Multi-Task Structured Reasoning and Explanation Benchmark',\n",
       "   'tldr': 'We introduce STREET, a unified multi-task and multi-domain natural language reasoning and explanation benchmark. Unlike most existing question-answering (QA) datasets, we expect models to not only answer questions, but also produce step-by-step structured explanations describing how premises in the ',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_99': {'abstract': 'Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xi Ye',\n",
       "    'Srinivasan Iyer',\n",
       "    'Asli Celikyilmaz',\n",
       "    'Veselin Stoyanov',\n",
       "    'Greg Durrett',\n",
       "    'Ramakanth Pasunuru'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLRSE'],\n",
       "   'id': 'ACL_99',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Complementary Explanations for Effective In-Context Learning',\n",
       "   'tldr': 'Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are us',\n",
       "   'track': '1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F1': {'abstract': \"Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader's perception of who is to blame for the violence, possibly reinforcing stereotypes which see the victim as partly responsible, too. As a contribution to raise awareness on perspective-based writing, and to facilitate access to alternative perspectives, we introduce the novel task of automatically rewriting GBV descriptions as a means to alter the perceived level of blame on the perpetrator. We present a quasi-parallel dataset of sentences with low and high perceived responsibility levels for the perpetrator, and experiment with unsupervised (mBART-based), zero-shot and few-shot (GPT3-based) methods for rewriting sentences. We evaluate our models using a questionnaire study and a suite of automatic metrics.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gosse Minnema',\n",
       "    'Huiyuan Lai',\n",
       "    'Benedetta Muscato',\n",
       "    'Malvina Nissim'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Responsibility Perspective Transfer for Italian Femicide News',\n",
       "   'tldr': \"Different ways of linguistically expressing the same real-world event can lead to different perceptions of what happened. Previous work has shown that different descriptions of gender-based violence (GBV) influence the reader's perception of who is to blame for the violence, possibly reinforcing ste\",\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F10': {'abstract': \"Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers and increases its internet frequency. While maintaining publications'' professional appearance, obfuscating profanities makes it challenging to evaluate the content, especially for non-native speakers.\\nSurveying 150 ACL papers, we discovered that obfuscation is usually used for English but not other languages, and even then, quite unevenly.\\nWe discuss the problems with obfuscation and suggest a multilingual community resource called PrOf with a Python module to standardize profanity obfuscation processes. We believe PrOf can help scientific publication policies to make hate speech work accessible and comparable, irrespective of language.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Debora Nozza', 'Dirk Hovy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] The State of Profanity Obfuscation in Natural Language Processing Scientific Publications',\n",
       "   'tldr': 'Work on hate speech has made considering rude and harmful examples in scientific publications inevitable. This situation raises various problems, such as whether or not to obscure profanities. While science must accurately disclose what it does, the unwarranted spread of hate speech can harm readers',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F11': {'abstract': 'Warning: This paper contains content that may be offensive or upsetting. \\nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. \\n\\nWe introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. \\n\\nTo study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement\\'s offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xuhui Zhou',\n",
       "    'Hao Zhu',\n",
       "    'Akhila Yerukola',\n",
       "    'Thomas Davidson',\n",
       "    'Jena D. Hwang',\n",
       "    'Swabha Swayamdipta',\n",
       "    'Maarten Sap'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements',\n",
       "   'tldr': 'Warning: This paper contains content that may be offensive or upsetting. \\nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good may implicitly signal an ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F12': {'abstract': \"Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this work, we are the first to investigate how multimodal models handle diverse gender identities. Concretely, we conduct a thorough analysis in which we compare the output of three image generation models for prompts containing cisgender vs. non-cisgender identity terms. Our findings demonstrate that certain non-cisgender identities are consistently (mis)represented as less human, more stereotyped and more sexualised. We complement our experimental analysis with (a) a survey among non-cisgender individuals and (b) a series of interviews, to establish which harms affected individuals anticipate, and how they would like to be represented. We find respondents are particularly concerned about misrepresentation, and the potential to drive harmful behaviours and beliefs. Simple heuristics to limit offensive content are widely rejected, and instead respondents call for community involvement, curated training data and the ability to customise. These improvements could pave the way for a future where change is led by the affected community, and technology is used to positively ''[portray] queerness in ways that we haven't even thought of''' rather than reproducing stale, offensive stereotypes.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Eddie Ungless', 'Bjorn Ross', 'Anne Lauscher'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Stereotypes and Smut: The (Mis)representation of Non-cisgender Identities by Text-to-Image Models',\n",
       "   'tldr': 'Cutting-edge image generation has been praised for producing high-quality images, suggesting a ubiquitous future in a variety of applications. However, initial studies have pointed to the potential for harm due to predictive bias, reflecting and potentially reinforcing cultural stereotypes. In this ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F2': {'abstract': 'The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant attention due to the growing importance of scientific and health discussions on online platforms. Automated scientific fact-checking methods based on NLP can help combat the spread of misinformation, assist researchers in knowledge discovery, and help individuals understand new scientific breakthroughs. In this paper, we present a comprehensive survey of existing research in this emerging field and its related tasks. We provide a task description, discuss the construction process of existing datasets, and analyze proposed models and approaches. Based on our findings, we identify intriguing challenges and outline potential future directions to advance the field.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Juraj Vladika', 'Florian Matthes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Scientific Fact-Checking: A Survey of Resources and Approaches',\n",
       "   'tldr': 'The task of fact-checking deals with assessing the veracity of factual claims based on credible evidence and background knowledge. In particular, scientific fact-checking is the variation of the task concerned with verifying claims rooted in scientific knowledge. This task has received significant a',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F3': {'abstract': 'The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP can support such efforts by quickly processing large collections of news articles to detect and summarize the characteristics of attacks on human rights defenders. To that end, we propose a new dataset for detecting Attacks on Human Rights Defenders (HRDsAttack) consisting of crowdsourced annotations on 500 online news articles. The annotations include fine-grained information about the type and location of the attacks, as well as information about the victim(s). We demonstrate the usefulness of the dataset by using it to train and evaluate baseline models on several sub-tasks to predict the annotated characteristics.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shihao Ran',\n",
       "    'Di Lu',\n",
       "    'Aoife Cahill',\n",
       "    'Joel Tetreault',\n",
       "    'Alejandro Jaimes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] A New Task and Dataset on Detecting Attacks on Human Rights Defenders',\n",
       "   'tldr': 'The ability to conduct retrospective analyses of attacks on human rights defenders over time and by location is important for humanitarian organizations to better understand historical or ongoing human rights violations and thus better manage the global impact of such events. We hypothesize that NLP',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F4': {'abstract': \"With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially on contentious political or economic issues. Our underlying assumption is that among the trusted sources, one's argument is not necessarily more true than the other, requiring comparison rather than verification. In this study, we propose ClaimDIff, a novel dataset that primarily focuses on comparing the nuance between claim pairs. In ClaimDiff, we provide human-labeled 2,941 claim pairs from 268 news articles. We observe that while humans are capable of detecting the nuances between claims, strong baselines struggle to detect them, showing over a 19% absolute gap with the humans. We hope this initial study could help readers to gain an unbiased grasp of contentious issues through machine-aided comparison.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Miyoung Ko',\n",
       "    'Ingyu Seong',\n",
       "    'Hwaran Lee',\n",
       "    'Joonsuk Park',\n",
       "    'Minsuk Chang',\n",
       "    'Minjoon Seo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] ClaimDiff: Comparing and Contrasting Claims on Contentious Issues',\n",
       "   'tldr': 'With the growing importance of detecting misinformation, many studies have focused on verifying factual claims by retrieving evidence. However, canonical fact verification tasks do not apply to catching subtle differences in factually consistent claims, which might still bias the readers, especially',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F5': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Connor Baumler', 'Anna Sotnikova', 'Hal Daum III'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree',\n",
       "   'tldr': '',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F6': {'abstract': \"Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge for automatic hate speech detection. In this paper, we propose a new framework for generating adversarial implicit HS short-text messages using Auto-regressive Language Models. Moreover, we propose a strategy to group the generated implicit messages in complexity levels (EASY, MEDIUM, and HARD categories) characterizing how challenging these messages are for supervised classifiers. Finally, relying on (Dinan et al., 2019; Vidgen et al., 2021), we propose a ``build it, break it, fix it'', training scheme using HARD messages showing how iteratively retraining on HARD messages substantially leverages SOTA models' performances on implicit HS benchmarks.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nicolas Ocampo', 'Elena Cabrio', 'Serena Villata'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Playing the Part of the Sharp Bully: Generating Adversarial Examples for Implicit Hate Speech Detection',\n",
       "   'tldr': 'Research on abusive content detection on social media has primarily focused on explicit forms of hate speech (HS), that are often identifiable by recognizing hateful words and expressions. Messages containing linguistically subtle and implicit forms of hate speech still constitute an open challenge ',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F7': {'abstract': \"Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard ``ground truth'' as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution. The model takes annotator-item pairs, rather than items alone, as input, and performs inference by aggregating over all annotators. Despite its simplicity, our experiments show that, on six benchmark datasets, our model is competitive with, and frequently outperforms, other, more complex models that either do not model specific annotators or were not designed for label distribution learning.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tharindu Cyril Weerasooriya',\n",
       "    'Alexander Ororbia',\n",
       "    'Raj Bhensadadia',\n",
       "    'Ashiqur KhudaBukhsh',\n",
       "    'Christopher Homan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo',\n",
       "   'tldr': \"Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard ``ground truth'' as the distribution of all labels that a populati\",\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F8': {'abstract': \"We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex education videos on TikTok. Children's exposure to sexually suggestive videos has been shown to have adversarial effects on their development (Collins et al. 2017). Meanwhile, virtual sex education, especially on subjects that are more relevant to the LGBTQIA+ community, is very valuable (Mitchell et al. 2014). The platform's current system removes/punishes some of both types of videos, even though they serve different purposes. Our dataset contains video URLs, and it is also audio transcribed. To validate its importance, we explore two transformer-based models for classifying the videos. Our preliminary results suggest that the task of distinguishing between these types of videos is learnable but challenging. These experiments suggest that this dataset is meaningful and invites further study on the subject.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Enfa George', 'Mihai Surdeanu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Its not Sexually Suggestive; Its Educative | Separating Sex Education from Suggestive Content on TikTok videos',\n",
       "   'tldr': \"We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled as sexually suggestive (from the annotator's point of view), sex-educational content, or neither. Such a dataset is necessary to address the challenge of distinguishing between sexually suggestive content and virtual sex ed\",\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_F9': {'abstract': 'Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to  one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Robert Morabito', 'Jad Kabbara', 'Ali Emami'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['WOAH'],\n",
       "   'id': 'ACL_F9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Findings] Debiasing should be Good and Bad',\n",
       "   'tldr': 'Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent',\n",
       "   'track': 'The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_G2P': {'abstract': 'Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The third iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous years task (Ashby et al., 2021), including additional languages, three subtasks varying the amount of available resources, extensive quality assurance procedures, and automated error analyses. Three teams submitted a total of fifteen systems, at best achieving relative reductions of word error rate of 14% in the crosslingual subtask and 14% in the very-low resource subtask. The generally consistent result is that cross-lingual transfer substantially helps grapheme-to-phoneme modeling, but not to the same degree as in-language examples.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Arya D. McCarthy',\n",
       "    'Jackson L. Lee',\n",
       "    'Alexandra DeLucia',\n",
       "    'Travis Bartley',\n",
       "    'Milind Agarwal',\n",
       "    'Lucas F.E. Ashby',\n",
       "    'Luca Del Signore',\n",
       "    'Cameron Gibson',\n",
       "    'Reuben Raff',\n",
       "    'Winston Wu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_G2P',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The SIGMORPHON 2022 Shared Task on Cross-lingual and Low-Resource Grapheme-to-Phoneme Conversion',\n",
       "   'tldr': 'Grapheme-to-phoneme conversion is an important component in many speech technologies, but until recently there were no multilingual benchmarks for this task. The third iteration of the SIGMORPHON shared task on multilingual grapheme-to-phoneme conversion features many improvements from the previous ',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_G2P1': {'abstract': 'This paper describes our participation in the Third SIGMORPHON Shared Task on Grapheme-to-Phoneme Conversion (Low-Resource and Cross-Lingual) (McCarthy et al.,2022). Our models rely on different sequence labelling methods. The main model predicts multiple phonemes from each grapheme and is trained using CTC loss (Graves et al., 2006). We find that sequence labelling methods yield worse performance than the baseline when enough data is available, but can still be used when very little data is available. Furthermore, we demonstrate that alignments learned by the sequence labelling models can be easily inspected.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Leander Girrbach'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_G2P1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SIGMORPHON 2022 Shared Task on Grapheme-to-Phoneme Conversion Submission Description: Sequence Labelling for G2P',\n",
       "   'tldr': 'This paper describes our participation in the Third SIGMORPHON Shared Task on Grapheme-to-Phoneme Conversion (Low-Resource and Cross-Lingual) (McCarthy et al.,2022). Our models rely on different sequence labelling methods. The main model predicts multiple phonemes from each grapheme and is trained u',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_G2P2': {'abstract': 'In this paper we explore a very simple nonneural approach to mapping orthography to phonetic transcription in a low-resource context with transfer data from a related language. We start from a baseline system and focus our efforts on data augmentation. We make three principal moves. First, we start with an HMMbased system (Novak et al., 2012). Second, we augment our basic system by recombining legal substrings in restricted fashion (Ryan and Hulden, 2020). Finally, we limit our transfer data by only using training pairs where the phonetic form shares all bigrams with the target language.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Michael Hammond'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_G2P2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Low-resource grapheme-to-phoneme mapping with phonetically-conditioned transfer',\n",
       "   'tldr': 'In this paper we explore a very simple nonneural approach to mapping orthography to phonetic transcription in a low-resource context with transfer data from a related language. We start from a baseline system and focus our efforts on data augmentation. We make three principal moves. First, we start ',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_G2P3': {'abstract': 'We propose a universal grapheme-phoneme transduction model using neuralized finite-state transducers.  Many computational models of grapheme-phoneme transduction nowadays are based on the (autoregressive) sequence-to-sequence string transduction paradigm. While such models have achieved state-of-the-art performance, they suffer from theoretical limitations of autoregressive models. On the other hand, neuralized finite-state transducers (NFSTs) have shown promising results on various string transduction tasks. NFSTs can be seen as a generalization of weighted finite-state transducers (WFSTs), and can be seen as pairs of a featurized finite-state machine (marked finite-state transducer or MFST in NFST terminology), and a string scoring function. Instead of taking a product of local contextual feature weights on FST arcs, NFSTs can employ arbitrary scoring functions to weight global contextual features of a string transduction, and therefore break the Markov property. Furthermore, NFSTs can be formally shown to be more expressive than (autoregressive) seq2seq models. Empirically, joint grapheme-phoneme transduction NFSTs have consistently outperformed vanilla seq2seq models on grapheme-tophoneme and phoneme-to-grapheme transduction tasks for English. Furthermore, they provide interpretable aligned string transductions, thanks to their finite-state machine component. In this talk, we propose a multilingual extension of the joint grapheme-phoneme NFST. We achieve this goal by modeling typological and phylogenetic features of languages and scripts as optional latent variables using a finite-state machine. The result is a versatile graphemephoneme transduction model: in addition to standard monolingual and multilingual transduction, the proposed multilingual NFST can also be used in various controlled generation scenarios, such as phoneme-to-grapheme transduction of an unseen language-script pair. We also plan to release an NFST software package.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chu-Cheng Lin Lin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_G2P3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A future for universal grapheme-phoneme transduction modeling with neuralized finite-state transducers',\n",
       "   'tldr': 'We propose a universal grapheme-phoneme transduction model using neuralized finite-state transducers.  Many computational models of grapheme-phoneme transduction nowadays are based on the (autoregressive) sequence-to-sequence string transduction paradigm. While such models have achieved state-of-the',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_G2P4': {'abstract': 'Grapheme-to-phoneme (G2P) conversion is a task that is inherently related to both written and spoken language. Therefore, our submission to the G2P shared task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model pretrained simultaneously on text from 101 languages and speech from 51 languages. For fine-tuning a G2P model, we combined mSLAMs text encoder, which uses characters as its input tokens, with an uninitialized single-layer RNN-T decoder (Graves, 2012) whose vocabulary is the set of all 381 phonemes appearing in the shared task data. We took an explicitly multilingual approach to modeling the G2P tasks, fine-tuning and evaluating a single model that covered all the languages in each task, and adding language codes as prefixes to the input strings as a means of specifying the language of each example. Our models perform well in the shared tasks high setting (in which they were trained on 1,000 words from each language), though they do poorly in the low task setting (training on only 100 words from each language). Our models also perform reasonably in the mixed setting (training on 100 words in the target language and 1000 words in a related language), hinting that mSLAMs multilingual pretraining may be enabling useful cross-lingual sharing.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dan Garrette'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_G2P4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Fine-tuning mSLAM for the SIGMORPHON 2022 Shared Task on Grapheme-to-Phoneme Conversion',\n",
       "   'tldr': 'Grapheme-to-phoneme (G2P) conversion is a task that is inherently related to both written and spoken language. Therefore, our submission to the G2P shared task builds off of mSLAM (Bapna et al., 2022), a 600M parameter encoder model pretrained simultaneously on text from 101 languages and speech fro',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_Gloss': {'abstract': 'This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natgu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce closed track and an open track, where participants are allowed to utilize external data resources.  Five teams participated in the shared task. The winning team T-CL achieved a 23.99%-point improvement over a baseline RoBERTa system in the closed track and a 17.42%-point improvement in the open track.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Michael Ginn',\n",
       "    'Sarah Moeller',\n",
       "    'Alexis Palmer',\n",
       "    'Anna Stacey',\n",
       "    'Garrett Nicolai',\n",
       "    'Mans Hulden',\n",
       "    'Miikka Silfverberg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_Gloss',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing',\n",
       "   'tldr': 'This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Natgu, Tsez and Uspanteko. The shared task encompasses two tracks: a resou',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_Inf1': {'abstract': \"The 2023 SIGMORPHONUniMorph shared task on typologically diverse morphological inflection included a wide range of languages: 26 languages from 9 primary language families. The data this year was all lemma-split, to allow testing models' generalization ability, and structured along the new hierarchical schema presented in (Batsuren et al., 2022). The systems submitted this year, 9 in number, showed ingenuity and innovativeness, including hard attention for explainability and bidirectional decoding. Special treatment was also given by many participants to the newly-introduced data in Japanese, due to the high abundance of unseen Kanji characters in its test set.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Omer Goldman',\n",
       "    'Khuyagbaatar Batsuren',\n",
       "    'Salam Khalifa',\n",
       "    'Aryaman Arora',\n",
       "    'Garrett Nicolai',\n",
       "    'Reut Tsarfaty',\n",
       "    'Ekaterina Vylomova'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_Inf1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SIGMORPHONUniMorph 2023 Shared Task 0: Typologically Diverse Morphological Inflection',\n",
       "   'tldr': \"The 2023 SIGMORPHONUniMorph shared task on typologically diverse morphological inflection included a wide range of languages: 26 languages from 9 primary language families. The data this year was all lemma-split, to allow testing models' generalization ability, and structured along the new hierarch\",\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ACL_Inf2': {'abstract': 'This paper summarises data collection and curation for Part 2 of the 2023 SIGMORPHON-UniMorph Shared Task 0, which focused on modeling speaker knowledge and generalization of a pair of interacting phonological processes in Korean. We briefly describe how modeling the generalization task could be of interest to researchers in both Natural Language Processing and linguistics, and then summarise the traditional description of the phonological processes that are at the center of the modeling challenge. We then describe the criteria we used to select and code cases of process application in two Korean speech corpora, which served as the primary learning data. We also report the technical details of the experiment we carried out that served as the primary test data.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Canaan Breiss', 'Jinyoung Jo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['SIGMORPHON'],\n",
       "   'id': 'ACL_Inf2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SIGMORPHONUniMorph 2023 Shared Task 0, Part 2: Cognitively Plausible Morphophonological Generalization in Korean',\n",
       "   'tldr': 'This paper summarises data collection and curation for Part 2 of the 2023 SIGMORPHON-UniMorph Shared Task 0, which focused on modeling speaker knowledge and generalization of a pair of interacting phonological processes in Korean. We briefly describe how modeling the generalization task could be of ',\n",
       "   'track': 'The 20th SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_10': {'abstract': 'The increasing use of AI chatbots as conversation partners for second-language learners highlights the importance of providing effective feedback. To ensure a successful learning experience, it is essential for researchers and practitioners to understand the optimal timing, methods of delivery, and types of feedback that are most beneficial to learners. Synchronous grammar corrective feedback (CF) has been shown to be more effective than asynchronous methods in online writing tasks. Additionally, self-correction by language learners has proven more beneficial than teacher-provided correction, particularly for spoken language skills and non-novice learners. However, existing language-learning AI chatbots often lack synchronous CF and self-correction capabilities. To address this, we propose a synchronous conversational corrective feedback (CCF) method, which allows self-correction and provides metalinguistic explanations (ME). Our study suggests that in chatbot-driven language-learning tools, corrective feedback is more effectively delivered through means other than the social chatbot, such as a GUI interface. Furthermore, we found that guided self-correction offers a superior learning experience compared to providing explicit corrections, particularly for learners with high learning motivation or lower linguistic ability.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kai-Hui Liang',\n",
       "    'Sam Davidson',\n",
       "    'Xun Yuan',\n",
       "    'Shehan Panditharatne',\n",
       "    'Chun-Yen Chen',\n",
       "    'Ryan Shea',\n",
       "    'Derek Pham',\n",
       "    'Yinghua Tan',\n",
       "    'Erik Voss',\n",
       "    'Luke Fryer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ChatBack: Investigating Methods of Providing Grammatical Error Feedback in a GUI-based Language Learning Chatbot',\n",
       "   'tldr': 'The increasing use of AI chatbots as conversation partners for second-language learners highlights the importance of providing effective feedback. To ensure a successful learning experience, it is essential for researchers and practitioners to understand the optimal timing, methods of delivery, and ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_100': {'abstract': 'We study the new problem of automatic question generation (QG) from multi-modal sources containing images and texts, significantly expanding the scope of most of the existing work that focuses exclusively on QG from only textual sources. We propose a simple solution for our new problem, called MultiQG-TI, which enables a text-only question generator to process visual input in addition to textual input. Specifically, we leverage an image-to-text model and an optical character recognition model to obtain the textual description of the image and extract any texts in the image, respectively, and then feed them together with the input texts to the question generator. We only fine-tune the question generator while keeping the other components fixed. On the challenging ScienceQA dataset, we demonstrate that MultiQG-TI significantly outperforms ChatGPT with few-shot prompting, despite having hundred-times less trainable parameters. Additional analyses empirically confirm the necessity of both visual and textual signals for QG and show the impact of various modeling choices. Code is available at https://anonymous.4open.science/r/multimodal-QG-47F2/',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zichao Wang', 'Richard Baraniuk'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_100',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MultiQG-TI: Towards Question Generation from Multi-modal Sources',\n",
       "   'tldr': 'We study the new problem of automatic question generation (QG) from multi-modal sources containing images and texts, significantly expanding the scope of most of the existing work that focuses exclusively on QG from only textual sources. We propose a simple solution for our new problem, called Multi',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_102': {'abstract': \"Enriching the quality of early childhood education with interactive math learning at home systems, empowered by recent advances in conversational AI technologies, is slowly becoming a reality. With this motivation, we implement a multimodal dialogue system to support play-based learning experiences at home, guiding kids to master basic math concepts. This work explores Spoken Language Understanding (SLU) pipeline within a task-oriented dialogue system developed for Kid Space, with cascading Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU) components evaluated on our home deployment data with kids going through gamified math learning activities. We validate the advantages of a multi-task architecture for NLU and experiment with a diverse set of pretrained language representations for Intent Recognition and Entity Extraction tasks in the math learning domain. To recognize kids' speech in realistic home environments, we investigate several ASR systems, including the commercial Google Cloud and the latest open-source Whisper solutions with varying model sizes. We evaluate the SLU pipeline by testing our best-performing NLU models on noisy ASR output to inspect the challenges of understanding children for math learning in authentic homes.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Eda Okur',\n",
       "    'Roddy Fuentes Alba',\n",
       "    'Saurav Sahay',\n",
       "    'Lama Nachman'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_102',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Inspecting Spoken Language Understanding from Kids for Basic Math Learning at Home',\n",
       "   'tldr': 'Enriching the quality of early childhood education with interactive math learning at home systems, empowered by recent advances in conversational AI technologies, is slowly becoming a reality. With this motivation, we implement a multimodal dialogue system to support play-based learning experiences ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_106': {'abstract': 'Socratic questioning is a teaching strategy where the student is guided towards solving a problem on their own, instead of being given the solution directly. In this paper, we introduce a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of GPT-based language models. While GPT-4 is observed to perform much better than GPT-3.5, its precision, and recall still fall short of human expert abilities, motivating further work in this area.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Erfan Al-Hossami',\n",
       "    'Razvan Bunescu',\n",
       "    'Ryan Teehan',\n",
       "    'Laurel Powell',\n",
       "    'Khyati Mahajan',\n",
       "    'Mohsen Dorodchi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_106',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Socratic Questioning of Novice Debuggers: A Benchmark Dataset and Preliminary Evaluations',\n",
       "   'tldr': 'Socratic questioning is a teaching strategy where the student is guided towards solving a problem on their own, instead of being given the solution directly. In this paper, we introduce a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple co',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_11': {'abstract': \"As the world regains its footing following the COVID-19 pandemic, academia is striving to consolidate the gains made in students' education experience. New technologies such as video-based learning have shown some early improvement in student learning and engagement. In this paper, we present ORBITS predictive engine at YOURIKA company, a video-based student support platform powered by knowledge tracing. In an exploratory case study of one master's level Speech Processing course at the Mohamed Bin Zayed University of Artificial Intelligence (MBZUAI) in Abu Dhabi, half the students used the system while the other half did not. Student qualitative feedback was universally positive and compared the system favorably against current available methods. These findings support the use of artificial intelligence techniques to improve the student learning experience.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shady Shehata',\n",
       "    'David Santandreu Calonge',\n",
       "    'Philip Purnell',\n",
       "    'Mark Thompson'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Enhancing Video-based Learning Using Knowledge Tracing: Personalizing Students' Learning Experience with ORBITS\",\n",
       "   'tldr': \"As the world regains its footing following the COVID-19 pandemic, academia is striving to consolidate the gains made in students' education experience. New technologies such as video-based learning have shown some early improvement in student learning and engagement. In this paper, we present ORBITS\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_110': {'abstract': \"The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94\\\\% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of evaluating and detecting AI-generated text.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ali Quidwai', 'Chunhui Li', 'Parijat Dube'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_110',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Beyond Black Box AI generated Plagiarism Detection: From Sentence to Document Level',\n",
       "   'tldr': 'The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quanti',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_111': {'abstract': 'Reinforcement Learning remains an underutilized method of training and fine-tuning Language Models (LMs) despite recent successes. This paper presents a simple approach of fine-tuning a language model with Reinforcement Learning to achieve competitive performance on the BEA 2023 Shared Task whose goal is to automatically generate teacher responses in educational dialogues. We utilized the novel NLPO algorithm that masks out tokens during generation to direct the model towards generations that maximize a reward function. We show results for both the t5-base model with 220 million parameters from the HuggingFace repository submitted to the leaderboard that, despite its comparatively small size, has achieved a good performance on both test and dev set, as well as GPT-2 with 124 million parameters. The presented results show that despite maximizing only one of the metrics used in the evaluation as a reward function our model scores highly in the other metrics as well.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Thomas Huber', 'Christina Niklaus', 'Siegfried Handschuh'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_111',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Educational Dialogues: A Reinforcement Learning Approach for Generating AI Teacher Responses',\n",
       "   'tldr': 'Reinforcement Learning remains an underutilized method of training and fine-tuning Language Models (LMs) despite recent successes. This paper presents a simple approach of fine-tuning a language model with Reinforcement Learning to achieve competitive performance on the BEA 2023 Shared Task whose go',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_112': {'abstract': \"(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilities of large language models in providing informative and helpful insights to students, thereby simulating the role of a knowledgeable teacher. To this end, we present an extensive evaluation of several benchmarking generative models, including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we fine-tuned the Flan-T5 model using reinforcement learning. Our experimental findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT. We hypothesize that several dataset characteristics, including sampling, representativeness, and dialog completeness, pose significant challenges to fine-tuning, thus contributing to the poor generalizability of the fine-tuned models. Finally, we note the need for these generative models to be evaluated with a metric that relies not only on dialog coherence and matched language modeling distribution but also on the model's ability to showcase pedagogical skills.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yann Hicke',\n",
       "    'Abhishek Masand',\n",
       "    'Wentao Guo',\n",
       "    'Tushaar Gangavarapu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_112',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Assessing the efficacy of large language models in generating accurate teacher responses',\n",
       "   'tldr': '(Tack et al., 2023) organized the shared task hosted by the 18th Workshop on Innovative Use of NLP for Building Educational Applications on generation of teacher language in educational dialogues. Following the structure of the shared task, in this study, we attempt to assess the generative abilitie',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_113': {'abstract': 'This paper presents the results of our participation in the BEA 2023 shared task, which focuses on generating AI teacher responses in educational dialogues. We conducted experiments using several Open-Source Large Language Models (LLMs) and explored fine-tuning techniques along with prompting strategies, including Few-Shot and Chain-of-Thought approaches. Our best model was ranked 4.5 in the competition with a BertScore F1 of 0.71 and a DialogRPT final (avg) of 0.35. Nevertheless, our internal results did not exactly correlate with those obtained in the competition, which showed the difficulty in evaluating this task. Other challenges we faced were data leakage on the train set and the irregular format of the conversations.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Alexis Baladn',\n",
       "    'Ignacio Sastre',\n",
       "    'Luis Chiruzzo',\n",
       "    'Aiala Ros'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_113',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'RETUYT-InCo at BEA 2023 Shared Task: Tuning Open-Source LLMs for Generating Teacher Responses',\n",
       "   'tldr': 'This paper presents the results of our participation in the BEA 2023 shared task, which focuses on generating AI teacher responses in educational dialogues. We conducted experiments using several Open-Source Large Language Models (LLMs) and explored fine-tuning techniques along with prompting strate',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_114': {'abstract': 'Language models are one of the biggest game changers in downstream NLP applications, especially in conversational agents. In spite of their awesome capabilities to generated responses to solve the inquireis, there are still some big challenges to using them. One challenge is how to enable the LLMs to use the private internal data to solve inquires. And secondly, how to keep the LLMs updated with newly incoming data without the burden of fine-tuning as it is not only expensive but also not an available option for some commercial LLMs, such as ChatGPT. In this work, we propose Semantic In-Context Learning (S-ICL) to address the aforementioned challenges. Our approach was participated in the BEA 2023 shared task and ended up having the fourth place in both development and evaluation phases.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Amin Omidvar', 'Aijun An'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_114',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Empowering Conversational Agents using Semantic In-Context Learning',\n",
       "   'tldr': 'Language models are one of the biggest game changers in downstream NLP applications, especially in conversational agents. In spite of their awesome capabilities to generated responses to solve the inquireis, there are still some big challenges to using them. One challenge is how to enable the LLMs t',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_115': {'abstract': 'This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies for candidate generation, including prompting for multiple candidates and employing iterative few-shot prompts with negative examples. We aggregate all candidate responses and rerank them based on DialogRPT scores. To handle consecutive turns in the dialogue data, we divide the task of generating teacher utterances into two components: teacher replies to the student and teacher continuations of previously sent messages. Through our proposed methodology, our system achieved the top score on both automated metrics and human evaluation, surpassing the reference human teachers on the latter.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Justin Vasselli',\n",
       "    'Christopher Vasselli',\n",
       "    'Adam Nohejl',\n",
       "    'Taro Watanabe'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_115',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NAISTeacher: A Prompt and Rerank Approach to Generating Teacher Utterances in Educational Dialogues',\n",
       "   'tldr': 'This paper presents our approach to the BEA 2023 shared task of generating teacher responses in educational dialogues, using the Teacher-Student Chatroom Corpus. Our system prompts GPT-3.5-turbo to generate initial suggestions, which are then subjected to reranking. We explore multiple strategies fo',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_116': {'abstract': 'This paper describes the results of the first shared task on generation of teacher responses in educational dialogues. The goal of the task was to benchmark the ability of generative language models to act as AI teachers, replying to a student in a teacherstudent dialogue. Eight teams participated in the competition hosted on CodaLab and experimented with a wide variety of state-of-the-art models, including Alpaca, Bloom, DialoGPT, DistilGPT-2, Flan-T5, GPT- 2, GPT-3, GPT-4, LLaMA, OPT-2.7B, and T5- base. Their submissions were automatically scored using BERTScore and DialogRPT metrics, and the top three among them were further manually evaluated in terms of pedagogical ability based on Tack and Piech (2022). The NAISTeacher system, which ranked first in both automated and human evaluation, generated responses with GPT-3.5 Turbo using an ensemble of prompts and DialogRPT-based ranking of responses for given dialogue contexts. Despite promising achievements of the participating teams, the results also highlight the need for evaluation metrics better suited to educational contexts.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Anas Tack',\n",
       "    'Ekaterina Kochmar',\n",
       "    'Zheng Yuan',\n",
       "    'Serge Bibauw',\n",
       "    'Chris Piech'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_116',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The BEA 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues',\n",
       "   'tldr': 'This paper describes the results of the first shared task on generation of teacher responses in educational dialogues. The goal of the task was to benchmark the ability of generative language models to act as AI teachers, replying to a student in a teacherstudent dialogue. Eight teams participated i',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_117': {'abstract': \"This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses within a student-teacher dialogue. Our system comprises evaluating various baseline models using OpenAI GPT-3 and designing diverse prompts to prompt the OpenAI models for teacher response generation. After the challenge, our system achieved second place by employing a few-shot prompt-based approach with the OpenAI text-davinci-003 model. The results highlight the few-shot learning capabilities of large-language models, particularly OpenAI's GPT-3, in the role of AI teachers.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Adaeze Adigwe', 'Zheng Yuan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_117',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The ADAIO System at the BEA-2023 Shared Task: Shared Task Generating AI Teacher Responses in Educational Dialogues',\n",
       "   'tldr': \"This paper presents the ADAIO team's system entry in the Building Educational Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in Educational Dialogues. The task aims to assess the performance of state-of-the-art generative models as AI teachers in producing suitable responses \",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_14': {'abstract': 'We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality QG than automatic summaries. In this paper, a) we show that advances in Large Language Models (LLMs) are not yet sufficient to generate quality summaries for QG and b) we introduce a new methodology for enhancing bullet point student notes into fully fledged summaries and find that our methodology yields higher quality QG. We conducted a large-scale human annotation study of generated question-answer pairs for the evaluation of our methodology. In order to aid in future research, we release a  new dataset of 9.2K human annotations of generated questions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hannah Gonzalez',\n",
       "    'Liam Dugan',\n",
       "    'Eleni Miltsakaki',\n",
       "    'Zhiqi Cui',\n",
       "    'Jiaxuan Ren',\n",
       "    'Bryan Li',\n",
       "    'Shriyash Upadhyay',\n",
       "    'Etan Ginsberg',\n",
       "    'Chris Callison-Burch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Human Summaries for Question-Answer Generation in Education',\n",
       "   'tldr': 'We address the problem of generating high-quality question-answer pairs for educational materials. Previous work on this problem showed that using summaries as input improves the quality of question generation (QG) over original textbook text and that human-written summaries result in higher quality',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_16': {'abstract': \"Question generation (QG) for reading comprehension, a technology for automatically generating questions related to given reading passages, has been used in various applications, including in education. Recently, QG methods based on deep neural networks have succeeded in generating fluent questions that are pertinent to given reading passages. One example of how QG can be applied in education is a reading tutor that automatically offers reading comprehension questions related to various reading materials. In such an application, QG methods should provide questions with difficulty levels appropriate for each learner's reading ability in order to improve learning efficiency. Several difficulty-controllable QG methods have been proposed for doing so. However, conventional methods focus only on generating questions and cannot generate answers to them. Furthermore, they ignore the relation between question difficulty and learner ability, making it hard to determine an appropriate difficulty for each learner. To resolve these problems, we propose a new method for generating question--answer pairs that considers their difficulty, estimated using item response theory. The proposed difficulty-controllable generation is realized by extending two pre-trained transformer models: BERT and GPT-2.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Masaki Uto', 'Yuto Tomikawa', 'Ayaka Suzuki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Difficulty-Controllable Neural Question Generation for Reading Comprehension using Item Response Theory',\n",
       "   'tldr': 'Question generation (QG) for reading comprehension, a technology for automatically generating questions related to given reading passages, has been used in various applications, including in education. Recently, QG methods based on deep neural networks have succeeded in generating fluent questions t',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_18': {'abstract': \"This paper presents Card-it, a web-based application for learning Italian verb conjugation. Card-it integrates a large-scale finite-state morphological\\\\textasciitilde{}(FSM) analyzer and a flashcard application as a user-friendly way for learners to utilize the analyzer. While Card-it can be used by individual learners, to support classroom adoption, we implemented simple classroom management functionalities such as sharing flashcards to a class and tracking students' progression. We evaluated Card-it with teachers of Italian. Card-it was reported as engaging and supportive, especially by featuring two different quiz types combined with a verb form look-up feature. Teachers were optimistic about the potential of Card-it as a classroom supplementary tool for learners of Italian as L2. Future work includes sample sentences and a complete learners evaluation.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mariana Shimabukuro',\n",
       "    'Jessica Zipf',\n",
       "    'Shawn Yama',\n",
       "    'Christopher Collins'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating Classroom Potential for Card-it: Digital Flashcards for Studying and Learning Italian Morphology',\n",
       "   'tldr': 'This paper presents Card-it, a web-based application for learning Italian verb conjugation. Card-it integrates a large-scale finite-state morphological\\\\textasciitilde{}(FSM) analyzer and a flashcard application as a user-friendly way for learners to utilize the analyzer. While Card-it can be used by',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_19': {'abstract': 'Open-ended constructed response math word problems (\"math plus text\", or MPT) are a powerful tool in the assessment of students\\' abilities to engage in mathematical reasoning and creative thinking. Such problems ask the student to compute a value or construct an expression and then explain, potentially in prose, what steps they took and why they took them. MPT items can be scored against highly structured rubrics, and we develop a novel technique for the automated scoring of MPT items that leverages these rubrics to provide explainable scoring. We show that our approach can be trained automatically and performs well on a large dataset of 34,417 responses across 14 MPT items.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Scott Hellman', 'Alejandro Andrade', 'Kyle Habermehl'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Scalable and Explainable Automated Scoring for Open-Ended Constructed Response Math Word Problems',\n",
       "   'tldr': 'Open-ended constructed response math word problems (\"math plus text\", or MPT) are a powerful tool in the assessment of students\\' abilities to engage in mathematical reasoning and creative thinking. Such problems ask the student to compute a value or construct an expression and then explain, potentia',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_20': {'abstract': 'In this paper we show that GEC systems display gender bias related to the use of masculine and feminine terms and the gender-neutral singular \"they\". We develop parallel datasets of texts with masculine and feminine terms, and singular \"they\", and use them to quantify gender bias in three competitive GEC systems. We contribute a novel data augmentation technique for singular \"they\" leveraging linguistic insights about its distribution relative to plural \"they\". We demonstrate that both this data augmentation technique and a refinement of a similar augmentation technique for masculine and feminine terms can generate training data that reduces bias in GEC systems, especially with respect to singular \"they\" while maintaining the same level of quality.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gunnar Lund', 'Kostiantyn Omelianchuk', 'Igor Samokhin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Gender-Inclusive Grammatical Error Correction through Augmentation',\n",
       "   'tldr': 'In this paper we show that GEC systems display gender bias related to the use of masculine and feminine terms and the gender-neutral singular \"they\". We develop parallel datasets of texts with masculine and feminine terms, and singular \"they\", and use them to quantify gender bias in three competitiv',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_21': {'abstract': 'We develop an interactive web-based user interface for performing textspeech alignment and creating digital interactive \"read-along audio books that highlight words as they are spoken and allow users to replay individual words when clicked. We build on an existing Python library for zero-shot multilingual textspeech alignment (Littell et al., 2022), extend it by exposing its functionality through a RESTful API, and rewrite the underlying speech recognition engine to run in the browser. The ReadAlong Studio Web App is open-source, user-friendly, prioritizes privacy and data sovereignty, allows for a variety of standard export formats, and is designed to work for the majority of the world\\'s languages.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aidan Pine',\n",
       "    'David Huggins-Daines',\n",
       "    'Eric Joanis',\n",
       "    'Patrick Littell',\n",
       "    'Marc Tessier',\n",
       "    'Delasie Torkornoo',\n",
       "    'Rebecca Knowles',\n",
       "    'Roland Kuhn',\n",
       "    'Delaney Lothian'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_21',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ReadAlong Studio Web Interface for Digital Interactive Storytelling',\n",
       "   'tldr': 'We develop an interactive web-based user interface for performing textspeech alignment and creating digital interactive \"read-along audio books that highlight words as they are spoken and allow users to replay individual words when clicked. We build on an existing Python library for zero-shot multil',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_24': {'abstract': 'A peer-assessment system allows students to provide feedback on each other\\'s work. An effective peer assessment system urgently requires helpful reviews to facilitate students to make improvements and progress. Automated evaluation of review helpfulness, with the help of deep learning models and natural language processing techniques, gains much interest in the field of peer assessment. However, collecting labeled data with the \"helpfulness\" tag to build these prediction models remains challenging. A straightforward solution would be using a supervised learning algorithm to train a prediction model on a similar domain and apply it to our peer review domain for inference. But naively doing so can degrade the model performance in the presence of the distributional gap between domains. Such a distributional gap can be effectively addressed by Domain Adaptation (DA). Self-training has recently been shown as a powerful branch of DA to address the distributional gap. The first goal of this study is to evaluate the performance of self-training-based DA in predicting the helpfulness of peer reviews as well as the ability to overcome the distributional gap. Our second goal is to propose an advanced self-training framework to overcome the weakness of the existing self-training by tailoring knowledge distillation and noise injection, to further improve the model performance and better address the distributional gap.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chengyuan Liu',\n",
       "    'Divyang Doshi',\n",
       "    'Muskaan Bhargava',\n",
       "    'Ruixuan Shang',\n",
       "    'Jialin Cui',\n",
       "    'Dongkuan Xu',\n",
       "    'Edward Gehringer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Labels are not necessary: Assessing peer-review helpfulness using domain adaptation based on self-training',\n",
       "   'tldr': \"A peer-assessment system allows students to provide feedback on each other's work. An effective peer assessment system urgently requires helpful reviews to facilitate students to make improvements and progress. Automated evaluation of review helpfulness, with the help of deep learning models and nat\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_25': {'abstract': 'This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). For reinforcement learning, we examine combinations of three reward functions that consider grammatical items, diversity, and fluency. Our experiments confirm that both approaches can generate responses including the specified grammatical items and that it is crucial to consider fluency rather than diversity as the reward function.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuki Okano',\n",
       "    'Kotaro Funakoshi',\n",
       "    'Ryo Nagata',\n",
       "    'Manabu Okumura'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generating Dialog Responses with Specified Grammatical Items for Second Language Learning',\n",
       "   'tldr': 'This paper proposes a new second language learning task of generating a response including specified grammatical items. We consider two approaches: 1) fine-tuning a pre-trained language model (DialoGPT) by reinforcement learning and 2) providing a few-shot prompt to a large language model (GPT-3). F',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_28': {'abstract': \"The exponential growth of question answering (QA) has made it an indispensable topic in any Natural Language Processing (NLP) course. Additionally, the breadth of QA derived from this exponential growth makes it an ideal scenario for teaching related NLP topics such as information retrieval, explainability, and adversarial attacks among others. In this paper, we introduce UKP-SQuARE as a platform for QA education. This platform provides an interactive environment where students can run, compare, and analyze various QA models from different perspectives, such as general behavior, explainability, and robustness. Therefore, students can get a first-hand experience in different QA techniques during the class. Thanks to this, we propose a learner-centered approach for QA education in which students proactively learn theoretical concepts and acquire problem-solving skills through interactive exploration, experimentation, and practical assignments, rather than solely relying on traditional lectures. To evaluate the effectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a postgraduate NLP course and surveyed the students after the course. Their positive feedback shows the platform's effectiveness in their course and invites a wider adoption.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Haishuo Fang', 'Haritz Puerto', 'Iryna Gurevych'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UKP-SQuARE: An Interactive Tool for Teaching Question Answering',\n",
       "   'tldr': 'The exponential growth of question answering (QA) has made it an indispensable topic in any Natural Language Processing (NLP) course. Additionally, the breadth of QA derived from this exponential growth makes it an ideal scenario for teaching related NLP topics such as information retrieval, explain',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_3': {'abstract': 'Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, no actively-maintained open-source library extracts a wide variety of handcrafted features. The current handcrafted feature extraction practices have several inefficiencies, and a researcher often has to build such an extraction system from the ground up. We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system to give the community a rich set of pre-implemented handcrafted features.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Bruce W. Lee', 'Jason Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LFTK: Handcrafted Features in Computational Linguistics',\n",
       "   'tldr': 'Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across res',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_30': {'abstract': \"Large-scale pre-trained language models such as GPT-3 have shown remarkable performance across various natural language processing tasks. However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored. Controllability in GEC is crucial for real-world applications, particularly in educational settings, where the ability to tailor feedback according to learner levels and specific error types can significantly enhance the learning process.This paper investigates the performance and controllability of prompt-based methods with GPT-3 for GEC tasks using zero-shot and few-shot setting. We explore the impact of task instructions and examples on GPT-3's output, focusing on controlling aspects such as minimal edits, fluency edits, and learner levels. Our findings demonstrate that GPT-3 could effectively perform GEC tasks, outperforming existing supervised and unsupervised approaches. We also showed that GPT-3 could achieve controllability when appropriate task instructions and examples are given.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mengsay Loem',\n",
       "    'Masahiro Kaneko',\n",
       "    'Sho Takase',\n",
       "    'Naoaki Okazaki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Effectiveness of GPT-3 in Grammatical Error Correction: A Study on Performance and Controllability in Prompt-Based Methods',\n",
       "   'tldr': 'Large-scale pre-trained language models such as GPT-3 have shown remarkable performance across various natural language processing tasks. However, applying prompt-based methods with GPT-3 for Grammatical Error Correction (GEC) tasks and their controllability remains underexplored. Controllability in',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_32': {'abstract': 'In various natural language processing tasks, such as named entity recognition and machine translation, example-based approaches have been used to improve performance by leveraging existing knowledge. However, the effectiveness of this approach for Grammatical Error Correction (GEC) is unclear. In this work, we explore how an example-based approach affects the accuracy and interpretability of the output of GEC systems and the trade-offs involved. The approach we investigate has shown great promise in machine translation by using the \\\\$k\\\\$-nearest translation examples to improve the results of a pretrained Transformer model. We find that using this technique increases precision by reducing the number of false positives, but recall suffers as the model becomes more conservative overall. Increasing the number of example sentences in the datastore does lead to better performing systems, but with diminishing returns and a high decoding cost. Synthetic data can be used as examples, but the effectiveness varies depending on the base model. Finally, we find that finetuning on a set of data may be more effective than using that data during decoding as examples.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Justin Vasselli', 'Taro Watanabe'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Closer Look at k-Nearest Neighbors Grammatical Error Correction',\n",
       "   'tldr': 'In various natural language processing tasks, such as named entity recognition and machine translation, example-based approaches have been used to improve performance by leveraging existing knowledge. However, the effectiveness of this approach for Grammatical Error Correction (GEC) is unclear. In t',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_33': {'abstract': \"By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models' decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['James Fiacco', 'David Adamson', 'Carolyn Ros'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models',\n",
       "   'tldr': 'By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_34': {'abstract': \"This paper analyzes winning solutions from the Feedback Prize competition series hosted from 2021-2022. The competition sought to improve Assisted Writing Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for evaluating student writing. The winning models are freely available for incorporation into educational applications, but the models need to be assessed for performance and other factors. This study reports the performance accuracy of Feedback Prize-winning models based on demographic factors such as student race/ethnicity, economic disadvantage, and English Language Learner status. Two competitions are analyzed. The first, which focused on identifying discourse elements, demonstrated minimal bias based on students' demographic factors. However, the second competition, which aimed to predict discourse effectiveness, exhibited moderate bias.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Perpetual Baffour', 'Tor Saxberg', 'Scott Crossley'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_34',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Analyzing Bias in Large Language Model Solutions for Assisted Writing Feedback Tools: Lessons from the Feedback Prize Competition Series',\n",
       "   'tldr': 'This paper analyzes winning solutions from the Feedback Prize competition series hosted from 2021-2022. The competition sought to improve Assisted Writing Feedback Tools (AWFTs) by crowdsourcing Large Language Model (LLM) solutions for evaluating student writing. The winning models are freely availa',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_37': {'abstract': 'Reading comprehension is a crucial skill in many aspects of education, including language learning, cognitive development, and fostering early literacy skills in children. Automated answer-aware reading comprehension question generation has significant potential to scale up learner support in educational activities. One key technical challenge in this setting is that there can be multiple questions, sometimes very different from each other, with the same answer; a trained question generation method may not necessarily know which question human educators would prefer. To address this challenge, we propose 1) a data augmentation method that enriches the training dataset with diverse questions given the same context and answer and 2) an overgenerate-and-rank method to select the best question from a pool of candidates. We evaluate our method on the FairytaleQA dataset, showing a 5\\\\% absolute improvement in ROUGE-L over the best existing method. We also demonstrate the effectiveness of our method in generating harder, \"implicit\" questions, where the answers are not contained in the context as text spans.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nischal Ashok Kumar',\n",
       "    'Nigel Fernandez',\n",
       "    'Zichao Wang',\n",
       "    'Andrew Lan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_37',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Reading Comprehension Question Generation with Data Augmentation and Overgenerate-and-rank',\n",
       "   'tldr': 'Reading comprehension is a crucial skill in many aspects of education, including language learning, cognitive development, and fostering early literacy skills in children. Automated answer-aware reading comprehension question generation has significant potential to scale up learner support in educat',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_39': {'abstract': \"The standard definition generation task requires to automatically produce mono-lingual definitions (e.g., English definitions for English words), but ignores that the generated definitions may also consist of unfamiliar words for language learners. In this work, we propose a novel task of Trans-Lingual Definition Generation (TLDG), which aims to generate definitions in another language, i.e., the native speaker's language. Initially, we explore the unsupervised manner of this task and build up a simple implementation of fine-tuning the multi-lingual machine translation model. Then, we develop two novel methods, Prompt Combination and Contrastive Prompt Learning, for further enhancing the quality of the generation. Our methods are evaluated against the baseline Pipeline method in both rich- and low-resource settings, and we empirically establish its superiority in generating higher-quality trans-lingual definitions.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hengyuan Zhang',\n",
       "    'Dawei Li',\n",
       "    'Yanran Li',\n",
       "    'Chenming Shang',\n",
       "    'Chufan Shi',\n",
       "    'Yong Jiang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Assisting Language Learners: Automated Trans-Lingual Definition Generation via Contrastive Prompt Learning',\n",
       "   'tldr': 'The standard definition generation task requires to automatically produce mono-lingual definitions (e.g., English definitions for English words), but ignores that the generated definitions may also consist of unfamiliar words for language learners. In this work, we propose a novel task of Trans-Ling',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_4': {'abstract': 'Large language models can solve reasoning tasks (like math problems) more effectively when they are allowed to generate rationales. However, a good tutoring system should not just generate solutions, but should also generate explanations and should be able to correct and guide students. We show that providing a code scratchpad improves performance on each tutoring step with a gradeschool mathematics dataset. On these tutoring tasks, GPT-3 models provided with a code scratchpad significantly outperform those given only a language scratchpad (77.7\\\\%  vs 48.7\\\\% cumulative accuracy).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shriyash Upadhyay', 'Etan Ginsberg', 'Chris Callison-Burch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Mathematics Tutoring With A Code Scratchpad',\n",
       "   'tldr': 'Large language models can solve reasoning tasks (like math problems) more effectively when they are allowed to generate rationales. However, a good tutoring system should not just generate solutions, but should also generate explanations and should be able to correct and guide students. We show that',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_40': {'abstract': \"The ability to revise in response to feedback is critical to students' writing success. In the case of argument writing in specific, identifying whether an argument revision (AR) is successful or not is a complex problem because AR quality is dependent on the overall content of an argument. For example, adding the same evidence sentence could strengthen or weaken existing claims in different argument contexts (ACs). To address this issue we developed Chain-of-Thought prompts to facilitate ChatGPT-generated ACs for AR quality predictions. The experiments on two corpora, our annotated elementary essays and existing college essays benchmark, demonstrate the superiority of the proposed ACs over baselines.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhexiong Liu',\n",
       "    'Diane Litman',\n",
       "    'Elaine Wang',\n",
       "    'Lindsay Matsumura',\n",
       "    'Richard Correnti'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Predicting the Quality of Revisions in Argumentative Writing',\n",
       "   'tldr': \"The ability to revise in response to feedback is critical to students' writing success. In the case of argument writing in specific, identifying whether an argument revision (AR) is successful or not is a complex problem because AR quality is dependent on the overall content of an argument. For exam\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_42': {'abstract': \"In intelligent language tutoring systems, student dashboards should display the learning progress and performance and support the navigation through the learning content. Designing an interface that transparently offers information on students' learning in relation to specific learning targets while linking to the overarching functional goal, that motivates and organizes the practice in current foreign language teaching, is challenging.This becomes even more difficult in systems that adaptively expose students to different learning material and individualize system interactions. If such a system is used in an ecologically valid setting of blended learning, this generates additional requirements to incorporate the needs of students and teachers for control and customizability.We present the conceptual design of a student dashboard for a task-based, user-adaptive intelligent language tutoring system intended for use in real-life English classes in secondary schools. We highlight the key challenges and spell out open questions for future research.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Leona Colling', 'Tanja Heck', 'Detmar Meurers'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Reconciling Adaptivity and Task Orientation in the Student Dashboard of an Intelligent Language Tutoring System',\n",
       "   'tldr': \"In intelligent language tutoring systems, student dashboards should display the learning progress and performance and support the navigation through the learning content. Designing an interface that transparently offers information on students' learning in relation to specific learning targets while\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_44': {'abstract': 'Improving conversational proficiency is a key target for students learning a new language. While acquiring conversational proficiency, students must learn the linguistic mechanisms of Repair and Grounding (R\\\\textbackslash{}\\\\&G) to  negotiate meaning and find common ground with their interlocutor so conversational breakdowns can be resolved. Task-oriented Spoken Dialogue Systems (SDS) have long been sought as a tool to hone conversational proficiency. However, the R\\\\&G patterns for language learners interacting with a task-oriented spoken dialogue system are not reflected explicitly in any existing datasets. Therefore, to move the needle in Spoken Dialogue Systems for language learning we present GrounDialog: an annotated dataset of spoken conversations where we elicit a rich set of R\\\\&G patterns.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xuanming Zhang', 'Rahul Divekar', 'Rutuja Ubale', 'Zhou Yu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GrounDialog: A Dataset for Repair and Grounding in Task-oriented Spoken Dialogues for Language Learning',\n",
       "   'tldr': 'Improving conversational proficiency is a key target for students learning a new language. While acquiring conversational proficiency, students must learn the linguistic mechanisms of Repair and Grounding (R\\\\textbackslash{}\\\\&G) to  negotiate meaning and find common ground with their interlocutor so ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_45': {'abstract': \"Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. Unfortunately, online student feedback is unstructured and abundant, making it challenging for teachers to learn and improve. We take a step towards tackling this challenge. First, we contribute a dataset for studying this problem: SIGHT is a large dataset of 288 math lecture transcripts and 15,784 comments collected from the Massachusetts Institute of Technology OpenCourseWare (MIT OCW) YouTube channel. Second, we develop a rubric for categorizing feedback types using qualitative analysis. Qualitative analysis methods are powerful in uncovering domain-specific insights, however they are costly to apply to large data sources. To overcome this challenge, we propose a set of best practices for using large language models (LLMs) to cheaply classify the comments at scale. We observe a striking correlation between the model's and humans' annotation: Categories with consistent human annotations (\\\\textgreater{}0.9 inter-rater reliability, IRR) also display higher human-model agreement (\\\\textgreater{}0.7), while categories with less consistent human annotations (0.7-0.8 IRR) correspondingly demonstrate lower human-model agreement (0.3-0.5). These techniques uncover useful student feedback from thousands of comments, costing around \\\\$0.002 per comment. We conclude by discussing exciting future directions on using online student feedback and improving automated annotation techniques for qualitative research.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Rose Wang',\n",
       "    'Pawan Wirawarn',\n",
       "    'Noah Goodman',\n",
       "    'Dorottya Demszky'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SIGHT: A Large Annotated Dataset on Student Insights Gathered from Higher Education Transcripts',\n",
       "   'tldr': 'Lectures are a learning experience for both students and teachers. Students learn from teachers about the subject material, while teachers learn from students about how to refine their instruction. Unfortunately, online student feedback is unstructured and abundant, making it challenging for teacher',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_47': {'abstract': 'This paper addresses the problem of providing automatic feedback on orthographic errors in handwritten text. Despite the availability of automatic error detection systems, the practical problem of digitizing the handwriting remains. Current handwriting recognition (HWR) systems produce highly accurate transcriptions but normalize away the very errors that are essential for providing useful feedback, e.g. orthographic errors. Our contribution is twofold:First, we create a comprehensive dataset of handwritten text with transcripts retaining orthographic errors by transcribing 1,350 pages from the German learner dataset FD-LEX. Second, we train a simple HWR system on our dataset, allowing it to transcribe words with orthographic errors.Thereby, we evaluate the effect of different dictionaries on recognition output, highlighting the importance of addressing spelling errors in these dictionaries.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Christian Gold', 'Ronja Laarmann-Quante', 'Torsten Zesch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Recognizing Learner Handwriting Retaining Orthographic Errors for Enabling Fine-Grained Error Feedback',\n",
       "   'tldr': 'This paper addresses the problem of providing automatic feedback on orthographic errors in handwritten text. Despite the availability of automatic error detection systems, the practical problem of digitizing the handwriting remains. Current handwriting recognition (HWR) systems produce highly accura',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_48': {'abstract': 'As in other NLP tasks, Automatic Short Answer Grading (ASAG) systems have evolved from using rule-based and interpretable machine learning models to utilizing deep learning architectures to boost accuracy. Since proper feedback is critical to student assessment, explainability will be crucial for deploying ASAG in real-world applications. This paper proposes a framework to generate explainable outcomes for assessing question-answer pairs of a Data Mining course in a binary manner. Our framework utilizes a fine-tuned Transformer-based classifier and an explainability module using SHAP or Integrated Gradients to generate language explanations for each prediction. We assess the outcome of our framework by calculating accuracy-based metrics for classification performance. Furthermore, we evaluate the quality of the explanations by measuring their agreement with human-annotated justifications using Intersection-Over-Union at a token level to derive a plausibility score.Despite the relatively limited sample, results show that our framework derives explanations that are, to some degree, aligned with domain-expert judgment. Furthermore, both explainability methods perform similarly in their agreement with human-annotated explanations. A natural progression of our work is to analyze the use of our explainable ASAG framework on a larger sample to determine the feasibility of implementing a pilot study in a real-world setting.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Maximilian Tornqvist',\n",
       "    'Mosleh Mahamud',\n",
       "    'Erick Mendez Guzman',\n",
       "    'Alexandra Farazouli'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ExASAG: Explainable Framework for Automatic Short Answer Grading',\n",
       "   'tldr': 'As in other NLP tasks, Automatic Short Answer Grading (ASAG) systems have evolved from using rule-based and interpretable machine learning models to utilizing deep learning architectures to boost accuracy. Since proper feedback is critical to student assessment, explainability will be crucial for de',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_49': {'abstract': 'Creating high-quality multiple-choice items requires careful attention to several factors, including ensuring that there is only one correct option, that options are independent of each other, that there is no overlap between options, and that each option is plausible. This attention is reflected in the explanations provided by human item-writers for each option. This study aimed to compare the creation of explanations of multiple-choice item options for reading comprehension by ChatGPT with those created by humans. We used two context-dependent multiple-choice item sets created based on EvidenceCentered Design. Results indicate that ChatGPT is capable of producing explanations with different type of information that are comparable to those created by humans. So that humans could benefit from additional information given to enhance their explanations. We conclude that ChatGPT ability to generate explanations for multiple-choice item options in reading comprehension tests is comparable to that of humans.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['George Duenas', 'Sergio Jimenez', 'Geral Mateus Ferro'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_49',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"You've Got a Friend in ... a Language Model? A Comparison of Explanations of Multiple-Choice Items of Reading Comprehension between ChatGPT and Humans\",\n",
       "   'tldr': 'Creating high-quality multiple-choice items requires careful attention to several factors, including ensuring that there is only one correct option, that options are independent of each other, that there is no overlap between options, and that each option is plausible. This attention is reflected in',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_50': {'abstract': 'We introduce a novel technique for automatically summarizing lecture videos using large language models such as GPT-3 and we present a user study investigating the effects on the studying experience when automatic summaries are added to lecture videos. We test students under different conditions and find that the students who are shown a summary next to a lecture video perform better on quizzes designed to test the course materials than the students who have access only to the video or the summary. Our findings suggest that adding automatic summaries to lecture videos enhances the learning experience. Qualitatively, students preferred summaries when studying under time constraints.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hannah Gonzalez',\n",
       "    'Jiening Li',\n",
       "    'Helen Jin',\n",
       "    'Jiaxuan Ren',\n",
       "    'Hongyu Zhang',\n",
       "    'Ayotomiwa Akinyele',\n",
       "    'Adrian Wang',\n",
       "    'Eleni Miltsakaki',\n",
       "    'Ryan Baker',\n",
       "    'Chris Callison-Burch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Automatically Generated Summaries of Video Lectures May Enhance Students' Learning Experience\",\n",
       "   'tldr': 'We introduce a novel technique for automatically summarizing lecture videos using large language models such as GPT-3 and we present a user study investigating the effects on the studying experience when automatic summaries are added to lecture videos. We test students under different conditions and',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_55': {'abstract': \"The popularization of large language models (LLMs) such as OpenAI's GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are difficult to identify automatically, such as discourse coherence. In addition, LLMs can provide rationales for their evaluations (ratings) which increases score interpretability and transparency. This paper investigates one approach to producing ratings by training GPT-4 to assess discourse coherence in a manner consistent with expert human raters. The findings of the study suggest that GPT-4 has strong potential to produce discourse coherence ratings that are comparable to human ratings, accompanied by clear rationales. Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with respect to agreement with human ratings. These results have implications for advancing AWE technology for learning and assessment.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ben Naismith', 'Phoebe Mulcaire', 'Jill Burstein'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Automated evaluation of written discourse coherence using GPT-4',\n",
       "   'tldr': \"The popularization of large language models (LLMs) such as OpenAI's GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are di\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_56': {'abstract': 'Lexical simplification (LS) automatically replaces words that are deemed difficult to understand for a given target population with simpler alternatives, whilst preserving the meaning of the original sentence. The TSAR-2022 shared task on LS provided participants with a multilingual lexical simplification test set. It contained nearly 1,200 complex words in English, Portuguese, and Spanish and presented multiple candidate substitutions for each complex word. The competition did not make training data available; therefore, teams had to use either off-the-shelf pre-trained large language models (LLMs) or out-domain data to develop their LS systems. As such, participants were unable to fully explore the capabilities of LLMs by re-training and/or fine-tuning them on in-domain data. To address this important limitation, we present ALEXSIS+, a multilingual dataset in the aforementioned three languages, and ALEXSIS++, an English monolingual dataset that together contains more than 50,000 unique sentences retrieved from news corpora and annotated with cosine similarities to the original complex word and sentence. Using these additional contexts, we are able to generate new high-quality candidate substitutions that improve LS performance on the TSAR-2022 test set regardless of the language or model.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kai North',\n",
       "    'Alphaeus Dmonte',\n",
       "    'Tharindu Ranasinghe',\n",
       "    'Matthew Shardlow',\n",
       "    'Marcos Zampieri'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ALEXSIS+: Improving Substitute Generation and Selection for Lexical Simplification with Information Retrieval',\n",
       "   'tldr': 'Lexical simplification (LS) automatically replaces words that are deemed difficult to understand for a given target population with simpler alternatives, whilst preserving the meaning of the original sentence. The TSAR-2022 shared task on LS provided participants with a multilingual lexical simplifi',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_57': {'abstract': 'Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Antonio Laverghetta Jr.', 'John Licato'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_57',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generating Better Items for Cognitive Assessments Using Large Language Models',\n",
       "   'tldr': 'Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_58': {'abstract': 'Responding to the increasing need for automated writing evaluation (AWE) systems to assess language use beyond lexis and grammar (Burstein et al., 2016), we introduce a new approach to identify rhetorical features of stance in academic English writing. Drawing on the discourse-analytic framework of engagement in the Appraisal analysis (Martin \\\\& White, 2005), we manually annotated 4,688 sentences (126,411 tokens) for eight rhetorical stance categories (e.g., PROCLAIM, ATTRIBUTION) and additional discourse elements. We then report an experiment to train machine learning models to identify and categorize the spans of these stance expressions. The best-performing model (RoBERTa + LSTM) achieved macro-averaged F1 of .7208 in the span identification of stance-taking expressions, slightly outperforming the intercoder reliability estimates before adjudication (F1 = .6629).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Masaki Eguchi', 'Kristopher Kyle'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_58',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Span Identification of Epistemic Stance-Taking in Academic Written English',\n",
       "   'tldr': 'Responding to the increasing need for automated writing evaluation (AWE) systems to assess language use beyond lexis and grammar (Burstein et al., 2016), we introduce a new approach to identify rhetorical features of stance in academic English writing. Drawing on the discourse-analytic framework of ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_6': {'abstract': \"Effective human learning depends on a wide selection of educational materials that align with the learner's current understanding of the topic. While the Internet has revolutionized human learning or education, a substantial resource accessibility barrier still exists. Namely, the excess of online information can make it challenging to navigate and discover high-quality learning materials in a given subject area. In this paper, we propose an automatic pipeline for building an educational resource discovery system for new domains. The pipeline consists of three main steps: resource searching, feature extraction, and resource classification. We first collect frequent queries from a set of seed documents, and search the web with these queries to obtain candidate resources such as lecture slides and introductory blog posts. Then, we process these resources for BERT-based features and meta-features. Next, we train a tree-based classifier to decide whether they are suitable learning materials. The pipeline achieves F1 scores of 0.94 and 0.82 when evaluated on two similar but novel domains. Finally, we demonstrate how this pipeline can benefit two applications: prerequisite chain learning and leading paragraph generation for surveys. We also release a corpus of 39,728 manually labeled web resources and 659 queries from NLP, Computer Vision (CV), and Statistics (STATS).\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Irene Li',\n",
       "    'Thomas George',\n",
       "    'Alex Fabbri',\n",
       "    'Tammy Liao',\n",
       "    'Benjamin Chen',\n",
       "    'Rina Kawamura',\n",
       "    'Richard Zhou',\n",
       "    'Vanessa Yan',\n",
       "    'Swapnil Hingmire',\n",
       "    'Dragomir Radev'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Transfer Learning Pipeline for Educational Resource Discovery with Application in Survey Generation',\n",
       "   'tldr': \"Effective human learning depends on a wide selection of educational materials that align with the learner's current understanding of the topic. While the Internet has revolutionized human learning or education, a substantial resource accessibility barrier still exists. Namely, the excess of online i\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_61': {'abstract': 'This paper presents the ACTA system, which performs automated short-answer grading in the domain of high-stakes medical exams. The system builds upon previous work on neural similarity-based grading approaches by applying these to the medical domain and utilizing contrastive learning as a means to optimize the similarity metric. ACTA is evaluated against three strong baselines and is developed in alignment with operational needs, where low-confidence responses are flagged for human review. Learning curves are explored to understand the effects of training data on performance. The results demonstrate that ACTA leads to substantially lower number of responses being flagged for human review, while maintaining high classification accuracy.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['King Yiu Suen',\n",
       "    'Victoria Yaneva',\n",
       "    'Le An Ha',\n",
       "    'Janet Mee',\n",
       "    'Yiyun Zhou',\n",
       "    'Polina Harik'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_61',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ACTA: Short-Answer Grading in High-Stakes Medical Exams',\n",
       "   'tldr': 'This paper presents the ACTA system, which performs automated short-answer grading in the domain of high-stakes medical exams. The system builds upon previous work on neural similarity-based grading approaches by applying these to the medical domain and utilizing contrastive learning as a means to o',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_62': {'abstract': 'Automatic readability assessment (ARA) predicts how difficult it is for the reader to understand a text.  While ARA has traditionally been performed at the passage level, there has been increasing interest in ARA at the sentence level, given its applications in downstream tasks such as text simplification and language exercise generation. Recent research has suggested the effectiveness of hybrid approaches for ARA, but they have yet to be applied on the sentence level. We present the first study that compares neural and hybrid models for sentence-level ARA. We conducted experiments on graded sentences from the Wall Street Journal (WSJ) and a dataset derived from the OneStopEnglish corpus. Experimental results show that both neural and hybrid models outperform traditional classifiers trained on linguistic features. Hybrid models obtained the best accuracy on both datasets, surpassing the previous best result reported on the WSJ dataset by almost 13\\\\% absolute.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Fengkai Liu', 'John Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_62',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Hybrid Models for Sentence Readability Assessment',\n",
       "   'tldr': 'Automatic readability assessment (ARA) predicts how difficult it is for the reader to understand a text.  While ARA has traditionally been performed at the passage level, there has been increasing interest in ARA at the sentence level, given its applications in downstream tasks such as text simplifi',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_63': {'abstract': \"Grammatical error correction (GEC) is a challenging task for non-native second language (L2) learners and learning machines. Data-driven GEC learning requires as much human-annotated genuine training data as possible. However, it is difficult to produce larger-scale human-annotated data, and synthetically generated large-scale parallel training data is valuable for GEC systems. In this paper, we propose a method for rebuilding a corpus of synthetic parallel data using target sentences predicted by a GEC model to improve performance. Experimental results show that our proposed pre-training outperforms that on the original synthetic datasets. Moreover, it is also shown that our proposed training without human-annotated L2 learners' corpora is as practical as conventional full pipeline training with both synthetic datasets and L2 learners' corpora in terms of accuracy.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mikio Oda'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_63',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Training for Grammatical Error Correction Without Human-Annotated L2 Learners' Corpora\",\n",
       "   'tldr': 'Grammatical error correction (GEC) is a challenging task for non-native second language (L2) learners and learning machines. Data-driven GEC learning requires as much human-annotated genuine training data as possible. However, it is difficult to produce larger-scale human-annotated data, and synthet',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_65': {'abstract': 'This paper explores the use of L2-specific grammatical microsystems as elements of the domain knowledge of an Intelligent Computer-assisted Language Learning (ICALL) system. We report on the design of new grammatico-functional measures and their association with proficiency. We illustrate the approach with the design of the IT, THIS, THAT proform microsystem. The measures rely on the paradigmatic relations between words of the same linguistic functions. They are operationalised with one frequency-based and two probabilistic methods, i.e., the relative proportions of the forms and their likelihood of occurrence. Ordinal regression models show that the measures are significant in terms of association with CEFR levels, paving the way for their introduction in a specific proform microsystem expert model.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cyriel Mallart',\n",
       "    'Andrew Simpkin',\n",
       "    'Rmi Venant',\n",
       "    'Nicolas Ballier',\n",
       "    'Bernardo Stearns',\n",
       "    'Jen Yu Li',\n",
       "    'Thomas Gaillat'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_65',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring a New Grammatico-functional Type of Measure as Part of a Language Learning Expert System',\n",
       "   'tldr': 'This paper explores the use of L2-specific grammatical microsystems as elements of the domain knowledge of an Intelligent Computer-assisted Language Learning (ICALL) system. We report on the design of new grammatico-functional measures and their association with proficiency. We illustrate the approa',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_66': {'abstract': \"Lexical complexity prediction (LCP) is the task of predicting the complexity of words in a text on a continuous scale. It plays a vital role in simplifying or annotating complex words to assist readers.To study lexical complexity in Japanese, we construct the first Japanese LCP dataset. Our dataset provides separate complexity scores for Chinese/Korean annotators and others to address the readers' L1-specific needs. In the baseline experiment, we demonstrate the effectiveness of a BERT-based system for Japanese LCP.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yusuke Ide',\n",
       "    'Masato Mita',\n",
       "    'Adam Nohejl',\n",
       "    'Hiroki Ouchi',\n",
       "    'Taro Watanabe'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_66',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Japanese Lexical Complexity for Non-Native Readers: A New Dataset',\n",
       "   'tldr': 'Lexical complexity prediction (LCP) is the task of predicting the complexity of words in a text on a continuous scale. It plays a vital role in simplifying or annotating complex words to assist readers.To study lexical complexity in Japanese, we construct the first Japanese LCP dataset. Our dataset ',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_67': {'abstract': 'The paper presents experiments on using a Grammatical Error Correction (GEC) model to assess the correctness of answers that language learners give to grammar exercises. We explored whether a GEC model can be applied in the language learning context for a language with complex morphology. We empirically check a hypothesis that a GEC model corrects only errors and leaves correct answers unchanged. We perform a test on assessing learner answers in a real but constrained language-learning setup: the learners answer only fill-in-the-blank and multiple-choice exercises. For this purpose, we use ReLCo, a publicly available manually annotated learner dataset in Russian (Katinskaia et al., 2022). In this experiment, we fine-tune a large-scale T5 language model for the GEC task and estimate its performance on the RULEC-GEC dataset (Rozovskaya and Roth, 2019) to compare with top-performing models. We also release an updated version of the RULEC-GEC test set, manually checked by native speakers. Our analysis shows that the GEC model performs reasonably well in detecting erroneous answers to grammar exercises and potentially can be used for best-performing error types in a real learning setup. However, it struggles to assess answers which were tagged by human annotators as alternative-correct using the aforementioned hypothesis. This is in large part due to a still low recall in correcting errors, and the fact that the GEC model may modify even correct words---it may generate plausible alternatives, which are hard to evaluate against the gold-standard reference.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Anisia Katinskaia', 'Roman Yangarber'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_67',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Grammatical Error Correction for Sentence-level Assessment in Language Learning',\n",
       "   'tldr': 'The paper presents experiments on using a Grammatical Error Correction (GEC) model to assess the correctness of answers that language learners give to grammar exercises. We explored whether a GEC model can be applied in the language learning context for a language with complex morphology. We empiric',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_69': {'abstract': \"An inclusive society needs to facilitate access to information for all of its members, including citizens with low literacy and with non-native language skills. We present an approach to assess Dutch text complexity on the sentence level and conduct an interpretability analysis to explore the link between neural models and linguistic complexity features. Building on these findings, we develop the first contextual lexical simplification model for Dutch and publish a pilot dataset for evaluation. We go beyondprevious work which primarily targeted lexical substitution and propose strategies for adjusting the model's linguistic register to generate simpler candidates. Our results indicate that continual pre-training and multi-task learning with conceptually related tasks are promising directions for ensuring the simplicity of the generated substitutions.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Eliza Hobo', 'Charlotte Pouw', 'Lisa Beinborn'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_69',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '\"Geen makkie\": Interpretable Classification and Simplification of Dutch Text Complexity',\n",
       "   'tldr': 'An inclusive society needs to facilitate access to information for all of its members, including citizens with low literacy and with non-native language skills. We present an approach to assess Dutch text complexity on the sentence level and conduct an interpretability analysis to explore the link b',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_7': {'abstract': \"Single Choice exercises constitute a central exercise type for language learning in a learner's progression from mere implicit exposure through input enhancement to productive language use in open exercises. Distractors that support learning in the individual zone of proximal development should not be derived from static analyses of learner corpora, but rely on dynamic learning analytics based on half-open exercises. We demonstrate how a system's error diagnosis module can be re-used for automatic and dynamic generation and adaptation of distractors, as well as to inform exercise generation in terms of relevant learning goals and reasonable chunking in Jumbled Sentences exercises.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tanja Heck', 'Detmar Meurers'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Using Learning Analytics for Adaptive Exercise Generation',\n",
       "   'tldr': \"Single Choice exercises constitute a central exercise type for language learning in a learner's progression from mere implicit exposure through input enhancement to productive language use in open exercises. Distractors that support learning in the individual zone of proximal development should not \",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_74': {'abstract': \"This paper describes a CEFR-based classifier of single-word and multi-word lexical complexity in context from a second language learner perspective in English and in French, developed as an analytical tool for the pedagogical team of the language learning application Mauril. We provide an overview of the required corpora and the way we transformed it into rich contextual representations that allow the disambiguation and accurate labelling in context of polysemous occurrences of a given lexical item. We report evaluation results for all models, including two multi-lingual lexical classifiers evaluated on novel French datasets created for this experiment. Finally, we share the perspective of Mauril's pedagogical team on the limitations of such systems.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Desislava Aleksandrova', 'Vincent Pouliot'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_74',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CEFR-based Contextual Lexical Complexity Classifier in English and French',\n",
       "   'tldr': 'This paper describes a CEFR-based classifier of single-word and multi-word lexical complexity in context from a second language learner perspective in English and in French, developed as an analytical tool for the pedagogical team of the language learning application Mauril. We provide an overview o',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_75': {'abstract': 'Classroom discourse is a core medium of instruction  analyzing it can provide a window into teaching and learning as well as driving the development of new tools for improving instruction. We introduce the largest dataset of mathematics classroom transcripts available to researchers, and demonstrate how this data can help improve instruction. The dataset consists of 1,660 45-60 minute long 4th and 5th grade elementary mathematics observations collected by the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The anonymized transcripts represent data from 317 teachers across 4 school districts that serve largely historically marginalized students. The transcripts come with rich metadata, including turn-level annotations for dialogic discourse moves, classroom observation scores, demographic information, survey responses and student test scores. We demonstrate that our natural language processing model, trained on our turn-level annotations, can learn to identify dialogic discourse moves and these moves are correlated with better classroom observation scores and learning outcomes. This dataset opens up several possibilities for researchers, educators and policymakers to learn about and improve K-12 instruction. The dataset can be found at https://github.com/ddemszky/classroom-transcript-analysis.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dorottya Demszky', 'Heather Hill'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_75',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts',\n",
       "   'tldr': 'Classroom discourse is a core medium of instruction  analyzing it can provide a window into teaching and learning as well as driving the development of new tools for improving instruction. We introduce the largest dataset of mathematics classroom transcripts available to researchers, and demonstrate',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_76': {'abstract': 'Online learning platforms offer a wealth of educational material, but as the amount of content on these platforms grows, students may struggle to determine the most efficient order in which to cover the material to achieve a particular learning objective. In this paper, we propose a feature-based method for identifying pre-requisite dependencies between academic videos. Our approach involves using a transcript engine with a language model to transcribe domain-specific terms and then extracting novel similarity-based features to determine pre-requisite dependencies between video transcripts. This approach succeeds due to the development of a novel corpus of K-12 academic text, which was created using a proposed feature-based document parser. We evaluate our method on hand-annotated datasets for transcript extraction, video pre-requisites determination, and textbook parsing, which we have released. Our method for pre-requisite edge determination shows significant improvement (+4.7\\\\%-10.24\\\\% F1-score) compared to existing methods.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Rushil Thareja',\n",
       "    'Ritik Garg',\n",
       "    'Shiva Baghel',\n",
       "    'Deep Dwivedi',\n",
       "    'Mukesh Mohania',\n",
       "    'Ritvik Kulshrestha'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_76',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Auto-req: Automatic detection of pre-requisite dependencies between academic videos',\n",
       "   'tldr': 'Online learning platforms offer a wealth of educational material, but as the amount of content on these platforms grows, students may struggle to determine the most efficient order in which to cover the material to achieve a particular learning objective. In this paper, we propose a feature-based me',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_77': {'abstract': 'Pre-trained large language models (PLMs) are adaptable to a wide range of downstream tasks by fine-tuning their rich contextual embeddings to the task, often without requiring much task-specific data. In this paper, we explore the use of a recently developed Hebrew PLM  aleph-BERT  for automated short answer grading of high school biology items. We show that the alephBERT-based system outperforms a strong CNN-based baseline, and that it general-izes unexpectedly well in a zero-shot paradigm to items on an unseen topic that address the same underlying biological concepts, opening up the possibility of automatically assessing new items without item-specific fine-tuning.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Abigail Gurin Schleifer',\n",
       "    'Beata Beigman Klebanov',\n",
       "    'Moriah Ariely',\n",
       "    'Giora Alexandron'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_77',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Transformer-based Hebrew NLP models for Short Answer Scoring in Biology',\n",
       "   'tldr': 'Pre-trained large language models (PLMs) are adaptable to a wide range of downstream tasks by fine-tuning their rich contextual embeddings to the task, often without requiring much task-specific data. In this paper, we explore the use of a recently developed Hebrew PLM  aleph-BERT  for automated sho',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_78': {'abstract': \"In recent decades, there has been a significant push to leverage technology to aid both teachers and students in the classroom. Language processing advancements have been harnessed to provide better tutoring services, automated feedback to teachers, improved peer-to-peer feedback mechanisms, and measures of student comprehension for reading. Automated question generation systems have the potential to significantly reduce teachers' workload in the latter. In this paper, we compare three differ- ent neural architectures for question generation across two types of reading material: narratives and textbooks. For each architecture, we explore the benefits of including question attributes in the input representation. Our models show that a T5 architecture has the best overall performance, with a RougeL score of 0.536 on a narrative corpus and 0.316 on a textbook corpus. We break down the results by attribute and discover that the attribute can improve the quality of some types of generated questions, including Action and Character, but this is not true for all models.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['E. Margaret Perkoff',\n",
       "    'Abhidip Bhattacharyya',\n",
       "    'Jon Cai',\n",
       "    'Jie Cao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_78',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Comparing Neural Question Generation Architectures for Reading Comprehension',\n",
       "   'tldr': 'In recent decades, there has been a significant push to leverage technology to aid both teachers and students in the classroom. Language processing advancements have been harnessed to provide better tutoring services, automated feedback to teachers, improved peer-to-peer feedback mechanisms, and mea',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_8': {'abstract': 'Large Language Models (LLMs) offer novel opportunities for educational applications that have the potential to transform traditional learning for students. Despite AI-enhanced applications having the potential to provide personalized learning experiences, more studies are needed on the design of generative AI systems and evidence for using them in real educational settings. In this paper, we design, implement and evaluate \\\\textbackslash{}texttt\\\\{Reviewriter\\\\}, a novel tool to provide students with AI-generated instructions for writing peer reviews in German. Our study identifies three key aspects: a) we provide insights into student needs when writing peer reviews with generative models which we then use to develop a novel system to provide adaptive instructions b) we fine-tune three German language models on a selected corpus of 11,925 student-written peer review texts in German and choose German-GPT2 based on quantitative measures and human evaluation, and c) we evaluate our tool with fourteen students, revealing positive technology acceptance based on quantitative measures. Additionally, the qualitative feedback presents the benefits and limitations of generative AI in peer review writing.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiaotian Su',\n",
       "    'Thiemo Wambsganss',\n",
       "    'Roman Rietsche',\n",
       "    'Seyed Parsa Neshaei',\n",
       "    'Tanja Kser'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Reviewriter: AI-Generated Instructions For Peer Review Writing',\n",
       "   'tldr': 'Large Language Models (LLMs) offer novel opportunities for educational applications that have the potential to transform traditional learning for students. Despite AI-enhanced applications having the potential to provide personalized learning experiences, more studies are needed on the design of gen',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_81': {'abstract': \"We present research aimed at solving a problem in  assessment of oral reading fluency using children's oral reading data from our online book reading app. It is known that properties of the passage being read aloud impact fluency estimates; therefore, passage-based measures are  used to remove passage-related variance when estimating growth in oral reading fluency. However, passage-based measures reported in the literature tend to treat passages as independent events, without explicitly modeling accumulation of lexical experience as one reads through a book. We propose such a model and show that it helps explain additional variance in the measurements of children's fluency as they read through a book, improving over a strong baseline. These results have implications for measuring growth in oral reading fluency.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Beata Beigman Klebanov',\n",
       "    'Michael Suhan',\n",
       "    'Zuowei Wang',\n",
       "    \"Tenaha O'reilly\"],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_81',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A dynamic model of lexical experience for tracking of oral reading fluency',\n",
       "   'tldr': \"We present research aimed at solving a problem in  assessment of oral reading fluency using children's oral reading data from our online book reading app. It is known that properties of the passage being read aloud impact fluency estimates; therefore, passage-based measures are  used to remove passa\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_86': {'abstract': \"Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes  language assessments.  While automated scoring approaches  are mature and have been around for decades,  human scoring is still considered the gold standard, despite its high costs and  well-known issues such as human rater fatigue and bias.  The recent introduction of large language models (LLMs) brings new opportunities for automated scoring.  In this paper, we evaluate how well GPT-3.5 and GPT-4 can rate short essay responses written by L2 English learners on a high-stakes language assessment, computing inter-rater agreement with human ratings. Results show that when calibration examples are provided, GPT-4 can perform almost as well as modern Automatic Writing Evaluation (AWE) methods, but agreement with human ratings can vary depending on the test-taker's first language (L1).\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kevin P. Yancey',\n",
       "    'Geoffrey Laflair',\n",
       "    'Anthony Verardi',\n",
       "    'Jill Burstein'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_86',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Rating Short L2 Essays on the CEFR Scale with GPT-4',\n",
       "   'tldr': 'Essay scoring is a critical task used to evaluate second-language (L2) writing proficiency on high-stakes  language assessments.  While automated scoring approaches  are mature and have been around for decades,  human scoring is still considered the gold standard, despite its high costs and  well-kn',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_88': {'abstract': 'L1-L2 parallel dependency treebanks are UD-annotated corpora of learner sentences paired with correction hypotheses. Automatic morphosyntactical annotation has the potential to remove the need for explicit manual error tagging and improve interoperability, but makes it more challenging to locate grammatical errors in the resulting datasets. We therefore propose a novel method for automatically extracting morphosyntactical error patterns and perform a preliminary bilingual evaluation of its first implementation through a similar example retrieval task. The resulting pipeline is also available as a prototype CALL application.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Arianna Masciolini', 'Elena Volodina', 'Dana Dannlls'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_88',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards automatically extracting morphosyntactical error patterns from L1-L2 parallel dependency treebanks',\n",
       "   'tldr': 'L1-L2 parallel dependency treebanks are UD-annotated corpora of learner sentences paired with correction hypotheses. Automatic morphosyntactical annotation has the potential to remove the need for explicit manual error tagging and improve interoperability, but makes it more challenging to locate gra',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_9': {'abstract': \"We introduce the Korean-Learner-Morpheme (KLM) corpus, a manually annotated dataset consisting of 129,784 morphemes from second language (L2) learners of Korean, featuring morpheme tokenization and part-of-speech (POS) tagging. We evaluate the performance of four Korean morphological analyzers in tokenization and POS tagging on the L2- Korean corpus. Results highlight the analyzers' reduced performance on L2 data, indicating the limitation of advanced deep-learning models when dealing with L2-Korean corpora. We further show that fine-tuning one of the models with the KLM corpus improves its accuracy of tokenization and POS tagging on L2-Korean dataset.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hakyung Sung', 'Gyu-Ho Shin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards L2-friendly pipelines for learner corpora: A case of written production by L2-Korean learners',\n",
       "   'tldr': 'We introduce the Korean-Learner-Morpheme (KLM) corpus, a manually annotated dataset consisting of 129,784 morphemes from second language (L2) learners of Korean, featuring morpheme tokenization and part-of-speech (POS) tagging. We evaluate the performance of four Korean morphological analyzers in to',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_90': {'abstract': 'Since performing exercises (including, e.g.,practice tests) forms a crucial component oflearning, and creating such exercises requiresnon-trivial effort from the teacher. There is agreat value in automatic exercise generationin digital tools in education. In this paper, weparticularly focus on automatic creation of gap-filling exercises for language learning, specifi-cally grammar exercises. Since providing anyannotation in this domain requires human ex-pert effort, we aim to avoid it entirely and ex-plore the task of converting existing texts intonew gap-filling exercises, purely based on anexample exercise, without explicit instructionor detailed annotation of the intended gram-mar topics. We contribute (i) a novel neuralnetwork architecture specifically designed foraforementioned gap-filling exercise generationtask, and (ii) a real-world benchmark datasetfor French grammar. We show that our modelfor this French grammar gap-filling exercisegeneration outperforms a competitive baselineclassifier by 8\\\\% in F1 percentage points, achiev-ing an average F1 score of 82\\\\%. Our model im-plementation and the dataset are made publiclyavailable to foster future research, thus offeringa standardized evaluation and baseline solutionof the proposed partially annotated data predic-tion task in grammar exercise creation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Semere Kiros Bitew',\n",
       "    'Johannes Deleu',\n",
       "    'A. Seza Doruz',\n",
       "    'Chris Develder',\n",
       "    'Thomas Demeester'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_90',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning',\n",
       "   'tldr': 'Since performing exercises (including, e.g.,practice tests) forms a crucial component oflearning, and creating such exercises requiresnon-trivial effort from the teacher. There is agreat value in automatic exercise generationin digital tools in education. In this paper, weparticularly focus on autom',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_93': {'abstract': \"The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI's ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Though researchers have recently explored how ChatGPT could possibly assist in student learning, few studies have applied these techniques to real-world classroom settings involving teachers and students. In this study, we implement a reading comprehension exercise generation system that provides high-quality and personalized reading materials for middle school English learners in China. Extensive evaluations of the generated reading passages and corresponding exercise questions, conducted both automatically and manually, demonstrate that the system-generated materials are suitable for students and even surpass the quality of existing human-written ones. By incorporating first-hand feedback and suggestions from experienced educators, this study serves as a meaningful pioneering application of ChatGPT, shedding light on the future design and implementation of LLM-based systems in the educational context.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Changrong Xiao',\n",
       "    'Sean Xin Xu',\n",
       "    'Kunpeng Zhang',\n",
       "    'Yufang Wang',\n",
       "    'Lei Xia'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_93',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating Reading Comprehension Exercises Generated by LLMs: A Showcase of ChatGPT in Education Applications',\n",
       "   'tldr': \"The recent advancement of pre-trained Large Language Models (LLMs), such as OpenAI's ChatGPT, has led to transformative changes across fields. For example, developing intelligent systems in the educational sector that leverage the linguistic capabilities of LLMs demonstrates a visible potential. Tho\",\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_97': {'abstract': \"Coaching, which involves classroom observation and expert feedback, is a widespread and fundamental part of teacher training. However, the majority of teachers do not have access to consistent, high quality coaching due to limited resources and access to expertise. We explore whether generative AI could become a cost-effective complement to expert feedback by serving as an automated teacher coach. In doing so, we propose three teacher coaching tasks for generative AI: (A) scoring transcript segments based on classroom observation instruments, (B)identifying highlights and missed opportunities for good instructional strategies, and (C) providing actionable suggestions for eliciting more student reasoning. We recruit expert math teachers to evaluate the zero-shot performance of ChatGPT on each of these tasks for elementary math classroom transcripts. Our results reveal that ChatGPT generates responses that are relevant to improving instruction, but they are often not novel or insightful.  For example, 82\\\\% of the model's suggestions point to places in the transcript where the teacher is already implementing that suggestion. Our work highlights the challenges of producing insightful, novel and truthful feedback for teachers while paving the way for future research to address these obstacles and improve the capacity of generative AI to coach teachers.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Rose Wang', 'Dorottya Demszky'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_97',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Is ChatGPT a Good Teacher Coach? Measuring Zero-Shot Performance For Scoring and Providing Actionable Insights on Classroom Instruction',\n",
       "   'tldr': 'Coaching, which involves classroom observation and expert feedback, is a widespread and fundamental part of teacher training. However, the majority of teachers do not have access to consistent, high quality coaching due to limited resources and access to expertise. We explore whether generative AI c',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BEA_99': {'abstract': \"In English speaking assessment, pretrained large language models (LLMs) such as BERT can score constructed response items as accurately as human raters. Less research has investigated whether LLMs perpetuate or exacerbate biases, which would pose problems for the fairness and validity of the test. This study examines gender and native language (L1) biases in human and automated scores, using an off-the-shelf (OOS) BERT model. Analyses focus on a specific type of bias known as differential item functioning (DIF), which compares examinees of similar English language proficiency. Results show that there is a moderate amount of DIF, based on examinees' L1 background in grade band 912. DIF is higher when scored by an OOS BERT model, indicating that BERT may exacerbate this bias; however, in practical terms, the degree to which BERT exacerbates DIF is very small. Additionally, there is more DIF for longer speaking items and for older examinees, but BERT does not exacerbate these patterns of DIF.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Alexander Kwako',\n",
       "    'Yixin Wan',\n",
       "    'Jieyu Zhao',\n",
       "    'Mark Hansen',\n",
       "    'Kai-Wei Chang',\n",
       "    'Li Cai'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BEA'],\n",
       "   'id': 'BEA_99',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Does BERT Exacerbate Gender or L1 Biases in Automated English Speaking Assessment?',\n",
       "   'tldr': 'In English speaking assessment, pretrained large language models (LLMs) such as BERT can score constructed response items as accurately as human raters. Less research has investigated whether LLMs perpetuate or exacerbate biases, which would pose problems for the fairness and validity of the test. T',\n",
       "   'track': '18th Workshop on Innovative Use of NLP for Building Educational Applications',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_1': {'abstract': 'We present a cross-domain approach for automated measurement and context extraction based on pre-trained language models. We construct a multi-source, multi-domain corpus and train an end-to-end extraction pipeline. We then apply multi-source task-adaptive pre-training and fine-tuning to benchmark the cross-domain generalization capability of our model. Further, we conceptualize and apply a task-specific error analysis and derive insights for future work. Our results suggest that multi-source training leads to the best overall results, while single-source training yields the best results for the respective individual domain. While our setup is successful at extracting quantity values and units, more research is needed to improve the extraction of contextual entities. We make the cross-domain corpus used in this work available online.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yueling Li', 'Sebastian Martschat', 'Simone Paolo Ponzetto'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi-Source (Pre-)Training for Cross-Domain Measurement, Unit and Context Extraction',\n",
       "   'tldr': 'We present a cross-domain approach for automated measurement and context extraction based on pre-trained language models. We construct a multi-source, multi-domain corpus and train an end-to-end extraction pipeline. We then apply multi-source task-adaptive pre-training and fine-tuning to benchmark t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_101': {'abstract': 'Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual errors. One popular intervention for improving factuality is using additional external knowledge to provide factual grounding. However, it is unclear how these grounding sources should be retrieved, selected, or integrated, and how supplementary grounding documents might affect the readability or relevance of the generated summaries. We develop a simple method for selecting grounding sources and integrating them with source documents. We then use the BioLaySum summarization dataset to evaluate the effects of different grounding sources on summary quality. We found that grounding source documents improves the relevance and readability of lay summaries but does not improve factuality of lay summaries. This continues to be true in zero-shot summarization settings where we hypothesized that grounding might be even more important for factual lay summaries.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Domenic Rosati'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_101',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GRASUM at BioLaySumm Task 1: Background Knowledge Grounding for Readable, Relevant, and Factual Biomedical Lay Summaries',\n",
       "   'tldr': 'Communication of scientific findings to the public is important for keeping non-experts informed of developments such as life-saving medical treatments. However, generating readable lay summaries from scientific documents is challenging, and currently, these summaries suffer from critical factual er',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_102': {'abstract': 'This paper summarizes two approaches developed for BioNLP2023 workshop task 1A: clinical problem list summarization. We develop two types of methods with either rules or pre-trained language models. In the rule-based summarization model, we leverage UMLS (Unified Medical Language System) and a negation detector to extract text spans to represent the summary. We also fine tune three pre-trained language models (BART, T5 and GPT2) to generate the summaries.  Experiment results show the rule based system returns extractive summaries but lower ROUGE-L score (0.043), while the fine tuned T5 returns a higher ROUGE-L score (0.208).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ming Liu', 'Dan Zhang', 'Weicong Tan', 'He Zhang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_102',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DeakinNLP at ProbSum 2023: Clinical Progress Note Summarization with Rules and Language ModelsClinical Progress Note Summarization with Rules and Languague Models',\n",
       "   'tldr': 'This paper summarizes two approaches developed for BioNLP2023 workshop task 1A: clinical problem list summarization. We develop two types of methods with either rules or pre-trained language models. In the rule-based summarization model, we leverage UMLS (Unified Medical Language System) and a negat',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_104': {'abstract': 'This paper describes the submission of the TALP-UPC team to the Problem List Summarization task from the BioNLP 2023 workshop. This task consists of automatically extracting a list of health issues from the e-health medical record of a given patient. Our submission combines additional steps of data annotationwith finetuning of BERT pre-trained language models. Our experiments focus on the impact of finetuning on different datasets as well as the addition of data augmentation techniques to delay overfitting.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Neil Torrero', 'Gerard Sant', 'Carlos Escolano'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_104',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TALP-UPC at ProbSum 2023: Fine-tuning and Data Augmentation Strategies for NER',\n",
       "   'tldr': 'This paper describes the submission of the TALP-UPC team to the Problem List Summarization task from the BioNLP 2023 workshop. This task consists of automatically extracting a list of health issues from the e-health medical record of a given patient. Our submission combines additional steps of data ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_106': {'abstract': \"Medical progress notes play a crucial role in documenting a patient's hospital journey, including his or her condition, treatment plan, and any updates for healthcare providers. Automatic summarisation of a patient's problems in the form of a ``problem list'' can aid stakeholders in understanding a patient's condition, reducing workload and cognitive bias. BioNLP 2023 Shared Task 1A focusses on generating a list of diagnoses and problems from the provider's progress notes during hospitalisation. In this paper, we introduce our proposed approach to this task, which integrates two complementary components. One component employs large language models (LLMs) for data augmentation; the other is an abstractive summarisation LLM with a novel pre-training objective for generating the patients' problems summarised as a list. Our approach was ranked second among all submissions to the shared task.  The performance of our model on the development and test datasets shows that our approach is more robust on unknown data, with an improvement of up to 3.1 points over the same size of the larger model.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hao Li',\n",
       "    'Yuping Wu',\n",
       "    'Viktor Schlegel',\n",
       "    'Riza Batista-Navarro',\n",
       "    'Thanh-Tung Nguyen',\n",
       "    'Abhinav Ramesh Kashyap',\n",
       "    'Xiao-Jun Zeng',\n",
       "    'Daniel Beck',\n",
       "    'Stefan Winkler',\n",
       "    'Goran Nenadic'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_106',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Team:PULSAR at ProbSum 2023:PULSAR: Pre-training with Extracted Healthcare Terms for Summarising Patients' Problems and Data Augmentation with Black-box Large Language Models\",\n",
       "   'tldr': \"Medical progress notes play a crucial role in documenting a patient's hospital journey, including his or her condition, treatment plan, and any updates for healthcare providers. Automatic summarisation of a patient's problems in the form of a ``problem list'' can aid stakeholders in understanding a \",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_107': {'abstract': \"In this paper, we elaborate on our approach for the shared task 1A issued by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in the digitization of health records, a need arises for quick and precise summarization of large amounts of records. With the help of summarization, medical professionals can sieve through multiple records in a short span of time without overlooking any crucial point. We use abstractive text summarization for this task and experiment with multiple state-of-the-art models like Pegasus, BART, and T5, along with various pre-processing and data augmentation techniques to generate summaries from patients' progress notes. For this task, the metric used was the ROUGE-L score. From our experiments, we conclude that Pegasus is the best-performing model on the dataset, achieving a ROUGE-L F1 score of 0.2744 on the test dataset (3rd rank on the leaderboard).\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gaurav Kolhatkar',\n",
       "    'Aditya Paranjape',\n",
       "    'Omkar Gokhale',\n",
       "    'Dipali Kadam'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_107',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Team Converge at ProbSum 2023: Abstractive Text Summarization of Patient Progress Notes',\n",
       "   'tldr': 'In this paper, we elaborate on our approach for the shared task 1A issued by BioNLP Workshop 2023 titled Problem List Summarization. With an increase in the digitization of health records, a need arises for quick and precise summarization of large amounts of records. With the help of summarization, ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_108': {'abstract': 'In this paper, we consider the challenge of summarizing patients medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that ClinicalT5 fine-tuned to 765 medical clinic notes outperforms other extractive, abstractive and zero-shot baselines, yielding reasonable baseline systems for medical note summarization. Further, we introduce Hierarchical Ensemble of Summarization Models (HESM), consisting of token-level ensembles of diverse fine-tuned ClinicalT5 models, followed by Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization performance boost, and when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which was the best-performing system at the top of the shared task leaderboard.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Potsawee Manakul',\n",
       "    'Yassir Fathullah',\n",
       "    'Adian Liusie',\n",
       "    'Vyas Raina',\n",
       "    'Vatsal Raina',\n",
       "    'Mark Gales'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_108',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models',\n",
       "   'tldr': 'In this paper, we consider the challenge of summarizing patients medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that ClinicalT5 fine-tuned to 765 medical clinic notes outperforms other extractive, abst',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_11': {'abstract': 'A common metric for evaluating Automatic Speech Recognition (ASR) is Word Error Rate (WER) which solely takes into account discrepancies at the word-level. Although useful, WER is not guaranteed to correlate well with human judgment or performance on downstream tasks that use ASR. Meaningful assessment of ASR mistakes becomes even more important in high-stake scenarios such as health-care. We propose 2 general measures to evaluate the severity of mistakes made by ASR systems, one based on sentiment analysis and another based on text embeddings. We evaluate these measures on simulated patient-doctor conversations using 5 ASR systems. Results show that these measures capture characteristics of ASR errors that WER does not. Furthermore, we train an ASR system incorporating severity and demonstrate the potential for using severity not only in the evaluation, but in the development of ASR. Advantages and limitations of this methodology are analyzed and discussed.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ryan Whetten', 'Casey Kennington'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating and Improving Automatic Speech Recognition using Severity',\n",
       "   'tldr': 'A common metric for evaluating Automatic Speech Recognition (ASR) is Word Error Rate (WER) which solely takes into account discrepancies at the word-level. Although useful, WER is not guaranteed to correlate well with human judgment or performance on downstream tasks that use ASR. Meaningful assessm',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_110': {'abstract': 'This paper presents our system at the Radiology Report Summarization Shared Task-1B of the 22nd BioNLP Workshop 2023. Inspired by the work of the BioBART model, we continuously pre-trained a general domain BART model with biomedical data to adapt it to this specific domain. In the pre-training phase, several pre-training tasks are aggregated to inject linguistic knowledge and increase the abstractivity of the generated summaries. We present the results of our models, and also, we have carried out an additional study on the lengths of the generated summaries, which has provided us with interesting information.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vicent Ahuir Esteve', 'Encarna Segarra', 'Lluis Hurtado'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_110',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ELiRF-VRAIN at BioNLP Task 1B: Radiology Report Summarization',\n",
       "   'tldr': 'This paper presents our system at the Radiology Report Summarization Shared Task-1B of the 22nd BioNLP Workshop 2023. Inspired by the work of the BioBART model, we continuously pre-trained a general domain BART model with biomedical data to adapt it to this specific domain. In the pre-training phase',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_111': {'abstract': 'This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained language models. The best performing system based on domain-specific model reached 33.96 F1RadGraph score which is the fourth best result among the challenge participants. This model was made publicly available on HuggingFace. We also describe an attempt of Proximal Policy Optimization Reinforcement Learning that was made in order to improve the factual correctness measured with F1RadGraph but did not lead to satisfactory results.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mariia Chizhikova',\n",
       "    'Manuel Diaz-Galiano',\n",
       "    'L. Alfonso Urena-Lopez',\n",
       "    'M. Teresa Martin-Valdivia'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_111',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SINAI at RadSum23: Radiology Report Summarization Based on Domain-Specific Sequence-To-Sequence Transformer Model',\n",
       "   'tldr': 'This paper covers participation of the SINAI team in the shared task 1B: Radiology Report Summarization at the BioNLP workshop held on ACL 2023. Our proposal follows a sequence-to-sequence approach which leverages pre-trained multilingual general domain and monolingual biomedical domain pre-trained ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_112': {'abstract': \"This paper presents our contribution to the RadSum23 shared task organized as part of the BioNLP 2023. We compared state-of-the-art generative language models in generating high-quality summaries from radiology reports. A two-stage fine-tuning approach was introduced for utilizing knowledge learnt from different datasets. We evaluated the performance of our method using a variety of metrics, including BLEU, ROUGE, bertscore, CheXbert, and RadGraph. Our results revealed the potentials of different models in summarizing radiology reports and demonstrated the effectiveness of the two-stage fine-tuning approach. We also discussed the limitations and future directions of our work, highlighting the need for better understanding the architecture design's effect and optimal way of fine-tuning accordingly in automatic clinical summarizations.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jinge Wu', 'Daqian Shi', 'Abul Hasan', 'Honghan Wu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_112',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'KnowLab at RadSum23: comparing pre-trained language models in radiology report summarization',\n",
       "   'tldr': 'This paper presents our contribution to the RadSum23 shared task organized as part of the BioNLP 2023. We compared state-of-the-art generative language models in generating high-quality summaries from radiology reports. A two-stage fine-tuning approach was introduced for utilizing knowledge learnt f',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_113': {'abstract': 'This paper describes the experiments undertaken and their results as part of the BioNLP 2023 workshop. We took part in Task 1B: Radiology Report Summarization. Multiple runs were submitted for evaluation from solutions utilizing transfer learning from pre-trained transformer models, which were then fine-tuned on MIMIC-III dataset, for abstractive report summarization.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sri Macharla', 'Ashok Madamanchi', 'Nikhilesh Kancharla'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_113',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'nav-nlp at RadSum23: Abstractive Summarization of Radiology Reports using BART Finetuning',\n",
       "   'tldr': 'This paper describes the experiments undertaken and their results as part of the BioNLP 2023 workshop. We took part in Task 1B: Radiology Report Summarization. Multiple runs were submitted for evaluation from solutions utilizing transfer learning from pre-trained transformer models, which were then ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_114': {'abstract': \"We describe the participation of team e-Health CSIRO in the BioNLP RadSum task of 2023. This task aims to develop automatic summarisation methods for radiology. The subtask that we participated in was multimodal; the impression section of a report was to be summarised from a given findings section and set of Chest X-rays (CXRs) of a subject's study. For our method, we adapted an encoder-to-decoder model for CXR report generation to the subtask. e-Health CSIRO placed seventh amongst the participating teams with a RadGraph ER F1 score of 23.9.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aaron Nicolson', 'Jason Dowling', 'Bevan Koopman'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_114',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'e-Health CSIRO at RadSum23: Adapting a Chest X-Ray Report Generator to Multimodal Radiology Report Summarisation',\n",
       "   'tldr': 'We describe the participation of team e-Health CSIRO in the BioNLP RadSum task of 2023. This task aims to develop automatic summarisation methods for radiology. The subtask that we participated in was multimodal; the impression section of a report was to be summarised from a given findings section a',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_116': {'abstract': 'Instruction-tuned generative large language models (LLMs), such as ChatGPT and Bloomz, possess excellent generalization abilities. However, they face limitations in understanding radiology reports, particularly when generating the IMPRESSIONS section from the FINDINGS section. These models tend to produce either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system that leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs, enhancing their medical knowledge and performance on specific medical tasks. We demonstrate that this system performs better in a zero-shot setting compared to several pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task. Furthermore, it ranks 1st among participating systems in Task 1B: Radiology Report Summarization.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sanjeev Kumar Karn',\n",
       "    'Rikhiya Ghosh',\n",
       "    'Kusuma P',\n",
       "    'Oladimeji Farri'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_116',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation',\n",
       "   'tldr': 'Instruction-tuned generative large language models (LLMs), such as ChatGPT and Bloomz, possess excellent generalization abilities. However, they face limitations in understanding radiology reports, particularly when generating the IMPRESSIONS section from the FINDINGS section. These models tend to p',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_117': {'abstract': 'Radiology report summarization aims to automatically provide concise summaries of radiology findings, reducing time and errors in manual summaries. However, current methods solely summarize the text, which overlooks critical details in the images. Unfortunately, directly using the images in a multimodal model is difficult. Multimodal models are susceptible to overfitting due to their increased capacity, and modalities tend to overfit and generalize at different rates. Thus, we propose a novel retrieval-based approach that uses image similarities to generate additional text features. We further employ few-shot with chain-of-thought and ensemble techniques to boost performance. Overall, our method achieves state-of-the-art performance in the F1RadGraph score, which measures the factual correctness of summaries. We rank second place in both MIMIC-CXR and MIMIC-III hidden tests among 11 teams.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tongnian Wang', 'Xingmeng Zhao', 'Anthony Rios'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_117',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UTSA-NLP at RadSum23: Multi-modal Retrieval-Based Chest X-Ray Report Summarization',\n",
       "   'tldr': 'Radiology report summarization aims to automatically provide concise summaries of radiology findings, reducing time and errors in manual summaries. However, current methods solely summarize the text, which overlooks critical details in the images. Unfortunately, directly using the images in a multim',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_118': {'abstract': 'In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various domain-specific tasks into a simple sequence-to-sequence schema.It enables the model to effectively learn the required knowledge and skills from limited resources in the domain.Demonstrating superior performance on the benchmark datasets provided by the BioNLP shared task (Delbrouck et al., 2023), our model benefits from its training across multiple tasks and domains.With subtle techniques including ensemble and factual calibration, our system achieves first place on the RadSum23 leaderboard for the hidden test set.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gangwoo Kim',\n",
       "    'Hajung Kim',\n",
       "    'Lei Ji',\n",
       "    'Seongsu Bae',\n",
       "    'chanhwi kim',\n",
       "    'Mujeen Sung',\n",
       "    'Hyunjae Kim',\n",
       "    'Kun Yan',\n",
       "    'Eric Chang',\n",
       "    'Jaewoo Kang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_118',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization',\n",
       "   'tldr': 'In this paper, we introduce CheXOFA, a new pre-trained vision-language model (VLM) for the chest X-ray domain. Our model is initially pre-trained on various multimodal datasets within the general domain before being transferred to the chest X-ray domain. Following a prominent VLM, we unify various d',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_12': {'abstract': \"The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT's ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break down the task and evaluate ChatGPT. Our experiments show that ChatGPT's performance has a large gap with that of supervised methods and can heavily rely on the design of prompts. We further demonstrate that ChatGPT can infer more small relation classes correctly than supervised methods. The current shortcomings of ChatGPT on temporal relation extraction are also discussed in this paper. We found that ChatGPT cannot keep consistency during temporal inference and it fails in actively long-dependency temporal inference.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chenhan Yuan', 'Qianqian Xie', 'Sophia Ananiadou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Zero-shot Temporal Relation Extraction with ChatGPT',\n",
       "   'tldr': \"The goal of temporal relation extraction is to infer the temporal relation between two events in the document. Supervised models are dominant in this task. In this work, we investigate ChatGPT's ability on zero-shot temporal relation extraction. We designed three different prompt techniques to break\",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_120': {'abstract': 'We describe our systems participated in the BioLaySumm 2023 Task 1, which aims at automatically generating lay summaries of scientific articles in a simplified way so that its content becomes easier to comprehend for non-expert readers. Our approaches are based on selecting key information by both explicit and implicit strategies. For explicit selection strategies, we conduct extractive summarization based on selecting key sentences for training abstractive summarization models. For implicit selection strategies, we utilize a method based on a factorized energy-based model, which is able to extract important information from long documents to generate summaries and achieve promising results. We build our systems using sequence-to-sequence models, which enable us to leverage powerful and biomedical domain pre-trained language models and apply different strategies to generate lay summaries from long documents. We conducted various experiments to carefully investigate the effects of different aspects of this long-document summarization task such as extracting different document lengths and utilizing different pre-trained language models. We achieve the third rank in the shared task (and the second rank excluding the baseline submission of the organizers).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Phuc Phan', 'Tri Tran', 'Hai-Long Trieu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_120',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'VBD-NLP at BioLaySumm Task 1: Explicit and Implicit Key Information Selection for Lay Summarization on Biomedical Long Documents',\n",
       "   'tldr': 'We describe our systems participated in the BioLaySumm 2023 Task 1, which aims at automatically generating lay summaries of scientific articles in a simplified way so that its content becomes easier to comprehend for non-expert readers. Our approaches are based on selecting key information by both e',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_121': {'abstract': 'In this paper we tackle a lay summarization task which aims to produce lay-summary of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023 (Goldsack et al., 2023), has presented us with this lay summarization task for biomedical articles. Our proposed models provide a three-step abstractive approach for summarizing biomedical articles. Our methodology involves breaking down the original document into distinct sections, generating candidate summaries for each subsection, then finally re-ranking and selecting the top performing paragraph for each section. We run ablation studies to establish that each step in our pipeline is critical for improvement in the quality of lay summary. This model achieved the second-highest rank in terms of readability scores (Luo et al., 2022). Our work distinguishes itself from previous studies by not only considering the content of the paper but also its structure, resulting in more coherent and comprehensible lay summaries. We hope that our model for generating lay summaries of biomedical articles will be a useful resource for individuals across various domains, including academia, industry, and healthcare, who require rapid comprehension of key scientific research.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['A.S. Poornash',\n",
       "    'Atharva Deshmukh',\n",
       "    'Archit Sharma',\n",
       "    'Sriparna Saha'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_121',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'APTSumm at BioLaySumm Task 1: Biomedical Breakdown, Improving Readability by Relevancy Based Selection',\n",
       "   'tldr': 'In this paper we tackle a lay summarization task which aims to produce lay-summary of biomedical articles. BioLaySumm in the BioNLP Workshop at ACL 2023 (Goldsack et al., 2023), has presented us with this lay summarization task for biomedical articles. Our proposed models provide a three-step abstra',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_122': {'abstract': 'This study describes the model design of the NCUEE-NLP system for BioLaySumm Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA models to independently generate technical abstracts and lay summaries of biomedical articles. A total of seven evaluation metrics across three criteria were used to compare system performance. Our best submission was ranked first for relevance, second for readability, and fourth for factuality, tying first for overall performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chao-Yi Chen', 'Jen-Hao Yang', 'Lung-Hao Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_122',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NCUEE-NLP at BioLaySumm Task 2: Readability-Controlled Summarization of Biomedical Articles Using the PRIMERA Models',\n",
       "   'tldr': 'This study describes the model design of the NCUEE-NLP system for BioLaySumm Task 2 at the BioNLP 2023 workshop. We separately fine-tune pretrained PRIMERA models to independently generate technical abstracts and lay summaries of biomedical articles. A total of seven evaluation metrics across three ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_123': {'abstract': 'Lay summarization aims to simplify complex scientific information for non-expert audiences. This paper investigates the trade-off between readability and relevance in the lay summarization of long biomedical documents. We introduce a two-stage framework that attains the best readability metrics in the first subtask of BioLaySumm 2023, with 8.924 FleschKincaid Grade Level and 9.188 DaleChall Readability Score. However, this comes at the cost of reduced relevance and factuality, emphasizing the inherent challenges of balancing readability and content preservation in lay summarization. The first stage generates summaries using a large language model, such as BART with LSG attention. The second stage uses a zero-shot sentence simplification method to improve the readability of the summaries. In the second subtask, a hybrid dataset is employed to train a model capable of generating both lay summaries and abstracts. This approach achieves the best readability score and shares the top overall rank with other leading methods. Our study underscores the importance of developing effective methods for creating accessible lay summaries while maintaining information integrity. Future work will integrate simplification and summary generation within a joint optimization framework that generates high-quality lay summaries that effectively communicate scientific content to a broader audience.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Irfan Al-Hussaini', 'Austin Wu', 'Cassie Mitchell'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_123',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pathology Dynamics at BioLaySumm: the trade-off between Readability, Relevance, and Factuality in Lay Summarization',\n",
       "   'tldr': 'Lay summarization aims to simplify complex scientific information for non-expert audiences. This paper investigates the trade-off between readability and relevance in the lay summarization of long biomedical documents. We introduce a two-stage framework that attains the best readability metrics in t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_124': {'abstract': 'This paper describes the entry by the Intelligent Knowledge Management (IKM) Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical articles into concise, reader-friendly summaries that can be easily comprehended by the general public. We utilized a long-text abstractive summarization longformer model and experimented with several prompt methods for this task. Our entry placed 10th overall, but we were particularly proud to achieve a 3rd place score in the readability evaluation metric.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yu-Hsuan Wu', 'Ying-Jia Lin', 'Hung-Yu Kao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_124',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'IKM_Lab at BioLaySumm Task 1: Longformer-based Prompt Tuning for Biomedical Lay Summary Generation',\n",
       "   'tldr': 'This paper describes the entry by the Intelligent Knowledge Management (IKM) Laboratory in the BioLaySumm 2023 task1. We aim to transform lengthy biomedical articles into concise, reader-friendly summaries that can be easily comprehended by the general public. We utilized a long-text abstractive sum',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_125': {'abstract': 'This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To address this issue, lay summaries can be used to explain research findings to non-experts in an accessible form. We conduct an evaluation of autoregressive language models, both general and specialized for the biomedical domain, to generate lay summaries from biomedical research article abstracts. Our findings demonstrate that a GPT-3.5 model combined with a straightforward few-shot prompt produces lay summaries that achieve significantly relevance and factuality compared to those generated by a fine-tuned BioGPT model. However, the summaries generated by the BioGPT model exhibit better readability. Notably, our submission for the shared task achieved 1st place in the competition.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Oisn Turbitt', 'Robert Bevan', 'Mouhamad Aboshokor'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_125',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MDC at BioLaySumm Task 1: Evaluating GPT Models for Biomedical Lay Summarization',\n",
       "   'tldr': 'This paper presents our approach to the BioLaySumm Task 1 shared task, held at the BioNLP 2023 Workshop. The effective communication of scientific knowledge to the general public is often limited by the technical language used in research, making it difficult for non-experts to comprehend. To addres',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_126': {'abstract': 'As part of our participation in BioLaySumm 2023, we explored the use of large language models (LLMs) to automatically generate concise and readable summaries of biomedical research articles. We utilized pre-trained LLMs to fine-tune our summarization models on two provided datasets, and adapt them to the shared task within the constraints of training time and computational power. Our final models achieved very high relevance and factuality scores on the test set, and ranked among the top five models in the overall performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Quancheng Liu', 'Xiheng Ren', 'V.G.Vinod Vydiswaran'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_126',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LHS712EE at BioLaySumm 2023: Using BART and LED to summarize biomedical research articles',\n",
       "   'tldr': 'As part of our participation in BioLaySumm 2023, we explored the use of large language models (LLMs) to automatically generate concise and readable summaries of biomedical research articles. We utilized pre-trained LLMs to fine-tune our summarization models on two provided datasets, and adapt them t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_127': {'abstract': \"Initially, we analyzed the datasets in a statistical way so as to learn about various sections' contributions to the final summary in both the pros and life datasets. We found that both the datasets have an Introduction and Abstract along with some initial parts of the results contributing to the summary. We considered only these sections in the next stage of analysis. We found the optimal length or no of sentences of each of the Introduction, abstract, and result which contributes best to the summary. After this statistical analysis, we took the pre-trained model Facebook/bart-base and fine-tuned it with both the datasets PLOS and eLife. While fine-tuning and testing the results we have used chunking because the text lengths are huge. So to not lose information due to the number of token constraints of the model, we used chunking. Finally, we saw the eLife model giving more accurate results than PLOS in terms of readability aspect, probably because the PLOS summary is closer to its abstract, we have considered the eLife model as our final model and tuned the hyperparameters. We are ranked 7th overall and 1st  in readability\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Venkat praneeth Reddy',\n",
       "    'Pinnapu Reddy Harshavardhan Reddy',\n",
       "    'Karanam Sai Sumedh',\n",
       "    'Raksha Sharma'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_127',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'IITR at BioLaySumm Task 1:Lay Summarization of BioMedical articles using Transformers',\n",
       "   'tldr': \"Initially, we analyzed the datasets in a statistical way so as to learn about various sections' contributions to the final summary in both the pros and life datasets. We found that both the datasets have an Introduction and Abstract along with some initial parts of the results contributing to the su\",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_128': {'abstract': 'Lay summarisation aims at generating a summary for non-expert audience which allows them to keep updated with latest research in a specific field. Despite the significant advancements made in the field of text summarisation, lay summarisation remains relatively under-explored. We present a comprehensive set of experiments and analysis to investigate the effectiveness of existing pre-trained language models in generating lay summaries. When evaluate our models using a BioNLP Shared Task, BioLaySumm, our submission ranked second for the relevance criteria and third overall among 21 competing teams.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mong Yuan Sim',\n",
       "    'Xiang Dai',\n",
       "    'Maciej Rybinski',\n",
       "    'Sarvnaz Karimi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_128',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CSIRO Data61 Team at BioLaySumm Task 1: Lay Summarisation of Biomedical Research Articles Using Generative Models',\n",
       "   'tldr': 'Lay summarisation aims at generating a summary for non-expert audience which allows them to keep updated with latest research in a specific field. Despite the significant advancements made in the field of text summarisation, lay summarisation remains relatively under-explored. We present a comprehen',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_13': {'abstract': 'The rapid growth of scientific publications, particularly during the COVID-19 pandemic, emphasizes the need for tools to help researchers efficiently comprehend the latest advancements. One essential part of understanding scientific literature is research aspect classification, which categorizes sentences in abstracts to Background, Purpose, Method, and Finding. In this study, we investigate the impact of different datasets on model performance for the crowd-annotated CODA-19 research aspect classification task. Specifically, we explore the potential benefits of using the large, automatically curated PubMed 200K RCT dataset and evaluate the effectiveness of large language models (LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that using the PubMed 200K RCT dataset does not improve performance for the CODA-19 task. We also observe that while GPT-4 performs well, it does not outperform the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance of a dedicated and task-aligned datasets dataset for the target task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shreya Chandrasekhar', 'Chieh-Yang Huang', 'Ting-Hao Huang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Good Data, Large Data, or No Data? Comparing Three Approaches in Developing Research Aspect Classifiers for Biomedical Papers',\n",
       "   'tldr': 'The rapid growth of scientific publications, particularly during the COVID-19 pandemic, emphasizes the need for tools to help researchers efficiently comprehend the latest advancements. One essential part of understanding scientific literature is research aspect classification, which categorizes sen',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_130': {'abstract': \"Communicating scientific research to the general public is an essential yet challenging task.Lay summaries, which provide a simplified version of research findings, can bridge the gapbetween scientific knowledge and public understanding. The BioLaySumm task (Goldsack et al., 2023) is a shared task that seeks to automate this process by generating lay summaries from biomedical articles. Two different datasets that have been created from curating two biomedical journals (PLOS and eLife) are provided by the task organizers. As a participant in this shared task, we developed a system to generate a lay summary from an article's abstract and main text.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Cagla Colak', 'lknur Karadeniz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_130',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ISIKSumm at BioLaySumm Task 1: BART-based Summarization System Enhanced with Bio-Entity Labels',\n",
       "   'tldr': 'Communicating scientific research to the general public is an essential yet challenging task.Lay summaries, which provide a simplified version of research findings, can bridge the gapbetween scientific knowledge and public understanding. The BioLaySumm task (Goldsack et al., 2023) is a shared task t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_14': {'abstract': \"Early identification of depression is beneficial to public health surveillance and disease treatment. There are many models that mainly treat the detection as a binary classification task, such as detecting whether a user is depressed. However, identifying users' depression severity levels from posts on social media is more clinically useful for future prevention and treatment. Existing severity detection methods mainly model the semantic information of posts while ignoring the relevant sentiment information, which can reflect the user's state of mind and could be helpful for severity detection. In addition, they treat all severity levels equally, making the model difficult to distinguish between closely-labeled categories. We propose a sentiment-guided Transformer model, which efficiently fuses social media posts' semantic information with sentiment information. Furthermore, we also utilize a supervised severity-aware contrastive learning framework to enable the model to better distinguish between different severity levels. The experimental results show that our model achieves superior performance on two public datasets, while further analysis proves the effectiveness of all proposed modules.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tianlin Zhang', 'Kailai Yang', 'Sophia Ananiadou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Sentiment-guided Transformer with Severity-aware Contrastive Learning for Depression Detection on Social Media',\n",
       "   'tldr': \"Early identification of depression is beneficial to public health surveillance and disease treatment. There are many models that mainly treat the detection as a binary classification task, such as detecting whether a user is depressed. However, identifying users' depression severity levels from post\",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_15': {'abstract': \"Social media (SM) can provide valuable information about patients' experiences with multiple drugs during treatments. Although information extraction from SM has been well-studied, drug switches detection and reasons behind these switches from SM have not been studied yet. Therefore, in this paper, we present a new SM listening approach for analyzing online patient conversations that contain information about drug switching, drug effectiveness, side effects, and adverse drug reactions. We describe a deep learning-based approach for identifying instances of drug switching in SM posts, as well as a method for extracting the reasons behind these switches. To train and test our models, we used annotated SM data from internal dataset which is automatically created using a rule-based method. We evaluated our models using Text-to-Text Transfer Transformer (T5) and found that our SM listening approach can extract medication change information and reasons with high accuracy, achieving  an F1-score of 98% and a ROUGE-1 score of 93%, respectively. Overall, our results suggest that our SM listening approach has the potential to provide valuable insights into patients' experiences with drug treatments, which can be used to improve patient outcomes and the effectiveness of drug treatments.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mourad Sarrouti', 'Carson Tao', 'Yoann Mamy Randriamihaja'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_15',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Drug Switching in Patients: A Deep Learning-based Approach to Extract Drug Changes and Reasons from Social Media',\n",
       "   'tldr': \"Social media (SM) can provide valuable information about patients' experiences with multiple drugs during treatments. Although information extraction from SM has been well-studied, drug switches detection and reasons behind these switches from SM have not been studied yet. Therefore, in this paper, \",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_16': {'abstract': 'The use of seed articles in information retrieval provides many advantages, such as a longercontext and more details about the topic being searched for. Given a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar articles to support the user in finding equivalent papers in the biomedical literature. We aimed at performing a quantitative evaluation of the PubMed Similar Articles based on three existing biomedical text similarity datasets, namely, RELISH, TREC-COVID, and SMAFIRA-c. Further, we carried out a survey and an evaluation of various text similarity methods on these three datasets. Our experiments considered the original title and abstract from PubMed as well as automatically detected sections and manually annotated relevant sentences. We provide an overview about which methods better performfor each dataset and compare them to the ranking in PubMed similar articles. While resultsvaried considerably among the datasets, we were able to obtain a better performance thanPubMed for all of them. Datasets and source codes are available at: https://github.com/mariananeves/reranking',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mariana Neves',\n",
       "    'Ines Schadock',\n",
       "    'Beryl Eusemann',\n",
       "    'Gilbert Schnfelder',\n",
       "    'Bettina Bert',\n",
       "    'Daniel Butzke'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Is the ranking of PubMed similar articles good enough? An evaluation of text similarity methods for three datasets',\n",
       "   'tldr': 'The use of seed articles in information retrieval provides many advantages, such as a longercontext and more details about the topic being searched for. Given a seed article (i.e., a PMID), PubMed provides a pre-compiled list of similar articles to support the user in finding equivalent papers in th',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_17': {'abstract': 'Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are commonly used in the biomedical domain to evaluate and compare their capacity for event trigger detection. We fine-tune the models using seven manually annotated corpora to assess their performance in different biomedical subdomains. SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. We demonstrate that adding the KG embeddings to the model improves the argument identification performance by around 20 \\\\%, and by around 15 \\\\% compared to two baseline models. Our results suggest that fine-tuning a transformer model that is pretrained from scratch with biomedical and general data allows to detect event triggers and identify arguments covering different biomedical subdomains, and therefore improving its generalization. Furthermore, the integration of KG embeddings into the model can significantly improve the performance of biomedical event argument identification, outperforming the results of baseline models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Laura Zanella', 'Yannick Toussaint'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'How Much do Knowledge Graphs Impact Transformer Models for Extracting Biomedical Events?',\n",
       "   'tldr': 'Biomedical event extraction can be divided into three main subtasks; (1) biomedical event trigger detection, (2) biomedical argument identification and (3) event construction. This work focuses in the two first subtasks. For the first subtask we analyze a set of transformer language models that are ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_18': {'abstract': 'We consider the task of automatically extracting various overlapping frames, i.e, structured entities composed of multiple labels and mentions, from long clinical breast radiology documents. While many methods exist for related topics such as event extraction, slot filling, or discontinuous entity recognition, a challenge in our study resides in the fact that clinical reports typically contain overlapping frames that span multiple sentences or paragraphs.We propose a new method that addresses these difficulties and evaluate it on a new annotated corpus.Despite the small number of documents, we show that the hybridization between knowledge injection and a learning-based system allows us to quickly obtain proper results.We will also introduce the concept of scope relations and show that it both improves the performance of our system, and provides a visual explanation of the predictions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Perceval Wajsburt', 'Xavier Tannier'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An end-to-end neural model based on cliques and scopes for frame extraction in long breast radiology reports',\n",
       "   'tldr': 'We consider the task of automatically extracting various overlapping frames, i.e, structured entities composed of multiple labels and mentions, from long clinical breast radiology documents. While many methods exist for related topics such as event extraction, slot filling, or discontinuous entity r',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_19': {'abstract': 'We propose a distantly supervised pipeline NER which executes entity span detection and entity classification in sequence named DISTANT (DIstantly Supervised enTity spAN deTection and classification).The former entity span detector extracts possible entity mention spans by the distant supervision. Then the later entity classifier assigns each entity span to one of the positive entity types or none by employing a positive and unlabeled (PU) learning framework. Two models were built based on the pre-trained SciBERT model and fine-tuned with the silver corpus generated by the distant supervision.Experimental results on BC5CDR and NCBI-Disease datasets show that our method outperforms the end-to-end NER baselines without PU learning by a large margin. In particular, it increases the recall score effectively.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ken Yano', 'Makoto Miwa', 'Sophia Ananiadou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DISTANT: Distantly Supervised Entity Span Detection and Classification',\n",
       "   'tldr': 'We propose a distantly supervised pipeline NER which executes entity span detection and entity classification in sequence named DISTANT (DIstantly Supervised enTity spAN deTection and classification).The former entity span detector extracts possible entity mention spans by the distant supervision. T',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_2': {'abstract': 'Automatically identifying genetic mutations in the cancer literature using text mining technology has been an important way to study the vast amount of cancer medical literature. However, novel knowledge regarding the genetic variants proliferates rapidly, though current supervised learning models struggle with discovering these unknown entity types. Few-shot learning allows a model to perform effectively with great generalization on new entity types, which has not been explored in recognizing cancer mutation detection. This paper addresses cancer mutation detection tasks with few-shot learning paradigms. We propose GDPN framework, which models the label dependency from the training examples in the support set and approximates the transition scores via Gaussian distribution. The experiments on three benchmark cancer mutation datasets show the effectiveness of our proposed model.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiarun Cao',\n",
       "    'Niels Peek',\n",
       "    'Andrew Renehan',\n",
       "    'Sophia Ananiadou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Gaussian Distributed Prototypical Network for Few-shot Genomic Variant Detection',\n",
       "   'tldr': 'Automatically identifying genetic mutations in the cancer literature using text mining technology has been an important way to study the vast amount of cancer medical literature. However, novel knowledge regarding the genetic variants proliferates rapidly, though current supervised learning models s',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_20': {'abstract': 'In clinical and other specialized domains, data are scarce due to their confidential nature. This lack of data is a major problem when fine-tuning language models.Nevertheless, very large language models (LLMs) are promising for the medical domain but cannot be used directly in healthcare facilities due to data confidentiality issues. We explore an approach of annotating training data with LLMs to train smaller models more adapted to our problem. We show that this method yields promising results for information extraction tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Simon Meoni', 'Eric De la Clergerie', 'Theo Ryffel'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Large Language Models as Instructors: A Study on Multilingual Clinical Entity Extraction',\n",
       "   'tldr': 'In clinical and other specialized domains, data are scarce due to their confidential nature. This lack of data is a major problem when fine-tuning language models.Nevertheless, very large language models (LLMs) are promising for the medical domain but cannot be used directly in healthcare facilities',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_22': {'abstract': 'Extracting temporal relations usually entails identifying and classifying the relation between two mentions. However, the definition of temporal mentions strongly depends on the text type and the application domain. Clinical text in particular is complex. It may describe events that occurred at different times, contain redundant information and a variety of domain-specific temporal expressions. In this paper, we propose a novel event-independent representation of temporal relations that is task-independent and, therefore, domain-independent. We are interested in identifying homogeneous text portions from a temporal standpoint and classifying the relation between each text portion and the document creation time. Temporal relation extraction is cast as a sequence labeling task and evaluated on oncology notes. We further evaluate our temporal representation by the temporal positioning of toxicity events of chemotherapy administrated to colon and lung cancer patients described in French clinical reports. An overall macro F-measure of 0.86 is obtained for temporal relation extraction by a neural token classification model trained on clinical texts written in French. Our results suggest that the toxicity event extraction task can be performed successfully by automatically identifying toxicity events and placing them within the patient timeline (F-measure .62). The proposed system has the potential to assist clinicians in the preparation of tumor board meetings.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nesrine Bannour',\n",
       "    'Bastien Rance',\n",
       "    'Xavier Tannier',\n",
       "    'Aurelie Neveol'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Event-independent temporal positioning: application to French clinical text',\n",
       "   'tldr': 'Extracting temporal relations usually entails identifying and classifying the relation between two mentions. However, the definition of temporal mentions strongly depends on the text type and the application domain. Clinical text in particular is complex. It may describe events that occurred at diff',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_24': {'abstract': \"Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting ADEs and the related suspect drugs using machine learning is a challenging task due to the complex linguistic relations between drug  ADE pairs in textual data and unavailability of large corpus of labelled datasets. This paper introduces ADEQA, a question- answer(QA) based approach using quasi supervised labelled data and sequence-to-sequence transformers to extract ADEs, drug suspects and the relationships between them. Unlike traditional QA models, natural language generation (NLG) based models don't require extensive token level labelling and thereby reduces the adoption barrier significantly. On a public ADE corpus, we were able to achieve state-of-the-art results with an F1 score of 94% on establishing the relationships between ADEs and the respective suspects.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vinayak Arannil', 'Tomal Deb', 'Atanu Roy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ADEQA: A Question Answer based approach for joint ADE-Suspect Extraction using Sequence-To-Sequence Transformers',\n",
       "   'tldr': 'Early identification of Adverse Drug Events (ADE) is critical for taking prompt actions while introducing new drugs into the market. These ADEs information are available through various unstructured data sources like clinical study reports, patient health records, social media posts, etc. Extracting',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_25': {'abstract': \"Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post online can aid providers in administering targeted healthcare and minimize false positives. Pre-trained Language Models (LMs) can assess users' social media data and classify them in terms of their mental health risk. We propose a Question-Answering (QA) approach to assess mental health risk using the Unified-QA model on two large mental health datasets. To protect user data, we extend Unified-QA by anonymizing the model training process using differential privacy. Our results demonstrate the effectiveness of modeling risk assessment as a QA task, specifically for mental health use cases. Furthermore, the model's performance decreases by less than 1% with the inclusion of differential privacy. The proposed system's performance is indicative of a promising research direction that will lead to the development of privacy-aware diagnostic systems.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Prateek Chhikara',\n",
       "    'Ujjwal Pasupulety',\n",
       "    'John Marshall',\n",
       "    'Dhiraj Chaurasia',\n",
       "    'Shweta Kumari'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Privacy Aware Question-Answering System for Online Mental Health Risk Assessment',\n",
       "   'tldr': 'Social media platforms have enabled individuals suffering from mental illnesses to share their lived experiences and find the online support necessary to cope. However, many users fail to receive genuine clinical support, thus exacerbating their symptoms. Screening users based on what they post onli',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_28': {'abstract': 'Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different down-stream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining and training time and with much smaller corpora.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aman Berhe',\n",
       "    'Guillaume Draznieks',\n",
       "    'Vincent Martenot',\n",
       "    'Valentin Masdeu',\n",
       "    'Lucas Davy',\n",
       "    'Jean-Daniel Zucker'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'AliBERT: A Pre-trained Language Model for French Biomedical Text',\n",
       "   'tldr': 'Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_29': {'abstract': 'Fact-checking of health-related claims has become necessary in this digital age, where any information posted online is easily available to everyone. The most effective way to verify such claims is by using evidences obtained from reliable sources of medical knowledge, such as PubMed. Recent advances in the field of NLP have helped automate such fact-checking tasks. In this work, we propose a domain-specific BERT-based model using a transfer learning approach for the task of predicting the veracity of claim-evidence pairs for the verification of health-related facts. We also improvise on a method to combine multiple evidences retrieved for a single claim, taking into consideration conflicting evidences as well. We also show how our model can be exploited when labelled data is available and how back-translation can be used to augment data when there is data scarcity.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Pritam Deka', 'Anna Jurek-Loughrey', 'Deepak P'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_29',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multiple Evidence Combination for Fact-Checking of Health-Related Information',\n",
       "   'tldr': 'Fact-checking of health-related claims has become necessary in this digital age, where any information posted online is easily available to everyone. The most effective way to verify such claims is by using evidences obtained from reliable sources of medical knowledge, such as PubMed. Recent advance',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_31': {'abstract': 'We present a manually annotated new corpus, Species-Species Interaction (SSI), for extracting meaningful binary relations between species, in biomedical texts, at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator to annotate species in full-text articles after evaluating different NER species taggers. Our first results are promising for extracting relations between species using BERT and its biomedical variants.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Oumaima El Khettari', 'Solen Quiniou', 'Samuel Chaffron'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_31',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Building a Corpus for Biomedical Relation Extraction of Species Mentions',\n",
       "   'tldr': 'We present a manually annotated new corpus, Species-Species Interaction (SSI), for extracting meaningful binary relations between species, in biomedical texts, at sentence level, with a focus on the gut microbiota. The corpus leverages PubTator to annotate species in full-text articles after evaluat',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_32': {'abstract': 'Understanding protein interactions and pathway knowledge is essential for comprehending living systems and investigating the mechanisms underlying various biological functions and complex diseases. While numerous databases curate such biological data obtained from literature and other sources, they are not comprehensive and require considerable effort to maintain. One mitigation strategies can be utilizing large language models to automatically extract biological information and explore their potential in life science research. This study presents an initial investigation of the efficacy of utilizing a large language model, Galactica in life science research by assessing its performance on tasks involving protein interactions, pathways, and gene regulatory relation recognition. The paper details the results obtained from the model evaluation, highlights the findings, and discusses the opportunities and challenges.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gilchan Park',\n",
       "    'Byung-Jun Yoon',\n",
       "    'Xihaier Luo',\n",
       "    'Vanessa Lpez-Marrero',\n",
       "    'Patrick Johnstone',\n",
       "    'Shinjae Yoo',\n",
       "    'Francis Alexander'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Automated Extraction of Molecular Interactions and Pathway Knowledge using Large Language Model, Galactica: Opportunities and Challenges',\n",
       "   'tldr': 'Understanding protein interactions and pathway knowledge is essential for comprehending living systems and investigating the mechanisms underlying various biological functions and complex diseases. While numerous databases curate such biological data obtained from literature and other sources, they ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_34': {'abstract': 'Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear definitions or descriptions in understandable language or often not available. Therefore, generating human-readable definitions for biomedical concepts might help make the information they encode more accessible and understandable to a wider public.Objective: In this article, we introduce the Automatic Glossary of Clinical Terminology (AGCT), a large-scale biomedical dictionary of clinical concepts generated using high-quality information extracted from the biomedical knowledge contained in SnomedCT.Methods: We generate a novel definition for every SnomedCT concept, after prompting the OpenAI Turbo model, a variant of GPT 3.5, using a high-quality verbalization of the SnomedCT relationships of the to-be-defined concept. A significant subset of the generated definitions was subsequently evaluated by NLP researchers with biomedical expertise on 5-point scales along the following three axes: factuality, insight, and fluency.Results: AGCT contains 422,070 computer-generated definitions for SnomedCT concepts, covering various domains such as diseases, procedures, drugs, and anatomy. The average length of the definitions is 49 words. The definitions were assigned average scores of over 4.5 out of 5 on all three axes, indicating a majority of factual, insightful, and fluent definitions.Conclusion: AGCT is a novel and valuable resource for biomedical tasks that require human-readable definitions for SnomedCT concepts. It can also serve as a base for developing robust biomedical retrieval models or other applications that leverage natural language understanding of biomedical knowledge.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Franois Remy', 'Kris Demuynck', 'Thomas Demeester'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_34',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Automatic Glossary of Clinical Terminology: a Large-Scale Dictionary of Biomedical Definitions Generated from Ontological Knowledge',\n",
       "   'tldr': 'Background: More than 400.000 biomedical concepts and some of their relationships are contained in SnomedCT, a comprehensive biomedical ontology. However, their concept names are not always readily interpretable by non-experts, or patients looking at their own electronic health records (EHR). Clear ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_35': {'abstract': \"We compare three simple and popular approaches for NER: 1) SEQ (sequence labeling with a linear token classifier) 2) SeqCRF (sequence labeling with Conditional Random Fields), and 3) SpanPred (span prediction with boundary token embeddings). We compare the approaches on 4 biomedical NER tasks: GENIA, NCBI-Disease, LivingNER (Spanish), and SocialDisNER (Spanish). The SpanPred model demonstrates state-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 1.3 and 0.6 F1 respectively. The SeqCRF model also demonstrates state-of-the-art performance on LivingNER and SocialDisNER, improving F1 by 0.2 F1 and 0.7 respectively. The SEQ model is competitive with the state-of-the-art on LivingNER dataset. We explore some simple ways of combining the three approaches. We find that majority voting consistently gives high precision and high F1 across all 4 datasets.Lastly, we implement a system that learns to combine SEQ's and SpanPred's predictions, generating systems that give high recall and high F1 across all 4 datasets. On the GENIA dataset, we find that our learned combiner system significantly boosts F1(+1.2) and recall(+2.1) over the systems being combined.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Harsh Verma', 'Sabine Bergler', 'Narjesossadat Tahaei'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Comparing and combining some popular NER approaches on Biomedical tasks',\n",
       "   'tldr': 'We compare three simple and popular approaches for NER: 1) SEQ (sequence labeling with a linear token classifier) 2) SeqCRF (sequence labeling with Conditional Random Fields), and 3) SpanPred (span prediction with boundary token embeddings). We compare the approaches on 4 biomedical NER tasks: GENIA',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_40': {'abstract': 'Understanding biological mechanisms requires determining mutual protein-protein interactions (PPI). Obtaining drug-drug interactions (DDI) from scientific articles provides important information about drugs. Extracting such medical entity interactions from biomedical articles is challenging due to complex sentence structures. To address this issue, our proposed model utilizes tree-transformers to generate the sentence representation first, and then a sentence-to-word update step to fine-tune the word embeddings which are again used by the tree-transformers to generate enriched sentence representations. Using the tree-transformers helps the model preserve syntactical information and provide semantic information. The fine-tuning provided by the continuous update step adds improved semantics to the representation of each sentence. Our model outperforms other prominent models with a significant performance boost on the five standard PPI corpora and a performance boost on the one benchmark DDI corpus that are used in our experiments.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sudipta Singha Roy', 'Robert E. Mercer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Extracting Drug-Drug and Protein-Protein Interactions from Text using a Continuous Update of Tree-Transformers',\n",
       "   'tldr': 'Understanding biological mechanisms requires determining mutual protein-protein interactions (PPI). Obtaining drug-drug interactions (DDI) from scientific articles provides important information about drugs. Extracting such medical entity interactions from biomedical articles is challenging due to c',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_41': {'abstract': 'Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination ellipsis, are a common phenomenon in German medical texts. As their presence is known to affect the performance in downstream tasks such as entity extraction and disambiguation, their resolution can be a useful preprocessing step in information extraction pipelines. In this work, we present a new comprehensive dataset of more than 4,000 manually annotated ECCNPs in German medical text, along with the respective ground truth resolutions. Based on this data, we propose a generative encoder-decoder Transformer model, allowing for a simple end-to-end resolution of ECCNPs from raw input strings with very high accuracy (90.5% exact match score). We compare our approach to an elaborate rule-based baseline, which the generative model outperforms by a large margin. We further investigate different scenarios for prompting large language models (LLM) to resolve ECCNPs. In a zero-shot setting, performance is remarkably poor (21.6% exact matches), as the LLM tends to apply complex changes to the inputs unrelated to our specific task. We also find no improvement over the generative model when using the LLM for post-filtering of generated candidate resolutions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Niklas Kammer',\n",
       "    'Florian Borchert',\n",
       "    'Silvia Winkler',\n",
       "    'Gerard de Melo',\n",
       "    'Matthieu-P. Schapranow'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_41',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Resolving Elliptical Compounds in German Medical Text',\n",
       "   'tldr': 'Elliptical coordinated compound noun phrases (ECCNPs), a special kind of coordination ellipsis, are a common phenomenon in German medical texts. As their presence is known to affect the performance in downstream tasks such as entity extraction and disambiguation, their resolution can be a useful pre',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_42': {'abstract': 'Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative AI techniques for data augmentation to enable further improvement in the pre-screening task of classifying WD. To this end, we propose a simple yet effective data augmentation approach through prompt-based Generative AI models, and evaluate the ROUGE scores and syntactic/ semantic similarity among existing interpretations and augmented data. Our approach with ChatGPT model surpasses all the other methods and achieves improvement over baselines such as Easy-Data Augmentation (EDA) and Backtranslation (BT).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chandreen Liyanage',\n",
       "    'Muskan Garg',\n",
       "    'Vijay Mago',\n",
       "    'Sunghwan Sohn'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Augmenting Reddit Posts to Determine Wellness Dimensions impacting Mental Health',\n",
       "   'tldr': 'Amid ongoing health crisis, there is a growing necessity to discern possible signs of Wellness Dimensions (WD) manifested in self-narrated text. As the distribution of WD on social media data is intrinsically imbalanced, we experiment the generative AI techniques for data augmentation to enable furt',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_43': {'abstract': 'Understanding temporal relationships in text from electronic health records can be valuable for many important downstream clinical applications. Since Clinical TempEval 2017, there has been little work on end-to-end systems for temporal relation extraction, with most work focused on the setting where gold standard events and time expressions are given. In this work, we make use of a novel multi-headed attention mechanism on top of a pre-trained transformer encoder to allow the learning process to attend to multiple aspects of the contextualized embeddings. Our system achieves state of the art results on the THYME corpus by a wide margin, in both the in-domain and cross-domain settings.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Timothy Miller',\n",
       "    'Steven Bethard',\n",
       "    'Dmitriy Dligach',\n",
       "    'Guergana Savova'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_43',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'End-to-end clinical temporal information extraction with multi-head attention',\n",
       "   'tldr': 'Understanding temporal relationships in text from electronic health records can be valuable for many important downstream clinical applications. Since Clinical TempEval 2017, there has been little work on end-to-end systems for temporal relation extraction, with most work focused on the setting wher',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_44': {'abstract': 'Accurate human-annotated data for real-worlduse cases can be scarce and expensive to obtain.In the clinical domain, obtaining such data is evenmore difficult due to privacy concerns which notonly restrict open access to quality data but also require that the annotation be done by domain experts.In this paper, we propose a novel framework - InterDAPT - that leverages Intermediate Domain Finetuning to allow language models to adapt to narrow domains with small, noisy datasets. By making use of peripherally-related, unlabeled datasets,this framework circumvents domain-specific datascarcity issues. Our results show that this weaklysupervised framework provides performance improvements in downstream clinical named entityrecognition tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shilpa Suresh',\n",
       "    'Nazgol Tavabi',\n",
       "    'Shahriar Golchin',\n",
       "    'Leah Gilreath',\n",
       "    'Rafael Garcia-Andujar',\n",
       "    'Alexander Kim',\n",
       "    'Joseph Murray',\n",
       "    'Blake Bacevich',\n",
       "    'Ata Kiapour'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Intermediate Domain Finetuning for Weakly Supervised Domain-adaptive Clinical NER',\n",
       "   'tldr': 'Accurate human-annotated data for real-worlduse cases can be scarce and expensive to obtain.In the clinical domain, obtaining such data is evenmore difficult due to privacy concerns which notonly restrict open access to quality data but also require that the annotation be done by domain experts.In t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_46': {'abstract': \"ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that lack large annotated data.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Israt Jahan',\n",
       "    'Md Tahmid Rahman Laskar',\n",
       "    'Chun Peng',\n",
       "    'Jimmy Huang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_46',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers',\n",
       "   'tldr': 'ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_47': {'abstract': 'Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language processing has offered a number of biomedical LMs pre-trained using different methods and techniques that advance results on many BioNLP tasks, including NER. However, there is still a lack of a comprehensive comparison of pre-training approaches that would work more optimally in the biomedical domain. This paper aims to investigate different pre-training methods, such as pre-training the biomedical LM from scratch and pre-training it in a continued fashion. We compare existing methods with our proposed pre-training method of initializing weights for new tokens by distilling existing weights from the BERT model inside the context where the tokens were found. The method helps to speed up the pre-training stage and improve performance on NER. In addition, we compare how masking rate, corruption strategy, and masking strategies impact the performance of the biomedical LM. Finally, using the insights from our experiments, we introduce a new biomedical LM (BIOptimus), which is pre-trained using Curriculum Learning (CL) and contextualized weight distillation method. Our model sets new states of the art on several biomedical Named Entity Recognition (NER) tasks. We release our code and all pre-trained models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vera Pavlova', 'Mohammed Makhlouf'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition',\n",
       "   'tldr': 'Using language models (LMs) pre-trained in a self-supervised setting on large corpora and then fine-tuning for a downstream task has helped to deal with the problem of limited label data for supervised learning tasks such as Named Entity Recognition (NER). Recent research in biomedical language proc',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_48': {'abstract': 'As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical language models (LMs) are pre-trained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the agglutinating nature of biomedical language. In this work, we first find that standard open-domain and biomedical tokenizers are largely unable to segment biomedical terms into meaningful components. Therefore, we hypothesize that using a tokenizer which segments biomedical terminology more accurately would enable biomedical LMs to improve their performance on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER) and entity linking. Surprisingly, we find that pre-training a biomedical LM using a more accurate biomedical tokenizer does not improve the entity representation quality of a language model as measured by several intrinsic and extrinsic measures such as masked language modeling prediction (MLM) accuracy as well as NER and entity linking performance. These quantitative findings, along with a case study which explores entity representation quality more directly, suggest that the biomedical pre-training process is quite robust to instances of sub-optimal tokenization.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Bernal Jimenez Gutierrez', 'Huan Sun', 'Yu Su'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Biomedical Language Models are Robust to Sub-optimal Tokenization',\n",
       "   'tldr': 'As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_49': {'abstract': 'We propose a novel distantly supervised document-level biomedical relation extraction model that uses partial knowledge graphs that include the graph neighborhood of the entities appearing in each input document. Most conventional distantly supervised relation extraction methods use only the entity relations automatically annotated by using knowledge base entries. They do not fully utilize the rich information in the knowledge base, such as entities other than the target entities and the network of heterogeneous entities defined in the knowledge base. To address this issue, our model integrates the representations of the entities acquired from the neighborhood knowledge graphs with the representations of the input document. We conducted experiments on the ChemDisGene dataset using Comparative Toxicogenomics Database (CTD) for document-level relation extraction with respect to interactions between drugs, diseases, and genes. Experimental results confirmed the performance improvement by integrating entities and their neighborhood biochemical information from the knowledge base.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Takuma Matsubara', 'Makoto Miwa', 'Yutaka Sasaki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_49',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Distantly Supervised Document-Level Biomedical Relation Extraction with Neighborhood Knowledge Graphs',\n",
       "   'tldr': 'We propose a novel distantly supervised document-level biomedical relation extraction model that uses partial knowledge graphs that include the graph neighborhood of the entities appearing in each input document. Most conventional distantly supervised relation extraction methods use only the entity ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_5': {'abstract': 'Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scenario partial knowledge base inference; training an EL model with one KB and inferring on the part of it without further training. In this work, we give a detailed definition and evaluation procedures for this practically valuable but significantly understudied scenario and evaluate methods from three representative EL paradigms. We construct partial KB inference benchmarks and witness a catastrophic degradation in EL performance due to dramatically precision drop.Our findings reveal these EL paradigms can not correctly handle unlinkable mentions (NIL), so they are not robust to partial KB inference. We also propose two simple-and-effective redemption methods to combat the NIL issue with little computational overhead.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hongyi Yuan', 'Keming Lu', 'Zheng Yuan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Partial Knowledge Base Inference in Biomedical Entity Linking',\n",
       "   'tldr': 'Biomedical entity linking (EL) consists of named entity recognition (NER) and named entity disambiguation (NED). EL models are trained on corpora labeled by a predefined KB. However, it is a common scenario that only entities within a subset of the KB are precious to stakeholders. We name this scena',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_50': {'abstract': 'We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder model, and both encoder and decoder are compatible with widely used BERT architecture, which allows benefiting from publicly available pre-trained biomedical language model checkpoints. We performed additional pre-training and fine-tuned BioNART on biomedical summarization and doctor-patient dialogue tasks. Experimental results show that our BioNART achieves about 94% of the ROUGE score to the pre-trained autoregressive model while realizing an 18 times faster inference speed on the iCliniq dataset.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Masaki Asada', 'Makoto Miwa'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BioNART: A Biomedical Non-AutoRegressive Transformer for Natural Language Generation',\n",
       "   'tldr': 'We propose a novel Biomedical domain-specific Non-AutoRegressive Transformer model for natural language generation: BioNART. Our BioNART is based on an encoder-decoder model, and both encoder and decoder are compatible with widely used BERT architecture, which allows benefiting from publicly availab',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_51': {'abstract': 'Recently, several methods have tackled the relation extraction task with QA and have shown successful results. However, the effectiveness of existing methods in specific domains, such as the biomedical domain, is yet to be verified. When there are multiple entity pairs that share an entity in a sentence, a QA-based relation extraction model that outputs only one single answer to a given question may not extract desired relations. In addition, these methods employ QA models that are not tuned for relation extraction. To address these issues, we first extend and apply a span QA-based relation extraction method to the drug-protein relation extraction by creating question templates and incorporating entity type markers. We further propose a binary QA-based method that directly uses the entity information available in the relation extraction task. The experimental results on the DrugProt dataset show that our QA-based methods, especially the proposed binary QA method, are effective for drug-protein relation extraction.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Koshi Yamada', 'Makoto Miwa', 'Yutaka Sasaki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_51',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Biomedical Relation Extraction with Entity Type Markers and Relation-specific Question Answering',\n",
       "   'tldr': 'Recently, several methods have tackled the relation extraction task with QA and have shown successful results. However, the effectiveness of existing methods in specific domains, such as the biomedical domain, is yet to be verified. When there are multiple entity pairs that share an entity in a sent',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_53': {'abstract': 'This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information.Recently, document classification performance has been significantly improved with large pre-trained language models; however, there still remain documents that are difficult to classify. External information, such as bibliographic information, citation links, descriptions of entities, and medical taxonomies, has been considered one of the keys to dealing with such documents in document classification. Although several document classification methods using external information have been proposed, they only consider limited relationships, e.g., word co-occurrence and citation relationships. However, there are multiple types of external information.To overcome the limitation of the conventional use of external information, we propose a document classification model that simultaneously considers bibliographic and entity information to deeply model the relationships among documents using the representations of the literature graph.The experimental results show that our proposed method outperforms existing methods on two document classification datasets in the biomedical domain with the help of the literature graph.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ryuki Ida', 'Makoto Miwa', 'Yutaka Sasaki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_53',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Biomedical Document Classification with Literature Graph Representations of Bibliographies and Entities',\n",
       "   'tldr': 'This paper proposes a new document classification method that incorporates the representations of a literature graph created from bibliographic and entity information.Recently, document classification performance has been significantly improved with large pre-trained language models; however, there ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_55': {'abstract': 'Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. This study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale. We perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of RCTs to traditional manual annotation methods. We analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. Our findings reveal that the best model for the two demonstrated tasks, ChatGPT can generally extract correct information and identify when the desired information is missing from an article. We additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['David Kartchner',\n",
       "    'Selvi Ramalingam',\n",
       "    'Irfan Al-Hussaini',\n",
       "    'Olivia Kronick',\n",
       "    'Cassie Mitchell'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models',\n",
       "   'tldr': 'Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. This study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale. We per',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_56': {'abstract': \"Social media offers an accessible avenue for individuals of diverse backgrounds and circumstances to share their unique perspectives and experiences. Our study focuses on the experience of low carbohydrate diets, motivated by recent research and clinical trials that elucidates the diet's promising health benefits. Given the lack of any suitable annotated dataset in this domain, we first define an annotation schema that reflects the interests of healthcare professionals and then manually annotate data from the Reddit social network.Finally, we benchmark the effectiveness of several classification approaches that are based on statistical Support Vector Machines (SVM) classifier, pre-train-then-finetune RoBERTa classifier, and, off-the-shelf ChatGPT API, on our annotated dataset.Our annotations and scripts that are used to download the Reddit posts are publicly available at https://data.csiro.au/collection/csiro:59208.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Skyler Zou',\n",
       "    'Xiang Dai',\n",
       "    'Grant Brinkworth',\n",
       "    'Pennie Taylor',\n",
       "    'Sarvnaz Karimi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Can Social Media Inform Dietary Approaches for Health Management? A Dataset and Benchmark for Low-Carb Diet',\n",
       "   'tldr': \"Social media offers an accessible avenue for individuals of diverse backgrounds and circumstances to share their unique perspectives and experiences. Our study focuses on the experience of low carbohydrate diets, motivated by recent research and clinical trials that elucidates the diet's promising h\",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_57': {'abstract': \"Automatically rating the quality of published research is a critical step in medical evidence synthesis. While several methods have been proposed, their algorithmic fairness has been   overlooked even though significant risks may follow when such systems are deployed in biomedical contexts. In this work, we study fairness on two systems along two sensitive attributes, participant sex and medical area. In some cases, we find important inequalities, leading us to apply various debiasing methods. Upon examining an interplay of systems' predictive performance, fairness, as well as medically critical selective classification capabilities and calibration performance, we find that fairness can sometimes improve through debiasing, but at a cost in other performance measures.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Simon Suster', 'Timothy Baldwin', 'Karin Verspoor'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_57',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Promoting Fairness in Classification of Quality of Medical Evidence',\n",
       "   'tldr': 'Automatically rating the quality of published research is a critical step in medical evidence synthesis. While several methods have been proposed, their algorithmic fairness has been   overlooked even though significant risks may follow when such systems are deployed in biomedical contexts. In this ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_58': {'abstract': \"Fine-tuning biomedical pre-trained language models (BioPLMs) such as BioBERT has become a common practice dominating leaderboards across various natural language processing tasks. Despite their success and wide adoption, prevailing fine-tuning approaches for named entity recognition (NER)  naively train BioPLMs on targeted datasets without considering class distributions. This is problematic especially when dealing with imbalanced biomedical gold-standard datasets for NER in which most biomedical entities are underrepresented.In this paper, we address the class imbalance problem and propose WeLT, a cost-sensitive fine-tuning approach based on new re-scaled class weights for the task of biomedical NER. We evaluate WeLT's fine-tuning performance on mixed-domain and domain-specific BioPLMs using eight biomedical gold-standard datasets. We compare our approach against vanilla fine-tuning and three other existing re-weighting schemes. Our results show the positive impact of handling the class imbalance problem. WeLT outperforms all the vanilla fine-tuned models. Furthermore, our method demonstrates advantages over other existing weighting schemes in most experiments.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ghadeer Mobasher',\n",
       "    'Wolfgang Mller',\n",
       "    'Olga Krebs',\n",
       "    'Michael Gertz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_58',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'WeLT: Improving Biomedical Fine-tuned Pre-trained Language Models with Cost-sensitive Learning',\n",
       "   'tldr': 'Fine-tuning biomedical pre-trained language models (BioPLMs) such as BioBERT has become a common practice dominating leaderboards across various natural language processing tasks. Despite their success and wide adoption, prevailing fine-tuning approaches for named entity recognition (NER)  naively t',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_59': {'abstract': \"Summarization of medical notes has been studied for decades with hospital discharge summaries garnering recent interest in the research community. While methods for summarizing these notes have been the focus, there has been little work in understanding the feasibility of this task. We believe this effort is warranted given the notes' length and complexity, and that they are often riddled with poorly formatted structured data and redundancy in copy and pasted text. In this work, we investigate the feasibility of the summarization task by finding the origin, or data provenance, of the discharge summary's source text. As a motivation to understanding the data challenges of the summarization task, we present DSProv, a new dataset of 51 hospital admissions annotated by clinical informatics physicians. The dataset is analyzed for semantics and the extent of copied text from human authored electronic health record (EHR) notes. We also present a novel unsupervised method of matching notes used in discharge summaries, and release our annotation dataset1 and source code to the community.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Paul Landes',\n",
       "    'Aaron Chaise',\n",
       "    'Kunal Patel',\n",
       "    'Sean Huang',\n",
       "    'Barbara Di Eugenio'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_59',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Hospital Discharge Summarization Data Provenance',\n",
       "   'tldr': 'Summarization of medical notes has been studied for decades with hospital discharge summaries garnering recent interest in the research community. While methods for summarizing these notes have been the focus, there has been little work in understanding the feasibility of this task. We believe this ',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_61': {'abstract': 'We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or parameter-efficient fine-tuning. Our results consistently achieve best performance by maximally adapting to the task via pretraining on clinical text and fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developing effective natural language processing solutions for clinical tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dave Van Veen',\n",
       "    'Cara Van Uden',\n",
       "    'Maayane Attias',\n",
       "    'Anuj Pareek',\n",
       "    'Christian Bluethgen',\n",
       "    'Malgorzata Polacin',\n",
       "    'Wah Chiu',\n",
       "    'Jean-Benoit Delbrouck',\n",
       "    'Juan Zambrano Chaves',\n",
       "    'Curtis Langlotz',\n",
       "    'Akshay Chaudhari',\n",
       "    'John Pauly'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_61',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models',\n",
       "   'tldr': 'We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, or clinical text) and via discrete prompting or paramete',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_62': {'abstract': \"The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant and accurate diagnoses will augment the healthcare providers' decision-making process and improve the quality of care for patients. The goal for participants is to develop models that generated a list of diagnoses and problems using input from the daily care notes collected from the hospitalization of critically ill patients. Eight teams submitted their final systems to the shared task leaderboard. In this paper, we describe the tasks, datasets, evaluation metrics, and baseline systems. Additionally, the techniques and results of the evaluation of the different approaches tried by the participating teams are summarized.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yanjun Gao',\n",
       "    'Dmitriy Dligach',\n",
       "    'Timothy Miller',\n",
       "    'Majid Afshar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_62',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Overview of the Problem List Summarization (ProbSum) 2023 Shared Task on Summarizing Patients' Active Diagnoses and Problems from Electronic Health Record Progress Notes\",\n",
       "   'tldr': 'The BioNLP Workshop 2023 initiated the launch of a shared task on Problem List Summarization (ProbSum) in January 2023. The aim of this shared task is to attract future research efforts in building NLP models for real-world diagnostic decision support applications, where a system generating relevant',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_63': {'abstract': \"This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023.  The goal of this shared task is to develop abstractive summarisation models capable of generating ``lay summaries'' (i.e., summaries that are comprehensible to non-technical audiences) in both a controllable and non-controllable setting.There are two subtasks: 1) Lay Summarisation, where the goal is for participants to build models for lay summary generation only, given the full article text and the corresponding abstract as input; and2) Readability-controlled Summarisation, where the goal is for participants to train models to generate both the technical abstract and the lay summary, given an article's main text as input.In addition to overall results, we report on the setup and insights from the BioLaySumm shared task, which attracted a total of 20 participating teams across both subtasks.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tomas Goldsack',\n",
       "    'Zheheng Luo',\n",
       "    'Qianqian Xie',\n",
       "    'Carolina Scarton',\n",
       "    'Matthew Shardlow',\n",
       "    'Sophia Ananiadou',\n",
       "    'Chenghua Lin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_63',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BioLaySumm 2023 Shared Task: Lay Summarisation of Biomedical Research Articles',\n",
       "   'tldr': \"This paper presents the results of the shared task on Lay Summarisation of Biomedical Research Articles (BioLaySumm), hosted at the BioNLP Workshop at ACL 2023.  The goal of this shared task is to develop abstractive summarisation models capable of generating ``lay summaries'' (i.e., summaries that \",\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_64': {'abstract': 'Radiology report summarization is a growing area of research. Given the Findings and/or Background sections of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. Recent efforts have released systems that achieve promising performance as measured by widely used summarization metrics such as BLEU and ROUGE. However, the research area of radiology report summarization currently faces two important limitations. First, most of the results are reported on private datasets. This limitation prevents the ability to reproduce results and fairly compare different systems and solutions. Secondly, to the best of our knowledge, most research is carried out on chest X-rays. To palliate these two limitations, we propose a radiology report summarization (RadSum) challenge on i) a new dataset of eleven different modalities and anatomies pairs based on the MIMIC-III database ii) a multimodal report summarization dataset based on MIMIC-CXR enhanced with a brand-new test-set from Stanford Hospital. In total, we received 112 submissions across 11 teams.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jean-Benoit Delbrouck',\n",
       "    'Maya Varma',\n",
       "    'Pierre Chambon',\n",
       "    'Curtis Langlotz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_64',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Short paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Overview of the RadSum23 Shared Task on Multi-modal and Multi-anatomical Radiology Report Summarization',\n",
       "   'tldr': 'Radiology report summarization is a growing area of research. Given the Findings and/or Background sections of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. Recent efforts have release',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_7': {'abstract': 'Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains; these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams. This discrepancy can be attributed to a knowledge gap between radiologists and the generation models. While radiologists possess patient-specific prior information, the models solely receive X-ray images at a specific time point. To tackle this issue, we propose a novel approach that leverages a rule-based labeler to extract comparison prior information from radiology reports. This extracted comparison prior is then seamlessly integrated into state-of-the-art transformer-based models, enabling them to produce more realistic and comprehensive reports. Our method is evaluated on English report datasets, such as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses baseline models in terms of natural language generation metrics. Notably, our model generates reports that are free from false references to non-existent prior exams, setting it apart from previous models. By addressing this limitation, our approach represents a significant step towards bridging the gap between radiologists and generation models in the domain of medical report generation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sanghwan Kim',\n",
       "    'Farhad Nooralahzadeh',\n",
       "    'Morteza Rohanian',\n",
       "    'Koji Fujimoto',\n",
       "    'Mizuho Nishio',\n",
       "    'Ryo Sakamoto',\n",
       "    'Fabio Rinaldi',\n",
       "    'Michael Krauthammer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Boosting Radiology Report Generation by Infusing Comparison Prior',\n",
       "   'tldr': 'Recent transformer-based models have made significant strides in generating radiology reports from chest X-ray images. However, a prominent challenge remains; these models often lack prior knowledge, resulting in the generation of synthetic reports that mistakenly reference non-existent prior exams.',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'BioNLP_9': {'abstract': 'Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP.  In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Omid Rohanian',\n",
       "    'Hannah Jauncey',\n",
       "    'Mohammadmahdi Nouriborji',\n",
       "    'Vinod Kumar',\n",
       "    'Bronner P. Gonalves',\n",
       "    'Christiana Kartsonaki',\n",
       "    'ISARIC Clinical Characterisation Group',\n",
       "    'Laura Merson',\n",
       "    'David Clifton'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['BioNLP-ST'],\n",
       "   'id': 'BioNLP_9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Long paper',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints',\n",
       "   'tldr': 'Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP.  In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset contai',\n",
       "   'track': 'BioNLP and BioNLP-ST 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2092': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aikaterini-Lida Kalouli',\n",
       "    'Hai Hu',\n",
       "    'Alexander Webb',\n",
       "    'Lawrence Moss',\n",
       "    'Valeria Paiva'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)'],\n",
       "   'id': 'C2092',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77586/slideshow/9fe13e46e570cbeb8baf4136e59aaf0c.pdf',\n",
       "   'title': '[CL] Curing the SICK and other NLI maladies',\n",
       "   'tldr': '',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 77586,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77586-an-exploration-of-encoder-decoder-approaches-to-multi-label-classification-for-legal-and-biomedical-text',\n",
       "   'video_url': None},\n",
       "  'C2121': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiehang Zeng',\n",
       "    'Jianhan Xu',\n",
       "    'Xiaoqing Zheng',\n",
       "    'Xuanjing Huang'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'C2121',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Certified Robustness to Text Adversarial Attacks by Randomized [MASK]',\n",
       "   'tldr': '',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2147': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vnia Mendona',\n",
       "    'Ricardo Rei',\n",
       "    'Lusa Coheur',\n",
       "    'Alberto Sardinha'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-6_-machine-translation-(poster)'],\n",
       "   'id': 'C2147',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Onception: Active Learning with Expert Advice for Real World Machine Translation',\n",
       "   'tldr': '',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77596,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2208': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Andrea Varda', 'Marco Marelli'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-1_-multilingualism-and-cross-lingual-nlp-(oral)'],\n",
       "   'id': 'C2208',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Data-driven Cross-lingual Syntax: An Agreement Study with Massively Multilingual Models',\n",
       "   'tldr': '',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77605,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2217': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chris Lee',\n",
       "    'Thiago Ferreira',\n",
       "    'Chris Emmery',\n",
       "    'Travis Wiltshire',\n",
       "    'Emiel Krahmer'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-5_-generation-(oral)'],\n",
       "   'id': 'C2217',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Neural Data-to-Text Generation Based on Small Datasets: Comparing the Added Value of Two Semi-Supervised Learning Approaches on Top of a Large Language Model',\n",
       "   'tldr': '',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2265': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chanapa Pananookooln',\n",
       "    'Jakrapop Akaranee',\n",
       "    'Chaklam Silpasuwanchai'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)'],\n",
       "   'id': 'C2265',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Comparing Selective Masking Methods for Depression Detection in Social Media',\n",
       "   'tldr': '',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'C2281': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Rochelle Choenni', 'Dan Garrette', 'Ekaterina Shutova'],\n",
       "   'category': 'CL',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'event_ids': ['session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)'],\n",
       "   'id': 'C2281',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'CL',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[CL] Cross-Lingual Transfer with Language-Specific Subnetworks for Low-Resource Dependency Parsing',\n",
       "   'tldr': '',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_10': {'abstract': 'We propose methods for transliterating English loanwords in Japanese from their Japanese written form (katakana/romaji) to their original English written form. Our data is a Japanese-English loanwords dictionary that we have created ourselves. We employ two approaches: the direct transliteration, which directly converts words from katakana to English, and the indirect transliteration, which utilizes the English pronunciation as an intermediate step. Additionally, we compare the effectiveness of using katakana versus romaji as input characters. We develop 6 models of 2 types for our experiments: one with an English lexicon-filter, and the other without. For each type, we built 3 models, including a pair n-gram based on WFSTs and two sequence-to-sequence models leveraging LSTM and transformer. Our best performing model was the pair n-gram model with a lexicon-filter, directly transliterating from katakana to English.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuying Ren'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Back-Transliteration of English Loanwords in Japanese',\n",
       "   'tldr': 'We propose methods for transliterating English loanwords in Japanese from their Japanese written form (katakana/romaji) to their original English written form. Our data is a Japanese-English loanwords dictionary that we have created ourselves. We employ two approaches: the direct transliteration, wh',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_12': {'abstract': 'Japanese writing is a complex system, and a large part of the complexity resides in the use of kanji. A single kanji character in modern Japanese may have multiple pronunciations, either as native vocabulary or as words borrowed from Chinese. This causes a problem for text-to-speech synthesis (TTS) because the system has to predict which pronunciation of each kanji character is appropriate in the context. The problem is called homograph disambiguation. To solve the problem, this research provides a new annotated Japanese single kanji character pronunciation data set and describes an experiment using the logistic regression (LR) classifier. A baseline is computed to compare with the LR classifier accuracy. This experiment provides the first experimental research in Japanese single kanji homograph disambiguation. The annotated Japanese data is freely released to the public to support further work.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wen Zhang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pronunciation Ambiguities in Japanese Kanji',\n",
       "   'tldr': 'Japanese writing is a complex system, and a large part of the complexity resides in the use of kanji. A single kanji character in modern Japanese may have multiple pronunciations, either as native vocabulary or as words borrowed from Chinese. This causes a problem for text-to-speech synthesis (TTS) ',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_13': {'abstract': \"Handwritten texts produced by young learners often contain orthographic features like spelling errors, capitalization errors, punctuation mistakes, and impurities such as strikethrough, inserts, and smudges that are typically normalized or ignored in existing transcriptions. For applications like handwriting recognition with the goal of automatically analyzing a learner's language performance, however, retaining such features would be necessary.\\nTo address this, we present transcription guidelines that retain the features addressed above. \\nOur guidelines were developed iteratively and include numerous example images to illustrate the various issues.\\nOn a subset of about 90 double-transcribed texts, we compute inter-annotator agreement and show that our guidelines can be applied with high levels of percentage agreement of about .98.\\nOverall, we transcribed 1,350 learner texts, which is about the same size as the widely adopted handwriting recognition datasets IAM (1,500 pages) and CVL (1,600 pages).\\nOur final corpus can be used to train a handwriting recognition system that transcribes closely to the real productions by young learners.\\nSuch a system is a prerequisite for applying automatic orthography feedback systems to handwritten texts in the future.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Christian Gold', 'Ronja Laarmann-quante', 'Torsten Zesch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Preserving the Authenticity of Handwritten Learner Language: Annotation Guidelines for Creating Transcripts Retaining Orthographic Features',\n",
       "   'tldr': 'Handwritten texts produced by young learners often contain orthographic features like spelling errors, capitalization errors, punctuation mistakes, and impurities such as strikethrough, inserts, and smudges that are typically normalized or ignored in existing transcriptions. For applications like ha',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_14': {'abstract': 'Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophisticated alternatives that explore multiple possibilities or make use of manual linguistic annotations. We show that the sophisticated systems are consistently better than simpler systems, quantitatively and qualitatively. We also show transliterating Maltese can be considered as an option to improve the cross-lingual transfer capabilities.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kurt Micallef',\n",
       "    'Fadhl Eryani',\n",
       "    'Nizar Habash',\n",
       "    'Houda Bouamor',\n",
       "    'Claudia Borg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring the Impact of Transliteration on NLP Performance for Low-Resource Languages: The Case of Maltese and Arabic',\n",
       "   'tldr': 'Maltese is a low-resource language of Arabic and Romance origins written in Latin script. We explore the impact of transliterating Maltese into Arabic script on a number of downstream tasks. We compare multiple transliteration pipelines ranging from simple one-to-one character maps to more sophistic',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_18': {'abstract': 'Writing systems have traditionally been classified by whether they prioritize encoding phonological information (phonographic) versus morphological or semantic information (logographic). Recent work has broached the question of how membership in these categories can be quantified. Sproat and Gutkin (2021) proposed a range of metrics by which degree of logography can be quantified, including mutual information and a metric based on contextual attention required by a sequence-to-sequence RNN that maps pronunciations to spellings. We aim to build on this work by treating a definition of logography which, in contrast to the definition used by Sproat and Gutkin, more directly incorporates morphological identity. We compare mutual information between graphic forms and phonological forms and between graphic forms and morphological identity for written Japanese and Sumerian. Our results suggest that our methods present a promising means of classifying the degree to which a writing system is logographic or phonographic.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Noah Hermalin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Mutual Information-based Approach to Quantifying Logography in Japanese and Sumerian',\n",
       "   'tldr': 'Writing systems have traditionally been classified by whether they prioritize encoding phonological information (phonographic) versus morphological or semantic information (logographic). Recent work has broached the question of how membership in these categories can be quantified. Sproat and Gutkin ',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_20': {'abstract': 'Natural language processing is largely focused on written text processing. However, many computational linguists tacitly endorse myths about the nature of writing. We highlight two of these myths---the conflation of language and writing, and the notion that Chinese, Japanese, and Korean writing is ideographic---and suggest how the community can dispel them.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kyle Gorman', 'Richard Sproat'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Myths about Writing Systems in Speech & Language Technology',\n",
       "   'tldr': 'Natural language processing is largely focused on written text processing. However, many computational linguists tacitly endorse myths about the nature of writing. We highlight two of these myths---the conflation of language and writing, and the notion that Chinese, Japanese, and Korean writing is i',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_4': {'abstract': 'Word error rate (WER) and character error rate (CER) are standard metrics in\\nSpeech Recognition (ASR), but one problem has always been alternative spellings: If one\\'s system transcribes adviser whereas the ground truth has advisor, this will count as an error even though the two spellings really represent the same word.\\n\\nJapanese is notorious for \"lacking orthography: most words can be spelled in multiple ways, presenting a problem for accurate ASR evaluation. In this paper we propose a new lenient evaluation metric as a more defensible CER measure for Japanese ASR. We create a lattice of plausible respellings of the reference transcription, using a combination of lexical resources, a Japanese text-processing system, and a neural machine translation model for reconstructing kanji from hiragana or katakana. In a\\nmanual evaluation, raters rated 95.4\\\\% of the proposed spelling variants as plausible. ASR results show that our method, which does not penalize the system for choosing a valid alternate spelling of a word, affords a 2.4\\\\%3.1\\\\% absolute reduction in CER depending on the task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shigeki Karita', 'Richard Sproat', 'Haruko Ishikawa'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally Occurring Spelling Inconsistency',\n",
       "   'tldr': \"Word error rate (WER) and character error rate (CER) are standard metrics in\\nSpeech Recognition (ASR), but one problem has always been alternative spellings: If one's system transcribes adviser whereas the ground truth has advisor, this will count as an error even though the two spellings really rep\",\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_5': {'abstract': 'This paper presents a new approach to the ancient scripts decipherment problem based on combinatorial optimisation and coupled simulated annealing. The proposed system is able to produce enhanced results in cognate identification when compared to the state-of-the-art systems on standard evaluation benchmarks used in literature.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Fabio Tamburini'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Decipherment of Lost Ancient Scripts as Combinatorial Optimisation Using Coupled Simulated Annealing',\n",
       "   'tldr': 'This paper presents a new approach to the ancient scripts decipherment problem based on combinatorial optimisation and coupled simulated annealing. The proposed system is able to produce enhanced results in cognate identification when compared to the state-of-the-art systems on standard evaluation b',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_6': {'abstract': \"A crucial step in deciphering a text is to identify what set of characters were used to write it. This requires grouping character tokens according to visual and contextual features, which can be challenging for human analysts when the number of tokens or underlying types is large. Prior work has shown that this process can be automated by clustering dense representations of character images, in a task which we call ``script clustering''. In this work, we present novel architectures which exploit varying degrees of contextual and visual information to learn representations for use in script clustering. We evaluate on a range of modern and ancient scripts, and find that our models produce representations which are more effective for script recovery than the current state-of-the-art, despite using just \\\\textasciitilde{}2\\\\textbackslash{}\\\\% as many parameters. Our analysis fruitfully applies these models to assess hypotheses about the character inventory of the partially-deciphered proto-Elamite script.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Logan Born',\n",
       "    'M. Willis Monroe',\n",
       "    'Kathryn Kelley',\n",
       "    'Anoop Sarkar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning the Character Inventories of Undeciphered Scripts Using Unsupervised Deep Clustering',\n",
       "   'tldr': 'A crucial step in deciphering a text is to identify what set of characters were used to write it. This requires grouping character tokens according to visual and contextual features, which can be challenging for human analysts when the number of tokens or underlying types is large. Prior work has sh',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_7': {'abstract': 'A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We contribute an automated conversion from PE notation to modern Hindu-Arabic notation, as well as two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Logan Born',\n",
       "    'M. Willis Monroe',\n",
       "    'Kathryn Kelley',\n",
       "    'Anoop Sarkar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora',\n",
       "   'tldr': 'A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_8': {'abstract': 'To gain a better understanding of the linguistic information encoded in character-based language models, we probe the multilingual contextual CANINE model. We design a range of phonetic probing tasks in six Nordic languages, including Faroese as an additional zero-shot instance. We observe that some phonetic information is indeed encoded in the character representations, as consonants and vowels can be well distinguished using a linear classifier. Furthermore, results for the Danish and Norwegian language seem to be worse for the consonant/vowel distinction in comparison to other languages. The information encoded in these representations can also be learned in a zero-shot scenario, as Faroese shows a reasonably good performance in the same vowel/consonant distinction task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Manex Agirrezabal', 'Sidsel Boldsen', 'Nora Hollenstein'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Hidden Folk: Linguistic Properties Encoded in Multilingual Contextual Character Representations',\n",
       "   'tldr': 'To gain a better understanding of the linguistic information encoded in character-based language models, we probe the multilingual contextual CANINE model. We design a range of phonetic probing tasks in six Nordic languages, including Faroese as an additional zero-shot instance. We observe that some',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CAWL_9': {'abstract': 'We examine the task of distinguishing between Hindi and Urdu when those languages are romanized, i.e., written in the Latin script.  Both languages are widely informally romanized, and to the extent that they are identified in the Latin script by language identification systems, they are typically conflated.  In the absence of large labeled collections of such text, we consider methods for generating training data. Beginning with a small set of seed words, each of which are strongly indicative of one of the languages versus the other, we prompt a pretrained large language model (LLM) to generate romanized text.  Treating text generated from an Urdu prompt as one class and text generated from a Hindi prompt as the other class, we build a binary language identification (LangID) classifier.  We demonstrate that the resulting classifier distinguishes manually romanized Urdu Wikipedia text from manually romanized Hindi Wikipedia text far better than chance.  We use this classifier to estimate the prevalence of Urdu in a large collection of text labeled as romanized Hindi that has been used to train large language models. These techniques can be applied to bootstrap classifiers in other cases where a dataset is known to contain multiple distinct but related classes, such as different dialects of the same language, but for which labels cannot easily be obtained.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Elizabeth Nielsen', 'Christo Kirov', 'Brian Roark'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CAWL'],\n",
       "   'id': 'CAWL_9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Distinguishing Romanized Hindi from Romanized Urdu',\n",
       "   'tldr': 'We examine the task of distinguishing between Hindi and Urdu when those languages are romanized, i.e., written in the Latin script.  Both languages are widely informally romanized, and to the extent that they are identified in the Latin script by language identification systems, they are typically c',\n",
       "   'track': 'The Workshop on Computation and Written Language (CAWL)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_1': {'abstract': 'Scientific publications follow conventionalized rhetorical structures. Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents. In this work, we adapt and extend this idea to the domain of materials science research. We present and release a new dataset of 50 manually annotated research articles. The dataset spans seven sub-topics and is annotated with a materials-science focused multi-label annotation scheme for AZ. We detail corpus statistics and demonstrate high inter-annotator agreement. Our computational experiments show that using domain-specific pre-trained transformer-based text encoders is key to high classification performance. We also find that AZ categories from existing datasets in other domains are transferable to varying degrees.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Timo Schrader',\n",
       "    'Teresa B{\\\\\"u}rkle',\n",
       "    'Sophie Henning',\n",
       "    'Sherry Tan',\n",
       "    'Matteo Finco',\n",
       "    'Stefan Gr{\\\\\"u}newald',\n",
       "    'Maira Indrikova',\n",
       "    'Felix Hildebrand',\n",
       "    'Annemarie Friedrich'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MuLMS-AZ: An Argumentative Zoning Dataset for the Materials Science Domain',\n",
       "   'tldr': 'Scientific publications follow conventionalized rhetorical structures. Classifying the Argumentative Zone (AZ), e.g., identifying whether a sentence states a Motivation, a Result or Background information, has been proposed to improve processing of scholarly documents. In this work, we adapt and ext',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_10': {'abstract': 'We directly embed easily extractable discourse structure information (subsection, paragraph and text type) in a transformer-based Dutch event coreference resolution model in order to more explicitly provide it with structural information that is known to be important in coreferential relationships. Results show that integrating this type of knowledge leads to a significant improvement in CONLL F1 for within-document settings (+ 8.6\\\\textbackslash{}\\\\%) and a minor improvement for cross-document settings (+ 1.1\\\\textbackslash{}\\\\%).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Loic De Langhe', 'Orphee De Clercq', 'Veronique Hoste'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Leveraging Structural Discourse Information for Event Coreference Resolution in Dutch',\n",
       "   'tldr': 'We directly embed easily extractable discourse structure information (subsection, paragraph and text type) in a transformer-based Dutch event coreference resolution model in order to more explicitly provide it with structural information that is known to be important in coreferential relationships. ',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_12': {'abstract': 'Biomedical argument mining (BAM) aims at automatically identifying the argumentative structure in biomedical texts. However, identifying and classifying argumentative relations (AR) between argumentative components (AC) is challenging since it not only needs to understand the semantics of ACs but also need to capture the interactions between them. We argue that entities can serve as bridges that connect different ACs since entities and their mentions convey significant semantic information in biomedical argumentation. For example, it is common that related AC pairs share a common entity. Capturing such entity information can be beneficial for the Relation Identification (RI) task. In order to incorporate this entity information into BAM, we propose an Entity Coreference and Co-occurrence aware Argument Mining (ECCAM) framework based on an edge-oriented graph model for BAM. We evaluate our model on a benchmark dataset and from the experimental results we find that our method improves upon state-of-the-art methods.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Boyang Liu',\n",
       "    'Viktor Schlegel',\n",
       "    'Riza Batista-navarro',\n",
       "    'Sophia Ananiadou'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Entity Coreference and Co-occurrence Aware Argument Mining from Biomedical Literature',\n",
       "   'tldr': 'Biomedical argument mining (BAM) aims at automatically identifying the argumentative structure in biomedical texts. However, identifying and classifying argumentative relations (AR) between argumentative components (AC) is challenging since it not only needs to understand the semantics of ACs but al',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_13': {'abstract': 'Machine-readable inventories of discourse connectives that provide information on multiple levels are valuable resources for automated discourse analysis, e.g. discourse parsing, machine translation, text summarization and argumentation mining. While there are already several connective lexicons available for certain languages (such as German, English, French, Czech, Portuguese, Hebrew, and Spanish), currently, there is no such resource available for Chinese, despite it being one of the most widely spoken languages in the world. To address this gap, we developed the Chinese-DimLex, a discourse lexicon for Chinese (Mandarin). It features 137 Chinese connectives () and is augmented with five layers of information, specifically morphological variations, syntactic categories (part-of-speech), semantic relations (PDTB3.0 sense inventory), usage examples, and English translations. Chinese-DimLex is publicly accessible in both XML format and through an easy-to-use web-interface, which enables browsing and searching of the lexicon, as well as comparison of discourse connectives across different languages based on their syntactic and semantic properties. In this extended abstract, we provide an overview of the data and the workflow used to populate the lexicon, followed by discussion of several Chinese-specific considerations and issues that arose during the process. By submitting this abstract, we aim to a) contribute to discourse research and b) receive feedback to promote and expand the lexicon for future work.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shujun Wan',\n",
       "    'Peter Bourgonje',\n",
       "    'Hongling Xiao',\n",
       "    'Clara Wan Ching Ho',\n",
       "    'Manfred Stede'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Chinese-DiMLex: A Lexicon of Chinese Discourse Connectives',\n",
       "   'tldr': 'Machine-readable inventories of discourse connectives that provide information on multiple levels are valuable resources for automated discourse analysis, e.g. discourse parsing, machine translation, text summarization and argumentation mining. While there are already several connective lexicons ava',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_14': {'abstract': 'Recently, the identification of free connective phrases as signals for discourse relations has received new attention with the introduction of statistical models for their automatic extraction. The limited amount of annotations makes it still challenging to develop well-performing models. In our work, we want to overcome this limitation with semi-supervised learning from unlabeled news texts. We implement a self-supervised sequence labeling approach and filter its predictions by a second model trained to disambiguate signal candidates. With our novel model design, we report state-of-the-art results and in addition, achieve an average improvement of about 5\\\\% for both exactly and partially matched alternativelylexicalized discourse signals due to weak supervision.',\n",
       "   'anthology_url': None,\n",
       "   'authors': [\"Ren{\\\\'e} Knaebel\"],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Weakly-Supervised Learning Approach to the Identification of \"Alternative Lexicalizations\" in Shallow Discourse Parsing',\n",
       "   'tldr': 'Recently, the identification of free connective phrases as signals for discourse relations has received new attention with the introduction of statistical models for their automatic extraction. The limited amount of annotations makes it still challenging to develop well-performing models. In our wor',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_16': {'abstract': 'Discourse-aware techniques, including entity-aware approaches, play a crucial role in summarization. In this paper, we propose an entity-based SpanCopy mechanism to tackle the entity-level factual inconsistency problem in abstractive summarization, i.e. reducing the mismatched entities between the generated summaries and the source documents. Complemented by a Global Relevance component to identify summary-worthy entities, our approach demonstrates improved factual consistency while preserving saliency on four summarization datasets, contributing to the effective application of discourse-aware methods summarization tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wen Xiao', 'Giuseppe Carenini'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Entity-based SpanCopy for Abstractive Summarization to Improve the Factual Consistency',\n",
       "   'tldr': 'Discourse-aware techniques, including entity-aware approaches, play a crucial role in summarization. In this paper, we propose an entity-based SpanCopy mechanism to tackle the entity-level factual inconsistency problem in abstractive summarization, i.e. reducing the mismatched entities between the g',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_17': {'abstract': 'In this study, we examine the benefits of incorporating discourse information into document-level temporal dependency parsing. Specifically, we evaluate the effectiveness of integrating both high-level discourse profiling information, which describes the discourse function of sentences, and surface-level sentence position information into temporal dependency graph (TDG) parsing. Unexpectedly, our results suggest that simple sentence position information, particularly when encoded using our novel sentence-position embedding method, performs the best, perhaps because it does not rely on noisy model-generated feature inputs. Our proposed system surpasses the current state-of-the-art TDG parsing systems in performance.Furthermore, we aim to broaden the discussion on the relationship between temporal dependency parsing and discourse analysis, given the substantial similarities shared between the two tasks. We argue that discourse analysis results should not be merely regarded as an additional input feature for temporal dependency parsing. Instead, adopting advanced discourse analysis techniques and research insights can lead to more effective and comprehensive approaches to temporal information extraction tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jingcheng Niu',\n",
       "    'Victoria Ng',\n",
       "    'Erin Rees',\n",
       "    'Simon De Montigny',\n",
       "    'Gerald Penn'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Discourse Information for Document-Level Temporal Dependency Parsing',\n",
       "   'tldr': 'In this study, we examine the benefits of incorporating discourse information into document-level temporal dependency parsing. Specifically, we evaluate the effectiveness of integrating both high-level discourse profiling information, which describes the discourse function of sentences, and surface-',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_18': {'abstract': 'We present a quantitative and qualitative comparison of the discourse trees defined by the Rhetorical Structure Theory and Questions under Discussion models. Based on an empirical analysis of parallel annotations for 28 texts (blog posts and podcast transcripts), we conclude that both discourse frameworks capture similar structural information. The qualitative analysis shows that while complex discourse units often match between analyses, QUD structures do not indicate the centrality of segments.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sara Shahmohammadi',\n",
       "    'Hannah Seemann',\n",
       "    'Manfred Stede',\n",
       "    'Tatjana Scheffler'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Encoding Discourse Structure: Comparison of RST and QUD',\n",
       "   'tldr': 'We present a quantitative and qualitative comparison of the discourse trees defined by the Rhetorical Structure Theory and Questions under Discussion models. Based on an empirical analysis of parallel annotations for 28 texts (blog posts and podcast transcripts), we conclude that both discourse fram',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_19': {'abstract': '[Finding paper]Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to build discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple tasks for fine-tuning and show that the dialogue-tailored Sentence Ordering task performs best. To locate and exploit discourse information in PLMs, we propose an unsupervised and a semi-supervised method. Our proposals thereby achieve encouraging results on the STAC corpus, with F1 scores of 57.2 and 59.3 for the unsupervised and semi-supervised methods, respectively. When restricted to projective trees, our scores improved to 63.3 and 68.1.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chuyuan Li',\n",
       "    'Patrick Huber',\n",
       "    'Wen Xiao',\n",
       "    'Maxime Amblard',\n",
       "    \"Chlo{\\\\'e} Braud\",\n",
       "    'Giuseppe Carenini'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Discourse Structure Extraction from Pre-Trained and Fine-Tuned Language Models in Dialogues',\n",
       "   'tldr': '[Finding paper]Discourse processing suffers from data sparsity, especially for dialogues. As a result, we explore approaches to build discourse structures for dialogues, based on attention matrices from Pre-trained Language Models (PLMs). We investigate multiple tasks for fine-tuning and show that t',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_20': {'abstract': \"In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another  on the contrary, there are several frameworks that describe the labels' similarities (by e.g. sorting them into a hierarchy or describing them interms of features (Sanders et al., 2021)). Recently, several methods for representing the similarities between labels have been proposed (Zhang et al., 2018; Wang et al., 2018; Xiong et al., 2021). We here explore and extend the Label Confusion Model (Guo et al., 2021) for learning a representation for discourse relation labels. We explore alternative ways of informing the model about the similarities between relations, by representing relations in terms of their names (and parent category), their typical markers, or in terms of CCR features that describe the relations. Experimental results show that exploiting label similarity improves classification results.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nobel Varghese',\n",
       "    'Frances Yung',\n",
       "    'Kaveri Anuranjana',\n",
       "    'Vera Demberg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploiting Knowledge about Discourse Relations for Implicit Discourse Relation Classification',\n",
       "   'tldr': \"In discourse relation recognition, the classification labels are typically represented as one-hot vectors. However, the categories are in fact not all independent of one another  on the contrary, there are several frameworks that describe the labels' similarities (by e.g. sorting them into a hierarc\",\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_22': {'abstract': 'Incorporating external knowledge, such as pre-trained language models (PLMs), into neural topic modeling has achieved great success in recent years. However, employing PLMs for topic modeling generally ignores the maximum sequence length of PLMs and the interaction between external knowledge and bag-of-words (BOW). To this end, we propose a sentence-aware encoder for neural topic modeling, which adopts fine-grained sentence embeddings as external knowledge to entirely utilize the semantic information of input documents. We introduce sentence-aware attention for document representation, where BOW enables the model to attend on topical sentences that convey topic-related cues. Experiments on three benchmark datasets show that our framework outperforms other state-of-the-art neural topic models in topic coherence. Further, we demonstrate that the proposed approach can yield better latent document-topic features through improvement on the document classification.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hao Liu',\n",
       "    'Jingsheng Gao',\n",
       "    'Suncheng Xiang',\n",
       "    'Ting Liu',\n",
       "    'Yuzhuo Fu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SAE-NTM: Sentence-Aware Encoder for Neural Topic Modeling',\n",
       "   'tldr': 'Incorporating external knowledge, such as pre-trained language models (PLMs), into neural topic modeling has achieved great success in recent years. However, employing PLMs for topic modeling generally ignores the maximum sequence length of PLMs and the interaction between external knowledge and bag',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_23': {'abstract': 'Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena.Many works have been published on the topic of document-level NMT, but most restrict the system to only local context, typically including just the one or two preceding sentences as additional information.This might be enough to resolve some ambiguous inputs, but it is probably not sufficient to capture some document-level information like the topic or style of a conversation.When increasing the context size beyond just the local context, there are two challenges: (i) the memory usage increases exponentially (ii) the translation performance starts to degrade.We argue that the widely-used attention mechanism is responsible for both issues.Therefore, we propose a constrained attention variant that focuses the attention on the most relevant parts of the sequence, while simultaneously reducing the memory consumption.For evaluation, we utilize targeted test sets in combination with novel evaluation techniques to analyze the translations in regards to specific discourse-related phenomena.We find that our approach is a good compromise between sentence-level NMT vs attending to the full context, especially in low resource scenarios.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Christian Herold', 'Hermann Ney'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_23',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Long Context Document-Level Machine Translation',\n",
       "   'tldr': 'Document-level context for neural machine translation (NMT) is crucial to improve the translation consistency and cohesion, the translation of ambiguous inputs, as well as several other linguistic phenomena.Many works have been published on the topic of document-level NMT, but most restrict the syst',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_24': {'abstract': 'In this paper, we present principles of constructing and resolving ambiguity in implicit discourse relations. Following these principles, we created a dataset in both English and Egyptian Arabic that controls for semantic disambiguation, enabling the investigation of prosodic features in future work. In these datasets, examples are two-part sentences with an implicit discourse relation that can be ambiguously read as either causal or concessive, paired with two different preceding context sentences forcing either the causal or the concessive reading. We also validated both datasets by humans and language models (LMs) to study whether context can help humans or LMs resolve ambiguities of implicit relations and identify the intended relation. As a result, this task posed no difficulty for humans, but proved challenging for BERT/CamelBERT and ELECTRA/AraELECTRA models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ahmed Ruby', 'Sara Stymne', 'Christian Hardmeier'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Unpacking Ambiguous Structure: A Dataset for Ambiguous Implicit Discourse Relations for English and Egyptian Arabic',\n",
       "   'tldr': 'In this paper, we present principles of constructing and resolving ambiguity in implicit discourse relations. Following these principles, we created a dataset in both English and Egyptian Arabic that controls for semantic disambiguation, enabling the investigation of prosodic features in future work',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_25': {'abstract': '[***NOTE: This is an ACL Findings paper***]Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguistic framework of Questions Under Discussion (QUD) for discourse analysis and seeks to derive QUD structures automatically. QUD views each sentence as an answer to a question triggered in prior context; thus, we characterize relationships between sentences as free-form questions, in contrast to exhaustive fine-grained taxonomies. We develop the first-of-its-kind QUD parser that derives a dependency structure of questions over full documents, trained using a large, crowdsourced question-answering dataset DCQA (Ko et al., 2022). Strong human evaluation results show that QUD dependency parsing is highly feasible under this crowdsourced, generalizable annotation scheme. We illustrate how our QUD structure is distinct from RST trees, and demonstrate the utility of QUD analysis in the context of document simplification. Our findings show that QUD parsing is an appealing alternative for automatic discourse processing.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wei-jen Ko',\n",
       "    'Yating Wu',\n",
       "    'Cutter Dalton',\n",
       "    'Dananjay Srinivas',\n",
       "    'Greg Durrett',\n",
       "    'Junyi Jessy Li'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Discourse Analysis via Questions and Answers: Parsing Dependency Structures of Questions Under Discussion',\n",
       "   'tldr': '[***NOTE: This is an ACL Findings paper***]Automatic discourse processing is bottlenecked by data: current discourse formalisms pose highly demanding annotation tasks involving large taxonomies of discourse relations, making them inaccessible to lay annotators. This work instead adopts the linguisti',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_26': {'abstract': 'Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits --- such as adding relevant background information or reordering content --- may require document-level context.Prior work has also predominantly framed simplification as a single-step, input-to-output task, only implicitly modeling the fine-grained, span-level edits that elucidate the simplification process.To address both gaps, we introduce the SWiPE dataset, which reconstructs the document-level editing process from English Wikipedia (EW) articles to paired Simple Wikipedia (SEW) articles. In contrast to prior work, SWiPE leverages the entire revision history when pairing pages in order to better identify simplification edits. We work with Wikipedia editors to annotate 5,000 EW-SEW document pairs, labeling more than 40,000 edits with proposed 19 categories.To scale our efforts, we propose several models to automatically label edits, achieving an F-1 score of up to 70.6, indicating that this is a tractable but challenging NLU task. Finally, we categorize the edits produced by several simplification models and find that SWiPE-trained models generate more complex edits while reducing unwanted edits.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Philippe Laban',\n",
       "    'Jesse Vig',\n",
       "    'Wojciech Kryscinski',\n",
       "    'Shafiq Joty',\n",
       "    'Caiming Xiong',\n",
       "    'Chien-sheng Wu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SWiPE: A Dataset for Document-Level Simplification of Wikipedia Pages',\n",
       "   'tldr': 'Text simplification research has mostly focused on sentence-level simplification, even though many desirable edits --- such as adding relevant background information or reordering content --- may require document-level context.Prior work has also predominantly framed simplification as a single-step,',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_27': {'abstract': \"While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by comparing InstructGPT's performance on learning referential biases with results from real psycholinguistic experiments. Recent psycholinguistic studies suggest that humans adapt their referential biases with exposure to referential patterns; closely replicating three relevant psycholinguistic experiments from Johnson and Arnold (2022) in an in-context learning (ICL) framework, we found that InstructGPT adapts its pronominal interpretations in response to the frequency of referential patterns in the local discourse, though in a limited fashion: adaptation was only observed relative to syntactic but not semantic biases. Our results provide further evidence that contemporary LLMs discourse representations are sensitive to syntactic patterns in the local context but less so to semantic patterns.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Suet-ying Lam',\n",
       "    'Qingcheng Zeng',\n",
       "    'Kexun Zhang',\n",
       "    'Chenyu You',\n",
       "    'Rob Voigt'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Replicate and Compare with Humans: LLMs Represent Partial Semantic Knowledge in Pronoun Interpretation',\n",
       "   'tldr': \"While a large body of literature suggests that large language models (LLMs) acquire rich linguistic representations, little is known about whether they adapt to linguistic biases in a human-like way. The present study probes this question by comparing InstructGPT's performance on learning referentia\",\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_28': {'abstract': 'Transforming narrative structure to implicit discourse relations in long-form text has recently seen a mindset shift toward assessing generation consistency. To this extent, summarization of lengthy biographical discourse is of practical benefit to readers, as it helps them decide whether immersing for days or weeks in a bulky book turns a rewarding experience. Machine-generated summaries can reduce the cognitive load and the time spent by authors to write the summary. Nevertheless, summarization faces significant challenges of factual inconsistencies with respect to the inputs. In this paper, we explored a two-step summary generation aimed to retain source-summary faithfulness. Our method uses a graph representation to rank sentence saliency in each of the novel chapters, leading to distributing summary segments in distinct regions of the chapter. Basing on the previously extracted sentences we produced an abstractive summary in a manner more computationally tractable for detecting inconsistent information. We conducted a series of quantitative analyses on a test set of four long biographical novels and showed to improve summarization quality in automatic evaluation over both single-tier settings and external baselines.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Avi Bleiweiss'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Two-step Text Summarization for Long-form Biographical Narrative Genre',\n",
       "   'tldr': 'Transforming narrative structure to implicit discourse relations in long-form text has recently seen a mindset shift toward assessing generation consistency. To this extent, summarization of lengthy biographical discourse is of practical benefit to readers, as it helps them decide whether immersing ',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_29': {'abstract': 'Time pressure and topic negotiation may impose constraints on how people leverage discourse relations (DRs) in spontaneous conversational contexts. In this work, we adapt a system of DRs for written language to spontaneous dialogue using crowdsourced annotations from novice annotators. We then test whether discourse relations are used differently across several types of multi-utterance contexts. We compare the patterns of DR annotation within and across speakers and within and across turns. Ultimately, we find that different discourse contexts produce distinct distributions of discourse relations, with single-turn annotations creating the most uncertainty for annotators. Additionally, we find that the discourse relation annotations are of sufficient quality to predict from embeddings of discourse units.',\n",
       "   'anthology_url': None,\n",
       "   'authors': [\"S. Magal\\\\'{i} L\\\\'{o}pez Cortez\", 'Cassandra L. Jacobs'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_29',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The distribution of discourse relations within and across turns in spontaneous conversation',\n",
       "   'tldr': 'Time pressure and topic negotiation may impose constraints on how people leverage discourse relations (DRs) in spontaneous conversational contexts. In this work, we adapt a system of DRs for written language to spontaneous dialogue using crowdsourced annotations from novice annotators. We then test ',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_30': {'abstract': 'Our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media. Social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests. However, with the vast number of online communities available, users may face difficulties in identifying relevant groups to address their mental health concerns. To address this challenge, we explore the integration of discourse information from various subreddit communities using embedding techniques to develop an effective recommendation system. Our approach involves the use of content-based and collaborative filtering techniques to enhance the performance of the recommendation system. Our findings indicate that the proposed approach outperforms the use of each technique separately and provides interpretability in the recommendation process.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hy Dang', 'Bang Nguyen', 'Noah Ziems', 'Meng Jiang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Embedding Mental Health Discourse for Community Recommendation',\n",
       "   'tldr': 'Our paper investigates the use of discourse embedding techniques to develop a community recommendation system that focuses on mental health support groups on social media. Social media platforms provide a means for users to anonymously connect with communities that cater to their specific interests.',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_31': {'abstract': 'We present a corpus of parallel German-language simplified newspaper articles. The articles have been aligned at sentence level and annotated according to the Rhetorical Structure Theory (RST) framework. These RST annotated texts could shed light on structural aspects of text complexity and how simplifications work on a text-level.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Freya Hewett'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_31',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'APA-RST: A Text Simplification Corpus with RST Annotations',\n",
       "   'tldr': 'We present a corpus of parallel German-language simplified newspaper articles. The articles have been aligned at sentence level and annotated according to the Rhetorical Structure Theory (RST) framework. These RST annotated texts could shed light on structural aspects of text complexity and how simp',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_33': {'abstract': '[Findings Paper]To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of sentence-level simplification operations) and using this plan to guide sentence-level simplification downstream. However, this is still limited in that the simplification model has no direct access to the local inter-sentence document context, likely having a negative impact on surface realisation. We explore various systems that use document context within the simplification process itself, either by iterating over larger text units or by extending the system architecture to attend over a high-level representation of document context. In doing so, we achieve state-of-the-art performance on the document simplification task, even when not relying on plan-guidance. Further, we investigate the performance and efficiency tradeoffs of system variants and make suggestions of when each should be preferred.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Liam Cripwell', 'Jol Legrand', 'Claire Gardent'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Context-Aware Document Simplification',\n",
       "   'tldr': '[Findings Paper]To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to subop',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_35': {'abstract': '(This is a findings paper).Collaboration increasingly happens online.This is especially true for large groups working on global tasks, with collaborators all around the world. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts for the study of decision-making mechanisms in such groups, and provides a new annotated dataset based on real-world data from the public mail-archives of one such organization  the Internet Engineering Task Force (IETF). Weprovide an initial data analysis showing that this dataset can be used to better understanddecision-making in such organizations. Finally, we experiment with a preliminary transformerbased dialog act tagging model.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mladen Karan',\n",
       "    'Prashant Khare',\n",
       "    'Ravi Shekhar',\n",
       "    'Stephen Mcquistin',\n",
       "    'Colin Perkins',\n",
       "    'Ignacio Castro',\n",
       "    'Gareth Tyson',\n",
       "    'Patrick Healey',\n",
       "    'Matthew Purver'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Email Dataset for Analyzing Large-Group Decision-Making',\n",
       "   'tldr': '(This is a findings paper).Collaboration increasingly happens online.This is especially true for large groups working on global tasks, with collaborators all around the world. The size and distributed nature of such groups makes decision-making challenging. This paper proposes a set of dialog acts f',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_37': {'abstract': 'The submitted paper (camera-ready version) has been accepted to the Findings of ACL 2023. We are also submitting it to the LAW-XVII workshop.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yang Janet Liu', 'Amir Zeldes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_37',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization',\n",
       "   'tldr': 'The submitted paper (camera-ready version) has been accepted to the Findings of ACL 2023. We are also submitting it to the LAW-XVII workshop.',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_38': {'abstract': '(Accepted to the Findings of ACL 2023) One crucial aspect of democracy is fair information sharing. While it is hard to prevent biases in news, they should be identified for better transparency. We propose an approach to automatically characterize biases that takes into account structural differences and that is efficient for long texts. This yields new ways to provide explanations for a textual classifier, going beyond mere lexical cues. We show that: (i) the use of discourse-based structure-aware document representations compare well to local, computationally heavy, or domain-specific models on classification tasks that deal with textual bias (ii) our approach based on different levels of granularity allows for the generation of better explanations of model decisions, both at the lexical and structural level, while addressing the challenge posed by long texts.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nicolas Devatine', 'Philippe Muller', \"Chlo{\\\\'e} Braud\"],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_38',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Integrated Approach for Political Bias Prediction and Explanation Based on Discursive Structure',\n",
       "   'tldr': '(Accepted to the Findings of ACL 2023) One crucial aspect of democracy is fair information sharing. While it is hard to prevent biases in news, they should be identified for better transparency. We propose an approach to automatically characterize biases that takes into account structural difference',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_39': {'abstract': 'Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in which they appear. Existing methods for training coreference systems sample from a largely skewed distribution, making it difficult for the algorithm to learn coreference beyond surface matching. Additionally, these methods are intractable because of the quadratic operations needed. To address these challenges, we break the problem of ECR into two parts: a) a heuristic to efficiently filter out a large number of non-coreferent pairs, and b) a training approach on a balanced set of coreferent and non-coreferent mention pairs. By following this approach, we show that we get comparable results to the state of the art on two popular ECR datasets while significantly reducing compute requirements. We also analyze the mention pairs that are \"hard\" to accurately classify as coreferent or non-coreferent. This is an accepted Findings paper at ACL 2023',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shafiuddin Rehan Ahmed',\n",
       "    'Abhijnan Nath',\n",
       "    'James H. Martin',\n",
       "    'Nikhil Krishnaswamy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '$2*n$ is better than $n^2$: Decomposing Event Coreference Resolution into Two Tractable Problems',\n",
       "   'tldr': 'Event Coreference Resolution (ECR) is the task of linking mentions of the same event either within or across documents. Most mention pairs are not coreferent, yet many that are coreferent can be identified through simple techniques such as lemma matching of the event triggers or the sentences in whi',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_4': {'abstract': 'Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse performance comparison of 7 pre-trained language models. We use PDTB-3, a popular discourse relation annotated dataset. Through our model search, we raise SOTA to 0.671 ACC and obtain novel observations. Some are contrary to what has been reported before (Shi and Demberg, 2019b), that sentence-level pre-training objectives (NSP, SBO, SOP) generally fail to produce the best-performing model for implicit discourse relation classification. Counterintuitively, similar-sized PLMs with MLM and full attention led to better performance. Our code is publicly released.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Bruce W. Lee', 'Bongseok Yang', 'Jason Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Side-by-side Comparison of Transformers for Implicit Discourse Relation Classification',\n",
       "   'tldr': 'Though discourse parsing can help multiple NLP fields, there has been no wide language model search done on implicit discourse relation classification. This hinders researchers from fully utilizing public-available models in discourse analysis. This work is a straightforward, fine-tuned discourse pe',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_40': {'abstract': '(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future goal utterances that are multiple turns away, given a dialogue context. As part of our analysis, we investigate characteristics that make conversations (un)plannable and find strong evidence of planning capability over multiple turns (in 61.56% over 3 turns) in conversations from the DailyDialog dataset. Finally, we show how we achieve higher efficiency in sequence modeling tasks compared to previous work thanks to our relativistic approach, where only the last utterance needs to be encoded and computed during inference.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Justus-jonas Erker', 'Stefan Schaffer', 'Gerasimos Spanakis'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Extended abstract',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning',\n",
       "   'tldr': '(ACL Findings paper)Inspired by the curvature of space-time, we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a ',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_5': {'abstract': \"Entity coreference resolution is an important research problem with many applications, including information extraction and question answering. Coreference resolution for English has been studied extensively. However, there is relatively little work for other languages. A problem that frequently occurs when working with a non-English language is the scarcity of annotated training data. To overcome this challenge, we design a simple but effective ensemble-based framework that combines various transfer learning (TL) techniques. We first train several models using different TL methods. Then, during inference, we compute the unweighted average scores of the models' predictions to extract the final set of predicted clusters. Furthermore, we also propose a low-cost TL method that bootstraps coreference resolution models by utilizing Wikipedia anchor texts. Leveraging the idea that the coreferential links naturally exist between anchor texts pointing to the same article, our method builds a sizeable distantly-supervised dataset for the target language that consists of tens of thousands of documents. We can pre-train a model on the pseudo-labeled dataset before finetuning it on the final target dataset. Experimental results on two benchmark datasets, OntoNotes and SemEval, confirm the effectiveness of our methods. Our best ensembles consistently outperform the baseline approach of simple training by up to 7.68\\\\% in the F1 score. These ensembles also achieve new state-of-the-art results for three languages: Arabic, Dutch, and Spanish.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tuan Lai', 'Heng Ji'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Ensemble Transfer Learning for Multilingual Coreference Resolution',\n",
       "   'tldr': 'Entity coreference resolution is an important research problem with many applications, including information extraction and question answering. Coreference resolution for English has been studied extensively. However, there is relatively little work for other languages. A problem that frequently occ',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'CODI_6': {'abstract': 'The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns effective sentence representations with dedicated designed hierarchical graph information aggregation. We also propose a graph contrastive learning module to learn global theme-aware sentence representations. Extensive experiments on the PubMed and arXiv benchmark datasets prove the effectiveness of CHANGES and the importance of capturing hierarchical structure information in modeling scientific papers.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Haopeng Zhang', 'Xiao Liu', 'Jiawei Zhang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['CODI'],\n",
       "   'id': 'CODI_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'Regular long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Contrastive Hierarchical Discourse Graph for Scientific Document Summarization',\n",
       "   'tldr': 'The extended structural context has made scientific paper summarization a challenging task. This paper proposes CHANGES, a contrastive hierarchical graph neural network for extractive scientific paper summarization. CHANGES represents a scientific paper with a hierarchical discourse graph and learns',\n",
       "   'track': '4th Workshop on Computational Approaches to Discourse',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_12': {'abstract': 'Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data management. In this paper, we study the effectiveness of dialog-aware pretraining objectives and multiphase training in using unlabeled data to improve LMs training for medical utterance classification. The objectives of pretraining for dialog awareness involve tasks that take into account the structure of conversations, including features such as turn-taking and the roles of speakers. The multiphase training process uses unannotated data in a sequence that prioritizes similarities and connections between different domains. We empirically evaluate these methods on conversational dialog classification tasks in the medical and counseling domains, and find that multiphase training can help achieve higher performance than standard pretraining or finetuning.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Do June Min', 'Veronica Perez-Rosas', 'Rada Mihalcea'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Navigating Data Scarcity: Pretraining for Medical Utterance Classification',\n",
       "   'tldr': 'Pretrained language models leverage self-supervised learning to use large amounts of unlabeled text for learning contextual representations of sequences. However, in the domain of medical conversations, the availability of large, public datasets is limited due to issues of privacy and data managemen',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_13': {'abstract': 'In developing countries like India, doctors and healthcare professionals working in public health spend significant time answering health queries that are fact-based and repetitive. Therefore, we propose an automated way to answer maternal and child health-related queries. A database of Frequently Asked Questions (FAQs) and their corresponding answers generated by experts is curated from rural health workers and young mothers. We develop a Hindi chatbot that identifies k relevant Question and Answer (QnA) pairs from the database in response to a healthcare query (q) written in Devnagri script or Hindi-English (Hinglish) code-mixed script. The curated database covers 80% of all the queries that a user of our study is likely to ask. We experimented with (i) rule-based methods, (ii) sentence embeddings, and (iii) a paraphrasing classifier, to calculate the q-Q similarity. We observed that paraphrasing classifier gives the best result when trained first on an open-domain text and then on the healthcare domain. Our chatbot uses an ensemble of all three approaches. We observed that if a given q can be answered using the database, then our chatbot can provide at least one relevant QnA pair among its top three suggestions for up to 70% of the queries.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ritwik Mishra',\n",
       "    'Simranjeet Singh',\n",
       "    'Jasmeet Kaur',\n",
       "    'Pushpendra Singh',\n",
       "    'Rajiv Shah'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Hindi Chatbot for Supporting Maternal and Child Health Related Queries in Rural India',\n",
       "   'tldr': 'In developing countries like India, doctors and healthcare professionals working in public health spend significant time answering health queries that are fact-based and repetitive. Therefore, we propose an automated way to answer maternal and child health-related queries. A database of Frequently A',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_16': {'abstract': 'Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a comprehensive generative AI framework, comprised of six tasks representing key components in clinical reasoning. We present a comparative analysis of in-domain versus out-of-domain language models as well as multi-task versus single task training with a focus on the problem summarization task in DR.BENCH. We demonstrate that a multi-task, clinically-trained language model outperforms its general domain counterpart by a large margin, establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55. This research underscores the value of domain-specific training for optimizing clinical diagnostic reasoning tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Brihat Sharma',\n",
       "    'Yanjun Gao',\n",
       "    'Timothy Miller',\n",
       "    'Matthew Churpek',\n",
       "    'Majid Afshar',\n",
       "    'Dmitriy Dligach'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi-Task Training with In-Domain Language Models for Diagnostic Reasoning',\n",
       "   'tldr': 'Generative artificial intelligence (AI) is a promising direction for augmenting clinical diagnostic decision support and reducing diagnostic errors, a leading contributor to medical errors. To further the development of clinical AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduce',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_19': {'abstract': 'Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly released Contextualized Medication Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical Challenges (n2c2) shared task. Our study evaluates the performance of various pretrained language models in this task. Further, we find that data augmentation coupled with domain-specific training provides notable improvements. With experiments, we also underscore the importance of careful data preprocessing in medical event detection.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Noushin Salek Faramarzi',\n",
       "    'Meet Patel',\n",
       "    'Sai Harika Bandarupally',\n",
       "    'Ritwik Banerjee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Context-aware Medication Event Extraction from Unstructured Text',\n",
       "   'tldr': 'Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly rele',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_2': {'abstract': 'Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We collect a benchmark of 18 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP) and make it publicly available for the community to further develop clinically-aware ASR metrics. To our knowledge, this is the first public dataset of its kind. We demonstrate that our metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Joel Shor',\n",
       "    'Ruyue Agnes Bi',\n",
       "    'Subhashini Venugopalan',\n",
       "    'Steven Ibara',\n",
       "    'Roman Goldenberg',\n",
       "    'Ehud Rivlin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings',\n",
       "   'tldr': 'Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcript',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_20': {'abstract': \"International Classification of Diseases (ICD) coding is the task of assigning a patient's electronic health records into standardized codes, which is crucial for enhancing medical services and reducing healthcare costs. In Korea, automatic Korean Standard Classification of Diseases (KCD) coding has been hindered by limited resources, differences in ICD systems, and language-specific characteristics. Therefore, we construct the Korean Dataset for Automatic KCD coding (KoDAK) by collecting and preprocessing Korean clinical documents. In addition, we propose a tokenization method optimized for Korean clinical documents. Our experiments show that our proposed method outperforms Korean Medical BERT (KM-BERT) in Macro-F1 performance by 0.14%p while using fewer model parameters, demonstrating its effectiveness in Korean clinical documents.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Geunyeong Jeong',\n",
       "    'Juoh Sun',\n",
       "    'Seokwon Jeong',\n",
       "    'Hyunjin Shin',\n",
       "    'Harksoo Kim'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Automatic KCD Coding: Introducing the KoDAK and an Optimized Tokenization Method for Korean Clinical Documents',\n",
       "   'tldr': \"International Classification of Diseases (ICD) coding is the task of assigning a patient's electronic health records into standardized codes, which is crucial for enhancing medical services and reducing healthcare costs. In Korea, automatic Korean Standard Classification of Diseases (KCD) coding has\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_22': {'abstract': \"Natural language processing (NLP) has shown great potential for Alzheimer's disease (AD) detection, particularly due to the adverse effect of AD on spontaneous speech. The current body of literature has directed attention toward context-based models, especially Bidirectional Encoder Representations from Transformers (BERTs), owing to their exceptional abilities to integrate contextual information in a wide range of NLP tasks.\\nThis comes at the cost of added model opacity and computational requirements. Taking this into consideration, we propose a Word2Vec-based model for AD detection in 108 age- and sex-matched participants who were asked to describe the Cookie Theft picture. We also investigate the effectiveness of our model by fine-tuning BERT-based sequence classification models, as well as incorporating linguistic features. Our results demonstrate that our lightweight and easy-to-implement model outperforms some of the state-of-the-art models available in the literature, as well as BERT models.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Behrad Taghibeyglou', 'Frank Rudzicz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Who needs context? Classical techniques for Alzheimers disease detection',\n",
       "   'tldr': \"Natural language processing (NLP) has shown great potential for Alzheimer's disease (AD) detection, particularly due to the adverse effect of AD on spontaneous speech. The current body of literature has directed attention toward context-based models, especially Bidirectional Encoder Representations \",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_26': {'abstract': 'In the medical field, there are many clinical texts such as electronic medical records, and research on Japanese natural language processing using these texts has been conducted.One such research involves Recognizing Textual Entailment (RTE) in clinical texts using a semantic analysis and logical inference system, ccg2lambda.However, it is difficult for existing inference systems to correctly determine the entailment relations , if the input sentence contains medical domain specific paraphrases such as disease names.\\nIn this study, we propose a method to supplement the equivalence relations of disease names as axioms by identifying candidates for paraphrases that lack in theorem proving.Candidates of paraphrases are identified by using a model for the NER task for disease names and a disease name dictionary.We also construct an inference test set that requires knowledge injection of disease names and evaluate our inference system.Experiments showed that our inference system was able to correctly infer for 106 out of 149 inference test sets.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Natsuki Murakami',\n",
       "    'Mana Ishida',\n",
       "    'Yuta Takahashi',\n",
       "    'Hitomi Yanaka',\n",
       "    'Daisuke Bekki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Knowledge Injection for Disease Names in Logical Inference between Japanese Clinical Texts',\n",
       "   'tldr': 'In the medical field, there are many clinical texts such as electronic medical records, and research on Japanese natural language processing using these texts has been conducted.One such research involves Recognizing Textual Entailment (RTE) in clinical texts using a semantic analysis and logical in',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_27': {'abstract': \"This work introduces a novel three-class annotation scheme for text-based dementia classification in patients, based on their recorded visit interactions. Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches were employed to improve the representation of dementia samples: oversampling the underrepresented data points in the original Pitt dataset and combining the Pitt with the Holland and Kempler datasets. The DistilBERT models trained on either an oversampled Pitt dataset or the combined dataset performed best in classifying the dementia class. Specifically, the model trained on the oversampled Pitt dataset and the one trained on the combined dataset obtained state-of-the-art performance with 98.8% overall accuracy and 98.6% macro-averaged F1-score, respectively. The models' outputs were manually inspected through saliency highlighting, using Local Interpretable Model-agnostic Explanations (LIME), to provide a better understanding of its predictions.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nadine Abdelhalim', 'Ingy Abdelhalim', 'Riza Batista-Navarro'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Training Models on Oversampled Data and a Novel Multi-class Annotation Scheme for Dementia Detection',\n",
       "   'tldr': 'This work introduces a novel three-class annotation scheme for text-based dementia classification in patients, based on their recorded visit interactions. Multiple models were developed utilising BERT, RoBERTa and DistilBERT. Two approaches were employed to improve the representation of dementia sam',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_3': {'abstract': 'Assessing the capacity of numerical understanding of vision-and-language models over images and texts is crucial for real vision-and-language applications, such as systems for automated medical image analysis.\\nWe provide a visual reasoning dataset focusing on numerical understanding in the medical domain.\\nThe experiments using our dataset show that current vision-and-language models fail to perform numerical inference in the medical domain.\\nHowever, the data augmentation with only a small amount of our dataset improves the model performance, while maintaining the performance in the general domain.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hitomi Yanaka',\n",
       "    'Yuta Nakamura',\n",
       "    'Yuki Chida',\n",
       "    'Tomoya Kurosawa'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Medical Visual Textual Entailment for Numerical Understanding of Vision-and-Language Models',\n",
       "   'tldr': 'Assessing the capacity of numerical understanding of vision-and-language models over images and texts is crucial for real vision-and-language applications, such as systems for automated medical image analysis.\\nWe provide a visual reasoning dataset focusing on numerical understanding in the medical d',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_30': {'abstract': 'Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised learning models with the world knowledge inside large language models (LLMs). Surprisingly, we find that zero-shot LLMs out-perform supervised BERT-based models applied to out-of-domain data. We also find that their strengths are synergistic, so that a simple ensemble technique leads to additional performance gains.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Weipeng Zhou',\n",
       "    'Majid Afshar',\n",
       "    'Dmitriy Dligach',\n",
       "    'Yanjun Gao',\n",
       "    'Timothy Miller'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving the Transferability of Clinical Note Section Classification Models with BERT and Large Language Model Ensembles',\n",
       "   'tldr': 'Text in electronic health records is organized into sections, and classifying those sections into section categories is useful for downstream tasks. In this work, we attempt to improve the transferability of section classification models by combining the dataset-specific knowledge in supervised lear',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_31': {'abstract': 'Recent advances in large language models (LLMs) have generated significant interest in their application across various domains including healthcare. However, there is limited data on their safety and performance in real-world scenarios. This study uses data collected using an autonomous telemedicine clinical assistant. The assistant asks symptom-based questions to elicit patient concerns and allows patients to ask questions about their post-operative recovery. We utilise real-world postoperative questions posed to the assistant by a cohort of 120 patients to examine the safety and appropriateness of responses generated by a recent popular LLM by OpenAI, ChatGPT. We demonstrate that LLMs have the potential to helpfully address routine patient queries following routine surgery.  However, important limitations around the safety of todays models exist which must be considered. \\n',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Mohita Chowdhury',\n",
       "    'Ernest Lim',\n",
       "    'Aisling Higham',\n",
       "    'Rory McKinnon',\n",
       "    'Nikoletta Ventoura',\n",
       "    'Yajie He',\n",
       "    'Nick De Pennington'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_31',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Can Large Language Models Safely Address Patient Questions Following Cataract Surgery?',\n",
       "   'tldr': 'Recent advances in large language models (LLMs) have generated significant interest in their application across various domains including healthcare. However, there is limited data on their safety and performance in real-world scenarios. This study uses data collected using an autonomous telemedicin',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_33': {'abstract': 'We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhancements to this baseline model which include using subword and multiword tokenization scheme, prefixing the targets with a chain-of-clinical-facts, and training with contrastive loss that is defined over various candidate summaries. We also use flash attention during training and query chunked attention during inference to be able to process long input and output sequences and to improve computational efficiency. Experiments are conducted on a dataset containing about 900K encounters from around 1800 healthcare providers covering 27 specialties. The results are broken down into primary care and non-primary care specialties. Consistent accuracy improvements are observed across both of these categories.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gagandeep Singh',\n",
       "    'Yue Pan',\n",
       "    'Jesus Andres-Ferrer',\n",
       "    'Miguel Del-Agua',\n",
       "    'Frank Diehl',\n",
       "    'Joel Pinto',\n",
       "    'Paul Vozila'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Large Scale Sequence-to-Sequence Models for Clinical Note Generation from Patient-Doctor Conversations',\n",
       "   'tldr': 'We present our work on building large scale sequence-to-sequence models for generating clinical note from patient-doctor conversation. This is formulated as an abstractive summarization task for which we use encoder-decoder transformer model with pointer-generator. We discuss various modeling enhanc',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_35': {'abstract': 'Clinical Natural Language Processing has been an increasingly popular research area in the NLP community. With the rise of large language models (LLMs) and their impressive abilities in NLP tasks, it is crucial to pay attention to their clinical applications. Sequence to sequence generative approaches with LLMs have been widely used in recent years. To be a part of the research in clinical NLP with recent advances in the field, we participated in task A of MEDIQA-Chat at ACL-ClinicalNLP Workshop 2023. In this paper, we explain our methods and findings as well as our comments on our results and limitations. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kadir Bulut Ozler', 'Steven Bethard'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'clulab at MEDIQA-Chat 2023: Summarization and classification of medical dialogues',\n",
       "   'tldr': 'Clinical Natural Language Processing has been an increasingly popular research area in the NLP community. With the rise of large language models (LLMs) and their impressive abilities in NLP tasks, it is crucial to pay attention to their clinical applications. Sequence to sequence generative approach',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_36': {'abstract': \"Early detection and automated classification of dementia has recently gained considerable attention using neuroimaging data and spontaneous speech. In this paper, we validate the possibility of dementia detection with in-hospital clinical notes. We collected 954 patients' clinical notes from a local hospital and assign dementia/non-dementia labels to those patients based on clinical assessment and telephone interview. Given the labeled dementia data sets, we fine tune a ClinicalBioBERT based on some filtered clinical notes and conducted experiments on both binary and three class dementia classification. Our experiment results show that the fine tuned ClinicalBioBERT achieved satisfied performance on binary classification but failed on three class dementia classification. Further analysis suggests that more human prior knowledge should be considered. \",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ming Liu',\n",
       "    'Richard Beare',\n",
       "    'Taya Collyer',\n",
       "    'Nadine Andrew',\n",
       "    'Velandai Srikanth'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_36',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Leveraging Natural Language Processing and Clinical Notes for Dementia Detection',\n",
       "   'tldr': \"Early detection and automated classification of dementia has recently gained considerable attention using neuroimaging data and spontaneous speech. In this paper, we validate the possibility of dementia detection with in-hospital clinical notes. We collected 954 patients' clinical notes from a local\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_37': {'abstract': 'We propose a method to automate orthodontic diagnosis with natural language processing. It is worthwhile to assist dentists with such technology to prevent errors by inexperienced dentists and to reduce the workload of experienced ones. However, text length and style inconsistencies in medical findings make an automated orthodontic diagnosis with deep-learning models difficult. In this study, we improve the performance of automatic diagnosis utilizing short summaries of medical findings written in a consistent style by experienced dentists. Experimental results on 970 Japanese medical findings show that summarization consistently improves the performance of various machine learning models for automated orthodontic diagnosis. Although BERT is the model that gains the most performance with the proposed method, the convolutional neural network achieved the best performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Takumi Ohtsuka',\n",
       "    'Tomoyuki Kajiwara',\n",
       "    'Chihiro Tanikawa',\n",
       "    'Yuujin Shimizu',\n",
       "    'Hajime Nagahara',\n",
       "    'Takashi Ninomiya'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_37',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Automated Orthodontic Diagnosis from a Summary of Medical Findings',\n",
       "   'tldr': 'We propose a method to automate orthodontic diagnosis with natural language processing. It is worthwhile to assist dentists with such technology to prevent errors by inexperienced dentists and to reduce the workload of experienced ones. However, text length and style inconsistencies in medical findi',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_38': {'abstract': 'Recent advancements in natural language processing (NLP) have been driven by large language models (LLMs), thereby revolutionizing the field. Our study investigates the impact of diverse pre-training strategies on the performance of Turkish clinical language models in a multi-label classification task involving radiology reports, with a focus on overcoming language resource limitations. Additionally, for the first time, we evaluated the simultaneous pre-training approach by utilizing limited clinical task data. We developed four models: TurkRadBERT-task v1, TurkRadBERT-task v2, TurkRadBERT-sim v1, and TurkRadBERT-sim v2. Our results revealed superior performance from BERTurk and TurkRadBERT-task v1, both of which leverage a broad general-domain corpus. Although task-adaptive pre-training is capable of identifying domain-specific patterns, it may be prone to overfitting because of the constraints of the task-specific corpus. Our findings highlight the importance of domain-specific vocabulary during pre-training to improve performance. They also affirmed that a combination of general domain knowledge and task-specific fine-tuning is crucial for optimal performance across various categories. This study offers key insights for future research on pre-training techniques in the clinical domain, particularly for low-resource languages.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hazal Trkmen',\n",
       "    'Oguz Dikenelli',\n",
       "    'Cenk Eraslan',\n",
       "    'Mehmet Calli',\n",
       "    'Suha Ozbek'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_38',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios',\n",
       "   'tldr': 'Recent advancements in natural language processing (NLP) have been driven by large language models (LLMs), thereby revolutionizing the field. Our study investigates the impact of diverse pre-training strategies on the performance of Turkish clinical language models in a multi-label classification ta',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_39': {'abstract': 'Over the last years, an increasing number of publicly available, semantically annotated medical corpora have been released for the German language. While their annotations cover comparable semantic classes, the synergies of such efforts have not been explored, yet. This is due to substantial differences in the data schemas (syntax) and annotated entities (semantics), which hinder the creation of common meta-datasets. For instance, it is unclear whether named entity recognition (NER) taggers trained on one or more of such datasets are useful to detect entities in any of the other datasets.  In this work, we create harmonized versions of German medical corpora using the BigBIO framework, and make them available to the community. Using these as a meta-dataset, we perform a series of cross-corpus evaluation experiments on two settings of aligned labels. These consist in fine-tuning various pre-trained Transformers on different combinations of training sets, and testing them against each dataset separately. We find that a) trained NER models generalize poorly, with F1 scores dropping approx. 20 pp. on unseen test data, and b) current pre-trained Transformer models for the German language do not systematically alleviate this issue. However, our results suggest that models benefit from additional training corpora in most cases, even if these belong to different medical fields or text genres.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ignacio Llorca', 'Florian Borchert', 'Matthieu-P. Schapranow'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Meta-dataset of German Medical Corpora: Harmonization of Annotations and Cross-corpus NER Evaluation',\n",
       "   'tldr': 'Over the last years, an increasing number of publicly available, semantically annotated medical corpora have been released for the German language. While their annotations cover comparable semantic classes, the synergies of such efforts have not been explored, yet. This is due to substantial differe',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_4': {'abstract': \"Valuable datasets that contain sensitive information are not shared due to privacy and copyright concerns. This hinders progress in many areas and prevents the use of machine learning solutions to solve relevant tasks. One possible solution is sharing models that are trained on such datasets. However, this is also associated with potential privacy risks due to data extraction attacks. In this work, we propose a solution based on sharing parts of the model's parameters, and using a proxy dataset for complimentary knowledge transfer. Our experiments show encouraging results, and reduced risk to potential training data identification attacks. We present a viable solution to sharing knowledge with data-disadvantaged parties, that do not have the resources to produce high-quality data, with reduced privacy risks to the sharing parties. We make our code publicly available.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Paul Youssef', 'Jrg Schltterer', 'Christin Seifert'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Privacy-Preserving Knowledge Transfer through Partial Parameter Sharing',\n",
       "   'tldr': 'Valuable datasets that contain sensitive information are not shared due to privacy and copyright concerns. This hinders progress in many areas and prevents the use of machine learning solutions to solve relevant tasks. One possible solution is sharing models that are trained on such datasets. Howeve',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_40': {'abstract': 'Post-stroke speech and language deficits (aphasia) significantly impact patients quality of life. Many with mild symptoms remain undiagnosed, and the majority do not receive the intensive doses of therapy recommended, due to healthcare costs and/or inadequate services. Automatic Speech Recognition (ASR) may help overcome these difficulties by improving diagnostic rates and providing feedback during tailored therapy. However, its performance is often unsatisfactory due to the high variability in speech errors and scarcity of training datasets. This study assessed the performance of Whisper, a recently released end-to-end model, in patients with post-stroke aphasia (PWA). We tuned its hyperparameters to achieve the lowest word error rate (WER) on aphasic speech. WER was significantly higher in PWA compared to age-matched controls (10.3% vs 38.5%, $p<0.001$). We demonstrated that worse WER was related to the more severe aphasia as measured by expressive (overt naming, and spontaneous speech production) and receptive (written and spoken comprehension) language assessments. Stroke lesion size did not affect the performance of Whisper. Linear mixed models accounting for demographic factors, therapy duration, and time since stroke, confirmed worse Whisper performance with left hemispheric frontal lesions.\\nWe discuss the implications of these findings for how future ASR can be improved in PWA.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Giulia Sanguedolce', 'Patrick Naylor', 'Fatemeh Geranmayeh'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Uncovering the Potential for a Weakly Supervised End-to-End Model in Recognising Speech from Patient with Post-Stroke Aphasia',\n",
       "   'tldr': 'Post-stroke speech and language deficits (aphasia) significantly impact patients quality of life. Many with mild symptoms remain undiagnosed, and the majority do not receive the intensive doses of therapy recommended, due to healthcare costs and/or inadequate services. Automatic Speech Recognition ',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_42': {'abstract': 'We explore temporal dependency graph (TDG) parsing in the clinical domain. We leverage existing annotations on the THYME dataset to semi-automatically construct a TDG corpus. Then we propose a new natural language inference (NLI) approach to TDG parsing, and evaluate it both on general domain TDGs from wikinews and the newly constructed clinical TDG corpus. We achieve competitive performance on general domain TDGs with a much simpler model than prior work. On the clinical TDGs, our method establishes the first result of TDG parsing on clinical data with 0.79/0.88 micro/macro F1. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiarui Yao',\n",
       "    'Steven Bethard',\n",
       "    'Kristin Wright-Bettner',\n",
       "    'Eli Goldner',\n",
       "    'David Harris',\n",
       "    'Guergana Savova'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Textual Entailment for Temporal Dependency Graph Parsing',\n",
       "   'tldr': 'We explore temporal dependency graph (TDG) parsing in the clinical domain. We leverage existing annotations on the THYME dataset to semi-automatically construct a TDG corpus. Then we propose a new natural language inference (NLI) approach to TDG parsing, and evaluate it both on general domain TDGs f',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_43': {'abstract': 'A medical provider\\'s summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing \"patient does not have a fever\" when a fever is present) can be detrimental to the outcome of care for the patient.\\n\\nThis paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Varun Nair', 'Elliot Schumacher', 'Anitha Kannan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_43',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models',\n",
       "   'tldr': \"A medical provider's summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant inform\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_45': {'abstract': \"Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is crucial to verify the identity of enrolled individuals and remove duplicates, but the size and quality of data influence ASV performance. However, there has been limited investigation into the factors that can affect ASV capabilities in clinical environments. In this paper, we bridge the gap by conducting analysis of how participant demographic characteristics, audio quality criteria, and severity level of Alzheimer's disease (AD) impact the performance of ASV utilizing a dataset of speech recordings from 659 participants with varying levels of AD, obtained through multiple speech tasks. Our results indicate that ASV performance: 1) is slightly better on male speakers than on female speakers; 2) degrades for individuals who are above 70 years old; 3) is comparatively better for non-native English speakers than for native English speakers; 4) is negatively affected by clinician interference, noisy background, and unclear participant speech; 5) tends to decrease with an increase in the severity level of AD. Our study finds that voice biometrics raise fairness concerns as certain subgroups exhibit different ASV performances owing to their inherent voice characteristics. Moreover, the performance of ASV is influenced by the quality of speech recordings, which underscores the importance of improving the data collection settings in clinical trials.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Malikeh Ehghaghi',\n",
       "    'Marija Stanojevic',\n",
       "    'Ali Akram',\n",
       "    'Jekaterina Novikova'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Factors Affecting the Performance of Automated Speaker Verification in Alzheimers Disease Clinical Trials',\n",
       "   'tldr': \"Detecting duplicate patient participation in clinical trials is a major challenge because repeated patients can undermine the credibility and accuracy of the trial's findings and result in significant health and financial risks. Developing accurate automated speaker verification (ASV) models is cruc\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_48': {'abstract': \"This paper describes Team Cadence's winning submission to Task C of the MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete summarized note, used to complete Task A and Task B, ranking highly on the leaderboard amongst stable and reproducible code submissions. The shared tasks invited participants to summarize, classify and generate patient-doctor conversations. Considering the small volume of training data available, we took a data-augmentation-first approach to the three tasks by focusing on the dialogue generation task, i.e., Task C. It proved effective in improving our models' performance on Task A and Task B. We also found the BART architecture to be highly versatile, as it formed the base for all our submissions. Finally, based on the results shared by the organizers, we note that Team Cadence was the only team to submit stable and reproducible runs to all three tasks.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ashwyn Sharma', 'David Feldman', 'Aneesh Jain'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Team Cadence at MEDIQA-Chat 2023: Generating, augmenting and summarizing clinical dialogue with large language models',\n",
       "   'tldr': \"This paper describes Team Cadence's winning submission to Task C of the MEDIQA-Chat 2023 shared tasks. We also present the set of methods, including a novel N-pass strategy to summarize a mix of clinical dialogue and an incomplete summarized note, used to complete Task A and Task B, ranking highly o\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_49': {'abstract': 'Annotated clinical text corpora are essential for machine learning studies that model and predict care processes and disease progression. However, few studies describe the necessary experimental design of the annotation guideline and annotation phases. This makes replication, reuse, and adoption challenging.\\n\\nUsing clinical questions about sepsis, we designed a semantic annotation guideline to capture sepsis signs from clinical text. The clinical questions aid guideline design, application, and evaluation. Our method incrementally evaluates each change in the guideline by testing the resulting annotated corpus using clinical questions. Additionally, our method uses inter-annotator agreement to judge the annotator compliance and quality of the guideline. We show that the method, combined with controlled design increments, is simple and allows the development and measurable improvement of a purpose-built semantic annotation guideline. We believe that our approach is useful for incremental design of semantic annotation guidelines in general.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Melissa Yan', 'Lise Gustad', 'Lise Hvik', 'ystein Nytr'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_49',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Method for Designing Semantic Annotation of Sepsis Signs in Clinical Text',\n",
       "   'tldr': 'Annotated clinical text corpora are essential for machine learning studies that model and predict care processes and disease progression. However, few studies describe the necessary experimental design of the annotation guideline and annotation phases. This makes replication, reuse, and adoption cha',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_5': {'abstract': \"Alzheimer's Disease (AD) is a neurodegenerative disorder that affects cognitive abilities and memory, especially in older adults. One of the challenges of AD is that it can be difficult to diagnose in its early stages. However, recent research has shown that changes in language, including speech decline and difficulty in processing information, can be important indicators of AD and may help with early detection. Hence, the speech narratives of the patients can be useful in diagnosing the early stages of Alzheimer's disease. While the previous works have presented the potential of using speech narratives to diagnose AD in high-resource languages, this work explores the possibility of using a low-resourced language, i.e., Hindi language, to diagnose AD. In this paper, we present a dataset specifically for analyzing AD in the Hindi language, along with experimental results using various state-of-the-art algorithms to assess the diagnostic potential of speech narratives in Hindi. Our analysis suggests that speech narratives in the Hindi language have the potential to aid in the diagnosis of AD. Our dataset and code are made publicly available at https://github.com/rkritesh210/DementiaBankHindi.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kritesh Rauniyar',\n",
       "    'Shuvam Shiwakoti',\n",
       "    'Sweta Poudel',\n",
       "    'Surendrabikram Thapa',\n",
       "    'Usman Naseem',\n",
       "    'Mehwish Nasim'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Breaking Barriers: Exploring the Diagnostic Potential of Speech Narratives in Hindi for Alzheimer's Disease\",\n",
       "   'tldr': \"Alzheimer's Disease (AD) is a neurodegenerative disorder that affects cognitive abilities and memory, especially in older adults. One of the challenges of AD is that it can be difficult to diagnose in its early stages. However, recent research has shown that changes in language, including speech dec\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_50': {'abstract': 'Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. \\nHowever, the potential of discriminative language models in biomedical tasks remains underexplored.\\nTo bridge this gap, we develop BioDLM, a method tailored for biomedical domain adaptation of discriminative language models that incorporates prompt-based continual pretraining and prompt tuning for downstream tasks. \\nBioDLM aims to maximize the potential of discriminative language models in low-resource scenarios by reformulating these tasks as span-level corruption detection, thereby enhancing performance on domain-specific tasks and improving the efficiency of continual pertaining.\\nIn this way, BioDLM provides a data-efficient domain adaptation method for discriminative language models, effectively enhancing performance on discriminative tasks within the biomedical domain.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Keming Lu',\n",
       "    'Peter Potash',\n",
       "    'Xihui Lin',\n",
       "    'Yuwen Sun',\n",
       "    'Zihan Qian',\n",
       "    'Zheng Yuan',\n",
       "    'Tristan Naumann',\n",
       "    'Tianxi Cai',\n",
       "    'Junwei Lu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Prompt Discriminative Language Models for Domain Adaptation',\n",
       "   'tldr': 'Prompt tuning offers an efficient approach to domain adaptation for pretrained language models, which predominantly focus on masked language modeling or generative objectives. \\nHowever, the potential of discriminative language models in biomedical tasks remains underexplored.\\nTo bridge this gap, we ',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_51': {'abstract': \"Information extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain knowledge transferring. The system builds on a pre-trained German language model and a token-level binary classifier, employing semantic types sourced from the Unified Medical Language System (UMLS) as entity labels to identify corresponding entity spans within the input text. To enhance the system's performance and robustness, we pre-train it using a medical literature corpus that incorporates UMLS semantic term annotations. We evaluate the system's effectiveness on two German annotated datasets obtained from different clinics in zero- and few-shot settings. The results show that our approach outperforms task-specific Condition Random Fields (CRF) classifiers in terms of accuracy. Our work contributes to developing robust and transparent German medical NER models that can support the extraction of information from various clinical texts.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Siting Liang', 'Mareike Hartmann', 'Daniel Sonntag'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_51',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Cross-domain German Medical Named Entity Recognition using a Pre-Trained Language Model and Unified Medical Semantic Types',\n",
       "   'tldr': 'Information extraction from clinical text has the potential to facilitate clinical research and personalized clinical care, but annotating large amounts of data for each set of target tasks is prohibitive. We present a German medical Named Entity Recognition (NER) system capable of cross-domain know',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_52': {'abstract': 'Graph-based techniques have gained traction for representing and analyzing data in various natural language processing (NLP) tasks. Knowledge graph-based language representation models have shown promising results in leveraging domain-specific knowledge for NLP tasks, particularly in the biomedical NLP field. However, such models have limitations, including knowledge noise and neglect of contextual relationships, leading to potential semantic errors and reduced accuracy. To address these issues, this paper proposes two novel methods. The first method combines knowledge graph-based language model with nearest-neighbor models to incorporate semantic and category information from neighboring instances. The second method involves integrating knowledge graph-based language model with graph neural networks (GNNs) to leverage feature information from neighboring nodes in the graph. Experiments on relation extraction (RE) and classification tasks in English and Chinese language datasets demonstrate significant performance improvements with both methods, highlighting their potential for enhancing the performance of language models and improving NLP applications in the biomedical domain.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Usman Naseem',\n",
       "    'Surendrabikram Thapa',\n",
       "    'Qi Zhang',\n",
       "    'Liang Hu',\n",
       "    'Anum Masood',\n",
       "    'Mehwish Nasim'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_52',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications',\n",
       "   'tldr': 'Graph-based techniques have gained traction for representing and analyzing data in various natural language processing (NLP) tasks. Knowledge graph-based language representation models have shown promising results in leveraging domain-specific knowledge for NLP tasks, particularly in the biomedical ',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_54': {'abstract': 'Artificial intelligence based diagnosis systems have emerged as powerful tools to reform traditional medical care. Each clinician now wants to have his own intelligent diagnostic partner to expand the range of services he can provide.  When reading a clinical note, experts make inferences with relevant knowledge. However, medical knowledge appears to be heterogeneous, including structured and unstructured knowledge. Existing approaches are incapable of uniforming them well. Besides, the descriptions of clinical findings in clinical notes, which are reasoned to diagnosis, vary a lot for different diseases or patients. To address these problems, we propose a Medical Knowledge-enhanced Prompt Learning (MedKPL) model for diagnosis classification. First, to overcome the heterogeneity of knowledge, given the knowledge relevant to diagnosis, MedKPL extracts and normalizes the relevant knowledge into a prompt sequence. Then, MedKPL integrates the knowledge prompt with the clinical note into a designed prompt for representation. Therefore, MedKPL can integrate medical knowledge into the models to enhance diagnosis and effectively transfer learned diagnosis capacity to unseen diseases using alternating relevant disease knowledge. The experimental results on two medical datasets show that our method can obtain better medical text classification results and can perform better in transfer and few-shot settings among datasets of different diseases. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuxing Lu', 'Xukai Zhao', 'Jinzhuo Wang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_54',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Medical knowledge-enhanced prompt learning for diagnosis classification from clinical text',\n",
       "   'tldr': 'Artificial intelligence based diagnosis systems have emerged as powerful tools to reform traditional medical care. Each clinician now wants to have his own intelligent diagnostic partner to expand the range of services he can provide.  When reading a clinical note, experts make inferences with relev',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_55': {'abstract': 'Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language models to be fine-tuned on a specific task in one language and to provide high accuracy for the same task in another language. However, other methods leveraging translation models can be used to perform NER without annotated data in the target language, by either translating the training set or test set. This paper compares cross-lingual transfer with these two alternative methods, to perform clinical NER in French and in German without any training data in those languages. To this end, we release MedNERF a medical NER test set extracted from French drug prescriptions and annotated with the same guidelines as an English dataset. Through extensive experiments on this dataset and on a German medical dataset (Frei and Kramer, 2021), we show that translation-based methods can achieve similar performance to CLT but require more care in their design. And while they can take advantage of monolingual clinical language models, those do not guarantee better results than large general-purpose multilingual models, whether with cross-lingual transfer or translation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Flix Gaschi',\n",
       "    'Xavier Fontaine',\n",
       "    'Parisa Rastin',\n",
       "    'Yannick Toussaint'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multilingual Clinical NER: Translation or Cross-lingual Transfer?',\n",
       "   'tldr': 'Natural language tasks like Named Entity Recognition (NER) in the clinical domain on non-English texts can be very time-consuming and expensive due to the lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent this issue thanks to the ability of multilingual large language model',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_56': {'abstract': 'Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS.\\nThis allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks. All pre-trained models, data processing pipelines and evaluation scripts will be made publicly available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aidan Mannion', 'Didier Schwab', 'Lorraine Goeuriot'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition',\n",
       "   'tldr': 'Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the bio',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_57': {'abstract': 'This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-context learning (ICL) with a large language model (LLM). Both achieve high performance as measured by automatic metrics (e.g. ROUGE, BERTScore) and ranked second and first, respectively, of all submissions to the shared task. Expert human scrutiny indicates that notes generated via the ICL-based approach with GPT-4 are preferred about as often as human-written notes, making it a promising path toward automated note generation from doctor-patient conversations.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['John Giorgi',\n",
       "    'Augustin Toma',\n",
       "    'Ronald Xie',\n",
       "    'Sondra Chen',\n",
       "    'Kevin An',\n",
       "    'Grace Zheng',\n",
       "    'BO Wang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_57',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'WangLab at MEDIQA-Chat 2023: Clinical Note Generation from Doctor-Patient Conversations using Large Language Models',\n",
       "   'tldr': 'This paper describes our submission to the MEDIQA-Chat 2023 shared task for automatic clinical note generation from doctor-patient conversations. We report results for two approaches: the first fine-tunes a pre-trained language model (PLM) on the shared task data, and the second uses few-shot in-con',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_58': {'abstract': \"The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and prevalence of diseases in a determined context. However, the manual coding process is subject to errors as it requires medical personnel to be competent in coding rules and terminology. In addition, this process consumes a lot of time and energy, which could be allocated to more clinically relevant tasks. These difficulties can be addressed by developing computational systems that automatically assign codes to diseases. In this way, we propose a two-step system for automatically coding diseases in referrals from the Chilean public healthcare system. Specifically, our model uses a state-of-the-art NER model for recognizing disease mentions and a search engine system based on Elasticsearch for assigning the most relevant codes associated with these disease mentions. The system's performance was evaluated on referrals manually coded by clinical experts. Our system obtained a MAP score of 0.63 for the subcategory level and 0.83 for the category level, close to the best-performing models in the literature. This system could be a support tool for health professionals, optimizing the coding and management process. Finally, to guarantee reproducibility, we publicly release the code of our models and experiments.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Fabin Villena',\n",
       "    'Matas Rojas',\n",
       "    'Felipe Arias',\n",
       "    'Jorge Pacheco',\n",
       "    'Paulina Vera',\n",
       "    'Jocelyn Dunstan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_58',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Automatic Coding at Scale: Design and Deployment of a Nationwide System for Normalizing Referrals in the Chilean Public Healthcare System',\n",
       "   'tldr': 'The disease coding task involves assigning a unique identifier from a controlled vocabulary to each disease mentioned in a clinical document. This task is relevant since it allows information extraction from unstructured data to perform, for example, epidemiological studies about the incidence and p',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_6': {'abstract': \"Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks.\\nThis work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen languages via transfer learning.\\nWe carry out an experimental investigation using Meta-AI's MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained on 7 language pairs and 14 translation directions including English to Czech, German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite direction.\\nWe fine-tune these MMPLMs towards English-\\\\textit{Spanish} language pair which \\\\textit{did not exist at all} in their original pre-trained corpora both implicitly and explicitly.\\nWe prepare carefully aligned \\\\textit{clinical} domain data for this fine-tuning, which is different from their original mixed domain knowledge.\\nOur experimental result shows that the fine-tuning is very successful using just 250k well-aligned in-domain EN-ES segments for three sub-task translation testings: clinical cases, clinical terms, and ontology concepts. It achieves very close evaluation scores to another MMPLM NLLB from Meta-AI, which included Spanish as a high-resource setting in the pre-training.\\nTo the best of our knowledge, this is the first work on using MMPLMs towards \\\\textit{clinical domain transfer-learning NMT} successfully for totally unseen languages during pre-training.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Lifeng Han',\n",
       "    'Gleb ',\n",
       "    'Irina Sorokina',\n",
       "    'Serge Gladkoff',\n",
       "    'Goran Nenadic'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Investigating Massive Multilingual Pre-Trained Machine Translation Models for Clinical Domain via Transfer Learning',\n",
       "   'tldr': 'Massively multilingual pre-trained language models (MMPLMs) are developed in recent years demonstrating superpowers and the pre-knowledge they acquire for downstream tasks.\\nThis work investigates whether MMPLMs can be applied to clinical domain machine translation (MT) towards entirely unseen langua',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_62': {'abstract': 'This paper explores methods for extracting information from radiology reports that generalize across exam modalities to reduce requirements for annotated data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit better generalization across exam modalities compared to approaches that employ BERT-based task-specific classification layers.  We then develop methods that reduce the inference cost of the model, making large-scale corpus processing more feasible for clinical applications. Specifically, we introduce a generative technique that decomposes complex tasks into smaller subtask blocks, which improves a single-pass model when combined with multitask training. In addition, we leverage target-domain contexts during inference to enhance domain adaptation, enabling use of smaller models. Analyses offer insights into the benefits of different cost reduction strategies. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sitong Zhou', 'Meliha Yetisgen', 'Mari Ostendorf'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_62',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Building blocks for complex tasks:  Robust generative event extraction for radiology reports under domain shifts',\n",
       "   'tldr': 'This paper explores methods for extracting information from radiology reports that generalize across exam modalities to reduce requirements for annotated data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit better generalization across exam modalities compared to appr',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_65': {'abstract': \"Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute to a patient's experience. Further, some injustices may only be evident when examining the nuances that arise through the lens of intersectionality. Ignoring such injustices can result in poor quality of care or life-endangering events. Thus, considering intersectionality could result in more accurate classifications and just decisions. To illustrate this, we use real-world medical data to determine whether medical records exhibit words that could lead to testimonial injustice, employ fairness metrics (e.g. demographic parity, differential intersectional fairness, and subgroup fairness) to assess the severity to which subgroups are experiencing testimonial injustice, and analyze how the intersectionality of demographic features (e.g. gender and race) make a difference in uncovering testimonial injustice. From our analysis we found that with intersectionality we can better see disparities in how subgroups are treated and there are differences in how someone is treated based on the intersection of their demographic attributes. This has not been previously studied in clinical records, nor has it been proven through empirical study. \",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kenya Andrews', 'Bhuvni Shah', 'Lu Cheng'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_65',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Intersectionality and Testimonial Injustice in Medical Records',\n",
       "   'tldr': 'Detecting testimonial injustice is an essential element of addressing inequities and promoting inclusive healthcare practices, many of which are life-critical. However, using a single demographic factor to detect testimonial injustice does not fully encompass the nuanced identities that contribute t',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_66': {'abstract': 'Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We first introduce unsupervised KRISS-Search and show that our method outperforms existing methods in identifying spans that are semantically similar to a given span of interest, with >50% AUPRC improvement relative to PubMedBERT. We then introduce supervised KRISS-Search, which leverages human interaction to improve the notion of similarity used by unsupervised KRISS-Search. Through simulated human feedback, we demonstrate an enhanced F1 score of 0.68 in classifying spans as semantically similar or different in the low-label setting, outperforming PubMedBERT by 2 F1 points. Finally, supervised KRISS-Search demonstrates competitive or superior performance compared to PubMedBERT in few-shot biomedical named entity recognition (NER) across five benchmark datasets, with an average improvement of 5.6 F1 points. We envision KRISS-Search increasing the efficiency of programmatic data labeling and also providing broader utility as an interactive biomedical search engine.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Louis Blankemeier',\n",
       "    'Theodore Zhao',\n",
       "    'Robert Tinn',\n",
       "    'Sid Kiblawi',\n",
       "    'Yu Gu',\n",
       "    'Akshay Chaudhari',\n",
       "    'Hoifung Poon',\n",
       "    'Sheng Zhang',\n",
       "    'Mu Wei',\n",
       "    'J. Preston'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_66',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Interactive Span Recommendation for Biomedical Text',\n",
       "   'tldr': 'Motivated by the scarcity of high-quality labeled biomedical text, as well as the success of data programming, we introduce KRISS-Search. By leveraging the Unified Medical Language Systems (UMLS) ontology, KRISS-Search addresses an interactive few-shot span recommendation task that we propose. We fi',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_68': {'abstract': 'Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-identified social history sections annotated for SDOH, including substance use, employment, and living status information. We explore the automatic extraction of SDOH information with SHAC in both standoff and inline annotation formats using GPT-4 in a one-shot prompting setting. We compare GPT-4 extraction performance with a high-performing supervised approach and perform thorough error analyses. Our prompt-based GPT-4 method achieved an overall 0.652 F1 on the SHAC test set, similar to the 7th best-performing system among all teams in the n2c2 challenge with SHAC. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Giridhar Kaushik Ramachandran',\n",
       "    'Yujuan Fu',\n",
       "    'Bin Han',\n",
       "    'Kevin Lybarger',\n",
       "    'Nic Dobbins',\n",
       "    'Ozlem Uzuner',\n",
       "    'Meliha Yetisgen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_68',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Prompt-based Extraction of Social Determinants of Health Using Few-shot Learning',\n",
       "   'tldr': 'Social determinants of health (SDOH) documented in the electronic health record through unstructured text are increasingly being studied to understand how SDOH impacts patient health outcomes. In this work, we utilize the Social History Annotation Corpus (SHAC), a multi-institutional corpus of de-id',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_69': {'abstract': 'In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note from doctor-patient consultation dialogues. This task has several challenging issues, such as lack of training data, handling long dialogue inputs, and generating semi-structured clinical note which have section heads. To address these issues, we conducted various experiments and analyzed their results. We utilized the DialogLED model pre-trained on long dialogue data to handle long inputs, and we pre-trained on other dialogue datasets to address the lack of training data. We also attempted methods such as using prompts and contrastive learning for handling sections. This paper provides insights into clinical note generation through analyzing experimental methods and results, and it suggests future research directions.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yongbin Jeong',\n",
       "    'Ju-Hyuck Han',\n",
       "    'Kyung Min Chae',\n",
       "    'Yousang Cho',\n",
       "    'Hyunbin Seo',\n",
       "    'KyungTae Lim',\n",
       "    'Key-Sun Choi',\n",
       "    'Younggyun Hahm'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_69',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Teddysum at MEDIQA-Chat 2023: an analysis of fine-tuning strategy for long dialog summarization',\n",
       "   'tldr': 'In this paper, we introduce the design and various attempts for TaskB of MEDIQA-Chat 2023. The goal of TaskB in MEDIQA-Chat 2023 is to generate full clinical note from doctor-patient consultation dialogues. This task has several challenging issues, such as lack of training data, handling long dialog',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_7': {'abstract': 'The Coronavirus pandemic has heightened the demand for technological solutions capable of gathering and monitoring data automatically, quickly, and securely. To achieve this need, the Planto Coronavirus chatbot has been made available to the population of Cear State in Brazil. This chatbot employs automated symptom detection technology through Natural Language Processing (NLP). The proposal of this work is a symptom tracker, which is a neural network that processes texts and captures symptoms in messages exchanged between citizens of the state and the Planto Coronavirus nurse/doctor, i.e., clinical conversations. The model has the ability to recognize new patterns and has identified a high incidence of altered psychological behaviors, including anguish, anxiety, and sadness, among users who tested positive or negative for Covid-19. As a result, the tool has emphasized the importance of expanding coverage through community mental health services in the state.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ticiana Coelho Da Silva',\n",
       "    'Jos Fernandes De Macdo',\n",
       "    'Rgis Magalhes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Tracking the Evolution of Covid-19 Symptoms through Clinical Conversations',\n",
       "   'tldr': 'The Coronavirus pandemic has heightened the demand for technological solutions capable of gathering and monitoring data automatically, quickly, and securely. To achieve this need, the Planto Coronavirus chatbot has been made available to the population of Cear State in Brazil. This chatbot employs',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_70': {'abstract': 'Multi-label clinical text classification, such as automatic ICD coding, has always been a challenging subject in Natural Language Processing, due to its long, domain-specific documents and long-tail distribution over a large label set. Existing methods adopt different model architectures to encode the clinical notes. Whereas without digging out the useful connections between labels, the model presents a huge gap in predicting performances between rare and frequent codes. In this work, we propose a novel method for further mining the helpful relations between different codes via a relation-enhanced code encoder to improve the rare code performance. Starting from the simple code descriptions, the model reaches comparable, even better performances than models with heavy external knowledge. Our proposed method is evaluated on MIMIC-III, a common dataset in the medical domain. It outperforms the previous state-of-art models on both overall metrics and rare code performances. Moreover, the interpretation results further prove the effectiveness of our methods. Our code is publicly available at https://github.com/jiaminchen-1031/Rare-ICD.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiamin Chen',\n",
       "    'Xuhong Li',\n",
       "    'Junting Xi',\n",
       "    'Lei Yu',\n",
       "    'Haoyi Xiong'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_70',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Rare Codes Count: Mining Inter-code Relations for Long-tail Clinical Text Classification',\n",
       "   'tldr': 'Multi-label clinical text classification, such as automatic ICD coding, has always been a challenging subject in Natural Language Processing, due to its long, domain-specific documents and long-tail distribution over a large label set. Existing methods adopt different model architectures to encode t',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_71': {'abstract': 'This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for \\\\textit{MEDIQA-Chat 2023 Task A: Short Dialogue2Note Summarization}. Manual creation of these clinical notes requires extensive human efforts, thus making it a time-consuming and expensive process. To address this, we propose an ensemble-based method over GPT-3, BART, BERT variants, and Rule-based systems to automatically generate clinical notes from these conversations. The proposed system achieves a score of 0.730 and 0.544 for both the sub-tasks on the test set (ranking 8th on the leaderboard for both tasks) and shows better performance compared to a baseline system using BART variants. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Prakhar Mishra', 'Ravi Theja Desetty'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_71',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NewAgeHealthWarriors at MEDIQA-Chat 2023 Task A: Summarizing Short Medical Conversation with Transformers',\n",
       "   'tldr': 'This paper presents the MEDIQA-Chat 2023 shared task organized at the ACL-Clinical NLP workshop. The shared task is motivated by the need to develop methods to automatically generate clinical notes from doctor-patient conversations. In this paper, we present our submission for \\\\textit{MEDIQA-Chat 20',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_72': {'abstract': 'Aphasia and dysarthria are both common symptoms of stroke, affecting around 30% and 50% of acute ischemic stroke patients. In this paper, we propose a storyline-centric approach to detect aphasia and dysarthria in acute stroke patients using transcribed picture descriptions alone. Our pipeline enriches the training set with healthy data to address the lack of acute stroke patient data and utilizes knowledge distillation to significantly improve upon a document classification baseline, achieving an AUC of 0.814 (aphasia) and 0.764 (dysarthria) on a patient-only validation set.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Peiqi Sui',\n",
       "    'Kelvin Wong',\n",
       "    'Xiaohui Yu',\n",
       "    'John Volpi',\n",
       "    'Stephen Wong'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_72',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Storyline-Centric Detection of Aphasia and Dysarthria in Stroke Patient Transcripts',\n",
       "   'tldr': 'Aphasia and dysarthria are both common symptoms of stroke, affecting around 30% and 50% of acute ischemic stroke patients. In this paper, we propose a storyline-centric approach to detect aphasia and dysarthria in acute stroke patients using transcribed picture descriptions alone. Our pipeline enric',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_74': {'abstract': \"The field of clinical natural language processing (NLP) can extract useful information from clinical text. Since 2017, the NLP field has shifted towards using pre-trained language models (PLMs), improving performance in several tasks. Most of the research in this field has focused on English text, but there are some available PLMs in Spanish. In this work, we use clinical PLMs to analyze text from admission and medical reports in Spanish for an insurance and health provider to give a probability of no coverage in a labor insurance process. Our results show that fine-tuning a PLM pre-trained with the provider's data leads to better results, but this process is time-consuming and computationally expensive. At least for this task, fine-tuning publicly available clinical PLM leads to comparable results to a custom PLM, but in less time and with fewer resources. Analyzing large volumes of insurance requests is burdensome for employers, and models can ease this task by pre-classifying reports that are likely not to have coverage. Our approach of entirely using clinical-related text improves the current models while reinforcing the idea of clinical support systems that simplify human labor but do not replace it. To our knowledge, the clinical corpus collected for this study is the largest one reported for the Spanish language.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Claudio Aracena',\n",
       "    'Nicols Rodrguez',\n",
       "    'Victor Rocco',\n",
       "    'Jocelyn Dunstan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_74',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pre-trained language models in Spanish for health insurance coverage',\n",
       "   'tldr': 'The field of clinical natural language processing (NLP) can extract useful information from clinical text. Since 2017, the NLP field has shifted towards using pre-trained language models (PLMs), improving performance in several tasks. Most of the research in this field has focused on English text, b',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_75': {'abstract': 'In response to the global challenge of mental health problems, we proposes a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis of mental disorders. Due to the lack of effective therapy coverage for mental disorders, there is a need for an AI solution that can assist therapists with the diagnosis. However, current Neural Network models lack explainability and may not be trusted by therapists. The LNN is a Recurrent Neural Network architecture that combines the learning capabilities of neural networks with the reasoning capabilities of classical logic-based AI. The proposed system uses input predicates from clinical interviews to output a mental disorder class, and different predicate pruning techniques are used to achieve scalability and higher scores. In addition, we provide an insight extraction method to aid therapists with their diagnosis. The proposed system addresses the lack of explainability of current Neural Network models and provides a more trustworthy solution for mental disorder diagnosis.\\n',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yeldar Toleubay',\n",
       "    'Don Joven Agravante',\n",
       "    'Daiki Kimura',\n",
       "    'Baihan Lin',\n",
       "    'Djallel Bouneffouf',\n",
       "    'Michiaki Tatsubori'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_75',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Utterance Classification with Logical Neural Network: Explainable AI for Mental Disorder Diagnosis',\n",
       "   'tldr': 'In response to the global challenge of mental health problems, we proposes a Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis of mental disorders. Due to the lack of effective therapy coverage for mental disorders, there is a need for an AI solution that can assist thera',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_76': {'abstract': 'Medical Report Generation (MRG) is a sub-task of  Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support information for decision-making. Given the specificity of the medical domain, the evaluation of automatically generated medical reports is of paramount importance to the validity of these systems. Therefore, in this paper, we focus on the evaluation of automatically generated medical reports from the perspective of automatic and human evaluation. We present evaluation methods for general NLG evaluation and how they have been applied to domain-specific medical tasks. The study shows that MRG evaluation methods are very diverse, and that further work is needed to build shared evaluation methods. The state of the art also emphasizes that such an evaluation must be task specific and include human assessments, requesting the participation of experts in the field.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yongxin Zhou', 'Fabien Ringeval', 'Franois Portet'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_76',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Survey of Evaluation Methods of Generated Medical Textual Reports',\n",
       "   'tldr': 'Medical Report Generation (MRG) is a sub-task of  Natural Language Generation (NLG) and aims to present information from various sources in textual form and synthesize salient information, with the goal of reducing the time spent by domain experts in writing medical reports and providing support inf',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_78': {'abstract': 'This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate that our approaches yield reasonable performance as evaluated by automatic metrics such as ROUGE, medical concept recall, BLEU, and Self-BLEU.  Furthermore, we conducted a comparative analysis between our proposed method and ChatGPT and GPT-4. This analysis also investigates the potential of utilizing cooperation LLMs to generate high-quality datasets.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Junda Wang',\n",
       "    'Zonghai Yao',\n",
       "    'Avijit Mitra',\n",
       "    'Samuel Osebe',\n",
       "    'Zhichao Yang',\n",
       "    'Hong yu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_78',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UMASS_BioNLP at MEDIQA-Chat 2023:   Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?',\n",
       "   'tldr': 'This paper presents UMASS_BioNLP team participation in the MEDIQA-Chat 2023 shared task for Task-A and Task-C. We focus especially on Task-C and propose a novel LLMs cooperation system named a doctor-patient loop to generate high-quality conversation data sets. The experiment results demonstrate tha',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_79': {'abstract': \"In recent years, we have seen many Transformer based models being created to address Dialog Summarization problem. While there has been a lot of work on understanding how these models stack against each other in summarizing regular conversations such as the ones found in DialogSum dataset, there haven't been many analysis of these models on Clinical Dialog Summarization. In this article, we describe our solution to MEDIQA-Chat 2023 Shared Tasks as part of ACL-ClinicalNLP 2023 workshop which benchmarks some of the popular Transformer Architectures such as BioBart, Flan-T5, DialogLED, and OpenAI GPT3 on the problem of Clinical Dialog Summarization. We analyse their performance on two tasks - summarizing short conversations and long conversations. In addition to this, we also benchmark two popular summarization ensemble methods and report their performance.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kunal Suri', 'Saumajit Saha', 'Atul Singh'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_79',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HealthMavericks@MEDIQA-Chat 2023: Benchmarking different Transformer based models for Clinical Dialogue Summarization',\n",
       "   'tldr': 'In recent years, we have seen many Transformer based models being created to address Dialog Summarization problem. While there has been a lot of work on understanding how these models stack against each other in summarizing regular conversations such as the ones found in DialogSum dataset, there hav',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_8': {'abstract': 'In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and align with human annotator preferences. Our work focuses on two tasks: Conclusion Generation and Review Generation. We train a CONFIT summarization model that outperforms GPT-3 and previous state-of-the-art models on the same datasets and collects expert and crowd-worker annotations to evaluate the quality and factual consistency of the generated summaries. These annotations enable us to measure the correlation of various automatic metrics, including modern factual evaluation metrics like QAFactEval, with human-assessed factual consistency. By employing top-correlated metrics as objectives for a reinforcement learning model, we demonstrate improved factuality in generated summaries that are preferred by human annotators. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiangru Tang', 'Arman Cohan', 'Mark Gerstein'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Aligning Factual Consistency for Clinical Studies Summarization through Reinforcement Learning',\n",
       "   'tldr': 'In the rapidly evolving landscape of medical research, accurate and concise summarization of clinical studies is crucial to support evidence-based practice. This paper presents a novel approach to clinical studies summarization, leveraging reinforcement learning to enhance factual consistency and al',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_80': {'abstract': 'Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminology\\nin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 summaries are more abstractive and shorter. We make our code publicly available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yash Mathur',\n",
       "    'Sanketh Rangreji',\n",
       "    'Raghav Kapoor',\n",
       "    'Medha Palavalli',\n",
       "    'Amanda Bertsch',\n",
       "    'Matthew Gormley'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_80',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization',\n",
       "   'tldr': 'Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminology\\nin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks ',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_81': {'abstract': \"Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors' workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summarization of doctor-patient conversations and on the generation of synthetic dialogues from clinical notes for data augmentation. Seventeen teams participated in the challenge and experimented with a broad range of approaches and models. In this paper, we describe the three MEDIQA-Chat 2023 tasks, the datasets, and the participants' results and methods. We hope that these shared tasks will lead to additional research efforts and insights on the automatic generation and evaluation of clinical notes. \",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Asma Ben Abacha',\n",
       "    'Wen-wai Yim',\n",
       "    'Griffin Adams',\n",
       "    'Neal Snider',\n",
       "    'Meliha Yetisgen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_81',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Overview of the MEDIQA-Chat 2023 Shared Tasks on the Summarization & Generation of Doctor-Patient Conversations',\n",
       "   'tldr': \"Automatic generation of clinical notes from doctor-patient conversations can play a key role in reducing daily doctors' workload and improving their interactions with the patients. MEDIQA-Chat 2023 aims to advance and promote research on effective solutions through shared tasks on the automatic summ\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_83': {'abstract': 'We propose a transfer learning method that adapts a high-resource English clinical NER model to low-resource languages and domains using only small amounts of in-domain annotated data. Our approach involves translating in-domain datasets to English, fine-tuning the English model on the translated data, and then transferring it to the target language/domain. Experiments on Spanish, French, and conversational clinical text datasets show accuracy gains over models trained on target data alone. Our method achieves state-of-the-art performance and can enable clinical NLP in more languages and modalities with limited resources.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nevasini Sasikumar', 'Krishna Sri Ipsit Mantri'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_83',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Transfer Learning for Low-Resource Clinical Named Entity Recognition',\n",
       "   'tldr': 'We propose a transfer learning method that adapts a high-resource English clinical NER model to low-resource languages and domains using only small amounts of in-domain annotated data. Our approach involves translating in-domain datasets to English, fine-tuning the English model on the translated da',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_84': {'abstract': 'Clinical conversation summarization has become an important application of Natural language Processing. In this work, we intend to analyze summarization model ensembling approaches, that can be utilized to improve the overall accuracy of the generated medical report called chart note. The work starts with a single summarization model creating the baseline. Then leads to an ensemble of summarization models trained on a separate section of the chart note. This leads to the final approach of passing the generated results to another summarization model in a multi-layer/stage fashion for better coherency of the generated text. Our results indicate that although an ensemble of models specialized in each section produces better results, the multi-layer/stage approach does not improve accuracy. The code for the above paper is available at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dhananjay Srivastava'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_84',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for multi layer summarization of clinical conversations?',\n",
       "   'tldr': 'Clinical conversation summarization has become an important application of Natural language Processing. In this work, we intend to analyze summarization model ensembling approaches, that can be utilized to improve the overall accuracy of the generated medical report called chart note. The work start',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_85': {'abstract': 'Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore and\\nBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Amal Alqahtani', 'Rana Salama', 'Mona Diab', 'Abdou Youssef'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_85',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues',\n",
       "   'tldr': 'Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_86': {'abstract': \"This paper presents our system for the MEDIQA-Chat 2023 shared task on medical conversation summarization. Our approach involves finetuning a LongT5 model on multiple tasks simultaneously, which we demonstrate improves the model's overall performance while reducing the number of factual errors and hallucinations in the generated summary. Furthermore, we investigated the effect of augmenting the data with in-text annotations from a clinical named entity recognition model, finding that this approach decreased summarization quality. Lastly, we explore using different text generation strategies for medical note generation based on the length of the note. Our findings suggest that the application of our proposed approach can be beneficial for improving the accuracy and effectiveness of medical conversation summarization.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kirill Milintsevich', 'Navneet Agarwal'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_86',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Calvados at MEDIQA-Chat 2023: Improving Clinical Note Generation with Multi-Task Instruction Finetuning',\n",
       "   'tldr': \"This paper presents our system for the MEDIQA-Chat 2023 shared task on medical conversation summarization. Our approach involves finetuning a LongT5 model on multiple tasks simultaneously, which we demonstrate improves the model's overall performance while reducing the number of factual errors and h\",\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_87': {'abstract': 'This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of one-shot prompts using GPT-3.5. We employ dialogues and summaries from the same category as prompts to generate summaries for novel dialogues. Our findings exceed the average benchmark score, offering a robust reference for assessing performance in this field.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Boya Zhang', 'Rahul Mishra', 'Douglas Teodoro'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_87',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DS4DH at MEDIQA-Chat 2023: Leveraging SVM and GPT-3 Prompt Engineering for Medical Dialogue Classification and Summarization',\n",
       "   'tldr': 'This paper presents the results of the Data Science for Digital Health (DS4DH) group in the MEDIQA-Chat Tasks at ACL-ClinicalNLP 2023. Our study combines the power of a classical machine learning method, Support Vector Machine, for classifying medical dialogues, along with the implementation of one-',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'ClinicalNLP_88': {'abstract': 'This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiangru Tang', 'Andrew Tran', 'Jeffrey Tan', 'Mark Gerstein'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['Clinical-NLP'],\n",
       "   'id': 'ClinicalNLP_88',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'N/A',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning',\n",
       "   'tldr': 'This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and ',\n",
       "   'track': 'The 5th Workshop on Clinical Natural Language Processing (ClinicalNLP)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'D103': {'abstract': 'CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.39',\n",
       "   'authors': ['Jacob Sharf', 'Mustafa Omer Gul', 'Yoav Artzi'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-1_-generation-(demo)-(poster)'],\n",
       "   'id': 'D103',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.39.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78266/poster_document/704e36b15870e1f18c39300ce3a0e6bc.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] CB2: Collaborative Natural Language Interaction Research Platform',\n",
       "   'tldr': 'CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at h...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78266,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78266-demo-cb2-collaborative-natural-language-interaction-research-platform',\n",
       "   'video_url': None},\n",
       "  'D104': {'abstract': \"Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.40',\n",
       "   'authors': ['Gabriele Sarti',\n",
       "    'Nils Feldhus',\n",
       "    'Ludwig Sickert',\n",
       "    'Oskar van der Wal'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'event_ids': ['demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D104',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.40.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78267/poster_document/52be043906859414da842326315e73fc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78267/poster/b9cf4960e9102f477b18e3d0c77805e3.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Inseq: An Interpretability Toolkit for Sequence Generation Models',\n",
       "   'tldr': 'Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of se...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'underline_id': 78267,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78267-demo-inseq-an-interpretability-toolkit-for-sequence-generation-models',\n",
       "   'video_url': None},\n",
       "  'D105': {'abstract': 'We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based on their semantic topics. We demonstrate the pipeline by modeling causal belief systems surrounding the Covid-19 vaccine from tweets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.41',\n",
       "   'authors': ['John Priniski', 'Ishaan Verma', 'Fred Morstatter'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['session-4_-nlp-applications-(demo)-(virtual-poster)'],\n",
       "   'id': 'D105',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.41.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78268/poster_document/2ccb9db045432751e89fcd4494acddbb.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78268/poster/1138f732b3240dbbae382074e07a6a4b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Pipeline for modeling causal beliefs from natural language',\n",
       "   'tldr': 'We present a causal language analysis pipeline that leverages a Large Language Model to identify causal claims made in natural language documents, and aggregates claims across a corpus to produce a causal claim network. The pipeline then applies a clustering algorithm that groups causal claims based...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78268,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/78268-bgglue-a-bulgarian-general-language-understanding-evaluation-benchmark',\n",
       "   'video_url': None},\n",
       "  'D106': {'abstract': 'Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie -- a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generation. In TabGenie, all inputs are represented as tables with associated metadata. The tables can be explored through a web interface, which also provides an interactive mode for debugging table-to-text generation, facilitates side-by-side comparison of generated system outputs, and allows easy exports for manual analysis. Furthermore, TabGenie is equipped with command line processing tools and Python bindings for unified dataset loading and processing. We release TabGenie as a PyPI package and provide its open-source code and a live demo at https://github.com/kasnerz/tabgenie.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.42',\n",
       "   'authors': ['Zdenk Kasner',\n",
       "    'Ekaterina Garanina',\n",
       "    'Ondrej Platek',\n",
       "    'Ondrej Dusek'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction (demo)',\n",
       "   'event_ids': ['demo-session-2_-information-extraction-(demo)-(poster)'],\n",
       "   'id': 'D106',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.42.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78269/poster_document/c9094bbb8d4651f5556435297da501c1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78269/poster/b772040efb170e9662ffdfc173be1a77.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] TabGenie: A Toolkit for Table-to-Text Generation',\n",
       "   'tldr': 'Heterogenity of data-to-text generation datasets limits the research on data-to-text generation systems. We present TabGenie -- a toolkit which enables researchers to explore, preprocess, and analyze a variety of data-to-text generation datasets through the unified framework of table-to-text generat...',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'underline_id': 78269,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78269-demo-tabgenie-a-toolkit-for-table-to-text-generation',\n",
       "   'video_url': None},\n",
       "  'D107': {'abstract': 'Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time.\\nThis paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. \\nHeuristics from different perspectives are designed to achieve the best trade-off between quality and latency.\\nFrom the modeling side, the decoder-only model exploited the previous turns of conversational history in a computation lightweight manner. \\nBesides, a novel phrase tokenizer is proposed to reduce latency without losing the composing quality further. \\nAdditionally, the caching mechanism is applied to the serving framework. \\nThe demo video of the system is available at https://youtu.be/U1KXkaqr60g.\\nWe open-sourced our phrase tokenizer in https://github.com/tensorflow/text.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.43',\n",
       "   'authors': ['Yun Zhu',\n",
       "    'Xiayu Chen',\n",
       "    'Lei Shu',\n",
       "    'Bowen Tan',\n",
       "    'Xinying Song',\n",
       "    'Lijuan Liu',\n",
       "    'Maria Wang',\n",
       "    'Jindong Chen',\n",
       "    'Ning Ruan'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'event_ids': ['demo-session-2_-dialogue-and-interactive-systems-(demo)-(poster)'],\n",
       "   'id': 'D107',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.43.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78270/poster_document/a3f31e60d23bdf5d1ff23925d986115d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78270/poster/d815b761065bb081660b810451b73461.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] An Efficient Conversational Smart Compose System',\n",
       "   'tldr': 'Online conversation is a ubiquitous way to share information and connect everyone but repetitive idiomatic text typing takes users a lot of time.\\nThis paper demonstrates a simple yet effective cloud based smart compose system to improve human-to-human conversation efficiency. \\nHeuristics from differ...',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'underline_id': 78270,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78270-demo-an-efficient-conversational-smart-compose-system',\n",
       "   'video_url': None},\n",
       "  'D110': {'abstract': \"We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. \\nThe dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. \\nAdditionally, users can receive feedback from a trained NLI model on how challenging the newly created example is and make refinements based on the feedback.\\nThrough our investigation, we discover several categories of spurious correlations that impact the reasoning of NLI models, which we group into three categories: Semantic Relevance, Logical Fallacies, and Bias. Based on our findings, we identify and describe various research opportunities, including diversifying training data and assessing NLI models' robustness by creating adversarial test suites.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.44',\n",
       "   'authors': ['Robin Chan', 'Afra Amini', 'Mennatallah El-Assady'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'event_ids': ['demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D110',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.44.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78271/poster_document/82330d1ca205ebea43747a2fdca9e2a6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78271/poster/73674f31e85300268ad315cebc5b3f55.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Which Spurious Correlations Impact Reasoning in NLI Models? A Visual Interactive Diagnosis through Data-Constrained Counterfactuals',\n",
       "   'tldr': 'We present a human-in-the-loop dashboard tailored to diagnosing potential spurious features that NLI models rely on for predictions. \\nThe dashboard enables users to generate diverse and challenging examples by drawing inspiration from GPT-3 suggestions. \\nAdditionally, users can receive feedback from...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'underline_id': 78271,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78271-crosssum-beyond-english-centric-cross-lingual-summarization-for-1-500-language-pairs',\n",
       "   'video_url': None},\n",
       "  'D113': {'abstract': 'We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an in-depth understanding of the problem description and a technical expertise to produce the modeling code. Thus, our proposed system TeX2Solver helps partially automate this conversion and help the users build optimization models more efficiently. In this paper, we describe its interface and the components of the hierarchical parsing system. A video demo walk-through is available online at http://bit.ly/3kuOm3x',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.45',\n",
       "   'authors': ['Rindra Ramamonjison',\n",
       "    'Timothy Yu',\n",
       "    'Linzi Xing',\n",
       "    'Mahdi Mostajabdaveh',\n",
       "    'Xiaorui Li',\n",
       "    'Xiaojin Fu',\n",
       "    'Xiongwei Han',\n",
       "    'Yuanzhe Chen',\n",
       "    'Ren Li',\n",
       "    'Kun Mao',\n",
       "    'Yong Zhang'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['demo-session-3_-nlp-applications-(demo)-(poster)'],\n",
       "   'id': 'D113',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.45.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78272/poster_document/5d69387853dedc05abe89b2cc8ffd7cf.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78272/poster/78f23f792c28dc91cef13e25227980cd.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] LaTeX2Solver: a Hierarchical Semantic Parsing of LaTeX Document into Code for an Assistive Optimization Modeling Application',\n",
       "   'tldr': 'We demonstrate an interactive system to help operations research (OR) practitioners convert the mathematical formulation of optimization problems from TeX document format into the solver modeling language. In practice, a manual translation is cumbersome and time-consuming. Moreover, it requires an i...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78272,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78272-it-s-not-sexually-suggestive-it-s-educative-or-separating-sex-education-from-suggestive-content-on-tiktok-videos',\n",
       "   'video_url': None},\n",
       "  'D117': {'abstract': 'Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural language prompts for language and vision-language models. Alfred provides a simple Python interface for the key steps of this emerging paradigm, with a high-throughput backend for large-scale data labeling. Users can quickly create, evaluate, and refine their prompt-based weak supervision sources; map the results to weak labels; and resolve their disagreements with a label model. Alfred enables a seamless local development experience backed by models served from self-managed computing clusters. It automatically optimizes the execution of prompts with optimized batching mechanisms.  We find that this optimization improves query throughput by 2.9x versus a naive approach.  We present two example use cases demonstrating Alfred on YouTube comment spam detection and pet breeds classification. Alfred is open source, available at https://github.com/BatsResearch/alfred.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.46',\n",
       "   'authors': ['Peilin Yu', 'Stephen Bach'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP (demo)',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(demo)-(virtual-poster)'],\n",
       "   'id': 'D117',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.46.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78273/poster_document/0e190baa11af3d066ce38c033a3fb769.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Alfred: A System for Prompted Weak Supervision',\n",
       "   'tldr': 'Alfred is the first system for programmatic weak supervision (PWS) that creates training data for machine learning by prompting. In contrast to typical PWS systems where weak supervision sources are programs coded by experts, Alfred enables users to encode their subject matter expertise via natural ...',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'underline_id': 78273,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/78273-demo-alfred-a-system-for-prompted-weak-supervision',\n",
       "   'video_url': None},\n",
       "  'D118': {'abstract': 'In recent years, In-context Learning (ICL) has gained increasing attention\\nand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates.\\nHowever, the implementation of ICL is sophisticated due to the diverse retrieval and inference methods involved, as well as the varying pre-processing requirements for different models, datasets, and tasks. A unified and flexible framework for ICL is urgently needed to ease the implementation of the aforementioned components.\\nTo facilitate ICL research, we introduce OpenICL, an open-source toolkit for ICL and LLM evaluation. OpenICL \\nis research-friendly with a highly flexible architecture that users can easily combine different components to suit their needs.\\nIt also provides various state-of-the-art retrieval and inference methods to streamline the process of adapting ICL to cutting-edge research.\\nThe effectiveness of OpenICL has been validated on a wide range of NLP tasks, including classification, QA, machine translation, and semantic parsing. As a side-product, we found OpenICL to be an efficient yet robust tool for LLMs evaluation. OpenICL is released at https://github.com/Shark-NLP/OpenICL.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.47',\n",
       "   'authors': ['Zhenyu Wu',\n",
       "    'Yaoxiang Wang',\n",
       "    'Jiacheng Ye',\n",
       "    'Zhiyong Wu',\n",
       "    'Jiangtao Feng',\n",
       "    'Jingjing Xu',\n",
       "    'Yu Qiao'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP (demo)',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(demo)-(virtual-poster)'],\n",
       "   'id': 'D118',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.47.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78274/poster_document/c23ec139a96184105a26f67c05901099.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78274/poster/732fc3476f4d71f535079d1827c12873.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] OpenICL: An Open-Source Framework for In-context Learning',\n",
       "   'tldr': 'In recent years, In-context Learning (ICL) has gained increasing attention\\nand emerged as the new paradigm for large language model (LLM) evaluation. Unlike traditional fine-tuning methods, ICL instead adapts the pre-trained models to unseen tasks without any parameter updates.\\nHowever, the implemen...',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'underline_id': 78274,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/78274-demo-openicl-an-open-source-framework-for-in-context-learning',\n",
       "   'video_url': None},\n",
       "  'D119': {'abstract': 'Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large number of parallel data by removing modifiers in the engaging sentences collected from public resources. Then we fine-tune LongLM to reconstruct the original sentences from the corrupted ones. Considering that much overlap between inputs and outputs may bias the model to completely copy the inputs, we split each source sentence into sub-sentences and only require the model to generate the modified sub-sentences. Furthermore, we design a retrieval augmentation algorithm to prompt the model to add suitable modifiers. Automatic and manual evaluation on the auto-constructed test set and real human texts show that our model can generate more engaging sentences with suitable modifiers than strong baselines while keeping fluency. We deploy the model at \\\\url{http://coai.cs.tsinghua.edu.cn/static/polishSent/}. A demo video is available at \\\\url{https://youtu.be/Y6gFHOgSv8Y}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.48',\n",
       "   'authors': ['Zhexin Zhang',\n",
       "    'Jian Guan',\n",
       "    'Xin Cui',\n",
       "    'Yu Ran',\n",
       "    'Bo Liu',\n",
       "    'Minlie Huang'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['session-4_-generation-(demo)-(virtual-poster)'],\n",
       "   'id': 'D119',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.48.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78275/poster_document/b2afcb6ff2f5666f69a31406a45be7c7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78275/poster/b870aeb2ca03fd3994f6e487646eb62c.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Self-Supervised Sentence Polishing by Adding Engaging Modifiers',\n",
       "   'tldr': 'Teachers often guide students to improve their essays by adding engaging modifiers to polish the sentences. In this work, we present the first study on automatic sentence polishing by adding modifiers. Since there is no available dataset for the new task, we first automatically construct a large num...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78275,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/78275-swipe-a-dataset-for-document-level-simplification-of-wikipedia-pages',\n",
       "   'video_url': None},\n",
       "  'D123': {'abstract': 'Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (\\\\textbf{Eff}icient and \\\\textbf{I}ntelligent E\\\\textbf{dit}ing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificial Intelligence (AI) and Natural Language Processing (NLP) technologies. We significantly expand the capacities of a writing assistant\\nby providing functions in three modules: text completion, hint recommendation, and writing refinement. Based on the above efforts, Effidit can efficiently assist users in creating their own text. Effidit has been deployed to several Tencent products and publicly released at \\\\url{https://effidit.qq.com/}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.49',\n",
       "   'authors': ['Shuming Shi',\n",
       "    'Enbo Zhao',\n",
       "    'Wei Bi',\n",
       "    'Deng Cai',\n",
       "    'Leyang Cui',\n",
       "    'Xinting Huang',\n",
       "    'Haiyun Jiang',\n",
       "    'Duyu Tang',\n",
       "    'Kaiqiang Song',\n",
       "    'Longyue Wang',\n",
       "    'Chenyan Huang',\n",
       "    'Guoping Huang',\n",
       "    'Yan Wang',\n",
       "    'Piji Li'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['demo-session-3_-nlp-applications-(demo)-(poster)'],\n",
       "   'id': 'D123',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.49.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78276/poster_document/25e2422ed8580f811f9549e6d85e4935.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78276/poster/1f66b8f9df7783e2be2dddbb33508b30.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Effidit: An Assistant for Improving Writing Efficiency',\n",
       "   'tldr': 'Writing assistants are valuable tools that can help writers improve their writing skills. We introduce Effidit (\\\\textbf{Eff}icient and \\\\textbf{I}ntelligent E\\\\textbf{dit}ing), a digital writing assistant that facilitates users to write higher-quality text more efficiently through the use of Artificia...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78276,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78276-dmlm-descriptive-masked-language-modeling',\n",
       "   'video_url': None},\n",
       "  'D130': {'abstract': \"Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can be challenging due to their opaqueness, high dimensionality, and the large size of modern datasets. To tackle these challenges, we present WizMap, an interactive visualization tool to help researchers and practitioners easily explore large embeddings. With a novel multi-resolution embedding summarization method and a familiar map-like interaction design, WizMap enables users to navigate and interpret embedding spaces with ease. Leveraging modern web technologies such as WebGL and Web Workers, WizMap scales to millions of embedding points directly in users' web browsers and computational notebooks without the need for dedicated backend servers. WizMap is open-source and available at the following public demo link: https://poloclub.github.io/wizmap.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.50',\n",
       "   'authors': ['Zijie J. Wang', 'Fred Hohman', 'Duen Horng Chau'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP (demo)',\n",
       "   'event_ids': ['demo-session-4_-machine-learning-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D130',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.50.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78277/poster_document/4620f7de20d7a62e2c2c87de7f462446.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78277/poster/65a3bd705c19f4c909be418f6f6a8f96.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] WizMap: Scalable Interactive Visualization for Exploring Large Machine Learning Embeddings',\n",
       "   'tldr': 'Machine learning models often learn latent embedding representations that capture the domain semantics of their training data. These embedding representations are valuable for interpreting trained models, building new models, and analyzing new datasets. However, interpreting and using embeddings can...',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'underline_id': 78277,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78277-demo-wizmap-scalable-interactive-visualization-for-exploring-large-machine-learning-embeddings',\n",
       "   'video_url': None},\n",
       "  'D132': {'abstract': 'Our research focuses on the most prevalent type of queries simple questions exemplified by questions like \"What is the capital of France?\". These questions reference an entity such as \"France\", which is directly connected (one hop) to the answer entity \"Paris\" in the underlying knowledge graph (KG). We propose a multilingual Knowledge Graph Question Answering (KGQA) technique that orders potential responses based on the distance between the question\\'s text embeddings and the answer\\'s graph embeddings. A system incorporating this novel method is also described in our work.\\n\\nThrough comprehensive experimentation using various English and multilingual datasets and two KGs  Freebase and Wikidata  we illustrate the comparative advantage of the proposed method across diverse KG embeddings and languages. This edge is apparent even against robust baseline systems, including seq2seq QA models, search-based solutions and intricate rule-based pipelines. Interestingly, our research underscores that even advanced AI systems like ChatGPT encounter difficulties when tasked with answering simple questions. This finding emphasizes the relevance and effectiveness of our approach, which consistently outperforms such systems. We are making the source code and trained models from our study publicly accessible to promote further advancements in multilingual KGQA.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.51',\n",
       "   'authors': ['Anton Razzhigaev',\n",
       "    'Mikhail Salnikov',\n",
       "    'Valentin Malykh',\n",
       "    'Pavel Braslavski',\n",
       "    'Alexander Panchenko'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering (demo)',\n",
       "   'event_ids': ['session-4_-question-answering-(demo)-(virtual-poster)'],\n",
       "   'id': 'D132',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.51.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78278/poster_document/43b2509fcaf285834afb939e6c6c2ab7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78278/poster/7cce85d63314769ded998a8c3f421c9e.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] A System for Answering Simple Questions in Multiple Languages',\n",
       "   'tldr': 'Our research focuses on the most prevalent type of queries simple questions exemplified by questions like \"What is the capital of France?\". These questions reference an entity such as \"France\", which is directly connected (one hop) to the answer entity \"Paris\" in the underlying knowledge graph (KG...',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'underline_id': 78278,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/78278-demo-a-system-for-answering-simple-questions-in-multiple-languages',\n",
       "   'video_url': None},\n",
       "  'D133': {'abstract': 'We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.\\nKWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS analysis, bridging reference resolution, coreference resolution, and discourse relation analysis, making it the most versatile among existing Japanese text analyzers.\\nKWJA solves these tasks in a multi-task manner but still achieves competitive or better performance compared to existing analyzers specialized for each task.\\nKWJA is publicly available under the MIT license at https://github.com/ku-nlp/kwja.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.52',\n",
       "   'authors': ['Nobuhiro Ueda',\n",
       "    'Kazumasa Omura',\n",
       "    'Takashi Kodama',\n",
       "    'Hirokazu Kiyomaru',\n",
       "    'Yugo Murawaki',\n",
       "    'Daisuke Kawahara',\n",
       "    'Sadao Kurohashi'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'event_ids': ['demo-session-2_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)'],\n",
       "   'id': 'D133',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.52.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78279/poster_document/e47a80d50b645a296fb69b799606706a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78279/poster/96a34d03c48c092dc27e5c2be5609881.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] KWJA: A Unified Japanese Analyzer Based on Foundation Models',\n",
       "   'tldr': 'We present KWJA, a high-performance unified Japanese text analyzer based on foundation models.\\nKWJA supports a wide range of tasks, including typo correction, word segmentation, word normalization, morphological analysis, named entity recognition, linguistic feature tagging, dependency parsing, PAS ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'underline_id': 78279,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78279-varta-a-large-scale-headline-generation-dataset-for-indic-languages',\n",
       "   'video_url': None},\n",
       "  'D134': {'abstract': \"We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific articles on idiopathic pulmonary fibrosis (IPF). The front-end web-base user interface of DNC includes two-dimensional (2D) and 3D visualizations of the constructed disease network. The back-end system of DNC includes several natural language processing (NLP) techniques to process biomedical text including BERT-based tokenization on the basis of Bidirectional Encoder Representations from Transformers (BERT), flat and nested named entity recognition (NER), candidate generation and candidate ranking for entity linking (EL) or, relation extraction (RE), and event extraction (EE) tasks. We evaluated the end-to-end EL and end-to-end nested EE systems to determine the DNC's back-end\\nimplementation performance. To the best of our knowledge, this is the first attempt that addresses neural NER, EL, RE, and EE tasks in an end-to-end manner that constructs a path-way visualization from events, which we name Disease Network Constructor. The demonstration video can be accessed from https://youtu.be/rFhWwAgcXE8. We release an online system for end users and the source code is available at https://github.com/aistairc/PRISM-APIs/.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.53',\n",
       "   'authors': ['Mohammad Golam Sohrab',\n",
       "    'Khoa Duong',\n",
       "    'Goran Topi',\n",
       "    'Masami Ikeda',\n",
       "    'Nozomi Nagano',\n",
       "    'Yayoi Natsume-Kitatani',\n",
       "    'Masakata Kuroda',\n",
       "    'Mari Itoh',\n",
       "    'Hiroya Takamura'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['demo-session-3_-nlp-applications-(demo)-(poster)'],\n",
       "   'id': 'D134',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.53.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78280/poster_document/25ffe5f1c53b338955d3e6897022a056.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78280/poster/f26f86094f7d6dd374a4bbf321708149.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Disease Network Constructor: a Pathway Extraction and Visualization',\n",
       "   'tldr': 'We present Disease Network Constructor (DNC), a system that extracts and visualizes a disease network, in which nodes are entities such as diseases, proteins, and genes, and edges represent regulation relation. We focused on the disease network derived through regulation events found in scientific a...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78280,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78280-stt4sg-350-a-speech-corpus-for-all-swiss-german-dialect-regions',\n",
       "   'video_url': None},\n",
       "  'D14': {'abstract': \"Scientific research is inherently shaped by its authors' perspectives, influenced by various factors\\nsuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewpoints. In response to this issue, we introduce PersLEARN , a tool designed to facilitate the cultivation of scientific perspectives, starting from a basic seed idea and progressing to a well-articulated framework. By interacting with a prompt-based model, researchers can develop their perspectives explicitly. Our human\\nstudy reveals that scientific perspectives developed by students using PersLEARN exhibit a superior level of logical coherence and depth compared to those that did not. Furthermore, our pipeline outperforms baseline approaches across multiple domains of literature from various perspectives. These results suggest that PersLEARN could help foster a greater appreciation of diversity in scientific perspectives as an essential component of research training.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.2',\n",
       "   'authors': ['Yu-Zhe Shi',\n",
       "    'Shiqian Li',\n",
       "    'Xinyi Niu',\n",
       "    'Qiao Xu',\n",
       "    'Jiawen Liu',\n",
       "    'Yifan Xu',\n",
       "    'Shiyu Gu',\n",
       "    'Bingru He',\n",
       "    'Xinyang Li',\n",
       "    'Xinyu Zhao',\n",
       "    'Zijian Zhao',\n",
       "    'Yidong Lyu',\n",
       "    'Zhen Li',\n",
       "    'Sijia Liu',\n",
       "    'Lin Qiu',\n",
       "    'Jinhao Ji',\n",
       "    'Lecheng Ruan',\n",
       "    'Yuxi Ma',\n",
       "    'Wenjuan Han',\n",
       "    'Yixin Zhu'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Diversity (demo)',\n",
       "   'event_ids': ['session-7_-linguistic-diversity-(demo)-(virtual-poster)'],\n",
       "   'id': 'D14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.2.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78229/poster_document/e66667bd9364e12ab613259cb2b836c9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78229/poster/4e49829755ec8716d757f57da165fa21.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] PersLEARN: Research Training through the Lens of Perspective Cultivation',\n",
       "   'tldr': \"Scientific research is inherently shaped by its authors' perspectives, influenced by various factors\\nsuch as their personality, community, or society. Junior researchers often face challenges in identifying the perspectives reflected in the existing literature and struggle to develop their own viewp...\",\n",
       "   'track': 'Linguistic Diversity (demo)',\n",
       "   'underline_id': 78229,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/78229-demo-perslearn-research-training-through-the-lens-of-perspective-cultivation',\n",
       "   'video_url': None},\n",
       "  'D141': {'abstract': 'Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. In some cases, LLMs can be used more affordably via RAM offloading or hosted APIs. However, these techniques have innate limitations: offloading is too slow for interactive inference, while APIs are not flexible enough for research that requires access to weights, attention or logits. In this work, we propose Petals - a system for inference and fine-tuning of large models collaboratively by joining the resources of multiple parties. We demonstrate that this strategy outperforms offloading for very large models, running inference of BLOOM-176B on consumer GPUs with 1 step per second, which is enough for many interactive LLM applications. Unlike most inference APIs, Petals also natively exposes hidden states of served models, allowing to train and share custom model extensions based on efficient fine-tuning methods. The system, its source code, and documentation are available at https://petals.ml\\n\\nVideo (2 min): https://youtu.be/F4muLI-0hTE',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.54',\n",
       "   'authors': ['Alexander Borzunov',\n",
       "    'Dmitry Baranchuk',\n",
       "    'Tim Dettmers',\n",
       "    'Maksim Riabinin',\n",
       "    'Younes Belkada',\n",
       "    'Artem Chumachenko',\n",
       "    'Pavel Samygin',\n",
       "    'Colin Raffel'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models (demo)',\n",
       "   'event_ids': ['demo-session-6_-large-language-models-(demo)-(poster)'],\n",
       "   'id': 'D141',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.54.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78281/poster_document/a5e33ac36c15822b4051eb3b11a16353.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78281/poster/62cdab5b171b9672c7fdcb1d5278da2d.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Petals: Collaborative Inference and Fine-tuning of Large Models',\n",
       "   'tldr': 'Many NLP tasks benefit from using large language models (LLMs) that often have more than 100 billion parameters. With the release of BLOOM-176B and OPT-175B, everyone can download pretrained models of this scale. Still, using these models requires high-end hardware unavailable to many researchers. I...',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'underline_id': 78281,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78281-demo-petals-collaborative-inference-and-fine-tuning-of-large-models',\n",
       "   'video_url': None},\n",
       "  'D144': {'abstract': \"The continuous development of Question Answering (QA) datasets has drawn the research community's attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dataset. However, with the proliferation of QA models in online repositories such as GitHub or Hugging Face, an alternative is becoming viable. Recent works have demonstrated that combining expert agents can yield large performance gains over multi-dataset models. To ease research in multi-agent models, we extend UKP-SQuARE, an online platform for QA research, to support three families of multi-agent systems: i) agent selection, ii) early-fusion of agents, and iii) late-fusion of agents. We conduct experiments to evaluate their inference speed and discuss the performance vs. speed trade-off compared to multi-dataset models. UKP-SQuARE is open-source and publicly available.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.55',\n",
       "   'authors': ['Haritz Puerto',\n",
       "    'Tim Baumgrtner',\n",
       "    'Rachneet Sachdeva',\n",
       "    'Haishuo Fang',\n",
       "    'Hao Zhang',\n",
       "    'Sewin Tariverdian',\n",
       "    'Kexin Wang',\n",
       "    'Iryna Gurevych'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering (demo)',\n",
       "   'event_ids': ['demo-session-6_-question-answering-(demo)-(poster)'],\n",
       "   'id': 'D144',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.55.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78282/poster_document/c397488a675215f412c824dc2c964da0.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78282/poster/7bd19bb4d5a337b3809142c7b6a91f6b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] UKP-SQuARE v3: A Platform for Multi-Agent QA Research',\n",
       "   'tldr': \"The continuous development of Question Answering (QA) datasets has drawn the research community's attention toward multi-domain models. A popular approach is to use multi-dataset models, which are models trained on multiple datasets to learn their regularities and prevent overfitting to a single dat...\",\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'underline_id': 78282,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78282-escoxlm-r-multilingual-taxonomy-driven-pre-training-for-the-job-market-domain',\n",
       "   'video_url': None},\n",
       "  'D147': {'abstract': \"In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and take-away messages less reliable. With Ranger, we aim to address this issue by providing a task-agnostic toolkit that combines the effect of a treatment on multiple tasks into one statistical evaluation, allowing for comparison of metrics and computation of an overall summary effect. Our toolkit produces publication-ready forest plots that enable clear communication of evaluation results over multiple tasks. Our goal with the ready-to-use Ranger toolkit is to promote robust, effect-size-based evaluation and improve evaluation standards in the community. We provide two case studies for common IR and NLP settings to highlight Ranger's benefits.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.56',\n",
       "   'authors': ['Mete Sertkan', 'Sophia Althammer', 'Sebastian Hofsttter'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP (demo)',\n",
       "   'event_ids': ['demo-session-7_-machine-learning-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D147',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.56.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78283/poster_document/d48b9357c25d8818995796956b7a5e0d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78283/poster/92e2964214736b64d908129cd2ba864d.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Ranger: A Toolkit for Effect-Size Based Multi-Task Evaluation',\n",
       "   'tldr': 'In this paper, we introduce Ranger - a toolkit to facilitate the easy use of effect-size-based meta-analysis for multi-task evaluation in NLP and IR. We observed that our communities often face the challenge of aggregating results over incomparable metrics and scenarios, which makes conclusions and ...',\n",
       "   'track': 'Machine Learning for NLP (demo)',\n",
       "   'underline_id': 78283,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15340/poster/78283-demo-ranger-a-toolkit-for-effect-size-based-multi-task-evaluation',\n",
       "   'video_url': None},\n",
       "  'D148': {'abstract': 'Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility.\\nWe include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub: https://github.com/huggingface/gaia.\\nWe then demonstrate how the ideas we present can be operationalized to create a powerful tool for qualitative data analysis in NLP. We present GAIA Search - a search engine built following previously laid out principles, giving access to four popular large-scale text collections. GAIA serves a dual purpose of illustrating the potential of methodologies we discuss but also as a standalone qualitative analysis tool that can be leveraged by NLP researchers aiming to understand datasets prior to using them in training. GAIA is hosted live on Hugging Face Spaces: https://huggingface.co/spaces/spacerini/gaia.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.57',\n",
       "   'authors': ['Aleksandra Piktus',\n",
       "    'Odunayo Ogundepo',\n",
       "    'Christopher Akiki',\n",
       "    'Akintunde Oladipo',\n",
       "    'Xinyu Zhang',\n",
       "    'Hailey Schoelkopf',\n",
       "    'Stella Biderman',\n",
       "    'Martin Potthast',\n",
       "    'Jimmy Lin'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation (demo)',\n",
       "   'event_ids': ['demo-session-3_-resources-and-evaluation-(demo)-(poster)'],\n",
       "   'id': 'D148',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.57.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78284/poster_document/465c4a92141365f5f13e650a1ace4161.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78284/poster/5c47f25a435dd21f9e95e9136c0b539b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration',\n",
       "   'tldr': 'Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-sca...',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'underline_id': 78284,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78284-demo-gaia-search-hugging-face-and-pyserini-interoperability-for-nlp-training-data-exploration',\n",
       "   'video_url': None},\n",
       "  'D156': {'abstract': 'An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It supports modular approach to implementation of conversational agents enabling their development through the choice of NLP components and conversational skills from a rich library organized into the distributions of ready-for-use multi-skill AI assistant systems. In DeepPavlov Dream, multi-skill Generative AI Assistant consists of NLP components that extract features from user utterances, conversational skills that generate or retrieve a response, skill and response selectors that facilitate choice of relevant skills and the best response, as well as a conversational orchestrator that enables creation of multi-skill Generative AI Assistants scalable up to industrial grade AI assistants. The platform allows to integrate large language models into dialog pipeline, customize with prompt engineering,  handle multiple prompts during the same dialog session and create simple multimodal assistants.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.58',\n",
       "   'authors': ['Diliara Zharikova',\n",
       "    'Daniel Kornev',\n",
       "    'Fedor Ignatov',\n",
       "    'Maxim Talimanchuk',\n",
       "    'Dmitry Evseev',\n",
       "    'Ksenya Petukhova',\n",
       "    'Veronika Smilga',\n",
       "    'Dmitry Karpov',\n",
       "    'Yana Shishkina',\n",
       "    'Dmitry Kosenko',\n",
       "    'Mikhail Burtsev'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-2_-generation-(demo)-(poster)'],\n",
       "   'id': 'D156',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.58.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78285/poster_document/4c6537b7176bc23c6af59595ca010037.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78285/poster/a222c2c78a85e4d42d60ac7e1d3e86aa.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] DeepPavlov Dream: Platform for Building Generative AI Assistants',\n",
       "   'tldr': 'An open-source DeepPavlov Dream Platform is specifically tailored for development of complex dialog systems like Generative AI Assistants. The stack prioritizes efficiency, modularity, scalability, and extensibility with the goal to make it easier to develop complex dialog systems from scratch. It s...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78285,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78285-demo-deeppavlov-dream-platform-for-building-generative-ai-assistants',\n",
       "   'video_url': None},\n",
       "  'D17': {'abstract': 'We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future research and development. It features a unified interface to easily access state-of-the-art image-language, video-language models and common datasets. LAVIS supports training, evaluation and benchmarking on a rich variety of tasks, including multimodal classification, retrieval, captioning, visual question answering, dialogue and pre-training. In the meantime, the library is also highly extensible and configurable, facilitating future development and customization. In this technical report, we describe design principles, key components and functionalities of the library, and also present benchmarking results across common language-vision tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.3',\n",
       "   'authors': ['Dongxu Li',\n",
       "    'Junnan Li',\n",
       "    'Hung Le',\n",
       "    'Guangsen Wang',\n",
       "    'silvio savarese',\n",
       "    'Steven C.H. Hoi'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics and Beyond (demo)',\n",
       "   'event_ids': ['session-1_-language-grounding-to-vision,-robotics-and-beyond-(demo)-(virtual-poster)'],\n",
       "   'id': 'D17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.3.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78230/poster_document/bfc801d095f48a04fbb20df9dab38ea2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78230/poster/99a889d4b6e34918abd206c2bbe90a8c.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] LAVIS: A One-stop Library for Language-Vision Intelligence',\n",
       "   'tldr': 'We introduce LAVIS, an open-source deep learning library for LAnguage-VISion research and applications. LAVIS aims to serve as a one-stop comprehensive library that brings recent advancements in the language-vision field accessible for researchers and practitioners, as well as fertilizing future res...',\n",
       "   'track': 'Language Grounding to Vision, Robotics and Beyond (demo)',\n",
       "   'underline_id': 78230,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/78230-gcarret3r-a-graph-guided-generate-and-rerank-framework-for-complex-and-cross-domain-text-to-sql-generation',\n",
       "   'video_url': None},\n",
       "  'D18': {'abstract': 'Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotypes. To address this issue, we propose Finspector, a human-centered visual inspection tool designed to detect biases in different categories through log-likelihood scores generated by language models. The goal of the tool is to enable researchers to easily identify potential biases using visual analytics, ultimately contributing to a fairer and more just deployment of these models in both academic and industrial settings. Finspector is available at https://github.com/IBM/finspector.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.4',\n",
       "   'authors': ['Bum Chul Kwon', 'Nandana Mihindukulasooriya'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models (demo)',\n",
       "   'event_ids': ['demo-session-7_-large-language-models-(demo)-(poster)'],\n",
       "   'id': 'D18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.4.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78231/poster_document/fbd6e486bce49486e4f753322998c93f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78231/poster/a97c5c7e06464438c7666c92c114a47a.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Finspector: A Human-Centered Visual Inspection Tool for Exploring and Comparing Biases among Foundation Models',\n",
       "   'tldr': 'Pre-trained transformer-based language models are becoming increasingly popular due to their exceptional performance on various benchmarks. However, concerns persist regarding the presence of hidden biases within these models, which can lead to discriminatory outcomes and reinforce harmful stereotyp...',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'underline_id': 78231,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15340/poster/78231-demo-finspector-a-human-centered-visual-inspection-tool-for-exploring-and-comparing-biases-among-foundation-models',\n",
       "   'video_url': None},\n",
       "  'D19': {'abstract': 'The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce PrimeQA: a one-stop and open-source QA repository with an aim to democratize QA research and facilitate easy replication of state-of-the-art (SOTA) QA methods. PrimeQA supports core QA functionalities like retrieval and reading comprehension as well as auxiliary capabilities such as question generation. It has been designed as an end-to-end toolkit for various use cases: building front-end applications, replicating SOTA methods on public benchmarks, and expanding pre-existing methods. PrimeQA is available at: https://github.com/primeqa.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.5',\n",
       "   'authors': ['Avi Sil',\n",
       "    'Jaydeep Sen',\n",
       "    'Bhavani Iyer',\n",
       "    'Martin Franz',\n",
       "    'Kshitij Fadnis',\n",
       "    'Mihaela Bornea',\n",
       "    'Sara Rosenthal',\n",
       "    'Scott McCarley',\n",
       "    'Rong Zhang',\n",
       "    'Vishwajeet Kumar',\n",
       "    'Yulong Li',\n",
       "    'Md Arafat Sultan',\n",
       "    'Riyaz Bhat',\n",
       "    'Juergen Bross',\n",
       "    'Radu Florian',\n",
       "    'Salim Roukos'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering (demo)',\n",
       "   'event_ids': ['demo-session-6_-question-answering-(demo)-(poster)'],\n",
       "   'id': 'D19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.5.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78232/poster_document/9044f3a1fed73575d850d5a54e95825a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78232/poster/b046b5a9bca33da3f0c433f7ff195ddc.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] PrimeQA: The Prime Repository for State-of-the-Art Multilingual Question Answering Research and Development',\n",
       "   'tldr': 'The field of Question Answering (QA) has made remarkable progress in recent years, thanks to the advent of large pre-trained language models, newer realistic benchmark datasets with leaderboards, and novel algorithms for key components such as retrievers and readers. In this paper, we introduce Prim...',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'underline_id': 78232,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78232-evaluating-open-domain-dialogues-in-latent-space-with-next-sentence-prediction-and-mutual-information',\n",
       "   'video_url': None},\n",
       "  'D24': {'abstract': 'Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and boredom and severely lower the diversity of the generated poetry. To address these issues, we present Lingxi, a diversity-aware Chinese modern poetry generation system. For the CWS issue, we propose a novel framework that incorporates CWS in the tokenization process. The proposed method can achieve a high vocabulary coverage rate with a reasonable vocabulary size. For the decoding method and the diversity issue, we propose a novel sampling algorithm that flattens the high likelihood part of the predicted distribution of the language model to emphasize the comparatively low-likelihood words and increase the diversity of generated poetry. Empirical results show that even when the top 60% of cumulative probability mass of the predicted distribution is flattened, our method achieves comparable or even better performance than baseline sampling methods. Our system is available at http://lingxi.website.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.6',\n",
       "   'authors': ['Xinran Zhang', 'Maosong Sun', 'Jiafeng Liu', 'Xiaobing Li'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Diversity (demo)',\n",
       "   'event_ids': ['demo-session-3_-linguistic-diversity-(demo)-(poster)'],\n",
       "   'id': 'D24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.6.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78233/poster_document/b5a961249cb403581513c2a4eef80363.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78233/poster/8b2a65d49d123f4f2aa10b578288214d.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Lingxi: A Diversity-aware Chinese Modern Poetry Generation System',\n",
       "   'tldr': 'Chinese modern poetry generation has been a challenging task. One issue is the Chinese word segmentation (CWS) which is critical to comprehend the Chinese language but was not always considered in common tokenization methods. Another is the decoding (sampling) method which may induce repetition and ...',\n",
       "   'track': 'Linguistic Diversity (demo)',\n",
       "   'underline_id': 78233,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78233-demo-lingxi-a-diversity-aware-chinese-modern-poetry-generation-system',\n",
       "   'video_url': None},\n",
       "  'D25': {'abstract': \"Scientific literature is always available in Adobe's Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, we present Autodive, an integrated onsite scientific literature annotation tool for natural scientists and Natural Language Processing (NLP) researchers. This tool provides six core functions of annotation that support the whole lifecycle of corpus generation including i)annotation project management, ii)resource management, iii)ontology management, iv)manual annotation, v)onsite auto annotation, and vi)annotation task statistic. Two experiments are carried out to verify efficiency of the presented tool. A live demo of Autodive is available at http://autodive.sciwiki.cn. The source code is available at https://github.com/Autodive.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.7',\n",
       "   'authors': ['Yi Du',\n",
       "    'Ludi Wang',\n",
       "    'Mengyi Huang',\n",
       "    'Dongze Song',\n",
       "    'Wenjuan Cui',\n",
       "    'Yuanchun Zhou'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['session-1_-nlp-applications-(demo)-(virtual-poster)'],\n",
       "   'id': 'D25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.7.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78234/poster_document/66ec85f3a997905080e370fad120a87b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78234/poster/a4be76209ae000b80a16b53100f98788.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Autodive: An Integrated Onsite Scientific Literature Annotation Tool',\n",
       "   'tldr': \"Scientific literature is always available in Adobe's Portable Document Format (PDF), which is friendly for scientists to read. Compared with raw text, annotating directly on PDF documents can greatly improve the labeling efficiency of scientists whose annotation costs are very high. In this paper, w...\",\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78234,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/78234-demo-autodive-an-integrated-onsite-scientific-literature-annotation-tool',\n",
       "   'video_url': None},\n",
       "  'D26': {'abstract': 'Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e.\\\\ a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small number of publicly accessible QAG models. In this paper, we introduce AutoQG, an online service for multilingual QAG along with \\\\texttt{lmqg}, an all-in-one python package for model fine-tuning, generation, and evaluation. We also release QAG models in eight languages fine-tuned on a few variants of pre-trained encoder-decoder language models, which can be used online via AutoQG or locally via \\\\texttt{lmqg}. With these resources, practitioners of any level can benefit from a toolkit that includes a web interface for end users, and easy-to-use code for developers who require custom models or fine-grained controls for generation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.8',\n",
       "   'authors': ['Asahi Ushio',\n",
       "    'Fernando Alva-Manchego',\n",
       "    'Jose Camacho-Collados'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering (demo)',\n",
       "   'event_ids': ['demo-session-6_-question-answering-(demo)-(poster)'],\n",
       "   'id': 'D26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.8.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78235/poster_document/c6038788be65333f3044204a63d954e6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78235/poster/0b4caef0277a23cbe4b4e2af67ad155a.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] A Practical Toolkit for Multilingual Question and Answer Generation',\n",
       "   'tldr': 'Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generat...',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'underline_id': 78235,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78235-demo-a-practical-toolkit-for-multilingual-question-and-answer-generation',\n",
       "   'video_url': None},\n",
       "  'D27': {'abstract': 'Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible toolkit for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models for both single-intent and multi-intent scenarios, which support both non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is highly modularized and extensible by decomposing the model architecture, inference, and learning process into reusable modules, which allows researchers to quickly set up SLU experiments with highly flexible configurations. OpenSLU is implemented based on PyTorch, and released at https://github.com/LightChen233/OpenSLU.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.9',\n",
       "   'authors': ['Libo Qin',\n",
       "    'Qiguang Chen',\n",
       "    'Xiao Xu',\n",
       "    'Yunlong Feng',\n",
       "    'Wanxiang Che'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality (demo)',\n",
       "   'event_ids': ['demo-session-3_-speech-and-multimodality-(demo)-(poster)'],\n",
       "   'id': 'D27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.9.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78236/poster_document/ec30da30fc040431459b5f397a4d4f4f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78236/poster/736c4e556b0dca70aef129393815561a.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding',\n",
       "   'tldr': 'Spoken Language Understanding (SLU) is one of the core components of a task-oriented dialogue system, which aims to extract the semantic meaning of user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an open-source toolkit to provide a unified, modularized, and extensible too...',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'underline_id': 78236,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78236-demo-openslu-a-unified-modularized-and-extensible-toolkit-for-spoken-language-understanding',\n",
       "   'video_url': None},\n",
       "  'D30': {'abstract': 'We present  a  neural  Sanskrit  Natural Language Processing (NLP)  toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems currently report state-of-the-art performance on available benchmark datasets for all tasks. SanskritShala is deployed as a web-based application, which allows a user to get real-time analysis for the given input. It is built with easy-to-use interactive data annotation features that allow annotators to correct the system predictions when it makes mistakes. We publicly release the source codes of the 4 modules included in the toolkit, 7 word embedding models that have been trained on publicly available Sanskrit corpora and multiple annotated datasets such as word similarity, relatedness, categorization, analogy prediction to assess intrinsic properties of word embeddings.  So far as we know, this is the first neural-based Sanskrit NLP toolkit that has a web-based interface and a number of NLP modules. We are sure that the people who are willing to work with Sanskrit will find it useful for pedagogical and annotative purposes. SanskritShala is available at: https://cnerg.iitkgp.ac.in/sanskritshala. The demo video of our platform can be accessed at: https://youtu.be/x0X31Y9k0mw4.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.10',\n",
       "   'authors': ['Jivnesh Sandhan',\n",
       "    'Anshul Agarwal',\n",
       "    'Laxmidhar Behera',\n",
       "    'Tushar Sandhan',\n",
       "    'Pawan Goyal'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Diversity (demo)',\n",
       "   'event_ids': ['session-7_-linguistic-diversity-(demo)-(virtual-poster)'],\n",
       "   'id': 'D30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.10.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78237/poster_document/af94ff77c51ac16cb7798b0f5c046798.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78237/poster/2fffd8b26c64d3b685f172fe0cee72c3.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] SanskritShala: A Neural Sanskrit NLP Toolkit with Web-Based Interface for Pedagogical and Annotation Purposes',\n",
       "   'tldr': 'We present  a  neural  Sanskrit  Natural Language Processing (NLP)  toolkit named SanskritShala (a school of Sanskrit) to facilitate computational linguistic analyses for several tasks such as word segmentation, morphological tagging, dependency parsing, and compound type identification. Our systems...',\n",
       "   'track': 'Linguistic Diversity (demo)',\n",
       "   'underline_id': 78237,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/78237-disease-network-constructor-a-pathway-extraction-and-visualization',\n",
       "   'video_url': None},\n",
       "  'D31': {'abstract': 'Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation problem and argue that well-orchestrated pipelines based on large language models (LLMs) and image generation models (IGMs) are suitable to addressing these tasks. We present LIDA, a novel tool for generating grammar-agnostic visualizations and infographics. LIDA comprises of 4 modules - A SUMMARIZER that converts data into a rich but compact natural language summary, a GOAL EXPLORER that enumerates visualization goals given the data, a VISGENERATOR that generates, refines, executes and filters visualization code and an INFOGRAPHER module that yields data-faithful stylized graphics using IGMs. LIDA provides a python api, and a hybrid user interface (direct manipulation and multilingual natural language) for interactive chart, infographics and data story generation. \\nCode and demo are available at this url - https://microsoft.github.io/lida/',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.11',\n",
       "   'authors': ['Victor Dibia'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-6_-generation-(demo)-(poster)'],\n",
       "   'id': 'D31',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.11.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78238/poster_document/d2c7f09b6a3d9ee6fbcbea0cf70e45f6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78238/poster/8102e5ab3511fa9e5c311d7a95014e24.png',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models',\n",
       "   'tldr': 'Systems that support users in the automatic creation of visualizations must address several subtasks - understand the semantics of data, enumerate relevant visualization goals and generate visualization specifications. In this work, we pose visualization generation as a multi-stage generation proble...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78238,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78238-demo-lida-a-tool-for-automatic-generation-of-grammar-agnostic-visualizations-and-infographics-using-large-language-models',\n",
       "   'video_url': None},\n",
       "  'D32': {'abstract': 'Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept mappings, attracting great attention from cognitive science and psychology communities. Thus, we aim to develop a computational metaphor processing online system, termed MetaPro Online, that allows users without a coding background, e.g., language learners and linguists, to easily query metaphoricity labels, metaphor paraphrases, and concept mappings for non-domain-specific text. The outputs of MetaPro can be directly used by language learners and natural language processing downstream tasks because MetaPro is an end-to-end system.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.12',\n",
       "   'authors': ['Rui Mao', 'Xiao Li', 'Kai He', 'Mengshi Ge', 'Erik Cambria'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['session-4_-generation-(demo)-(virtual-poster)'],\n",
       "   'id': 'D32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.12.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78239/poster_document/2e2392e365eff023f945e629015ae5de.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78239/poster/6df8eb5cac14b51d38a0ea40b9ce500b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] MetaPro Online: A Computational Metaphor Processing Online System',\n",
       "   'tldr': 'Metaphoric expressions are a special linguistic phenomenon, frequently appearing in everyday language. Metaphors do not take their literal meanings in contexts, which may cause obstacles for language learners to understand them. Metaphoric expressions also reflect the cognition of humans via concept...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78239,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/78239-demo-metapro-online-a-computational-metaphor-processing-online-system',\n",
       "   'video_url': None},\n",
       "  'D39': {'abstract': 'In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. \\nOur goal for this tool is threefold: \\n1) To support subject-experts to intuitively create complex and flexible dialog systems,\\n2) To support rapid prototyping of dialog system behavior, e.g., for research, and \\n3) To provide a hands-on test bed for students learning about dialog systems.\\nTo facilitate this, DIAGRAPH aims to provide a clean and intuitive graphical interface for creating dialog systems without requiring any coding knowledge.\\nOnce a dialog graph has been created, it is automatically turned into a dialog system using state of the art language models. This allows for rapid prototyping and testing.\\nDialog designers can then distribute a link to their finished  dialog system or embed it into a website.\\nAdditionally, to support scientific experiments and data collection, dialog designers can access chat logs. \\nFinally, to verify the usability of DIAGRAPH, we performed evaluation with subject-experts who extensively worked with the tool and users testing it for the first time, receiving above average System Usability Scale (SUS) scores from both (82 out 100 and 75 out of 100, respectively).\\nIn this way, we hope DIAGRAPH helps reduce the barrier to entry for creating dialog interactions.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.13',\n",
       "   'authors': ['Dirk Vth', 'Lindsey Vanderlyn', 'Ngoc Thang Vu'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'event_ids': ['demo-session-3_-dialogue-and-interactive-systems-(demo)-(poster)'],\n",
       "   'id': 'D39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.13.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78240/poster_document/39237908770e4244a30a7ac5dc0a418a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78240/poster/d870605bef7fbddc2e1f550a7bfadcef.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] DIAGRAPH: An Open-Source Graphic Interface for Dialog Flow Design',\n",
       "   'tldr': 'In this work, we present DIAGRAPH, an open-source graphical dialog flow editor built on the ADVISER toolkit. \\nOur goal for this tool is threefold: \\n1) To support subject-experts to intuitively create complex and flexible dialog systems,\\n2) To support rapid prototyping of dialog system behavior, e.g....',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'underline_id': 78240,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78240-demo-diagraph-an-open-source-graphic-interface-for-dialog-flow-design',\n",
       "   'video_url': None},\n",
       "  'D44': {'abstract': \"Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researchers have introduced distributional control techniques. These techniques, not limited to language, allow controlling the prevalence (i.e. expectations) of any features of interest in the model's outputs. Despite their potential, the widespread adoption of these techniques has been hindered by the difficulty in adapting the complex, disconnected code. Here, we present disco, an open-source Python library that brings these techniques to the broader public\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.14',\n",
       "   'authors': ['Germn Kruszewski', 'Jos Rozen', 'Marc Dymetman'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-1_-generation-(demo)-(poster)'],\n",
       "   'id': 'D44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.14.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78241/poster_document/2ed6327b093888ae2d9c2b1b2aedf84a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78241/poster/22b1c7631bb9da9f7b8dccaabd72c487.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Disco: a toolkit for Distributional Control of Generative Models',\n",
       "   'tldr': 'Pre-trained language models and other generative models have revolutionized NLP and beyond. However, these models tend to reproduce undesirable biases present in their training data. Also, they may overlook patterns that are important but challenging to capture. To address these limitations, researc...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78241,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78241-demo-disco-a-toolkit-for-distributional-control-of-generative-models',\n",
       "   'video_url': None},\n",
       "  'D45': {'abstract': 'Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort.\\n\\nCode: https://github.com/kevinduh/sockeye-recipes3\\n\\nVideo demo: https://cs.jhu.edu/~kevinduh/j/demo.mp4',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.15',\n",
       "   'authors': ['Xuan Zhang', 'Kevin Duh', 'Paul McNamee'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation (demo)',\n",
       "   'event_ids': ['demo-session-4_-machine-translation-(demo)-(poster)'],\n",
       "   'id': 'D45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.15.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78242/poster_document/9d4a728cafba3e1d23456139b0c371ee.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78242/poster/466b3a5c267223354713b3b7b23f076f.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] A Hyperparameter Optimization Toolkit for Neural Machine Translation Research',\n",
       "   'tldr': 'Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequi...',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'underline_id': 78242,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78242-demo-a-hyperparameter-optimization-toolkit-for-neural-machine-translation-research',\n",
       "   'video_url': None},\n",
       "  'D46': {'abstract': 'Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces the original speech of a live video stream in a simultaneous manner. Tests on a collection of 90 public videos show that our system achieves a low average latency of 11.90 seconds for smooth playback. Our method is general and can be extended to other language pairs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.16',\n",
       "   'authors': ['Xiaolin Wang', 'Masao Utiyama', 'Eiichiro Sumita'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation (demo)',\n",
       "   'event_ids': ['session-4_-machine-translation-(demo)-(virtual-poster)'],\n",
       "   'id': 'D46',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.16.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78243/poster_document/7d9a74b10c074600bfcb6b6a0c988ba8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78243/poster/16c2452fd6ea99bc2224e4da5606b1ef.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Japanese-to-English Simultaneous Dubbing Prototype',\n",
       "   'tldr': 'Live video streaming has become an important form of communication such as virtual conferences. However, for cross-language communication in live video streaming, reading subtitles degrades the viewing experience. To address this problem, our simultaneous dubbing prototype translates and replaces th...',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'underline_id': 78243,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/78243-demo-japanese-to-english-simultaneous-dubbing-prototype',\n",
       "   'video_url': None},\n",
       "  'D47': {'abstract': \"We present Visual Knowledge oriented Programming platform (<b>VisKoP</b>), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as <i>''dragging''</i> to add knowledge operators and <i>''slot filling''</i> to designate operator arguments. Moreover, VisKoP provides auto-completion for its knowledge base schema and users can easily debug the KoPL program by checking its intermediate results. To facilitate the practical KBQA on a million-entity-level KB, we design a highly efficient KoPL execution engine for the back-end. Experiment results show that VisKoP is highly efficient and user interaction can fix a large portion of wrong KoPL programs to acquire the correct answer. The VisKoP online demo, highly efficient KoPL engine, and screencast video are now publicly available.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.17',\n",
       "   'authors': ['Zijun Yao',\n",
       "    'YUANYONG CHEN',\n",
       "    'Xin Lv',\n",
       "    'Shulin Cao',\n",
       "    'Amy Xin',\n",
       "    'Jifan Yu',\n",
       "    'Hailong Jin',\n",
       "    'jianjun xu',\n",
       "    'Peng Zhang',\n",
       "    'Lei Hou',\n",
       "    'Juanzi Li'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering (demo)',\n",
       "   'event_ids': ['demo-session-5_-question-answering-(demo)-(poster)'],\n",
       "   'id': 'D47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.17.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78244/poster_document/596e91a6836dcb2b7de54e5d269dd684.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78244/poster/1d000670ce3bba702536fd5d360198c5.png',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering',\n",
       "   'tldr': 'We present Visual Knowledge oriented Programming platform (<b>VisKoP</b>), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural langu...',\n",
       "   'track': 'Question Answering (demo)',\n",
       "   'underline_id': 78244,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78244-viskop-visual-knowledge-oriented-programming-for-interactive-knowledge-base-question-answering',\n",
       "   'video_url': None},\n",
       "  'D49': {'abstract': 'English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-world situational dialogue-based chatbot designed for English education. It also naturally switches to a new topic or situation in response to out-of-topic utterances, which are common among English beginners. Furthermore, PEEP-Talk provides feedback score on conversation and grammar error correction. We performed automatic and user evaluations to validate performance and education efficiency of our system. The results show that PEEP-Talk generates appropriate responses in various real-life situations while providing accurate feedback to learners. Moreover, we demonstrate a positive impact on English-speaking, grammar, and English learning anxiety, implying that PEEP-Talk can lower the barrier to learning natural conversation in effective ways.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.18',\n",
       "   'authors': ['Seugnjun Lee',\n",
       "    'Yoonna Jang',\n",
       "    'Chanjun Park',\n",
       "    'Jungseob Lee',\n",
       "    'Jaehyung Seo',\n",
       "    'Hyeonseok Moon',\n",
       "    'Sugyeong Eo',\n",
       "    'Seounghoon Lee',\n",
       "    'Bernardo Yahya',\n",
       "    'Heuiseok Lim'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'event_ids': ['demo-session-7_-dialogue-and-interactive-systems-(demo)-(poster)'],\n",
       "   'id': 'D49',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.18.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78245/poster_document/336971c74ec38cdd07da62d552f89e1b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78245/poster/ae87b31f0ceea96633d5d084409feb64.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] PEEP-Talk: A Situational Dialogue-based Chatbot for English Education',\n",
       "   'tldr': 'English is acknowledged worldwide as a mode of communication. However, due to the absence of realistic practicing scenarios, students learning English as a foreign language (EFL) typically have limited chances to converse and share feedback with others. In this paper, we propose PEEP-Talk, a real-wo...',\n",
       "   'track': 'Dialogue and Interactive Systems (demo)',\n",
       "   'underline_id': 78245,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15340/poster/78245-demo-peep-talk-a-situational-dialogue-based-chatbot-for-english-education',\n",
       "   'video_url': None},\n",
       "  'D53': {'abstract': 'Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, current approaches do not consider the human interactions that occur in real post-editing scenarios. To address this issue, we present OpenTIPE, a flexible and extensible framework that aims at supporting research on interactive post-editing. Specifically, the interactive environment of OpenTIPE allows researchers to explore human-centered approaches for the post-editing task. We release the OpenTIPE source code and showcase its main functionalities with a demonstration video and an online live demo.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.19',\n",
       "   'authors': ['Fabian Landwehr', 'Thomas Steinmann', 'Laura Mascarell'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation (demo)',\n",
       "   'event_ids': ['demo-session-4_-machine-translation-(demo)-(poster)'],\n",
       "   'id': 'D53',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.19.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78246/poster_document/b625afec58ce7b4de83ac4e16ef4170c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78246/poster/3404c0ae10b862f2f40f42a72d0b8362.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] OpenTIPE: An Open-source Translation Framework for Interactive Post-Editing Research',\n",
       "   'tldr': 'Despite the latest improvements on machine translation, professional translators still must review and post-edit the automatic output to ensure high-quality translations. The research on automating this process lacks an interactive post-editing environment implemented for this purpose; therefore, cu...',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'underline_id': 78246,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78246-demo-opentipe-an-open-source-translation-framework-for-interactive-post-editing-research',\n",
       "   'video_url': None},\n",
       "  'D55': {'abstract': 'Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different pre-training models within a uniform framework. In this paper, we present TencentPretrain, a toolkit supporting pre-training models of different modalities. The core feature of TencentPretrain is the modular design. The toolkit uniformly divides pre-training models into 5 components: embedding, encoder, target embedding, decoder, and target. As almost all of common modules are provided in each component, users can choose the desired modules from different components to build a complete pre-training model. The modular design enables users to efficiently reproduce existing pre-training models or build brand-new one. We test the toolkit on text, vision, and audio benchmarks and show that it can match the performance of the original implementations.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.20',\n",
       "   'authors': ['Zhe Zhao',\n",
       "    'Yudong Li',\n",
       "    'Cheng Hou',\n",
       "    'Jing Zhao',\n",
       "    'Rong Tian',\n",
       "    'Weijie Liu',\n",
       "    'Yiren Chen',\n",
       "    'Ningyuan Sun',\n",
       "    'Haoyan Liu',\n",
       "    'Weiquan Mao',\n",
       "    'Han Guo',\n",
       "    'Weigang Gou',\n",
       "    'Taiqiang Wu',\n",
       "    'Tao Zhu',\n",
       "    'Wenhang Shi',\n",
       "    'Chen Chen',\n",
       "    'Shan Huang',\n",
       "    'Sihong Chen',\n",
       "    'Liqun Liu',\n",
       "    'Feifei Li',\n",
       "    'Xiaoshuai Chen',\n",
       "    'Xingwu Sun',\n",
       "    'Zhanhui Kang',\n",
       "    'Xiaoyong Du',\n",
       "    'Linlin Shen',\n",
       "    'Kimmo Yan'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality (demo)',\n",
       "   'event_ids': ['session-1_-speech-and-multimodality-(demo)-(virtual-poster)'],\n",
       "   'id': 'D55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.20.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78247/poster_document/70c97e48edb7cf29c8c85a55062065e6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78247/poster/6e5a9733fd657aa50d6d2e3cc7a7cebb.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models of Different Modalities',\n",
       "   'tldr': 'Recently, the success of pre-training in text domain has been fully extended to vision, audio, and cross-modal scenarios. The proposed pre-training models of different modalities are showing a rising trend of homogeneity in their model structures, which brings the opportunity to implement different ...',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'underline_id': 78247,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/78247-demo-tencentpretrain-a-scalable-and-flexible-toolkit-for-pre-training-models-of-different-modalities',\n",
       "   'video_url': None},\n",
       "  'D56': {'abstract': 'Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search. We present NeuroX, a comprehensive open-source toolkit to conduct neuron analysis of natural language processing models. It implements various interpretation methods under a unified API, and provides a framework for data processing and evaluation, thus making it easier for researchers and practitioners to perform neuron analysis. The Python toolkit is available at https://www.github.com/fdalvi/NeuroX.\\n\\nDemo Video available at: https://youtu.be/mLhs2YMx4u8',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.21',\n",
       "   'authors': ['Fahim Dalvi', 'Hassan Sajjad', 'Nadir Durrani'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'event_ids': ['demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.21.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78248/poster_document/c3d7e261fc98ade469cecb05e8b6671f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78248/poster/39d96c7b3e78d329c60d472b28594883.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] NeuroX Library for Neuron Analysis of Deep NLP Models',\n",
       "   'tldr': 'Neuron analysis provides insights into how knowledge is structured in representations and discovers the role of neurons in the network. In addition to developing an understanding of our models, neuron analysis enables various applications such as debiasing, domain adaptation and architectural search...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'underline_id': 78248,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78248-demo-neurox-library-for-neuron-analysis-of-deep-nlp-models',\n",
       "   'video_url': None},\n",
       "  'D63': {'abstract': 'Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s). Our assistive tool for literature discovery and scientific writing is available at https://scilit.vercel.app',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.22',\n",
       "   'authors': ['Nianlong Gu', 'Richard H.R. Hahnloser'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-1_-generation-(demo)-(poster)'],\n",
       "   'id': 'D63',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.22.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78249/poster_document/6a9acf6ff41d3945a697f9e8b3e201b9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78249/poster/cd85ccbb11b4fd64ed65abf1739d06dd.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation',\n",
       "   'tldr': 'Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes. Although in many workflows these processes are serially linked, there are opportunities for natural language processing (NLP) to provide end-to-end assistive tools. We propose SciL...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78249,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78249-demo-scilit-a-platform-for-joint-scientific-literature-discovery-summarization-and-citation-generation',\n",
       "   'video_url': None},\n",
       "  'D66': {'abstract': 'In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 languages ranging from Afrikaans to Yiddish. We provide multiple event-centric views of extracted events, including both a graphical representation and a document-level summary. We also integrate existing cross-lingual search algorithms with event extraction capabilities to provide cross-lingual event-centric search, allowing English-speaking users to search over events automatically extracted from a corpus of non-English documents, using either English natural language queries (e.g. \"cholera outbreaks in Iran\") or structured queries (e.g. find all events of type Disease-Outbreak with agent \"cholera\" and location \"Iran\").',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.23',\n",
       "   'authors': ['Chris Jenkins',\n",
       "    'Shantanu Agarwal',\n",
       "    'Joel Barry',\n",
       "    'Steven Fincke',\n",
       "    'Elizabeth Boschee'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'event_ids': ['demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)'],\n",
       "   'id': 'D66',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.23.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78250/poster_document/a948fc4b7504c5ebc34361c45ea60480.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78250/poster/897d1ccd491a01833d40451f82cf5165.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Massively Multi-Lingual Event Understanding: Extraction, Visualization, and Search',\n",
       "   'tldr': 'In this paper, we present ISI-Clear, a state-of-the-art, cross-lingual, zero-shot event extraction system and accompanying user interface for event visualization & search. Using only English training data, ISI-Clear makes global events available on-demand, processing user-supplied text in 100 langua...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'underline_id': 78250,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78250-demo-massively-multi-lingual-event-understanding-extraction-visualization-and-search',\n",
       "   'video_url': None},\n",
       "  'D69': {'abstract': 'In this paper, we present our open-source neural machine translation (NMT) toolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning and enables easy pre-training and fine-tuning of sequence-to-sequence models at scale. It can be used for training parameter-heavy models with minimal parameter sharing and efficient, lightweight models via heavy parameter sharing. Additionally, it supports parameter-efficient fine-tuning (PEFT) through adapters and prompts. Our toolkit also comes with a user interface that can be used to demonstrate these models and visualize various parts of the model. Apart from these core features, our toolkit also provides other advanced functionalities such as but not limited to document/multi-source NMT, simultaneous NMT, mixtures-of-experts, model compression and continual learning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.24',\n",
       "   'authors': ['Raj Dabre',\n",
       "    'Diptesh Kanojia',\n",
       "    'Chinmay Sawant',\n",
       "    'Eiichiro Sumita'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation (demo)',\n",
       "   'event_ids': ['demo-session-3_-machine-translation-(demo)-(poster)'],\n",
       "   'id': 'D69',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.24.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78251/poster_document/3e32999b79777cdaef609fdc13be204d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78251/poster/11f95f7deb193343c8318ec05ea32052.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] YANMTT: Yet Another Neural Machine Translation Toolkit',\n",
       "   'tldr': 'In this paper, we present our open-source neural machine translation (NMT) toolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as YANMTT - https://github.com/prajdabre/yanmtt, which is built on top of the HuggingFace Transformers library. YANMTT focuses on transfer learning a...',\n",
       "   'track': 'Machine Translation (demo)',\n",
       "   'underline_id': 78251,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78251-demo-yanmtt-yet-another-neural-machine-translation-toolkit',\n",
       "   'video_url': None},\n",
       "  'D71': {'abstract': \"NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on the behavior, then\\nusing the feedback to update the model. While existing model debugging methods have shown promise, their prototype-level implementations provide limited practical utility. Thus, we propose XMD: the first open-source, end-to-end framework for explanation-based model debugging. Given task- or instance-level explanations,\\nusers can flexibly provide various forms of feedback via an intuitive, web-based UI. After receiving user feedback, XMD automatically updates the model in real time, by regularizing the model so that its explanations\\nalign with the user feedback. The new model can then be easily deployed into real-world applications via Hugging Face. Using XMD, we can improve the model's OOD performance on text classification tasks by up to 18%.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.25',\n",
       "   'authors': ['Dong-Ho Lee',\n",
       "    'Akshen Kadakia',\n",
       "    'Brihi Joshi',\n",
       "    'Aaron Chan',\n",
       "    'Ziyi Liu',\n",
       "    'Kiran Narahari',\n",
       "    'Takashi Shibuya',\n",
       "    'Ryosuke Mitani',\n",
       "    'Toshiyuki Sekiya',\n",
       "    'Jay Pujara',\n",
       "    'Xiang Ren'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'event_ids': ['demo-session-6_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)'],\n",
       "   'id': 'D71',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.25.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78252/poster_document/23c230112191fbf9428a0566adac21a3.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] XMD: An End-to-End Framework for Interactive Explanation-Based Debugging of NLP Models',\n",
       "   'tldr': 'NLP models are susceptible to learning spurious biases (i.e., bugs) that work on some datasets but do not properly reflect the underlying task. Explanation-based model debugging aims to resolve spurious biases by showing human users explanations of model behavior, asking users to give feedback on th...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "   'underline_id': 78252,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78252-demo-xmd-an-end-to-end-framework-for-interactive-explanation-based-debugging-of-nlp-models',\n",
       "   'video_url': None},\n",
       "  'D74': {'abstract': \"The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as ``delta tuning'' in Ding et al. (2022), which updates only a small subset of parameters, known as ``delta modules'', while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, modular, and extensible, providing a comprehensive platform for researchers and practitioners to adapt large PTMs efficiently.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.26',\n",
       "   'authors': ['Shengding Hu',\n",
       "    'Ning Ding',\n",
       "    'Weilin Zhao',\n",
       "    'Xingtai Lv',\n",
       "    'Zhen Zhang',\n",
       "    'Zhiyuan Liu',\n",
       "    'Maosong Sun'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models (demo)',\n",
       "   'event_ids': ['demo-session-1_-large-language-models-(demo)-(poster)'],\n",
       "   'id': 'D74',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.26.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models',\n",
       "   'tldr': 'The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as ``del...',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'underline_id': 78253,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78253-opendelta-a-plug-and-play-library-for-parameter-efficient-adaptation-of-pre-trained-models',\n",
       "   'video_url': None},\n",
       "  'D77': {'abstract': \"Information extraction systems often produce\\nhundreds to thousands of strings on a specific\\ntopic. We present a method that facilitates\\nbetter consumption of these strings, in an ex-\\nploratory setting in which a user wants to both\\nget a broad overview of what's available, and a\\nchance to dive deeper on some aspects. The sys-\\ntem works by grouping similar items together,\\nand arranging the remaining items into a hierar-\\nchical navigable DAG structure. We apply the\\nmethod to medical information extraction.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.27',\n",
       "   'authors': ['Itay Yair', 'Hillel Taub-Tabib', 'Yoav Goldberg'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models (demo)',\n",
       "   'event_ids': ['demo-session-5_-large-language-models-(demo)-(poster)'],\n",
       "   'id': 'D77',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.27.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78254/poster_document/289988e9476b8e358fc61069a3996a78.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78254/poster/37b02dd0866ffd970b92226939e9f743.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation',\n",
       "   'tldr': \"Information extraction systems often produce\\nhundreds to thousands of strings on a specific\\ntopic. We present a method that facilitates\\nbetter consumption of these strings, in an ex-\\nploratory setting in which a user wants to both\\nget a broad overview of what's available, and a\\nchance to dive deeper...\",\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'underline_id': 78254,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78254-demo-hierarchy-builder-organizing-textual-spans-into-a-hierarchy-to-facilitate-navigation',\n",
       "   'video_url': None},\n",
       "  'D78': {'abstract': 'Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and reading. CARE facilitates data collection for inline commentaries in a commonplace collaborative reading environment, and provides a framework for enhancing reading with NLP-based assistance, such as text classification, generation or question answering. The extensible behavioral logging allows unique insights into the reading and commenting behavior, and flexible configuration makes the platform easy to deploy in new scenarios. To evaluate CARE in action, we apply the platform in a user study dedicated to scholarly peer review. CARE facilitates the data collection and study of inline commentary in NLP, extrinsic evaluation of NLP assistance, and application prototyping. We invite the community to explore and build upon the open source implementation of CARE.\\n\\nGithub Repository: https://github.com/UKPLab/CARE\\nPublic Live Demo: https://care.ukp.informatik.tu-darmstadt.de',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.28',\n",
       "   'authors': ['Dennis Zyska',\n",
       "    'Nils Dycke',\n",
       "    'Jan Buchmann',\n",
       "    'Ilia Kuznetsov',\n",
       "    'Iryna Gurevych'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['demo-session-3_-nlp-applications-(demo)-(poster)'],\n",
       "   'id': 'D78',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.28.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78255/poster_document/8c29cf0e6cfa348f5b96c1f57bb42e61.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78255/poster/af0cfb0db2d4839f351c6870e14c66fb.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] CARE: Collaborative AI-Assisted Reading Environment',\n",
       "   'tldr': 'Recent years have seen impressive progress in AI-assisted writing, yet the developments in AI-assisted reading are lacking. We propose inline commentary as a natural vehicle for AI-based reading assistance, and present CARE: the first open integrated platform for the study of inline commentary and r...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78255,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78255-demo-care-collaborative-ai-assisted-reading-environment',\n",
       "   'video_url': None},\n",
       "  'D80': {'abstract': 'ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offering both fuzzy and exact search capabilities. ROOTS is the largest corpus to date that can be investigated this way. The ROOTS Search Tool is open-sourced and available on Hugging Face Spaces: https://huggingface.co/spaces/bigscience-data/roots-search. We describe our implementation and the possible use cases of our tool.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.29',\n",
       "   'authors': ['Aleksandra Piktus',\n",
       "    'Christopher Akiki',\n",
       "    'Paulo Villegas',\n",
       "    'Hugo Laurenon',\n",
       "    'Grard Dupont',\n",
       "    'Sasha Luccioni',\n",
       "    'Yacine Jernite',\n",
       "    'Anna Rogers'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models (demo)',\n",
       "   'event_ids': ['demo-session-5_-large-language-models-(demo)-(poster)'],\n",
       "   'id': 'D80',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.29.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78256/poster_document/b0b6637c606d72ff0b89e5a4262806e5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78256/poster/18ae996c11c98a2fb135ba2935cc2972.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] The ROOTS Search Tool: Data Transparency for LLMs',\n",
       "   'tldr': 'ROOTS is a 1.6TB multilingual text corpus developed for the training of BLOOM, currently the largest language model explicitly accompanied by commensurate data governance efforts. In continuation of these efforts, we present the ROOTS Search Tool: a search engine over the entire ROOTS corpus offerin...',\n",
       "   'track': 'Large Language Models (demo)',\n",
       "   'underline_id': 78256,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78256-demo-the-roots-search-tool-data-transparency-for-llms',\n",
       "   'video_url': None},\n",
       "  'D84': {'abstract': 'The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS-MT and Tatoeba models as well as external models from the Huggingface repository and user-contributed translations. The functionalities of the evaluation tool include summaries of benchmarks for over 2,300 models covering 4,560 language directions and 294 languages, as well as the inspection of predicted translations against their human reference. We focus on centralization, reproducibility and coverage of MT evaluation combined with scalability. The dashboard can be accessed live at https://opus.nlpl.eu/dashboard/.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.30',\n",
       "   'authors': ['Jrg Tiedemann', 'Ona de Gibert'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation (demo)',\n",
       "   'event_ids': ['demo-session-6_-resources-and-evaluation-(demo)-(poster)'],\n",
       "   'id': 'D84',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.30.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78257/poster_document/ea963148565292bf2d83ce95acec1d85.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] The OPUS-MT Dashboard  A Toolkit for a Systematic Evaluation of Open Machine Translation Models',\n",
       "   'tldr': 'The OPUS-MT dashboard is a web-based platform that provides a comprehensive overview of open translation models. We focus on a systematic collection of benchmark results with verifiable translation performance and large coverage in terms of languages and domains. We provide results for in-house OPUS...',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'underline_id': 78257,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15265/poster/78257-learning-neuro-symbolic-world-models-with-conversational-proprioception',\n",
       "   'video_url': None},\n",
       "  'D85': {'abstract': \"This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in which the discourses of contemporary societies are encoded. To provide meaningful insights from such data, our system leverages and combines state-of-the-art machine learning technologies from Natural Language Processing and Com-\\nputer Vision. Further, the DWTS is conceived and developed by an interdisciplinary team of\\ncultural anthropologists and computer scientists to ensure the tool's usability for modern\\nDH research. Central features of the DWTS are: a) import of multi-modal data like text, image, audio, and video b) preprocessing pipelines for automatic annotations c) lexical and semantic search of documents d) manual span, bounding box, time-span, and frame annotations e) documentation of the research process.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.31',\n",
       "   'authors': ['Florian Schneider',\n",
       "    'Tim Fischer',\n",
       "    'Fynn Petersen-Frey',\n",
       "    'Isabel Eiser',\n",
       "    'Gertraud Koch',\n",
       "    'Chris Biemann'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality (demo)',\n",
       "   'event_ids': ['demo-session-2_-speech-and-multimodality-(demo)-(poster)'],\n",
       "   'id': 'D85',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.31.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78258/poster_document/6a9a3f33580712622bb1d6756b7b58ab.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78258/poster/ae8fa86a2d15dca9be204fd8cbd0b697.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] The D-WISE Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis',\n",
       "   'tldr': 'This work introduces the D-WISE Tool Suite (DWTS), a novel working environment for digital qualitative discourse analysis in the Digital Humanities (DH). The DWTS addresses limitations of current DH tools induced by the ever-increasing amount of heterogeneous, unstructured, and multi-modal data in w...',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'underline_id': 78258,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78258-sub-character-tokenization-for-chinese-pretrained-language-models',\n",
       "   'video_url': None},\n",
       "  'D89': {'abstract': 'There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a lack of unified, standardized, fair, and comprehensive comparison between methods. This paper presents OpenRT, the first open-source framework for reasoning over tabular data, to reproduce existing table pre-training models for performance comparison and develop new models quickly. We implemented and compared six table pre-training models on four question answering, one fact checking, and one faithful text generation datasets. Moreover, to enable the community to easily construct new table reasoning datasets, we developed TaRAT, an annotation tool which supports multi-person collaborative annotations for various kinds of table reasoning tasks. The researchers are able to deploy the newly-constructed dataset to OpenRT and compare the performances of different baseline systems.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.32',\n",
       "   'authors': ['Yilun Zhao',\n",
       "    'Boyu Mi',\n",
       "    'Zhenting Qi',\n",
       "    'Linyong Nan',\n",
       "    'Minghao Guo',\n",
       "    'Arman Cohan',\n",
       "    'Dragomir Radev'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction (demo)',\n",
       "   'event_ids': ['demo-session-3_-information-extraction-(demo)-(poster)'],\n",
       "   'id': 'D89',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.32.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78259/poster_document/5434aea21322285c28af9f6b133c5cf7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78259/poster/255a27020e55344a879d6484255e220b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] OpenRT: An Open-source Framework for Reasoning Over Tabular Data',\n",
       "   'tldr': 'There are a growing number of table pre-training methods proposed for reasoning over tabular data (e.g., question answering, fact checking, and faithful text generation). However, most existing methods are benchmarked solely on a limited number of datasets, varying in configuration, which leads to a...',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'underline_id': 78259,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15228/poster/78259-cross-lingual-amr-aligner-paying-attention-to-cross-attention',\n",
       "   'video_url': None},\n",
       "  'D9': {'abstract': 'Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3. We first describe the different modules of our system, including prompting to generate schematic elements, manual edit of those elements, and conversion of those into a schema graph. By qualitatively comparing our system to previous ones, we show that our system not only transfers to new domains more easily than previous approaches, but also reduces efforts of human curation thanks to our interactive interface.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.1',\n",
       "   'authors': ['Tianyi Zhang',\n",
       "    'Isaac Tham',\n",
       "    'Zhaoyi Hou',\n",
       "    'Jiaxuan Ren',\n",
       "    'Leon Zhou',\n",
       "    'Hainiu Xu',\n",
       "    'Li Zhang',\n",
       "    'Lara Martin',\n",
       "    'Rotem Dror',\n",
       "    'Sha Li',\n",
       "    'Heng Ji',\n",
       "    'Martha Palmer',\n",
       "    'Susan Windisch Brown',\n",
       "    'Reece Suchocki',\n",
       "    'Chris Callison-Burch'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications (demo)',\n",
       "   'event_ids': ['session-1_-nlp-applications-(demo)-(virtual-poster)'],\n",
       "   'id': 'D9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.1.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78228/poster_document/2048a211b156ef75f41ae50bee8d1495.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78228/poster/7eac33a3773fb3e4111ccd2f18de7ad8.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Human-in-the-loop Schema Induction',\n",
       "   'tldr': 'Schema induction builds a graph representation explaining how events unfold in a scenario. Existing approaches have been based on information retrieval (IR) and information extraction (IE), often with limited human curation. We demonstrate a human-in-the-loop schema induction system powered by GPT-3...',\n",
       "   'track': 'NLP Applications (demo)',\n",
       "   'underline_id': 78228,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/78228-demo-human-in-the-loop-schema-induction',\n",
       "   'video_url': None},\n",
       "  'D90': {'abstract': 'This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes the data format, and exposes functionalities to facilitates data manipulation and the evaluation of custom models. We also present the results of tests conducted with available Italian and multilingual language models on UINAUIL, providing an updated picture of the current state of the art in Italian NLU.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.33',\n",
       "   'authors': ['Valerio Basile',\n",
       "    'Livio Bioglio',\n",
       "    'Alessio Bosca',\n",
       "    'Cristina Bosco',\n",
       "    'Viviana Patti'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation (demo)',\n",
       "   'event_ids': ['demo-session-5_-resources-and-evaluation-(demo)-(poster)'],\n",
       "   'id': 'D90',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.33.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78260/poster_document/1f509fcd6b98e184567fa5446d08e7e4.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] UINAUIL: A Unified Benchmark for Italian Natural Language Understanding',\n",
       "   'tldr': 'This paper introduces the Unified Interactive Natural Understanding of the Italian Language (UINAUIL), a benchmark of six tasks for Italian Natural Language Understanding. We present a description of the tasks and software library that collects the data from the European Language Grid, harmonizes th...',\n",
       "   'track': 'Resources and Evaluation (demo)',\n",
       "   'underline_id': 78260,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15255/poster/78260-decoding-symbolism-in-language-models',\n",
       "   'video_url': None},\n",
       "  'D91': {'abstract': 'The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. With the advent of large pretrained language models, several novel methods have been proposed, resulting in substantial improvements in ZSL performance. There is a growing demand, both in the research community and industry, for a comprehensive ZSL framework that facilitates the development and accessibility of the latest methods and pretrained models.\\nIn this study, we propose a novel ZSL framework called Zshot that aims to address the aforementioned challenges. Our primary objective is to provide a platform that allows researchers to compare different state-of-the-art ZSL methods with standard benchmark datasets. Additionally, we have designed our framework to support the industry with readily available APIs for production under the standard SpaCy NLP pipeline. Our API is extendible and evaluable, moreover, we include numerous enhancements such as boosting the accuracy with pipeline ensembling and visualization utilities available as a SpaCy extension.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.34',\n",
       "   'authors': ['Gabriele Picco',\n",
       "    'Marcos Martinez Galindo',\n",
       "    'Alberto Purpura',\n",
       "    'Leopold Fuchs',\n",
       "    'Vanessa Lopez',\n",
       "    'Thanh Lam Hoang'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction (demo)',\n",
       "   'event_ids': ['demo-session-2_-information-extraction-(demo)-(poster)'],\n",
       "   'id': 'D91',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.34.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78261/poster_document/63e66c6529a7002dcdc84495a84ec4ec.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78261/poster/5a8805657fad470b9b4d3aabcffbb762.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Zshot: An Open-source Framework for Zero-Shot Named Entity Recognition and Relation Extraction',\n",
       "   'tldr': 'The Zero-Shot Learning (ZSL) task pertains to the identification of entities or relations in texts that were not seen during training. ZSL has emerged as a critical research area due to the scarcity of labeled data in specific domains, and its applications have grown significantly in recent years. W...',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'underline_id': 78261,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15208/poster/78261-towards-parameter-efficient-integration-of-pre-trained-language-models-in-temporal-video-grounding',\n",
       "   'video_url': None},\n",
       "  'D93': {'abstract': 'In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. These technologies often allow users to access external resources, such as dictionaries, terminologies or bilingual concordancers, thereby interrupting and considerably hindering the writing process. In addition, CAT systems assume that the source sentence is fixed and also restrict the possible changes on the target side. In order to make the writing process smoother, we present BiSync, a bilingual writing assistant that allows users to freely compose text in two languages, while maintaining the two monolingual texts synchronized. We also include additional functionalities, such as the display of alternative prefix translations and paraphrases, which are intended to facilitate the authoring of texts. We detail the model architecture used for synchronization and evaluate the resulting tool, showing that high accuracy can be attained with limited computational resources. The interface and models are publicly available at https://github.com/jmcrego/BiSync and a demonstration video can be watched on YouTube https://youtu.be/_l-ugDHfNgU.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.35',\n",
       "   'authors': ['Josep Crego', 'Jitao Xu', 'Franois Yvon'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'event_ids': ['demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)'],\n",
       "   'id': 'D93',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.35.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78262/poster_document/bf2b2f8f29c382a00ac8af003e172a9e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78262/poster/08a7e8c052c7fa7837a7bad7cb0e32b2.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] BiSync: A Bilingual Editor for Synchronized Monolingual Texts',\n",
       "   'tldr': 'In our globalized world, a growing number of situations arise where people are required to communicate in one or several foreign languages. In the case of written communication, users with a good command of a foreign language may find assistance from computer-aided translation (CAT) technologies. Th...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "   'underline_id': 78262,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78262-demo-bisync-a-bilingual-editor-for-synchronized-monolingual-texts',\n",
       "   'video_url': None},\n",
       "  'D94': {'abstract': 'Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.36',\n",
       "   'authors': ['Maria Antoniak',\n",
       "    'Anjalie Field',\n",
       "    'Jimin Mun',\n",
       "    'Melanie Walsh',\n",
       "    'Lauren Klein',\n",
       "    'Maarten Sap'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction (demo)',\n",
       "   'event_ids': ['demo-session-4_-information-extraction-(demo)-(poster)'],\n",
       "   'id': 'D94',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.36.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78263/poster_document/9fe7c726aeb3d9888b1509b184203268.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78263/poster/b2bf3f059a433589bc98641bbd6dc8ec.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Riveter: Measuring Power and Social Dynamics Between Entities',\n",
       "   'tldr': 'Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a ...',\n",
       "   'track': 'Information Extraction (demo)',\n",
       "   'underline_id': 78263,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15238/poster/78263-demo-riveter-measuring-power-and-social-dynamics-between-entities',\n",
       "   'video_url': None},\n",
       "  'D95': {'abstract': 'The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text \"whi te space correctio nwithTransf or mers\", produce \"whitespace correction with Transformers\". We compare two Transformer-based models, a character-level encoder-decoder model and a byte-level encoder-only model. We find that the encoder-only model is both faster and achieves higher quality. We provide an easy-to-use tool that is over 900 times faster than the previous best tool, with the same high quality. Our tool repairs text at a rate of over 200 kB/s on GPU, with a sequence-averaged F1-score ranging from 87.5% for hard-to-correct text up to 99% for text without any spaces.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.37',\n",
       "   'authors': ['Hannah Bast', 'Matthias Hertel', 'Sebastian Walter'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation (demo)',\n",
       "   'event_ids': ['demo-session-1_-generation-(demo)-(poster)'],\n",
       "   'id': 'D95',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.37.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/78264/poster_document/b1a52195f8f98e1e9d7d1c33e30fb6f8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/78264/poster/46edfc195c93135079994b3f445c828b.jpg',\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] Fast Whitespace Correction with Encoder-Only Transformers',\n",
       "   'tldr': 'The goal of whitespace correction is to fix space errors in arbitrary given text. For example, given the text \"whi te space correctio nwithTransf or mers\", produce \"whitespace correction with Transformers\". We compare two Transformer-based models, a character-level encoder-decoder model and a byte-l...',\n",
       "   'track': 'Generation (demo)',\n",
       "   'underline_id': 78264,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78264-how-many-answers-should-i-givequestion-an-empirical-study-of-multi-answer-reading-comprehension',\n",
       "   'video_url': None},\n",
       "  'D96': {'abstract': 'ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-demo.38',\n",
       "   'authors': ['Brian Yan',\n",
       "    'Jiatong Shi',\n",
       "    'Yun Tang',\n",
       "    'Hirofumi Inaguma',\n",
       "    'Yifan Peng',\n",
       "    'Siddharth Dalmia',\n",
       "    'Peter Polak',\n",
       "    'Patrick Fernandes',\n",
       "    'Dan Berrebbi',\n",
       "    'Tomoki Hayashi',\n",
       "    'Xiaohui Zhang',\n",
       "    'Zhaoheng Ni',\n",
       "    'Moto Hira',\n",
       "    'Soumi Maiti',\n",
       "    'Juan Pino',\n",
       "    'Shinji Watanabe'],\n",
       "   'category': 'Demo',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality (demo)',\n",
       "   'event_ids': ['demo-session-1_-speech-and-multimodality-(demo)-(poster)'],\n",
       "   'id': 'D96',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-demo.38.pdf',\n",
       "   'paper_type': 'demo',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Demo',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Demo] ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit',\n",
       "   'tldr': 'ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech tra...',\n",
       "   'track': 'Speech and Multimodality (demo)',\n",
       "   'underline_id': 78265,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15202/poster/78265-espnet-st-v2-multipurpose-spoken-language-translation-toolkit',\n",
       "   'video_url': None},\n",
       "  'DialDoc_10': {'abstract': 'Multilingual  document-grounded dialogue, where the system is required to generate responses based on both the conversation Multilingual  context and external knowledge sources.\\nTraditional pipeline methods for knowledge identification and response generation, while effective in certain scenarios, suffer from error propagation issues and fail to capture the interdependence between these two sub-tasks. To overcome these challenges, we propose the application of the SLDT method, which treats passage-knowledge selection as a sequential decision process rather than a single-step decision process.\\nWe achieved winner 3rd in dialdoc 2023 and we also validated the effectiveness of our method on other datasets. The ablation experiment also shows that our method significantly improves the basic model compared to other methods.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhanyu Ma', 'Zeming Liu', 'Jian Ye'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SLDT: Sequential Latent Document Transformer for Multilingual Document-based Dialogue',\n",
       "   'tldr': 'Multilingual  document-grounded dialogue, where the system is required to generate responses based on both the conversation Multilingual  context and external knowledge sources.\\nTraditional pipeline methods for knowledge identification and response generation, while effective in certain scenarios, s',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_12': {'abstract': 'In healthcare, the ability to care for oneself is reflected in the \"Activities of Daily Living (ADL),\" which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need of support, assistance programs continuously evaluate participants\\' functioning across various domains. However, the assessment process may encounter consistency issues when multiple assessors with varying levels of expertise are involved. Novice assessors, in particular, may lack the necessary preparation for real-world interactions with participants. To address this issue, we developed a dialogue system that simulates interactions between assessors and individuals of varying functioning in a natural and reproducible way. The dialogue system consists of two major modules, one for natural language understanding (NLU) and one for natural language generation (NLG), respectively. In order to generate responses consistent with the underlying knowledge base, the dialogue system requires both an understanding of the user\\'s query and of biographical details of an individual being simulated. To fulfill this requirement, we experimented with query classification and generated responses based on those biographical details using some recently released InstructGPT-like models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhecheng Sheng',\n",
       "    'Raymond Finzel',\n",
       "    'lucke096@umn.edu lucke096@umn.edu',\n",
       "    'gahmx008@umn.edu gahmx008@umn.edu',\n",
       "    'Maria Gini',\n",
       "    'Serguei Pakhomov'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge',\n",
       "   'tldr': 'In healthcare, the ability to care for oneself is reflected in the \"Activities of Daily Living (ADL),\" which serve as a measure of functional ability (functioning). A lack of functioning may lead to poor living conditions requiring personal care and assistance. To accurately identify those in need o',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_14': {'abstract': 'Collecting and constructing human-annotated corpora for training conversational question-answering (CQA) models has recently been shown to be inefficient and costly. To solve this problem, previous works have proposed training QA models with automatically generated QA data. In this work, we extend earlier studies on QA synthesis, and propose an efficient QA data generation algorithm under conversational settings. Our model recognizes potential dialogue topics, generates corresponding questions, and extracts answers from grounding passages. To improve the quality of generated QAs and downstream self-training of CQA models, we propose dropout and agreement-based QA selection methods. We conduct experiments on both data augmentation and domain adaptation settings. Experiments on the QuAC and Doc2Dial tasks show that the proposed method can significantly improve the quality of generated QA data, and also improves the accuracy of self-trained CQA models based on the constructed training corpora.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tianhua Zhang',\n",
       "    'Liping Tang',\n",
       "    'Wei Fang',\n",
       "    'Hongyin Luo',\n",
       "    'Xixin Wu',\n",
       "    'Helen Meng',\n",
       "    'James Glass'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ConvRGX: Recognition, Generation, and Extraction for Self-trained Conversational Question Answering',\n",
       "   'tldr': 'Collecting and constructing human-annotated corpora for training conversational question-answering (CQA) models has recently been shown to be inefficient and costly. To solve this problem, previous works have proposed training QA models with automatically generated QA data. In this work, we extend e',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_15': {'abstract': 'The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Srinivas Gowriraj',\n",
       "    'Soham Dinesh Tiwari',\n",
       "    'Mitali Potnis',\n",
       "    'Srijan Bansal',\n",
       "    'Teruko Mitamura',\n",
       "    'Eric Nyberg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_15',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA',\n",
       "   'tldr': 'The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-e',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_18': {'abstract': \"Crowd-sourcing has been one of the primary ways to curate conversational data, specially for certain scenarios like grounding in knowledge. In this setting, using online platforms like AMT, non-expert participants are hired to converse with each other, following  instructions which try to guide the outcome towards the desired format. The resulting data then is used for different parts of dialog modelling like knowledge selection and response selection/generation.\\nIn this work, we take a closer look into two of the most popular knowledge grounded dialog (KGD) datasets. Investigating potential biases and artefacts in knowledge selection labels, we observe that in many cases the `knowledge selection flow' simply follows the order of presented knowledge pieces. In Wizard of Wikipedia (the most popular KGD dataset) we use simple content-agnostic models based on this bias to get significant knowledge selection performance. In Topical-Chat we see a similar correlation between the knowledge selection sequence and the order of entities and their segments, as provided to crowd-source workers. We believe that the observed results, question the significance and origin of the presumed dialog-level attributes like `knowledge flow' in these crowd-sourced datasets. \",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ehsan Lotfi',\n",
       "    'Maxime De Bruyn',\n",
       "    'jeska.buhmann@uantwerpen.be jeska.buhmann@uantwerpen.be',\n",
       "    'Walter Daelemans'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Follow the Knowledge: Structural Biases and Artefacts in Knowledge Grounded Dialog Datasets',\n",
       "   'tldr': 'Crowd-sourcing has been one of the primary ways to curate conversational data, specially for certain scenarios like grounding in knowledge. In this setting, using online platforms like AMT, non-expert participants are hired to converse with each other, following  instructions which try to guide the ',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_2': {'abstract': 'This paper proposes a framework to address the issue of data scarcity in Document-Grounded Dialogue Systems(DGDS). Our model leverages high-resource languages to enhance the capability of dialogue generation in low-resource languages. Specifically, We present a novel pipeline CLEM (Cross-Lingual Enhanced Model) including adversarial training retrieval (Retriever and Re-ranker), and Fid (fusion-in-decoder) generator. To further leverage high-resource language, we also propose an innovative architecture to conduct alignment across different languages with translated training. Extensive experiment results demonstrate the effectiveness of our model and we achieved 4th place in the DialDoc 2023 Competition. Therefore, CLEM can serve as a solution to resource scarcity in DGDS and provide useful guidance for multi-lingual alignment tasks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Qi Gou', 'Zehua Xia', 'Wenzhe Du'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Cross-lingual Data Augmentation for Document-grounded Dialog Systems in Low Resource Languages',\n",
       "   'tldr': 'This paper proposes a framework to address the issue of data scarcity in Document-Grounded Dialogue Systems(DGDS). Our model leverages high-resource languages to enhance the capability of dialogue generation in low-resource languages. Specifically, We present a novel pipeline CLEM (Cross-Lingual Enh',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_3': {'abstract': 'Previous research on open-domain question answering (QA) mainly focuses on questions with short answers. However, information-seeking QA often requires various formats of answers depending on the nature of the questions, e.g., why/how questions typically require a long answer. In this paper, we present MoQA, a benchmark for open-domain QA that requires building one system that can provide short, medium, long, and yes/no answers to different questions accordingly. MoQA builds upon Natural Questions with multiple types of questions and additional crowdsourcing efforts to ensure high query quality. We adapt state-of-the-art models, and reveal unique findings in multi-type open-domain QA: (1) For retriever-reader models, training one retriever on all types achieves the overall best performance, but it is challenging to train one reader model to output answers of different formats, or to train a question classifier to distinguish between types; (2) An end-to-end closed-book QA model trained on multiple types struggles with the task across the board; (3) State-of-the-art large language models such as the largest GPT-3 models (Brown et al., 2020; Ouyang et al., 2022) also lag behind open-book QA models. Our benchmark and analysis call for more effort into building versatile open-domain QA models in the future. ',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Howard Yen', 'Tianyu Gao', 'Jinhyuk Lee', 'Danqi Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MoQA: Benchmarking Multi-Type Open-Domain Question Answering',\n",
       "   'tldr': 'Previous research on open-domain question answering (QA) mainly focuses on questions with short answers. However, information-seeking QA often requires various formats of answers depending on the nature of the questions, e.g., why/how questions typically require a long answer. In this paper, we pres',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_4': {'abstract': 'Transferring DGD models from high-resource languages to low-resource languages is a meaningful but challenging task. Being able to provide multilingual responses to multilingual documents further complicates the task. This paper describes our method at DialDoc23 Shared Task (Document-Grounded Dialogue and Conversational Question Answering) for generate responses based on the most relevant passage retrieved. We divide it into three steps of retrieval, re-ranking and generation. Our methods include negative sample augmentation, prompt learning, pseudo-labeling and ensemble. On the submission page, we rank 2nd based on the sum\\xa0of token-level F1, SacreBleu\\xa0and Rouge-L scores used for the final evaluation, and get the total score of 210.25.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiaocheng Zhang', 'Huang Qing', 'Fu Lin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploration of multilingual prompts in document-grounded dialogue',\n",
       "   'tldr': 'Transferring DGD models from high-resource languages to low-resource languages is a meaningful but challenging task. Being able to provide multilingual responses to multilingual documents further complicates the task. This paper describes our method at DialDoc23 Shared Task (Document-Grounded Dialog',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_5': {'abstract': \"With the power of large pretrained language models, various research works have integrated knowledge into dialogue systems. The traditional techniques treat knowledge as part of the input sequence for the dialogue system, prepending a set of knowledge statements in front of dialogue history.\\nHowever, such a mechanism forces knowledge sets to be concatenated in an ordered manner, making models implicitly pay imbalanced attention to the sets during training.\\nIn this paper, we first investigate how the order of the knowledge set can influence autoregressive dialogue systems' responses. \\nWe conduct experiments on two commonly used dialogue datasets with two types of transformer-based models and find that models view the input knowledge unequally. \\nTo this end, we propose a simple and novel technique to alleviate the order effect by modifying the position embeddings of knowledge input in these models. With the proposed position embedding method, the experimental results show that each knowledge statement is uniformly considered to generate responses.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hsuan Su',\n",
       "    'Shachi H. Kumar',\n",
       "    'Sahisnu Mazumder',\n",
       "    'Wenda Chen',\n",
       "    'Ramesh Manuvinakurike',\n",
       "    'Eda Okur',\n",
       "    'Saurav Sahay',\n",
       "    'Lama Nachman',\n",
       "    'Shang-Tse Chen',\n",
       "    'Hung-yi Lee'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Position Matters! Empirical Study of Order Effect in Knowledge-grounded Dialogue',\n",
       "   'tldr': 'With the power of large pretrained language models, various research works have integrated knowledge into dialogue systems. The traditional techniques treat knowledge as part of the input sequence for the dialogue system, prepending a set of knowledge statements in front of dialogue history.\\nHowever',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_6': {'abstract': \"The Dialdoc23 shared task presents a Multilingual Document-Grounded Dialogue Systems (MDGDS) challenge, where system responses are generated in multiple languages using user's queries, historical dialogue records and relevant passages. A major challenge for this task is the limited training data available in low-resource languages such as French and Vietnamese. In this paper, we propose Cascaded Prompt-based Post-training Models, dividing the task into three subtasks: Retrieval, Reranking and Generation. We conduct post-training on high-resource language such as English and Chinese to enhance performance of low-resource languages by using the similarities of languages. Additionally, we utilize the prompt method to activate model's ability on diverse languages within the dialogue domain and explore which prompt is a good prompt. Our comprehensive experiments demonstrate the effectiveness of our proposed methods, which achieved the first place on the leaderboard with a total score of 215.40 in token-level F1, SacreBleu, and Rouge-L metrics.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jun Liu',\n",
       "    'Shuang Cheng',\n",
       "    'Zineng Zhou',\n",
       "    'Yang Gu',\n",
       "    'Jian Ye',\n",
       "    'Haiyong Luo'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Multilingual Document-Grounded Dialogue Using Cascaded Prompt-Based Post-Training Models',\n",
       "   'tldr': \"The Dialdoc23 shared task presents a Multilingual Document-Grounded Dialogue Systems (MDGDS) challenge, where system responses are generated in multiple languages using user's queries, historical dialogue records and relevant passages. A major challenge for this task is the limited training data ava\",\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'DialDoc_7': {'abstract': 'Document-grounded dialogue generation based on multilingual is a challenging and realistic task. Unlike previous tasks, it need to tackle with multiple high-resource languages facilitating low-resource languages. This paper summarizes our research based on a three-stage pipeline that includes retrieval, re-rank and generation where each component is individually optimized.  In different languages with limited data scenarios, we mainly improve the robustness of the pipeline through data augmentation and embedding perturbation with purpose of improving the performance designing three training methods: cross-language enhancement training, weighted training with neighborhood distribution augmentation, and ensemble adversarial training, all of that can be used as plug and play modules. Through experiments with different settings, it has been shown that our methods can effectively improve the generalization performance of pipeline  with score ranking 6th  among the public submissions on leaderboards.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hai Li', 'Yang Li'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['DialDoc'],\n",
       "   'id': 'DialDoc_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short -',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhanced Training Methods for Multiple Languages',\n",
       "   'tldr': 'Document-grounded dialogue generation based on multilingual is a challenging and realistic task. Unlike previous tasks, it need to tackle with multiple high-resource languages facilitating low-resource languages. This paper summarizes our research based on a three-stage pipeline that includes retrie',\n",
       "   'track': 'Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'I100': {'abstract': 'Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuses on a single metric such as FLOPs or general engine which is not good at auto-regressive decoding. In this paper, we present MobileNMT, a system that can translate in 15MB and 30ms on devices. We propose a series of principles for model compression when combined with quantization. Further, we implement an engine that is friendly to INT8 and decoding. With the co-design of model and engine, compared with the existing system, we speed up 47.0x and save 99.5\\\\% of memory with only 11.6\\\\% loss of BLEU. Our code will be publicly available after the anonymity period.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.36',\n",
       "   'authors': ['Ye Lin',\n",
       "    'Xiaohui Wang',\n",
       "    'Zhexi Zhang',\n",
       "    'Mingxuan Wang',\n",
       "    'Tong Xiao',\n",
       "    'Jingbo Zhu'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I100',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.36.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] MobileNMT: Enabling Translation in 15MB and 30ms',\n",
       "   'tldr': 'Deploying NMT models on mobile devices is essential for privacy, low latency, and offline scenarios. For high model capacity, NMT models are rather large. Running these models on devices is challenging with limited storage, memory, computation, and power consumption. Existing work either only focuse...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79809,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79809-mobilenmt-enabling-translation-in-15mb-and-30ms',\n",
       "   'video_url': None},\n",
       "  'I102': {'abstract': 'Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extracts corresponding key evidences based on multi-doc inputs. To fulfill that purpose, we crafted a salient representation learning method to induce latent salient features, which are effective for joint evidence extraction and summary generation. In order to train this model, we conducted multi-task learning to optimize a composited loss, constructed over extractive and abstractive sub-components in a hierarchical way. We implemented the system based on a ubiquiotously adopted transformer architecture and conducted experimental studies on multiple datasets across two domains, achieving superior performance over the baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.37',\n",
       "   'authors': ['Min Xiao'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I102',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.37.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Multi-doc Hybrid Summarization via Salient Representation Learning',\n",
       "   'tldr': 'Multi-document summarization is gaining more and more attention recently and serves as an invaluable tool to obtain key facts among a large information pool. In this paper, we proposed a multi-document hybrid summarization approach, which simultaneously generates a human-readable summary and extract...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79810,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79810-multi-doc-hybrid-summarization-via-salient-representation-learning',\n",
       "   'video_url': None},\n",
       "  'I104': {'abstract': 'Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100\\\\% accurate labels is difficult, thus many datasets are accompanied by label noise at different levels. Previous work has shown that existing noise-handling methods could not improve the peak performance of BERT on noisy datasets, and might even deteriorate it. In this paper, we propose SaFER, a robust and efficient fine-tuning framework for BERT-based text classifiers, combating label noises without access to any clean data for training or validation. Utilizing a label-agnostic early-stopping strategy and self-supervised learning, our proposed framework achieves superior performance in terms of both accuracy and speed on multiple text classification benchmarks. The trained model is finally fully deployed in several industrial biomedical literature mining tasks and demonstrates high effectiveness and efficiency.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.38',\n",
       "   'authors': ['Zhenting Qi',\n",
       "    'Xiaoyu Tan',\n",
       "    'Chao Qu',\n",
       "    'Yinghui Xu',\n",
       "    'Yuan Qi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I104',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.38.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] SaFER: A Robust and Efficient Framework for Fine-tuning BERT-based Classifier with Noisy Labels',\n",
       "   'tldr': 'Learning on noisy datasets is a challenging problem when pre-trained language models are applied to real-world text classification tasks. In numerous industrial applications, acquiring task-specific datasets with 100\\\\% accurate labels is difficult, thus many datasets are accompanied by label noise a...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79797,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79797-safer-a-robust-and-efficient-framework-for-fine-tuning-bert-based-classifier-with-noisy-labels',\n",
       "   'video_url': None},\n",
       "  'I107': {'abstract': 'In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemical language understanding benchmark datasets consisted of tasks for both patent and literature articles provided by industrial organization. All the datasets are internally made by chemists from scratch. Finally, we evaluate the datasets on the various language models based on BERT and RoBERTa, and demonstrate the model performs better when the domain of the pretrained models are closer to chemistry domain. We provide baselines for our benchmark as 0.8054 in average, and we hope this benchmark is used by many researchers in both industry and academia.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.39',\n",
       "   'authors': ['Yunsoo Kim',\n",
       "    'Hyuk Ko',\n",
       "    'Jane Lee',\n",
       "    'Hyun Young Heo',\n",
       "    'Jinyoung Yang',\n",
       "    'Sungsoo Lee',\n",
       "    'Kyu-hwang Lee'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I107',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.39.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79819/poster_document/4113d44d613aad88c4187c143109900f.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79819/slideshow/23a49303cc699beb7353a694e17a6459.pdf',\n",
       "   'title': '[Industry] Chemical Language Understanding Benchmark',\n",
       "   'tldr': 'In this paper, we introduce the benchmark datasets named CLUB (Chemical Language Understanding Benchmark) to facilitate NLP research in the chemical industry. We have 4 datasets consisted of text and token classification tasks. As far as we have recognized, it is one of the first examples of chemica...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79819,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79819-industry-chemical-language-understanding-benchmark',\n",
       "   'video_url': None},\n",
       "  'I109': {'abstract': 'Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or communities whose main focus is languages with less accessible resources (e.g., non-English). This paper explores the sequence-to-sequence (seq2seq) language model architecture as a more practical and compute-efficient alternative to the decoder-oriented approach (e.g., GPT-3), accompanied by novel findings in compute-optimality analyses. We successfully trained billion-scale Korean-language seq2seq language models that strongly outperform other competitive models in Korean benchmarks. Moreover, we demonstrate that such language models can be more efficiently utilized by employing a heavy pre-finetuning strategy, by showcasing a case study on dialog-task adaptation. Our case study shows that adopting language models with more readily available domain-specific unlabeled data greatly improves fine-tuning data efficiency in low-resource settings.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.40',\n",
       "   'authors': ['Dongju Park',\n",
       "    'Soonwon Ka',\n",
       "    'Kang Min Yoo',\n",
       "    'Gichang Lee',\n",
       "    'Jaewook Kang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I109',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.40.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] HyperT5: Towards Compute-Efficient Korean Language Modeling',\n",
       "   'tldr': 'Pretraining and fine-tuning language models have become the standard practice in industrial natural language processing (NLP), but developing and deploying general-purpose language models without the abundant computation or data resources is a real-world issue faced by smaller organizations or commu...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79833,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79833-hypert5-towards-compute-efficient-korean-language-modeling',\n",
       "   'video_url': None},\n",
       "  'I110': {'abstract': 'Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design considering ambiguity is possible. We utilize similarity in a semantic space to detect ambiguity in service scenarios and training data. In addition, we apply task-specific embedding to improve performance. Our results demonstrate that ambiguities and resulting labeling errors in training data or scenarios can be detected. Additionally, we confirm that it can be used to debug services',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.41',\n",
       "   'authors': ['Jong Myoung Kim',\n",
       "    'Young-jun Lee',\n",
       "    'Sangkeun Jung',\n",
       "    'Ho-jin Choi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I110',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.41.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Semantic Ambiguity Detection in Sentence Classification using Task-Specific Embeddings',\n",
       "   'tldr': 'Ambiguity is a major obstacle to providing services based on sentence classification. However, because of the structural limitations of the service, there may not be sufficient contextual information to resolve the ambiguity. In this situation, we focus on ambiguity detection so that service design ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79792,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79792-semantic-ambiguity-detection-in-sentence-classification-using-task-specific-embeddings',\n",
       "   'video_url': None},\n",
       "  'I111': {'abstract': 'Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards effective re-training of the model. In this study we propose an end-to-end framework for reliable model-agnostic change-point detection and interpretation in large task-oriented dialog systems, proven effective in multiple customer deployments. We evaluate our approach and demonstrate its benefits with a novel variant of intent classification training dataset, simulating customer requests to a dialog system. We make the data publicly available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.42',\n",
       "   'authors': ['Ella Rabinovich',\n",
       "    'Matan Vetzler',\n",
       "    'Samuel Ackerman',\n",
       "    'Ateret Anaby Tavor'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I111',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.42.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79765/poster/1e9afa15c1ef0484093123ed7d988c98.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Reliable and Interpretable Drift Detection in Streams of Short Texts',\n",
       "   'tldr': 'Data drift is the change in model input data that is one of the key factors leading to machine learning models performance degradation over time. Monitoring drift helps detecting these issues and preventing their harmful consequences. Meaningful drift interpretation is a fundamental step towards eff...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79765,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79765-industry-reliable-and-interpretable-drift-detection-in-streams-of-short-texts',\n",
       "   'video_url': None},\n",
       "  'I112': {'abstract': 'Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks.\\nLarger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (especially on CPU machines).\\nWe evaluate using a larger 170M parameter BERT encoder that shares representations across languages, domains and tasks for SLU compared to using smaller 17M parameter BERT encoders with language-, domain- and task-decoupled finetuning.\\nRunning inference with a larger shared encoder on GPU is latency neutral and reduces infrastructure cost compared to running inference for decoupled smaller encoders on CPU machines.\\nThe larger shared encoder reduces semantic error rates by 4.62\\\\% for test sets representing user requests to voice-controlled devices and 5.79\\\\% on the tail of the test sets on average across four languages.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.43',\n",
       "   'authors': ['Jonathan Hueser',\n",
       "    'Judith Gaspers',\n",
       "    'Thomas Gueudre',\n",
       "    'Chandana Prakash',\n",
       "    'Jin Cao',\n",
       "    'Daniil Sorokin',\n",
       "    'Quynh Do',\n",
       "    'Nicolas Anastassacos',\n",
       "    'Tobias Falke',\n",
       "    'Turan Gojayev'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I112',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.43.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79767/poster/e53127a2e02cfa5ecf42ef1b65a0a3b7.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Sharing Encoder Representations across Languages, Domains and Tasks in Large-Scale Spoken Language Understanding',\n",
       "   'tldr': 'Leveraging representations from pre-trained transformer-based encoders achieves state-of-the-art performance on numerous NLP tasks.\\nLarger encoders can improve accuracy for spoken language understanding (SLU) but are challenging to use given the inference latency constraints of online systems (espec...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79767,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79767-industry-sharing-encoder-representations-across-languages-domains-and-tasks-in-large-scale-spoken-language-understanding',\n",
       "   'video_url': None},\n",
       "  'I119': {'abstract': 'In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such as evaluating the impact of RI investment, and supporting Open Science and research reproducibility. These links can also be used to establish a profile of the RI portfolio of each institution and associate each equipment with scientific output. The system we are describing here is already in production, and has been used to address real business use cases, some of which we discuss in this paper. The computational pipeline at the heart of the system comprises both supervised and unsupervised modules to detect the usage of research equipment by processing the full text of the articles. Additionally, we have created a knowledge graph of RI, which is utilized to annotate the articles with metadata. Finally, examples of the business value of the insights made possible by this NLP pipeline are illustrated.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.44',\n",
       "   'authors': ['Seyed Amin Tabatabaei',\n",
       "    'Georgios Cheirmpos',\n",
       "    'Marius Doornenbal',\n",
       "    'Alberto Zigoni',\n",
       "    'Veronique Moore',\n",
       "    'Georgios Tsatsaronis'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I119',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.44.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79763/poster/a438d8d5be8c70e3e4386a56b049b9a0.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Annotating Research Infrastructure in Scientific Papers: An NLP-driven Approach',\n",
       "   'tldr': 'In this work, we present a natural language processing (NLP) pipeline for the identification, extraction and linking of Research Infrastructure (RI) used in scientific publications. Links between scientific equipment and publications where the equipment was used can support multiple use cases, such ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79763,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79763-industry-annotating-research-infrastructure-in-scientific-papers-an-nlp-driven-approach',\n",
       "   'video_url': None},\n",
       "  'I120': {'abstract': 'In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the QE system used in a famous Chinese search engine. EQE utilizes a novel event retrieval framework that consists of four stages, i.e., event collection, event reformulation, semantic retrieval and online ranking, which can select the best expansion from a significant amount of potential events rapidly and accurately. Specifically, we first collect and filter news headlines from websites. Then we propose a generation model that incorporates contrastive learning and prompt-tuning techniques to reformulate these headlines to concise candidates. Additionally, we fine-tune a dual-tower semantic model to serve as an encoder for event retrieval and explore a two-stage contrastive training approach to enhance the accuracy of event retrieval. Finally, we rank the retrieved events and select the optimal one as QE, which is then used to improve the retrieval of event-related documents. Through offline analysis and online A/B testing, we observed that the EQE system has significantly improved many indicators compared to the baseline. The system has been deployed in a real production environment and serves hundreds of millions of users.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.45',\n",
       "   'authors': ['Yanan Zhang',\n",
       "    'Weijie Cui',\n",
       "    'Yangfan Zhang',\n",
       "    'Xiaoling Bai',\n",
       "    'Zhe Zhang',\n",
       "    'Jin Ma',\n",
       "    'Xiang Chen',\n",
       "    'Tianhua Zhou'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I120',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.45.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79774/poster_document/24c84a8bf92c3edc7390fc0a48c836a5.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79774/slideshow/b88eac1969c21a57a2c40ff2a3736089.pptx',\n",
       "   'title': '[Industry] Event-Centric Query Expansion in Web Search',\n",
       "   'tldr': 'In search engines, query expansion (QE) is a crucial technique to improve search experience. Previous studies often rely on long-term search log mining, which leads to slow updates and is sub-optimal for time-sensitive news searches. In this work, we present Event-Centric Query Expansion (EQE), the ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79774,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79774-industry-event-centric-query-expansion-in-web-search',\n",
       "   'video_url': None},\n",
       "  'I124': {'abstract': 'As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. \\nIn order to unify the categorization process and ensure efficiency, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space. \\nTo further enhance domain transferability and better exploit cross-domain data, we design two plug-in modules: a heuristic mapping scorer and a pretrained contrastive ranking module with the help of meta concepts, which represent \\nkeyword knowledge shared across domains.\\nComprehensive offline experiments show that our method outperforms strong baselines\\non three dynamic multi-domain product categorization (DMPC) tasks,\\nand online experiments reconfirm its efficacy with a\\n5\\\\% increase on seasonal purchase revenue. Related datasets will be released.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.46',\n",
       "   'authors': ['Shansan Gong',\n",
       "    'Zelin Zhou',\n",
       "    'Shuo Wang',\n",
       "    'Fengjiao Chen',\n",
       "    'Xiujie Song',\n",
       "    'Xuezhi Cao',\n",
       "    'Yunsen Xian',\n",
       "    'Kenny Zhu'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I124',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.46.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79780/poster_document/cfa173d7dd9f87287c313af40ace8842.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79780/slideshow/f24123118cdd0de84274f10a35898c1e.pptx',\n",
       "   'title': '[Industry] Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization',\n",
       "   'tldr': 'As e-commerce platforms develop different business lines, a special but challenging product categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time. \\nIn order to unify the categorization process and ensure efficienc...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79780,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79780-industry-transferable-and-efficient-unifying-dynamic-multi-domain-product-categorization',\n",
       "   'video_url': None},\n",
       "  'I125': {'abstract': 'Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems.\\nKnowledge Base (KB) databases are an effective way of storing and accessing such information to scale.\\nIn this work we present a system, developed for the European Space Agency, that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a program sketch from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database.\\nThis pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shortcut learning even with limited amount of in-domain training data.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.47',\n",
       "   'authors': ['Paul Darm',\n",
       "    'Antonio Valerio Miceli Barone',\n",
       "    'Shay B. Cohen',\n",
       "    'Annalisa Riccardi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I125',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.47.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] DISCOSQA: A Knowledge Base Question Answering System for Space Debris based on Program Induction',\n",
       "   'tldr': 'Space program agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems.\\nKnowledge Base (KB) databases are an effective way of storing and accessing such information to scale.\\nIn this work we present a system, ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79806,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79806-discosqa-a-knowledge-base-question-answering-system-for-space-debris-based-on-program-induction',\n",
       "   'video_url': None},\n",
       "  'I128': {'abstract': \"Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs' early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers' exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\\\textbackslash{}footnote\\\\{Code will be made publicly available to the research community upon acceptance.\\\\}\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.48',\n",
       "   'authors': ['Wei Zhu',\n",
       "    'Peng Wang',\n",
       "    'Yuan Ni',\n",
       "    'Guotong Xie',\n",
       "    'Xiaoling Wang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I128',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.48.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79759/poster/88a8c27522f86608c16ec48eb1757ea4.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting',\n",
       "   'tldr': 'Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79759,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79759-industry-badge-speeding-up-bert-inference-after-deployment-via-block-wise-bypasses-and-divergence-based-early-exiting',\n",
       "   'video_url': None},\n",
       "  'I13': {'abstract': 'Patent applicants write patent specifications\\nthat describe embodiments of inventions.\\nSome embodiments are claimed for a patent,\\nwhile others may be unclaimed\\ndue to strategic considerations.\\nUnclaimed embodiments may be extracted by\\napplicants later and claimed in\\ncontinuing applications to\\ngain advantages over competitors.\\nDespite being essential for corporate intellectual property (IP) strategies,\\nunclaimed embodiment extraction is conducted manually,\\nand little research has been conducted on its automation.\\nThis paper presents a novel task of\\nunclaimed embodiment extraction (UEE)\\nand a novel dataset for the task.\\nOur experiments with Transformer-based models\\ndemonstrated\\nthat the task was challenging as it required\\nconducting natural language inference on\\npatent specifications, which consisted of\\ntechnical, long, syntactically and semantically\\ninvolved sentences.\\nWe release the dataset and code \\nto foster this new area of research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.3',\n",
       "   'authors': ['Chikara Hashimoto',\n",
       "    'Gautam Kumar',\n",
       "    'Shuichiro Hashimoto',\n",
       "    'Jun Suzuki'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.3.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79817/poster_document/1337b36956fcc5f947568abb6dd0f496.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79817/slideshow/066ca93bcfcf96ba969b9c443dffcfcc.pdf',\n",
       "   'title': '[Industry] Hunt for Buried Treasures: Extracting Unclaimed Embodiments from Patent Specifications',\n",
       "   'tldr': 'Patent applicants write patent specifications\\nthat describe embodiments of inventions.\\nSome embodiments are claimed for a patent,\\nwhile others may be unclaimed\\ndue to strategic considerations.\\nUnclaimed embodiments may be extracted by\\napplicants later and claimed in\\ncontinuing applications to\\ngain a...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79817,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79817-industry-hunt-for-buried-treasures-extracting-unclaimed-embodiments-from-patent-specifications',\n",
       "   'video_url': None},\n",
       "  'I131': {'abstract': 'Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard and soft data (sensors and texts), either by tweaking their AIS emitters or by emitting false information on pseudo-newspapers.\\n\\nMany techniques exist to identify these pieces of false information, including using knowledge base population techniques to build a structured view of the information. This paper presents a use case for suspect data identification in a maritime setting. The proposed system UMBAR ingests data from sensors and texts, processing them through an information extraction step, in order to feed a Knowledge Base and finally perform coherence checks between the extracted facts.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.49',\n",
       "   'authors': ['Maxime Prieur',\n",
       "    'Souhir Gahbiche',\n",
       "    'Guillaume Gadek',\n",
       "    'Sylvain Gatepaille',\n",
       "    'Kilian Vasnier',\n",
       "    'Valerian Justine'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I131',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.49.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79760/poster/02483b7e3f750e640fca47f7da092602.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] K-pop and fake facts: from texts to smart alerting for maritime security',\n",
       "   'tldr': 'Maritime security requires full-time monitoring of the situation, mainly based on technical data (radar, AIS) but also from OSINT-like inputs (e.g., newspapers). Some threats to the operational reliability of this maritime surveillance, such as malicious actors, introduce discrepancies between hard ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79760,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79760-industry-k-pop-and-fake-facts-from-texts-to-smart-alerting-for-maritime-security',\n",
       "   'video_url': None},\n",
       "  'I134': {'abstract': 'The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vector representations of a given text. With a growing number of APIs at our disposal, in this paper, our goal is to analyze semantic embedding APIs in realistic retrieval scenarios in order to assist practitioners and researchers in finding suitable services according to their needs. Specifically, we wish to investigate the capabilities of existing APIs on domain generalization and multilingual retrieval. For this purpose, we evaluate the embedding APIs on two standard benchmarks, BEIR, and MIRACL. We find that re-ranking BM25 results using the APIs is a budget-friendly approach and is most effective on English, in contrast to the standard practice, i.e., employing them as first-stage retrievers. For non-English retrieval, re-ranking still improves the results, but a hybrid model with BM25 works best albeit at a higher cost. We hope our work lays the groundwork for thoroughly evaluating APIs that are critical in search and more broadly, in information retrieval.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.50',\n",
       "   'authors': ['Ehsan Kamalloo',\n",
       "    'Xinyu Zhang',\n",
       "    'Odunayo Ogundepo',\n",
       "    'Nandan Thakur',\n",
       "    'David Alfonso-hermelo',\n",
       "    'Mehdi Rezagholizadeh',\n",
       "    'Jimmy Lin'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I134',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.50.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79779/poster_document/e48280b2a64f16a24ccfa513f6ddd113.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Evaluating Embedding APIs for Information Retrieval',\n",
       "   'tldr': 'The ever-increasing size of language models curtails their widespread access to the community, thereby galvanizing many companies and startups into offering access to large language models through APIs. One particular API, suitable for dense retrieval, is the semantic embedding API that builds vecto...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79779,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79779-evaluating-embedding-apis-for-information-retrieval',\n",
       "   'video_url': None},\n",
       "  'I135': {'abstract': 'Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. \\nParticularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradient learning have been shown to struggle in such setups or have limitations like memory buffers, and being restricted to specific domains that disable its usage in real-world scenarios. For this reason, we present a fully differentiable architecture based on the Mixture of Experts model, that enables the training of high-performance classifiers when examples from each class are presented separately. We conducted exhaustive experiments that proved its applicability in various domains and ability to learn online in production environments. The proposed technique achieves SOTA results without a memory buffer and clearly outperforms the reference methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.51',\n",
       "   'authors': ['Mateusz Wjcik',\n",
       "    'Witold Kociukiewicz',\n",
       "    'Mateusz Baran',\n",
       "    'Tomasz Kajdanowicz',\n",
       "    'Adam Gonczarek'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I135',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.51.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform',\n",
       "   'tldr': 'Production deployments in complex systems require ML architectures to be highly efficient and usable against multiple tasks. \\nParticularly demanding are classification problems in which data arrives in a streaming fashion and each class is presented separately. Recent methods with stochastic gradien...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79787,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79787-domain-agnostic-neural-architecture-for-class-incremental-continual-learning-in-document-processing-platform',\n",
       "   'video_url': None},\n",
       "  'I139': {'abstract': 'In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, focal distillation, to model updates in a goal-oriented dialog system and assess its usefulness in practice. In particular, we evaluate its effectiveness for key language understanding tasks, including sentence classification and sequence labeling tasks, we further assess its effect when applied to repeated model updates over time, and test its compatibility with mislabeled data. Our experiments on a public benchmark and data from a deployed dialog system demonstrate that focal distillation can substantially reduce regressions, at only minor drops in accuracy, and that it further outperforms naive supervised training in challenging mislabeled data and label expansion settings.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.52',\n",
       "   'authors': ['Andrea Caciolai',\n",
       "    'Verena Weber',\n",
       "    'Tobias Falke',\n",
       "    'Alessandro Pedrani',\n",
       "    'Davide Bernardi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I139',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.52.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79768/poster/9550f352f48d79a83d735e7e585305a0.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Regression-Free Model Updates for Spoken Language Understanding',\n",
       "   'tldr': 'In real-world systems, an important requirement for model updates is to avoid regressions in user experience caused by flips of previously correct classifications to incorrect ones. Multiple techniques for that have been proposed in the recent literature. In this paper, we apply one such technique, ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79768,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79768-industry-regression-free-model-updates-for-spoken-language-understanding',\n",
       "   'video_url': None},\n",
       "  'I14': {'abstract': \"Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose 'MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the 'MultiArith' dataset (78.7\\\\% -\\\\textgreater{} 92.5\\\\%) evaluated using 175B parameter GPT-based LLM.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.4',\n",
       "   'authors': ['Shima Imani', 'Liang Du', 'Harsh Shrivastava'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.4.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79772/poster_document/6878702800f4d6dc568b38f1f5e6a769.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79772/slideshow/61521c4fff4ff46382c4c6f647ee9d1a.pptx',\n",
       "   'title': '[Industry] MathPrompter: Mathematical Reasoning using Large Language Models',\n",
       "   'tldr': 'Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79772,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79772-industry-mathprompter-mathematical-reasoning-using-large-language-models',\n",
       "   'video_url': None},\n",
       "  'I140': {'abstract': \"Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is observed that natural language understanding models often have lower performance when dealing with requests coming from new users rather than experienced users. To mitigate this problem, we propose a framework that consists of two phases (1) a fixing phase with four active learning strategies used to identify important samples coming from new users, and (2) a self training phase where a teacher model trained from the first phase is used to annotate semi-supervised samples to expand the training data with relevant cohort utterances. We explain practical strategies that involve an identification of representative cohort-based samples through density clustering as well as employing implicit customer feedbacks to improve new customers' experience. We demonstrate the effectiveness of our approach in a real world large scale voice assistant system for two languages, German and French through both offline experiments as well as A/B testings.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.53',\n",
       "   'authors': ['Dieu-thu Le',\n",
       "    'Gabriela Hernandez',\n",
       "    'Bei Chen',\n",
       "    'Melanie Bradford'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I140',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.53.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Reducing cohort bias in natural language understanding systems with targeted self-training scheme',\n",
       "   'tldr': 'Bias in machine learning models can be an issue when the models are trained on particular types of data that do not generalize well, causing under performance in certain groups of users. In this work, we focus on reducing the bias related to new customers in a digital voice assistant system. It is o...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79793,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79793-reducing-cohort-bias-in-natural-language-understanding-systems-with-targeted-self-training-scheme',\n",
       "   'video_url': None},\n",
       "  'I141': {'abstract': 'Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled data, which make effective policy compliance challenging.  We propose to model content moderation as a binary question answering problem where the questions validate the loosely coupled themes constituting a policy. A decision logic is applied on top to aggregate the theme-specific validations. This way the questions pass theme information to a transformer network as explicit policy prompts, that in turn enables explainability. This setting further allows for faster adaptation to policy updates by leveraging zero-shot capabilities of pre-trained transformers. We showcase improved recall for our proposed method at 95\\\\textbackslash{}\\\\% precision on two proprietary datasets of social media posts and comments respectively annotated under curated Hate Speech and Commercial Spam policies.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.54',\n",
       "   'authors': ['Sankha Subhra Mullick',\n",
       "    'Mohan Bhambhani',\n",
       "    'Suhit Sinha',\n",
       "    'Akshat Mathur',\n",
       "    'Somya Gupta',\n",
       "    'Jidnya Shah'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I141',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.54.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79771/poster_document/9297d08868ed5d9507c1ed537366ed10.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79771/slideshow/fb94daee76c51f3ecd6106ec949ca412.pdf',\n",
       "   'title': '[Industry] Content Moderation for Evolving Policies using Binary Question Answering',\n",
       "   'tldr': 'Content moderation on social media is governed by policies that are intricate and frequently updated with evolving world events. However, automated content moderation systems often restrict easy adaptation to policy changes and are expected to learn policy intricacies from limited amounts of labeled...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79771,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79771-industry-content-moderation-for-evolving-policies-using-binary-question-answering',\n",
       "   'video_url': None},\n",
       "  'I146': {'abstract': \"Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be poor due to scarce supervision, causing many issues in real-life applications. To address IC task's long-tail issue, K-positive contrastive loss (KCL) is proposed on image classification task and can be applied on the IC task when using text-based contrastive learning, e.g., SimCSE. However, one shortcoming of using KCL has been neglected in previous research: false negative (FN) instances may harm the KCL's representation learning. To address the FN issue in the KCL, we proposed to re-weight the positive pairs in the KCL loss with a regularization that the sum of weights should be constrained to K+1 as close as possible. After controlling FN instances with the proposed method, IC performance has been further improved and is superior to other LT-addressing methods.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.55',\n",
       "   'authors': ['Tianqi Wang',\n",
       "    'Lei Chen',\n",
       "    'Xiaodan Zhu',\n",
       "    'Younghun Lee',\n",
       "    'Jing Gao'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I146',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.55.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Weighted Contrastive Learning With False Negative Control to Help Long-tailed Product Classification',\n",
       "   'tldr': 'Item categorization (IC) aims to classify product descriptions into leaf nodes in a categorical taxonomy, which is a key technology used in a wide range of applications. Along with the fact that most datasets often has a long-tailed distribution, classification performances on tail labels tend to be...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79798,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79798-weighted-contrastive-learning-with-false-negative-control-to-help-long-tailed-product-classification',\n",
       "   'video_url': None},\n",
       "  'I148': {'abstract': 'Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\\\textbackslash{}texttt\\\\{ToxicTrap\\\\}, introducing small word-level perturbations to fool SOTA text classifiers to predict toxic text samples as benign. \\\\textbackslash{}texttt\\\\{ToxicTrap\\\\} exploits greedy based search strategies to enable fast and effective generation of toxic adversarial examples. Two novel goal function designs allow \\\\textbackslash{}texttt\\\\{ToxicTrap\\\\} to identify weaknesses in both multiclass and multilabel toxic language detectors. Our empirical results show that SOTA toxicity text classifiers are indeed vulnerable to the proposed attacks, attaining over 98\\\\textbackslash{}\\\\% attack success rates in multilabel cases. We also show how a vanilla adversarial training and its improved version can help increase robustness of a toxicity detector even against unseen attacks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.56',\n",
       "   'authors': ['Dmitriy Bespalov',\n",
       "    'Sourav Bhabesh',\n",
       "    'Yi Xiang',\n",
       "    'Liutong Zhou',\n",
       "    'Yanjun Qi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I148',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.56.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79816/slideshow/341dabe8e2bb0bd25a501f677041930f.pdf',\n",
       "   'title': '[Industry] Towards Building a Robust Toxicity Predictor',\n",
       "   'tldr': 'Recent NLP literature pays little attention to the robustness of toxicity language predictors, while these systems are most likely to be used in adversarial contexts. This paper presents a novel adversarial attack, \\\\textbackslash{}texttt\\\\{ToxicTrap\\\\}, introducing small word-level perturbations to fo...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79816,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79816-towards-building-a-robust-toxicity-predictor',\n",
       "   'video_url': None},\n",
       "  'I15': {'abstract': 'Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making abrupt policy changes that break the current user experience. In this study, we introduce a scalable framework for supporting fine-grained exploration targets for individual domains via user-defined constraints. For example, we may want to ensure fewer policy deviations in business-critical domains such as shopping, while allocating more exploration budget to domains such as music. We present a novel meta-gradient learning approach that is scalable and practical to address this problem. The proposed method adjusts constraint violation penalty terms adaptively through a meta objective that encourages balanced constraint satisfaction across domains. We conducted extensive experiments on a real-world conversational AI and using a set of realistic constraint benchmarks. The proposed approach has been deployed in production for a large-scale commercial assistant, enabling the best balance between the policy value and constraint satisfaction rate.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.5',\n",
       "   'authors': ['Mohammad Kachuee', 'Sungjin Lee'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I15',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.5.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Constrained Policy Optimization for Controlled Self-Learning in Conversational AI Systems',\n",
       "   'tldr': 'Recently, self-learning methods based on user satisfaction metrics and contextual bandits have shown promising results to enable consistent improvements in conversational AI systems. However, directly targeting such metrics by off-policy bandit learning objectives often increases the risk of making ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79830,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79830-constrained-policy-optimization-for-controlled-self-learning-in-conversational-ai-systems',\n",
       "   'video_url': None},\n",
       "  'I156': {'abstract': 'In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coaching purposes, and provide relevant feedback and insights to the contact center manager or supervisor. In this paper, we present \"AI Coach Assis\", which leverages the pre-trained transformer-based language models to determine whether a given call is coachable or not based on the quality assurance (QA) queries/questions asked by the contact center managers or supervisors. The system was trained and evaluated on a large dataset collected from real-world contact centers and provides an efficient and effective way to determine which calls are most relevant for coaching purposes. Extensive experimental evaluation demonstrates the potential of AI Coach Assist to improve the coaching process, resulting in enhancing the performance of contact center agents.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.57',\n",
       "   'authors': ['Md Tahmid Rahman Laskar',\n",
       "    'Cheng Chen',\n",
       "    'Xue-yong Fu',\n",
       "    'Mahsa Azizi',\n",
       "    'Shashi Bhushan',\n",
       "    'Simon Corston-oliver'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I156',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.57.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] AI Coach Assist: An Automated Approach for Call Recommendation in Contact Centers for Agent Coaching',\n",
       "   'tldr': 'In recent years, the utilization of Artificial Intelligence (AI) in the contact center industry is on the rise. One area where AI can have a significant impact is in the coaching of contact center agents. By analyzing call transcripts, AI can quickly determine which calls are most relevant for coach...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79789,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79789-ai-coach-assist-an-automated-approach-for-call-recommendation-in-contact-centers-for-agent-coaching',\n",
       "   'video_url': None},\n",
       "  'I16': {'abstract': 'Large pre-trained language models based on transformer architecturehave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4\\\\% and 97.8\\\\% the performance of mBERT on MTOP and multiATIS, while using 170x less parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8\\\\% on MTOP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.6',\n",
       "   'authors': ['Francesco Fusco',\n",
       "    'Damian Pascual',\n",
       "    'Peter Staar',\n",
       "    'Diego Antognini'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.6.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79758/poster/9e3cd2f4a859adf77153d52deb4f9f95.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] pNLP-Mixer: an Efficient all-MLP Architecture for Language',\n",
       "   'tldr': 'Large pre-trained language models based on transformer architecturehave drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and infere...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79758,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79758-industry-pnlp-mixer-an-efficient-all-mlp-architecture-for-language',\n",
       "   'video_url': None},\n",
       "  'I17': {'abstract': 'Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 times faster, even on high-end GPUs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.7',\n",
       "   'authors': ['Francesco Fusco', 'Diego Antognini'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.7.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Extracting Text Representations for Terms and Phrases in Technical Domains',\n",
       "   'tldr': 'Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79795,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79795-extracting-text-representations-for-terms-and-phrases-in-technical-domains',\n",
       "   'video_url': None},\n",
       "  'I171': {'abstract': 'Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separately. Although these two tasks have many similarities such as they both use the previous dialogue along with the current request as model input, there is no unified model to solve them jointly. To this end, we propose a unified contextual query rewriting model that unifies QR for both reducing friction and contextual carryover purpose. Moreover, we involve multiple auxiliary tasks such as trigger prediction and NLU interpretation tasks to boost the performance of the rewrite. We leverage the text-to-text unified framework which uses independent tasks with weighted loss to account for task importance. Then we propose new unified multitask learning strategies including a sequential model which outputs one sentence for multi-tasks, and a hybrid model where some tasks are independent and some tasks are sequentially generated. Our experimental results demonstrate the effectiveness of the proposed unified learning methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.58',\n",
       "   'authors': ['Yingxue Zhou',\n",
       "    'Jie Hao',\n",
       "    'Mukund Rungta',\n",
       "    'Yang Liu',\n",
       "    'Eunah Cho',\n",
       "    'Xing Fan',\n",
       "    'Yanbin Lu',\n",
       "    'Vishal Vasudevan',\n",
       "    'Kellen Gillespie',\n",
       "    'Zeynab Raeesy'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I171',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.58.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79781/poster_document/cb2230f1dad4457d6246761f6c936a97.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Unified Contextual Query Rewriting',\n",
       "   'tldr': 'Query rewriting (QR) is an important technique for user friction (i.e. recovering ASR error or system error) reduction and contextual carryover (i.e. ellipsis and co-reference) in conversational AI systems. Recently, generation-based QR models have achieved promising results on these two tasks separ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79781,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79781-unified-contextual-query-rewriting',\n",
       "   'video_url': None},\n",
       "  'I173': {'abstract': \"E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain contextual insights about users' true shopping intents. Therefore, modeling such contextual information is critical to a better query rewriting model. However, existing query rewriting models ignore users' history behaviors and consider only the instant search query, which is often a short string offering limited information about the true shopping intent.\\nWe propose an end-to-end context-aware query rewriting model to bridge this gap, which takes the search context into account. Specifically, our model builds a session graph using the history search queries and their contained words. We then employ a graph attention mechanism that models cross-query relations and computes contextual information of the session. The model subsequently calculates session representations by combining the contextual information with the instant search query using an aggregation network. The session representations are then decoded to generate rewritten queries. Empirically, we demonstrate the superiority of our method to state-of-the-art approaches under various metrics.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.59',\n",
       "   'authors': ['Simiao Zuo',\n",
       "    'Qingyu Yin',\n",
       "    'Haoming Jiang',\n",
       "    'Shaohui Xi',\n",
       "    'Bing Yin',\n",
       "    'Chao Zhang',\n",
       "    'Tuo Zhao'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I173',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.59.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79761/poster/c08e592ba2d96aaf96ec0ae5faddbbea.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"[Industry] Context-Aware Query Rewriting for Improving Users' Search Experience on E-commerce Websites\",\n",
       "   'tldr': 'E-commerce queries are often short and ambiguous. Consequently, query understanding often uses query rewriting to disambiguate user-input queries. While using e-commerce search tools, users tend to enter multiple searches, which we call context, before purchasing. These history searches contain cont...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79761,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79761-industry-context-aware-query-rewriting-for-improving-users-search-experience-on-e-commerce-websites',\n",
       "   'video_url': None},\n",
       "  'I18': {'abstract': 'Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial indexing and inference time and the large consumption of computational resources. Although knowledge distillation techniques have been widely utilized for uni-modal model compression, how to expand them to the situation when the numbers of modalities and teachers/students are doubled has been rarely studied. In this paper, we conduct comprehensive experiments on this topic and propose the fully-Connected knowledge interaction graph (Coca) technique for cross-modal pre-training distillation. Based on our findings, the resulting CocaCLIP achieves SOTA performances on the widely-used Flickr30K and MSCOCO benchmarks under the lightweight setting. An industry application of our method on an e-commercial platform further demonstrates the significant effectiveness of CocaCLIP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.8',\n",
       "   'authors': ['Jiapeng Wang',\n",
       "    'Chengyu Wang',\n",
       "    'Xiaodan Wang',\n",
       "    'Jun Huang',\n",
       "    'Lianwen Jin'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.8.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] CocaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval',\n",
       "   'tldr': 'Large-scale pre-trained text-image models with dual-encoder architectures (such as CLIP) are typically adopted for various vision-language applications, including text-image retrieval. However, these models are still less practical on edge devices or for real-time situations, due to the substantial ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79831,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79831-cocaclip-exploring-distillation-of-fully-connected-knowledge-interaction-graph-for-lightweight-text-image-retrieval',\n",
       "   'video_url': None},\n",
       "  'I186': {'abstract': 'We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. \\nTo provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. \\nWith the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and \\\\$\\\\textbackslash{}rho-\\\\$zCDP privacy guarantees with \\\\$\\\\textbackslash{}rho \\\\textbackslash{}in (0.3, 2)\\\\$, with one model additionally trained with secure aggregation.\\nWe summarize our experience and provide concrete suggestions on DP training for practitioners.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.60',\n",
       "   'authors': ['Zheng Xu',\n",
       "    'Yanxiang Zhang',\n",
       "    'Galen Andrew',\n",
       "    'Christopher Choquette',\n",
       "    'Peter Kairouz',\n",
       "    'Brendan Mcmahan',\n",
       "    'Jesse Rosenstock',\n",
       "    'Yuanbo Zhang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I186',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.60.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79777/poster_document/147f6a8c20b71af2c477a0ea45e77d99.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79777/slideshow/db4525e9b3b360c9b5f2e3b2f9ee3b95.pdf',\n",
       "   'title': '[Industry] Federated Learning of Gboard Language Models with Differential Privacy',\n",
       "   'tldr': 'We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. \\nTo...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79777,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79777-industry-federated-learning-of-gboard-language-models-with-differential-privacy',\n",
       "   'video_url': None},\n",
       "  'I187': {'abstract': 'Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning tasks. We present RadLing, a continuously pretrained language model using ELECTRA-small architecture, trained using over 500K radiology reports that can compete with state-of-the-art results for fine tuning tasks in radiology domain. Our main contribution in this paper is knowledge-aware masking which is an taxonomic knowledge-assisted pre-training task that dynamically masks tokens to inject knowledge during pretraining. In addition, we also introduce an knowledge base-aided vocabulary extension to adapt the general tokenization vocabulary to radiology domain.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.61',\n",
       "   'authors': ['Rikhiya Ghosh',\n",
       "    'Oladimeji Farri',\n",
       "    'Sanjeev Kumar Karn',\n",
       "    'Manuela Danu',\n",
       "    'Ramya Vunikili',\n",
       "    'Larisa Micu'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I187',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.61.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] RadLing: Towards Efficient Radiology Report Understanding',\n",
       "   'tldr': 'Most natural language tasks in the radiology domain use language models pre-trained on biomedical corpus. There are few pretrained language models trained specifically for radiology, and fewer still that have been trained in a low data setting and gone on to produce comparable results in fine-tuning...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79799,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79799-radling-towards-efficient-radiology-report-understanding',\n",
       "   'video_url': None},\n",
       "  'I188': {'abstract': 'In a typical call center, only up to 8\\\\% of callers\\nleave a Customer Satisfaction (CSAT) survey\\nresponse at the end of the call, and these tend to\\nbe customers with strongly positive or negative\\nexperiences. To manage this data sparsity and\\nresponse bias, we outline a predictive CSAT\\ndeep learning algorithm that infers CSAT on\\nthe 1-5 scale on inbound calls to the call center\\nwith minimal latency. The key metric to maximize is the precision for CSAT = 1 (lowest\\nCSAT). We maximize this metric in two ways.\\nFirst, reframing the problem\\nas a binary class, rather than five-class problem during model fine-tuning, and then mapping binary outcomes back to five classes using\\ntemperature-scaled model probabilities. Second, using soft labels to represent the classes. \\nThe\\nresult is a production model able to support key\\ncustomer workflows with high accuracy over\\nmillions of calls a month.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.62',\n",
       "   'authors': ['Etienne Manderscheid', 'Matthias Lee'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I188',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.62.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Predicting Customer Satisfaction with Soft Labels for Ordinal Classification',\n",
       "   'tldr': 'In a typical call center, only up to 8\\\\% of callers\\nleave a Customer Satisfaction (CSAT) survey\\nresponse at the end of the call, and these tend to\\nbe customers with strongly positive or negative\\nexperiences. To manage this data sparsity and\\nresponse bias, we outline a predictive CSAT\\ndeep learning a...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79790,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79790-predicting-customer-satisfaction-with-soft-labels-for-ordinal-classification',\n",
       "   'video_url': None},\n",
       "  'I189': {'abstract': 'Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models.\\nUnfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback data collected from deployed QA systems.\\nThis data can be collected from tens of millions of user with no additional cost, for real-world QA services, e.g., Alexa, Google Home, and etc. The main drawback is the noise affecting feedback on individual examples. \\nRecent literature on QA systems has shown the benefit of training models even with noisy feedback. However, these studies have multiple limitations: (i) they used uniform random noise to simulate feedback responses, which is typically an unrealistic approximation as noise follows specific patterns, depending on target examples and users; and (ii) they do not show how to aggregate feedback for improving training signals.\\nIn this paper, we first collect a large scale (16M) QA dataset with real feedback sampled from the QA traffic of a popular Virtual Assistant.\\nSecond, we use this data to develop two strategies for filtering unreliable users and thus de-noise feedback: (i) ranking users with an automatic classifier, and (ii) aggregating feedback over similar instances and comparing users between each other. Finally, we train QA models on our filtered feedback data, showing a significant improvement over the state of the art.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.63',\n",
       "   'authors': ['Liang Wang', 'Ivano Lauriola', 'Alessandro Moschitti'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I189',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.63.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79764/poster/5d75b6abb765170c8db2d3e2b69291b4.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Accurate Training of Web-based Question Answering Systems with Feedback from Ranked Users',\n",
       "   'tldr': 'Recent work has shown that large-scale annotated datasets are essential for training state-of-the-art Question Answering (QA) models.\\nUnfortunately, creating this data is expensive and requires a huge amount of annotation work. An alternative and cheaper source of supervision is given by feedback da...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79764,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79764-industry-accurate-training-of-web-based-question-answering-systems-with-feedback-from-ranked-users',\n",
       "   'video_url': None},\n",
       "  'I191': {'abstract': 'In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label classification, which cannot generalize to utterances with more intents than training samples. Meanwhile, it lacks the ability to assign slots to each corresponding intent. To overcome these problems, we propose a Split-Parsing Method (SPM) for joint multiple intent detection and slot filling, which is a two-stage method. It first splits an input sentence into multiple sub-sentences which contain a single-intent, and then a joint single intent detection and slot filling model is applied to parse each sub-sentence recurrently. Finally, we integrate the parsed results. The sub-sentence split task is also treated as a sequence labelling problem with only one entity-label, which can effectively generalize to a sentence with more intents unseen in the training set. Experimental results on three multi-intent datasets show that our method obtains substantial improvements over different baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.64',\n",
       "   'authors': ['Sheng Jiang',\n",
       "    'Su Zhu',\n",
       "    'Ruisheng Cao',\n",
       "    'Qingliang Miao',\n",
       "    'Kai Yu'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I191',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.64.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79818/slideshow/ff4042d6f60e6d91c7a027ff73d1a2b5.pptx',\n",
       "   'title': '[Industry] SPM: A Split-Parsing Method for Joint Multi-Intent Detection and Slot Filling',\n",
       "   'tldr': 'In a task-oriented dialogue system, joint intent detection and slot filling for multi-intent utterances become meaningful since users tend to query more. The current state-of-the-art studies choose to process multi-intent utterances through a single joint model of sequence labelling and multi-label ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79818,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79818-industry-spm-a-split-parsing-method-for-joint-multi-intent-detection-and-slot-filling',\n",
       "   'video_url': None},\n",
       "  'I196': {'abstract': 'Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering from error propagation and inefficient decoding. In this work, we propose a unified non-autoregressive generation (NAG) framework for general NER tasks, referred to as NAG-NER. First, we propose to generate entities as a set instead of a sequence, avoiding error propagation. Second, we propose incorporating NAG in NER tasks for efficient decoding by treating each entity as a target sequence. Third, to enhance the generation performances of the NAG decoder, we employ the NAG encoder to detect potential entity mentions. Extensive experiments show that our NAG-NER model outperforms the state-of-the-art generative NER models on three benchmark NER datasets of different types and two of our proprietary NER tasks.\\\\textbackslash{}footnote\\\\{Code will be publicly available to the research community upon acceptance.\\\\}',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.65',\n",
       "   'authors': ['Xinpeng Zhang', 'Ming Tan', 'Jingfan Zhang', 'Wei Zhu'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I196',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.65.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] NAG-NER: a Unified Non-Autoregressive Generation Framework for Various NER Tasks',\n",
       "   'tldr': 'Recently, the recognition of flat, nested, and discontinuous entities by a unified generative model framework has received increasing attention both in the research field and industry. However, the current generative NER methods force the entities to be generated in a predefined order, suffering fro...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79800,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79800-nag-ner-a-unified-non-autoregressive-generation-framework-for-various-ner-tasks',\n",
       "   'video_url': None},\n",
       "  'I197': {'abstract': 'Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling patterns for a single correct query. In this paper, we first systematically analyze and group different spell errors into error classes and then leverage the state-of-the-art Transformer model for contextual spell correction. We overcome the constraint of limited human labelled data by proposing novel synthetic data generation techniques for voluminous generation of training pairs needed by data hungry Transformers, without any human intervention. We further utilize weakly supervised data coupled with curriculum learning strategies to improve on tough spell mistakes without regressing on the easier ones. We show significant improvements from our model on human labeled data and online A/B experiments against multiple state-of-art models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.66',\n",
       "   'authors': ['Vishal Kakkar',\n",
       "    'Chinmay Sharma',\n",
       "    'Madhura Pande',\n",
       "    'Surender Kumar'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I197',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.66.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Search Query Spell Correction with Weak Supervision in E-commerce',\n",
       "   'tldr': 'Misspelled search queries in e-commerce can lead to empty or irrelevant products. Besides inadvertent typing mistakes, most spell mistakes occur because the user does not know the correct spelling, hence typing it as it is pronounced colloquially. This colloquial typing creates countless misspelling...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79814,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79814-search-query-spell-correction-with-weak-supervision-in-e-commerce',\n",
       "   'video_url': None},\n",
       "  'I199': {'abstract': 'Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified Vision-Language (VL) model based on the One For All (OFA) model, with a focus on context-assisted image captioning where the caption is generated based on both the image and its context. Our approach aims to overcome the context-independent (image and text are treated independently) nature of the existing approaches. We exploit context by pretraining our model with datasets of three tasks- news image captioning where the news article is the context, contextual visual entailment, and keyword extraction from the context. The second pretraining task is a new VL task, and we construct and release two datasets for the task with  1.1M and 2.2K data instances.  Our system achieves state-of-the-art results with an improvement of up to 8.34 CIDEr score on the benchmark news image captioning datasets. To the best of our knowledge, ours is the first effort at incorporating contextual information in pretraining the models for the VL tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.67',\n",
       "   'authors': ['Abisek Rajakumar Kalarani',\n",
       "    'Pushpak Bhattacharyya',\n",
       "    'Niyati Chhaya',\n",
       "    'Sumit Shekhar'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I199',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.67.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79769/poster_document/eb002a251cb4f184ef42b88b0e182913.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79769/poster/7c8e85fd55b5fd8110545b66b906f5c1.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79769/slideshow/666ce1cc49afdd41b952152883a43d0a.pdf',\n",
       "   'title': '[Industry] \"Let\\'s not Quote out of Context\": Unified Vision-Language Pretraining for Context Assisted Image Captioning',\n",
       "   'tldr': 'Well-formed context aware image captions and tags in enterprise content such as marketing material are critical to ensure their brand presence and content recall. Manual creation and updates to ensure the same is non trivial given the scale and the tedium towards this task. We propose a new unified ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79769,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79769-industry-let-s-not-quote-out-of-context-unified-vision-language-pretraining-for-context-assisted-image-captioning',\n",
       "   'video_url': None},\n",
       "  'I201': {'abstract': 'This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of WWH in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.68',\n",
       "   'authors': ['Deuksin Kwon',\n",
       "    'Sunwoo Lee',\n",
       "    'Ki Hyun Kim',\n",
       "    'Seojin Lee',\n",
       "    'Taeyoon Kim',\n",
       "    'Eric Davis'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I201',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.68.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79784/poster_document/0b95d3ccabaa63daf1cf84bad4f5fd45.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] What, When, and How to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue',\n",
       "   'tldr': 'This paper presents a method for building a personalized open-domain dialogue system to address the WWH (WHAT, WHEN, and HOW) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approa...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79784,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79784-what-when-and-how-to-ground-designing-user-persona-aware-conversational-agents-for-engaging-dialogue',\n",
       "   'video_url': None},\n",
       "  'I205': {'abstract': 'Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model uses query text and product title as input features, and estimates if the product is relevant for the customer query. While cross-attention in Transformers enables a more accurate relevance prediction in such a setting, its high evaluation latency makes it unsuitable for real-time predictions in which thousands of products must be evaluated against a user query within few milliseconds. To address this issue, we propose CUPID: a Curriculum learning based real-time Prediction using Distillation that utilizes knowledge distillation within a curriculum learning setting to learn a simpler architecture that can be evaluated within low latency budgets. In a bi-lingual relevance prediction task, our approach shows an 302 bps improvement on English and 676 bps improvement for low-resource Arabic, while maintaining the low evaluation latency on CPUs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.69',\n",
       "   'authors': ['Arindam Bhattacharya',\n",
       "    'Ankith Ms',\n",
       "    'Ankit Gandhi',\n",
       "    'Vijay Huddar',\n",
       "    'Atul Saroop',\n",
       "    'Rahul Bhagat'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I205',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.69.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] CUPID: Curriculum Learning Based Real-Time Prediction using Distillation',\n",
       "   'tldr': 'Relevance in E-commerce Product Search is crucial for providing customers with accurate results that match their query intent. With recent advancements in NLP and Deep Learning, Transformers have become the default choice for relevance classification tasks. In such a setting, the relevance model use...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79801,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79801-cupid-curriculum-learning-based-real-time-prediction-using-distillation',\n",
       "   'video_url': None},\n",
       "  'I207': {'abstract': 'Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant answers, leading to bad user experiences. We analyze failed QA requests to identify core challenges: lexical gaps, proposition types, complex syntactic structure, and high specificity. We propose a Semantic Question Reformulation (SURF) model offering three linguistically-grounded operations (repair, syntactic reshaping, generalization) to rewrite questions to facilitate answering. Offline evaluation on 1M unanswered questions from a leading voice assistant shows that SURF significantly improves answer rates: up to 24\\\\% of previously unanswered questions obtain relevant answers (75\\\\%). Live deployment shows positive impact for millions of customers with unanswered questions; explicit relevance feedback shows high user satisfaction.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.70',\n",
       "   'authors': ['Pedro Faustini',\n",
       "    'Zhiyu Chen',\n",
       "    'Besnik Fetahu',\n",
       "    'Oleg Rokhlenko',\n",
       "    'Shervin Malmasi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-6_-industry-(oral)'],\n",
       "   'id': 'I207',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.70.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79766/poster/0efe4ad5116ef5058c9660dd832e8cf3.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Answering Unanswered Questions through Semantic Reformulations in Spoken QA',\n",
       "   'tldr': 'Spoken Question Answering (QA) is a key feature of voice assistants, usually backed by multiple QA systems. Users ask questions via spontaneous speech that can contain disfluencies, errors, and informal syntax or phrasing. This is a major challenge in QA, causing unanswered questions or irrelevant a...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79766,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15266/lecture/79766-industry-answering-unanswered-questions-through-semantic-reformulations-in-spoken-qa',\n",
       "   'video_url': None},\n",
       "  'I208': {'abstract': 'Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe on Flan-T5 yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions is also very competitive.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.71',\n",
       "   'authors': ['Soham Parikh',\n",
       "    'Mitul Tiwari',\n",
       "    'Prashil Tumbade',\n",
       "    'Quaizar Vohra'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I208',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.71.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79778/poster_document/53f14782aeb2c9df78c1472ac4c8b7b6.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79778/slideshow/0aa712bfc3ebba7aa3fbb02a8df1a9d0.pptx',\n",
       "   'title': '[Industry] Exploring Zero and Few-shot Techniques for Intent Classification',\n",
       "   'tldr': 'Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classificati...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79778,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79778-industry-exploring-zero-and-few-shot-techniques-for-intent-classification',\n",
       "   'video_url': None},\n",
       "  'I213': {'abstract': \"Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants  have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phone numbers, addresses, email addresses, urls, and dates on their phone screens. We focus on reference understanding, which is particularly interesting when, similar to visual grounding, there are multiple similar texts on screen. We collect a dataset and propose a lightweight general purpose model for this novel experience. Since consuming pixels directly is expensive, our system is designed to rely only on text extracted from the UI. Our model is modular, offering flexibility, better interpretability and efficient run time memory.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.72',\n",
       "   'authors': ['Shruti Bhargava',\n",
       "    'Anand Dhoot',\n",
       "    'Ing-marie Jonsson',\n",
       "    'Hoang Long Nguyen',\n",
       "    'Alkesh Patel',\n",
       "    'Hong Yu',\n",
       "    'Vincent Renkens'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I213',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.72.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Referring to Screen Texts with Voice Assistants',\n",
       "   'tldr': \"Voice assistants help users make phone calls, send messages, create events, navigate and do a lot more. However assistants  have limited capacity to understand their users' context. In this work, we aim to take a step in this direction. Our work dives into a new experience for users to refer to phon...\",\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79815,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79815-referring-to-screen-texts-with-voice-assistants',\n",
       "   'video_url': None},\n",
       "  'I215': {'abstract': \"Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Providing FAQ content without disrupting user's shopping experience poses challenges on deciding when and how to show FAQ results. \\n\\nOur proposed intent-aware FAQ retrieval consists of (1) an intent classifier that predicts whether the query is looking for an FAQ; (2) a reformulation model that rewrites query into a natural question. \\nOffline evaluation demonstrates that our approach improves 12\\\\% in Hit@1 on retrieving ground-truth FAQs, while reducing latency by 95\\\\% compared to baseline systems. These improvements are further validated by real user feedback, where more than 99\\\\% of users consider FAQs displayed on top of product search results is helpful. Overall, our findings show promising directions for integrating FAQ retrieval into product search at scale.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.73',\n",
       "   'authors': ['Zhiyu Chen',\n",
       "    'Jason Choi',\n",
       "    'Besnik Fetahu',\n",
       "    'Oleg Rokhlenko',\n",
       "    'Shervin Malmasi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I215',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.73.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search',\n",
       "   'tldr': 'Frequently Asked Question (FAQ) retrieval aims at retrieving question-answer pairs for a given a user query. Integrating FAQ retrieval with product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Pro...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79807,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79807-generate-then-retrieve-intent-aware-faq-retrieval-in-product-search',\n",
       "   'video_url': None},\n",
       "  'I222': {'abstract': 'Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational vision-language models (VLMs) featuring impressive generalizability and adaptability. In this paper, we perform the first empirical study of image ad understanding through the lens of pre-trained VLMs. We benchmark and reveal practical challenges in adapting these VLMs to image ad understanding. We propose a simple feature adaptation strategy to effectively fuse multimodal information for image ads and further empower it with knowledge of real-world entities. We hope our study draws more attention to image ad understanding which is broadly relevant to the advertising industry.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.74',\n",
       "   'authors': ['Zhiwei Jia',\n",
       "    'Pradyumna Narayana',\n",
       "    'Arjun Akula',\n",
       "    'Garima Pruthi',\n",
       "    'Hao Su',\n",
       "    'Sugato Basu',\n",
       "    'Varun Jampani'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I222',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.74.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models',\n",
       "   'tldr': 'Image ad understanding is a crucial task with wide real-world applications. Although highly challenging with the involvement of diverse atypical scenes, real-world entities, and reasoning over scene-texts, how to interpret image ads is relatively under-explored, especially in the era of foundational...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79802,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79802-kafa-rethinking-image-ad-understanding-with-knowledge-augmented-feature-adaptation-of-vision-language-models',\n",
       "   'video_url': None},\n",
       "  'I225': {'abstract': 'Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are looking before making a purchase decision and helps enrich the catalog quality to improve the overall customer experience (CX). We propose a weakly supervised Hierarchical Multi-task Classification Framework (HMCF) to identify topics from customer questions at various granularities. Complexity lies in creating a list of granular topics (taxonomy) for 1000s of product categories and building a scalable classification system. To this end, we introduce a clustering based Taxonomy Creation and Data Labeling (TCDL) module for creating taxonomy and labelled data with minimal supervision. Using TCDL module, taxonomy and labelled data creation task reduces to 2 hours as compared to 2 weeks of manual efforts by a subject matter expert. For classification, we propose a two level HMCF that performs multi-class classification to identify coarse level-1 topic and leverages NLI based label-aware approach to identify granular level-2 topic. We showcase that HMCF (based on BERT and NLI) a) achieves absolute improvement of 13\\\\% in Top-1 accuracy over single-task non-hierarchical baselines b) learns a generic domain invariant function that can adapt to constantly evolving taxonomy (open label set) without need of re-training. c) reduces model deployment efforts significantly since it needs only one model that caters to 1000s of product categories.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.75',\n",
       "   'authors': ['Jitenkumar Rana',\n",
       "    'Promod Yenigalla',\n",
       "    'Chetan Aggarwal',\n",
       "    'Sandeep Sricharan Mukku',\n",
       "    'Manan Soni',\n",
       "    'Rashmi Patange'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I225',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.75.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79782/poster_document/0f9bbe9e64a5f1d8aec09291a13b4c77.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Weakly supervised hierarchical multi-task classification of customer questions',\n",
       "   'tldr': 'Identifying granular and actionable topics from customer questions (CQ) posted on e-commerce websites helps surface the missing information expected by customers on the product detail page (DP), provide insights to brands and sellers on what critical product information that the customers are lookin...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79782,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79782-weakly-supervised-hierarchical-multi-task-classification-of-customer-questions',\n",
       "   'video_url': None},\n",
       "  'I227': {'abstract': 'Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place orders. In this paper, we present prescription digitization system for online medicine ordering built with minimal supervision. Our system uses a modular pipeline comprising a mix of ML and rule-based components for (a) image to text extraction, (b) segmentation into blocks and medication items, (c) medication attribute extraction, (d) matching against medicine catalog, and (e) shopping cart building. Our approach efficiently utilizes multiple signals like layout, medical ontologies, and semantic embeddings via LayoutLMv2 model to yield substantial improvement relative to strong baselines on medication attribute extraction. Our pipeline achieves +5.9\\\\% gain in precision@3 and +5.6\\\\% in recall@3 over catalog-based fuzzy matching baseline for shopping cart building for printed prescriptions.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.76',\n",
       "   'authors': ['Megha Sharma',\n",
       "    'Tushar Vatsal',\n",
       "    'Srujana Merugu',\n",
       "    'Aruna Rajan'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I227',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.76.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Automated Digitization of Unstructured Medical Prescriptions',\n",
       "   'tldr': 'Automated digitization of prescription images is a critical prerequisite to scale digital healthcare services such as online pharmacies. This is challenging in emerging markets since prescriptions are not digitized at source and patients lack the medical expertise to interpret prescriptions to place...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79829,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79829-automated-digitization-of-unstructured-medical-prescriptions',\n",
       "   'video_url': None},\n",
       "  'I27': {'abstract': 'Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without leveraging external knowledge. Adapting these models to downstream applications in specific domains like fashion requires fine-grained in-domain image-text corpus, which are usually less semantically aligned and in small scale that requires efficient pre-training strategies. In this paper, we propose a knowledge-guided fashion-domain language-image pre-training (FLIP) framework that focuses on learning fine-grained representations in e-commerce domain and utilizes external knowledge (i.e., product attribute schema), to improve the pre-training efficiency. Experiments demonstrate that FLIP outperforms previous state-of-the-art  VLP models  on Amazon data and on the Fashion-Gen dataset by large margins. FLIP has been successfully deployed in the Amazon catalog system to backfill missing attributes and improve the customer shopping experience.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.9',\n",
       "   'authors': ['Qinjin Jia',\n",
       "    'Yang Liu',\n",
       "    'Daoping Wu',\n",
       "    'Shaoyuan Xu',\n",
       "    'Huidong Liu',\n",
       "    'Jinmiao Fu',\n",
       "    'Roland Vollgraf',\n",
       "    'Bryan Wang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.9.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] KG-FLIP: Knowledge-guided Fashion-domain Language-Image Pre-training for E-commerce',\n",
       "   'tldr': 'Various Vision-Language Pre-training (VLP) models (e.g., CLIP, BLIP) have sprung up and dramatically advanced the benchmarks for public general-domain datasets (e.g., COCO, Flickr30k). Such models usually learn the cross-modal alignment from large-scale well-aligned image-text datasets without lever...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79785,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79785-kg-flip-knowledge-guided-fashion-domain-language-image-pre-training-for-e-commerce',\n",
       "   'video_url': None},\n",
       "  'I29': {'abstract': 'Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products.\\nDue to a large non-English speaking population in India, we observe a significant percentage of code-mix Hinglish search queries e.g., sasta atta. An intuitive approach to dealing with code-mix queries is to train an encoder-decoder model to translate the query to English to perform the search. However, the problem becomes non-trivial when the brand names themselves have Hinglish names and possibly have a literal English translation. In such queries, only the context (non-brand name) Hinglish words needs to be translated. In this paper, we propose a simple yet effective modification to the transformer training to preserve/correct Grocery brand names in the output while selectively translating the context words. To achieve this, we use an additional dataset of popular Grocery brand names. Brand names are added as tokens to the model vocabulary, and the token embeddings are randomly initialized. Further, we introduce a Brand loss in training the translation model. Brand loss is a cross entropy loss computed using a denoising auto-encoder objective with brand name data. We warm-start the training from a public pre-trained checkpoint (such as BART/T5) and further adapt it for query translation using the domain data. The proposed model is generic and can be used with English as well as code-mix Hinglish queries alleviating the need for language detection. To reduce the latency of the model for the production deployment, we use knowledge distillation and quantization. Experimental evaluation indicates that the proposed approach improves translation results by preserving/correcting English/Hinglish brand names. After positive results with A/B testing, the model is currently deployed in production.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.10',\n",
       "   'authors': ['Mandar Kulkarni', 'Nikesh Garera', 'Anusua Trivedi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I29',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.10.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Domain-specific transformer models for query translation',\n",
       "   'tldr': 'Due to the democratization of e-commerce, many product companies are listing their goods for online shopping. For periodic buying within a domain such as Grocery, consumers are generally inclined to buy certain brands of products.\\nDue to a large non-English speaking population in India, we observe a...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79827,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79827-domain-specific-transformer-models-for-query-translation',\n",
       "   'video_url': None},\n",
       "  'I3': {'abstract': 'In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in the deployment. It includes the balance between performance and cost, segmentation ambiguity due to domain diversity and vague words boundary, and multi-grained segmentation. In this context, we propose a simple yet effective approach, namely CWSeg, to augment PLM-based schemes by developing cohort training and versatile decoding strategies. Extensive experiments on benchmark datasets demonstrate the efficiency and generalization of our approach. The corresponding segmentation system is also implemented for practical usage and the demo is recorded.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.1',\n",
       "   'authors': ['Dedong Li', 'Rui Zhao', 'Fei Tan'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.1.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] CWSeg: An Efficient and General Approach to Chinese Word Segmentation',\n",
       "   'tldr': 'In this work, we report our efforts in advancing Chinese Word Segmentation for the purpose of rapid deployment in different applications. The pre-trained language model (PLM) based segmentation methods have achieved state-of-the-art (SOTA) performance, whereas this paradigm also poses challenges in ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79794,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79794-cwseg-an-efficient-and-general-approach-to-chinese-word-segmentation',\n",
       "   'video_url': None},\n",
       "  'I30': {'abstract': \"To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A chatbot should be able to understand and answer a variety of such queries to help users with relevant information. One of the important modules in the chatbot is automated intent identification, i.e., understanding the user's intention from the query text. \\nDue to non-English speaking users interacting with the chatbot, we often get a significant percentage of code mix queries and queries with grammatical errors, which makes the problem more challenging. \\nThis paper proposes a simple yet competent Semi-Supervised Learning (SSL) approach for label-efficient intent classification. We use a small labeled corpus and relatively larger unlabeled query data to train a transformer model. For training the model with labeled data, we explore supervised MixUp data augmentation. To train with unlabeled data, we explore label consistency with dropout noise. We experiment with different pre-trained transformer architectures, such as BERT and sentence-BERT. \\nExperimental results demonstrate that the proposed approach significantly improves over the supervised baseline, even with a limited labeled set. A variant of the model is currently deployed in production.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.11',\n",
       "   'authors': ['Mandar Kulkarni',\n",
       "    'Kyung Kim',\n",
       "    'Nikesh Garera',\n",
       "    'Anusua Trivedi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.11.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Label efficient semi-supervised conversational intent classification',\n",
       "   'tldr': 'To provide a convenient shopping experience and to answer user queries at scale, conversational platforms are essential for e-commerce. The user queries can be pre-purchase questions, such as product specifications and delivery time related, or post-purchase queries, such as exchange and return. A c...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79825,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79825-label-efficient-semi-supervised-conversational-intent-classification',\n",
       "   'video_url': None},\n",
       "  'I32': {'abstract': \"Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers' questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate.\\nWe evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that in-domain data is essential as cross-lingual rankers trained on other domains perform poorly on the PQA task, and that translation-based approaches are most effective for candidate ranking while multilingual finetuning works best for answer generation. Still, there remains a significant performance gap between the English and the cross-lingual test sets.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.12',\n",
       "   'authors': ['Xiaoyu Shen', 'Akari Asai', 'Bill Byrne', 'Adria De Gispert'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.12.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] xPQA: Cross-Lingual Product Question Answering in 12 Languages',\n",
       "   'tldr': \"Product Question Answering (PQA) systems are key in e-commerce applications as they provide responses to customers' questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product inf...\",\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79832,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79832-xpqa-cross-lingual-product-question-answering-in-12-languages',\n",
       "   'video_url': None},\n",
       "  'I36': {'abstract': 'Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on past data and testing on future data. In this paper, we observe that the appearances of news events on the same topic may display discernible patterns over time, and posit that such patterns can assist in selecting training instances that could make the model adapt better to future data. Specifically, we design an effective framework FTT (Forecasting Temporal Trends), which could forecast the temporal distribution patterns of news data and then guide the detector to fast adapt to future distribution. Experiments on the real-world temporally split dataset demonstrate the superiority of our proposed framework.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.13',\n",
       "   'authors': ['Beizhe Hu',\n",
       "    'Qiang Sheng',\n",
       "    'Juan Cao',\n",
       "    'Yongchun Zhu',\n",
       "    'Danding Wang',\n",
       "    'Zhengjia Wang',\n",
       "    'Zhiwei Jin'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I36',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.13.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79821/poster_document/fa57638f5848831ae70bbd59a080ebbe.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79821/slideshow/c45978b300695c4f01ca39023f6078ae.pdf',\n",
       "   'title': '[Industry] Learn over Past, Evolve for Future: Forecasting Temporal Trends for Fake News Detection',\n",
       "   'tldr': 'Fake news detection has been a critical task for maintaining the health of the online news ecosystem. However, very few existing works consider the temporal shift issue caused by the rapidly-evolving nature of news data in practice, resulting in significant performance degradation when training on p...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79821,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79821-industry-learn-over-past-evolve-for-future-forecasting-temporal-trends-for-fake-news-detection',\n",
       "   'video_url': None},\n",
       "  'I41': {'abstract': 'Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient and able to capture early emerging market trends. Query Attribute Understanding (QAU) is a sub-component of QU that involves extracting named attributes from user queries and linking them to existing e-commerce entities such as brand, material, color, etc. While extracting named entities from text has been extensively explored in the literature, QAU requires specific attention due to the nature of the queries, which are often short, noisy, ambiguous, and constantly evolving. \\nThis paper makes three contributions to QAU. First, we propose a novel end-to-end approach that jointly solves Named Entity Recognition (NER) and Entity Linking (NEL) and enables open-world reasoning for QAU. Second, we introduce a novel method for utilizing product graphs to enhance the representation of query entities. Finally, we present a new dataset constructed from public sources that can be used to evaluate the performance of future QAU systems.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.14',\n",
       "   'authors': ['Thomas Ricatte', 'Donato Crisostomi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I41',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.14.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] AVEN-GR: Attribute Value Extraction and Normalization using product GRaphs',\n",
       "   'tldr': 'Getting a good understanding of the user intent is vital for e-commerce applications to surface the right product to a given customer query. Query Understanding (QU) systems are essential for this purpose, and many e-commerce providers are working on complex solutions that need to be data efficient ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79770,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79770-aven-gr-attribute-value-extraction-and-normalization-using-product-graphs',\n",
       "   'video_url': None},\n",
       "  'I42': {'abstract': 'Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world industrial-strength applications, which require the use of complex distillation methods on even larger-scale PLMs (over 10B), limited by memory on GPUs and the switching of methods. To overcome these challenges, we propose GKD, a general knowledge distillation framework that supports distillation on larger-scale PLMs using various distillation methods. With GKD, developers can build larger distillation models on memory-limited GPUs and easily switch and combine different distillation methods within a single framework. Experimental results show that GKD can support the distillation of at least 100B-scale PLMs and 25 mainstream methods on 8 NVIDIA A100 (40GB) GPUs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.15',\n",
       "   'authors': ['Shicheng Tan',\n",
       "    'Weng Lam Tam',\n",
       "    'Yuanchun Wang',\n",
       "    'Wenwen Gong',\n",
       "    'Shu Zhao',\n",
       "    'Peng Zhang',\n",
       "    'Jie Tang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.15.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] GKD: A General Knowledge Distillation Framework for Large-scale Pre-trained Language Model',\n",
       "   'tldr': 'Currently, the reduction in the parameter scale of large-scale pre-trained language models (PLMs) through knowledge distillation has greatly facilitated their widespread deployment on various devices. However, the deployment of knowledge distillation systems faces great challenges in real-world indu...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79824,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79824-gkd-an-general-knowledge-distillation-framework-for-large-scale-pre-trained-language-model',\n",
       "   'video_url': None},\n",
       "  'I43': {'abstract': 'Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained interactions between objects from different modalities are far from well-established. This issue becomes more severe in the e-commerce domain, which lacks sufficient training data and fine-grained cross-modal knowledge. To alleviate the problem, this paper proposes a novel e-commerce knowledge-enhanced VLP model FashionKLIP. We first automatically establish a multi-modal conceptual knowledge graph from large-scale e-commerce image-text data, and then inject the prior knowledge into the VLP model to align across modalities at the conceptual level. The experiments conducted on a public benchmark dataset demonstrate that FashionKLIP effectively enhances the performance of e-commerce image-text retrieval upon state-of-the-art VLP models by a large margin. The application of the method in real industrial scenarios also proves the feasibility and efficiency of FashionKLIP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.16',\n",
       "   'authors': ['Xiaodan Wang',\n",
       "    'Chengyu Wang',\n",
       "    'Lei Li',\n",
       "    'Zhixu Li',\n",
       "    'Ben Chen',\n",
       "    'Linbo Jin',\n",
       "    'Jun Huang',\n",
       "    'Yanghua Xiao',\n",
       "    'Ming Gao'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I43',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.16.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] FashionKLIP: Enhancing E-Commerce Image-Text Retrieval with Fashion Multi-Modal Conceptual Knowledge Graph',\n",
       "   'tldr': 'Image-text retrieval is a core task in the multi-modal domain, which arises a lot of attention from both research and industry communities. Recently, the booming of visual-language pre-trained (VLP) models has greatly enhanced the performance of cross-modal retrieval. However, the fine-grained inter...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79796,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79796-fashionklip-enhancing-e-commerce-image-text-retrieval-with-fashion-multi-modal-conceptual-knowledge-graph',\n",
       "   'video_url': None},\n",
       "  'I45': {'abstract': 'Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of interest in a spoken utterance. We investigate improving joint IC and NER models using entity contrastive learning that attempts to cluster similar entities together in a learned representation space. We compare a full virtual assistant system trained using entity contrastive learning to a production baseline system that does not use contrastive learning. We present both offline results, using retrospective test sets, as well as live online results from an A/B test that compared the two systems. In both the offline and online settings, entity contrastive training improved overall performance against production baselines. Furthermore, we provide a detailed analysis of learned entity embeddings, including both qualitative analysis via dimensionality-reduced visualizations and quantitative analysis by computing alignment and uniformity metrics. We show that entity contrastive learning improves alignment metrics and produces well-formed embedding clusters in representation space.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.17',\n",
       "   'authors': ['Jonathan Rubin',\n",
       "    'Jason Crowley',\n",
       "    'George Leung',\n",
       "    'Morteza Ziyadi',\n",
       "    'Maria Minakova'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.17.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Entity Contrastive Learning in a Large-Scale Virtual Assistant System',\n",
       "   'tldr': 'Conversational agents are typically made up of domain (DC) and intent classifiers (IC) that identify the general subject an utterance belongs to and the specific action a user wishes to achieve. In addition, named entity recognition (NER) performs per token labeling to identify specific entities of ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79788,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79788-entity-contrastive-learning-in-a-large-scale-virtual-assistant-system',\n",
       "   'video_url': None},\n",
       "  'I46': {'abstract': 'Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text content, such as pre-trained language models, are not particularly effective on structured tabular data since they are typically trained on free-form natural language texts. In this paper, we present Tab-Cleaner, a model designed to handle error detection over text-rich tabular data following a pre-training / fine-tuning paradigm. We train Tab-Cleaner on a real-world Amazon Product Catalog table w.r.t millions of products and show improvements over state-of-the-art methods by 16\\\\textbackslash{}\\\\% on PR AUC over attribute applicability classification task and by 11\\\\textbackslash{}\\\\%  on PR AUC over attribute value validation task.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.18',\n",
       "   'authors': ['Kewei Cheng',\n",
       "    'Xian Li',\n",
       "    'Zhengyang Wang',\n",
       "    'Chenwei Zhang',\n",
       "    'Binxuan Huang',\n",
       "    'Yifan Ethan Xu',\n",
       "    'Xin Luna Dong',\n",
       "    'Yizhou Sun'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I46',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.18.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Tab-Cleaner: Weakly Supervised Tabular Data Cleaning via Pre-training for E-commerce Catalog',\n",
       "   'tldr': 'Product catalogs, conceptually in the form of text-rich tables, are self-reported by individual retailers and thus inevitably contain noisy facts. Verifying such textual attributes in product catalogs is essential to improve their reliability. However, popular methods for processing free-text conten...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79828,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79828-tab-cleaner-weakly-supervised-tabular-data-cleaning-via-pre-training-for-e-commerce-catalog',\n",
       "   'video_url': None},\n",
       "  'I47': {'abstract': 'Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately following an interaction. In contrast to these approaches, we introduce a new dialog-level annotation workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate the quality of dialogs as a whole, and also label dialogs for attributes such as goal completion and user sentiment. In this contribution, we show that: (i) while dialog quality cannot be completely decomposed into dialog-level attributes, there is a strong relationship between some objective dialog attributes and judgments of dialog quality; (ii) for the task of dialog-level quality estimation, a supervised model trained on dialog-level annotations outperforms methods based purely on aggregating turn-level features; and (iii) the proposed evaluation model shows better domain generalization ability compared to the baselines. On the basis of these results, we argue that having high-quality human-annotated data is an important component of evaluating interaction quality for large industrial-scale voice assistant platforms.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.19',\n",
       "   'authors': ['Abishek Komma',\n",
       "    'Nagesh Panyam Chandrasekarasastry',\n",
       "    'Timothy Leffel',\n",
       "    'Anuj Goyal',\n",
       "    'Angeliki Metallinou',\n",
       "    'Spyros Matsoukas',\n",
       "    'Aram Galstyan'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.19.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs',\n",
       "   'tldr': 'Measurement of interaction quality is a critical task for the improvement of large-scale spoken dialog systems. Existing approaches to dialog quality estimation either focus on evaluating the quality of individual turns, or collect dialog-level quality measurements from end users immediately followi...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79811,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79811-toward-more-accurate-and-generalizable-evaluation-metrics-for-task-oriented-dialogs',\n",
       "   'video_url': None},\n",
       "  'I54': {'abstract': 'Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in the past 30 years. From these reports, we select 2,463 tables, and manually generate 2,463 conversations with 35,494 QA pairs. Additionally,  we select 4,578 tables, from which 4,578 conversations with 73,595 QA pairs are automatically created via a template-based method. With the manually- and automatically-generated conversations, Tab-CQA contains answerable and unanswerable questions. For the answerable questions, we further diversify them to cover a wide range of skills, e.g., table retrieval, fact checking, numerical reasoning, so as to accommodate real-world scenarios. We further propose two different tabular CQA models, a text-based model and an operation-based model, and evaluate them on Tab-CQA. Experiment results show that Tab-CQA is a very challenging dataset, where a huge performance gap exists between human and neural models. We will publicly release Tab-CQA as a benchmark testbed to promote further research on Chinese tabular CQA.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.20',\n",
       "   'authors': ['Chuang Liu', 'Junzhuo Li', 'Deyi Xiong'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I54',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.20.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79820/poster_document/1156ee7b8427fcc529e3743ba81c8a1b.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Tab-CQA: A Tabular Conversational Question Answering Dataset on Financial Reports',\n",
       "   'tldr': 'Existing conversational question answering (CQA) datasets have been usually constructed from unstructured texts in English. In this paper, we propose Tab-CQA, a tabular CQA dataset created from Chinese financial reports that are extracted from listed companies in a wide range of different sectors in...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79820,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79820-tab-cqa-a-tabular-conversational-question-answering-dataset-on-financial-reports',\n",
       "   'video_url': None},\n",
       "  'I55': {'abstract': 'Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47\\\\%p on average for HyperClova (30B and 82B), and GPT-3.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.21',\n",
       "   'authors': ['Hwaran Lee',\n",
       "    'Seokhee Hong',\n",
       "    'Joonsuk Park',\n",
       "    'Takyoung Kim',\n",
       "    'Gunhee Kim',\n",
       "    'Jung-woo Ha'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.21.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79776/poster_document/89608b79720bb64a5b3d5e00751a4196.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications',\n",
       "   'tldr': 'Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79776,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79776-kosbi-a-dataset-for-mitigating-social-bias-risks-towards-safer-large-language-model-applications',\n",
       "   'video_url': None},\n",
       "  'I60': {'abstract': 'Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question answering (QA) on these conversations. However, most existing researches focus on document-based QA tasks, and there is a lack of researches on conversation-based QA and related datasets, especially in Chinese language. The challenges of conversation-based QA include: 1) answers may be scattered among multiple dialogue turns; 2) understanding complex dialogue contexts is more complicated than documents. To address these challenges, we propose a multi-span extraction model on this task and introduce continual pre-training and multi-task learning schemes to further improve model performance. To validate our approach, we construct two Chinese datasets using dialogues as the knowledge source, namely cs-qaconv and kd-qaconv, respectively. Experimental results demonstrate that the proposed model outperforms the baseline on both datasets. The online application also verifies the effectiveness of our method. The dataset kd-qaconv will be released publicly for research purposes.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.22',\n",
       "   'authors': ['Changlin Yang',\n",
       "    'Siye Liu',\n",
       "    'Sen Hu',\n",
       "    'Wangshu Zhang',\n",
       "    'Teng Xu',\n",
       "    'Jing Zheng'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I60',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.22.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Improving Knowledge Production Efficiency With Question Answering on Conversation',\n",
       "   'tldr': 'Through an online customer service application, we have collected many conversations between customer service agents and customers. Building a knowledge production system can help reduce the labor cost of maintaining the FAQ database for the customer service chatbot, whose core module is question an...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79826,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79826-improving-knowledge-production-efficiency-with-question-answering-on-conversation',\n",
       "   'video_url': None},\n",
       "  'I65': {'abstract': 'Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times.\\nThis paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets.\\nOur approach eliminates duplicates at the batch level, without altering the data distribution observed by the model, making it model-agnostic and easy to implement as a plug-and-play module. \\nWe also provide a mathematical expression to estimate the reduction in training time that our approach provides. \\nThrough empirical evidence, we show that our approach significantly reduces training times on various models across datasets with varying redundancy factors, without impacting their performance on the Named Entity Recognition task, both on publicly available datasets and in real industrial settings.\\nIn the latter, the approach speeds training by up to 87\\\\%, and by 46\\\\% on average, with a drop in model performance of 0.2\\\\% relative at worst.\\nWe finally release a modular and reusable codebase to further advance research in this area.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.23',\n",
       "   'authors': ['Donato Crisostomi',\n",
       "    'Andrea Caciolai',\n",
       "    'Alessandro Pedrani',\n",
       "    'Kay Rottmann',\n",
       "    'Alessandro Manzotti',\n",
       "    'Enrico Palumbo',\n",
       "    'Davide Bernardi'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I65',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.23.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Mitigating the Burden of Redundant Datasets via Batch-Wise Unique Samples and Frequency-Aware Losses',\n",
       "   'tldr': 'Datasets used to train deep learning models in industrial settings often exhibit skewed distributions with some samples repeated a large number of times.\\nThis paper presents a simple yet effective solution to reduce the increased burden of repeated computation on redundant datasets.\\nOur approach eli...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79812,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79812-mitigating-the-burden-of-redundant-datasets-via-batch-wise-unique-samples-and-frequency-aware-losses',\n",
       "   'video_url': None},\n",
       "  'I66': {'abstract': \"Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model's utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM --- prompt engineering, fine-tuning, and knowledge distillation --- using feedback from the brand's customer service agents. We find that the usability of a model's responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.24',\n",
       "   'authors': ['Kristen Howell',\n",
       "    'Gwen Christian',\n",
       "    'Pavel Fomitchov',\n",
       "    'Gitit Kehat',\n",
       "    'Julianne Marzulla',\n",
       "    'Leanne Rolston',\n",
       "    'Jadin Tredup',\n",
       "    'Ilana Zimmerman',\n",
       "    'Ethan Selfridge',\n",
       "    'Joseph Bradley'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I66',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.24.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Distilled Language Models are economically efficient for the enterprise. ...mostly.',\n",
       "   'tldr': 'Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context wind...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79803,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79803-distilled-language-models-are-economically-efficient-for-the-enterprise-mostly',\n",
       "   'video_url': None},\n",
       "  'I69': {'abstract': 'On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at once, such as communicating with a virtual assistant and speech-to-text. The simplest solution to serve multiple applications is to build application-specific (language) models, but this leads to an increase in memory. Therefore, we explore different data- and architecture-driven language modeling approaches to build a single application-agnostic model. We propose two novel feed-forward architectures that find an optimal trade off between different on-device constraints. In comparison to the application-specific solution, one of our novel approaches reduces the disk size by half, while maintaining speed and accuracy of the original model.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.25',\n",
       "   'authors': ['Markus Nussbaum-thom', 'Lyan Verwimp', 'Youssef Oualil'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I69',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.25.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Application-Agnostic Language Modeling for On-Device ASR',\n",
       "   'tldr': 'On-device automatic speech recognition systems face several challenges compared to server-based systems. They have to meet stricter constraints in terms of speed, disk size and memory while maintaining the same accuracy. Often they have to serve several ap- plications with different distributions at...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79791,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79791-application-agnostic-language-modeling-for-on-device-asr',\n",
       "   'video_url': None},\n",
       "  'I75': {'abstract': \"Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency streaming applications but have lower accuracy when compared to the state-of-the-art models. In this work, we build accurate LSTM, attention and CTC based streaming ASR models for large-scale Hinglish (blend of Hindi and English) Voice Search. We evaluate how various modifications in vanilla LSTM training improve the system's accuracy while preserving the streaming capabilities. We also discuss a simple integration of end-of-speech (EOS) detection with CTC models, which helps reduce the overall search latency. Our model achieves a word error rate (WER) of 3.69\\\\% without EOS and 4.78\\\\% with EOS, with \\\\textasciitilde{}1300 ms (\\\\textasciitilde{}46.64\\\\%) reduction in latency.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.26',\n",
       "   'authors': ['Abhinav Goyal', 'Nikesh Garera'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I75',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.26.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79813/poster_document/e7b75f1aff45d56f2df4cd44a81b4636.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Building Accurate Low Latency ASR for Streaming Voice Search in E-commerce',\n",
       "   'tldr': 'Automatic Speech Recognition (ASR) is essential for any voice-based application. The streaming capability of ASR becomes necessary to provide immediate feedback to the user in applications like Voice Search. LSTM/RNN and CTC based ASR systems are very simple to train and deploy for low latency strea...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79813,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79813-building-accurate-low-latency-asr-for-streaming-voice-search-in-e-commerce',\n",
       "   'video_url': None},\n",
       "  'I77': {'abstract': 'Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of Lists Attribute Extraction) benchmark dataset as a challenging new web extraction task. PLAtE focuses on shopping data, specifically extractions from product review pages with multiple items encompassing the tasks of: (1) finding product list segmentation boundaries and (2) extracting attributes for each product. PLAtE is composed of 52,898 items collected from 6,694 pages and 156,014 attributes, making it the first large-scale list page web extraction dataset. We use a multi-stage approach to collect and annotate the dataset and adapt three state-of-the-art web extraction models to the two tasks comparing their strengths and weaknesses both quantitatively and qualitatively.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.27',\n",
       "   'authors': ['Aidan San',\n",
       "    'Yuan Zhuang',\n",
       "    'Jan Bakus',\n",
       "    'Colin Lockard',\n",
       "    'David Ciemiewicz',\n",
       "    'Sandeep Atluri',\n",
       "    'Kevin Small',\n",
       "    'Yangfeng Ji',\n",
       "    'Heba Elfardy'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I77',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.27.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] PLAtE: A Large-scale Dataset for List Page Web Extraction',\n",
       "   'tldr': 'Recently, neural models have been leveraged to significantly improve the performance of information extraction from semi-structured websites. However, a barrier for continued progress is the small number of datasets large enough to train these models. In this work, we introduce the PLAtE (Pages of L...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79786,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79786-plate-a-large-scale-dataset-for-list-page-web-extraction',\n",
       "   'video_url': None},\n",
       "  'I78': {'abstract': 'Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the open-source community cannot support industrial domain-specific applications due to the lack of entity knowledge and low inference speed.\\nIn this paper, we propose Rapid Diffusion, a novel framework for training and deploying super-resolution, text-to-image latent diffusion models with rich entity knowledge injected and optimized networks.\\nFurthermore, we employ BladeDISC, an end-to-end Artificial Intelligence (AI) compiler, and FlashAttention techniques to optimize computational graphs of the generated models for online deployment. Experiments verify the effectiveness of our approach in terms of image quality and inference speed. In addition, we present industrial use cases and integrate Rapid Diffusion to an AI platform to show its practical values.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.28',\n",
       "   'authors': ['Bingyan Liu',\n",
       "    'Weifeng Lin',\n",
       "    'Zhongjie Duan',\n",
       "    'Chengyu Wang',\n",
       "    'Wu Ziheng',\n",
       "    'Zhang Zipeng',\n",
       "    'Kui Jia',\n",
       "    'Lianwen Jin',\n",
       "    'Cen Chen',\n",
       "    'Jun Huang'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I78',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.28.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Rapid Diffusion: Building Domain-Specific Text-to-Image Synthesizers with Fast Inference Speed',\n",
       "   'tldr': 'Text-to-Image Synthesis (TIS) aims to generate images based on textual inputs. Recently, several large pre-trained diffusion models have been released to create high-quality images with pre-trained text encoders and diffusion-based image synthesizers. However, popular diffusion-based models from the...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79804,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79804-rapid-diffusion-building-domain-specific-text-to-image-synthesizers-with-fast-inference-speed',\n",
       "   'video_url': None},\n",
       "  'I8': {'abstract': \"In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists' dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our work is the set of knowledge graphs (KGs) of ten abdominal organs- Liver, Kidney, Gallbladder, Uterus, Urinary bladder, Ovary, Pancreas, Prostate, Biliary Tree, and Bowel. Our method for constructing these KGs relies on extracting entity1-relation-entity2 triplets from a large collection (about 10,000) of free-text radiology reports. The quality and coverage of the KGs are verified by two experienced radiologists (practicing for the last 30 years and 8 years, respectively). The dictation of the radiologist is automatically converted to what is called a pathological description which is the clinical description of the findings of the radiologist during ultrasonography (USG).\\xa0\\nOur knowledge-enhanced deep learning model improves the reported BLEU-3, ROUGE-L, METEOR, and CIDEr scores of the pathological description generation by 2\\\\%, 4\\\\%, 2\\\\% and 2\\\\% respectively. To the best of our knowledge, this is the first attempt at representing the abdominal organs in the form of knowledge graphs and utilising these graphs for the automatic generation of USG reports. A Minimum Viable Product (MVP) has been made available to the beta users, i.e., radiologists of reputed hospitals, for testing and evaluation. Our solution guarantees report generation within 30 seconds of running a scan.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.2',\n",
       "   'authors': ['Kaveri Kale',\n",
       "    'Pushpak Bhattacharyya',\n",
       "    'Aditya Shetty',\n",
       "    'Milind Gune',\n",
       "    'Kush Shrivastava',\n",
       "    'Rustom Lawyer',\n",
       "    'Spriha Biswas'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.2.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79823/poster_document/881eb9492728179747e752bc3beb1984.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79823/slideshow/59c42574fd13f7bfa5d35134602e639d.pptx',\n",
       "   'title': '[Industry] \"Knowledge is Power\": Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation',\n",
       "   'tldr': 'In conventional radiology practice, the radiologist dictates the diagnosis to the transcriptionist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workflow is prone to delay and error. ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79823,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79823-industry-knowledge-is-power-constructing-knowledge-graph-of-abdominal-organs-and-using-them-for-automatic-radiology-report-generation',\n",
       "   'video_url': None},\n",
       "  'I81': {'abstract': \"E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute values from an eCommerce product page that contains both text and images is a challenging task, especially when the attribute value is not explicitly mentioned in the catalog. In this paper, we present a scalable solution for this problem where we pose attribute extraction problem as a question-answering task, which we solve using MXT, that consists of three key components: (i) MAG (Multimodal Adaptation Gate), (ii) Xception network, and (iii) T5 encoder-decoder. Our system consists of a generative model that generates attribute-values for a given product by using both textual and visual characteristics (e.g. images) of the product. We show that our system is capable of handling zero-shot attribute prediction (when attribute value is not seen in training data) and value-absent prediction (when attribute value is not mentioned in the text) which are missing in traditional classification-based and NER-based models respectively. We have trained our models using distant supervision, removing dependency on human labeling, thus making them practical for real-world applications. With this framework, we are able to train a single model for 1000s of (product-type, attribute) pairs, thus reducing the overhead of training and maintaining separate models. Extensive experiments on two real world datasets (total 57 attributes) show that our framework improves the absolute recall@90P by 10.16\\\\% and 6.9\\\\textbackslash{} from the existing state of the art models. In a popular e-commerce store, we have productionized our models that cater to \\\\textgreater{}12K (product-type, attribute) pairs, and have extracted \\\\textgreater{}150MM attribute values.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.29',\n",
       "   'authors': ['Anant Khandelwal',\n",
       "    'Happy Mittal',\n",
       "    'Shreyas Kulkarni',\n",
       "    'Deepak Gupta'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-3_-industry-(oral)'],\n",
       "   'id': 'I81',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.29.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/79762/poster/e8177be4443196b6eadc3b2b5a7ea6a4.jpg',\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Large Scale Generative Multimodal Attribute Extraction for E-commerce Attributes',\n",
       "   'tldr': \"E-commerce websites (e.g. Amazon, Alibaba) have a plethora of structured and unstructured information (text and images) present on the product pages. Sellers often don't label or mislabel values of the attributes (e.g. color, size etc.) for their products. Automatically identifying these attribute v...\",\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79762,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15229/lecture/79762-industry-large-scale-generative-multimodal-attribute-extraction-for-e-commerce-attributes',\n",
       "   'video_url': None},\n",
       "  'I83': {'abstract': \"The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications.\\nAt its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.\\n\\nTo address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. We use a semi-supervised approach for data augmentation and presents two different methods for utilizing unlabeled samples. One method relies directly on existing catalogs, while the other uses a generative model. We compare the pros and cons of each approach and present our experimental results.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.30',\n",
       "   'authors': ['Noa Avigdor',\n",
       "    'Guy Horowitz',\n",
       "    'Ariel Raviv',\n",
       "    'Stav Yanovsky Daye'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I83',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.30.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79783/poster_document/61c72c3ecffb7cffb46ece9393476001.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79783/slideshow/7b6cc043f9e2d33dbf970a7840e3c454.pdf',\n",
       "   'title': '[Industry] Consistent Text Categorization using Data Augmentation in e-Commerce',\n",
       "   'tldr': 'The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications.\\nAt its core, the product ca...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79783,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79783-industry-consistent-text-categorization-using-data-augmentation-in-e-commerce',\n",
       "   'video_url': None},\n",
       "  'I90': {'abstract': 'We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following \"bread and butter\" NLP tasks: (a) Domain classification, for choosing which DB table to query, whether the question is out-of-scope (b) Multi-head slot/entity extraction (SE) to extract the field criteria and other attributes such as its role (filter, sort etc) from the raw text and (c) Slot value disambiguation (SVD) to resolve/normalize raw spans from SE to format suitable to query a DB. This is a general purpose, DB language agnostic approach and the output can be used to query any DB and return results to the user. Also each of these tasks is extremely well studied, mature, easier to collect data for and enables better error analysis by tracing problems to specific components when something goes wrong.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.31',\n",
       "   'authors': ['Hanoz Bhathena', 'Aviral Joshi', 'Prateek Singh'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I90',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.31.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79775/poster_document/4146a5f8b1d4323f3b3ecffa31bb26a4.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] An efficient method for Natural Language Querying on Structured Data',\n",
       "   'tldr': 'We present an efficient and reliable approach to Natural Language Querying (NLQ) on databases (DB) which is not based on text-to-SQL type semantic parsing. Our approach simplifies the NLQ on structured data problem to the following \"bread and butter\" NLP tasks: (a) Domain classification, for choosin...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79775,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79775-an-efficient-method-for-natural-language-querying-on-structured-data',\n",
       "   'video_url': None},\n",
       "  'I92': {'abstract': \"Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immunotherapy using real-world patients' clinical data and molecular profiles. This paper investigates the potential of transformers to improve clinical prediction compared to conventional machine learning approaches and addresses the challenge of few-shot learning in predicting rare disease areas. The study benchmarks the efficacy of baselines and language models on prognostic prediction across multiple cancer types and investigates the impact of different pretrained language models under few-shot regimes. The results demonstrate significant improvements in accuracy and highlight the potential of NLP in clinical research to improve early detection and intervention for different diseases.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.32',\n",
       "   'authors': ['Zekai Chen', 'Mariann Micsinai Balan', 'Kevin Brown'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I92',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.32.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79822/poster_document/890cf431b628e1dccbfe339755d293c3.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/79822/slideshow/b9ce2b04446734ebbacd3dfa536ab86d.pdf',\n",
       "   'title': '[Industry] Boosting Transformers and Language Models for Clinical Prediction in Immunotherapy',\n",
       "   'tldr': 'Clinical prediction is an essential task in the healthcare industry. However, the recent success of transformers, on which large language models are built, has not been extended to this domain. In this research, we explore the use of transformers and language models in prognostic prediction for immu...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79822,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79822-boosting-transformers-and-language-models-for-clinical-prediction-in-immunotherapy',\n",
       "   'video_url': None},\n",
       "  'I93': {'abstract': \"This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural quality estimation metric supervises the method without requiring reference translations. The method's online learning capability enables it to adapt to changes in the domain or MT engines dynamically, eliminating the requirement for retraining. The method selects a subset of translation engines to be called based on the source sentence features. The degree of exploration is configurable according to the desired quality-cost trade-off. Results from custom datasets demonstrate that EvolveMT achieves similar translation accuracy at a lower cost than selecting the best translation of each segment from all translations using an MT quality estimator. To the best of our knowledge, EvolveMT is the first MT system that adapts itself after deployment to incoming translation requests from the production environment without needing costly retraining on human feedback.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.33',\n",
       "   'authors': ['Kamer Yksel',\n",
       "    'Ahmet Gunduz',\n",
       "    'Mohamed Al-badrashiny',\n",
       "    'Hassan Sawaf'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I93',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.33.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] EvolveMT: an Ensemble MT Engine Improving Itself with Usage Only',\n",
       "   'tldr': 'This work proposes a method named EvolveMT for the efficient combination of multiple machine translation (MT) engines. The method selects the output from one engine for each segment, using online learning techniques to predict the most appropriate system for each translation request. A neural qualit...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79808,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79808-evolvemt-an-ensemble-mt-engine-improving-itself-with-usage-only',\n",
       "   'video_url': None},\n",
       "  'I94': {'abstract': \"Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the other hand, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name and Unused Variable are the most common errors among others made by language models.\\nThrough extensive studies, we also show the impact of sampling temperature, model size, and context on static errors in code completions.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.34',\n",
       "   'authors': ['Hantian Ding',\n",
       "    'Varun Kumar',\n",
       "    'Yuchen Tian',\n",
       "    'Zijian Wang',\n",
       "    'Rob Kwiatkowski',\n",
       "    'Xiaopeng Li',\n",
       "    'Murali Krishna Ramanathan',\n",
       "    'Baishakhi Ray',\n",
       "    'Parminder Bhatia',\n",
       "    'Sudipta Sengupta'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-5_-industry-(poster)'],\n",
       "   'id': 'I94',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.34.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/79773/poster_document/ab504be29b12554b5dd1edef4b4b9bbf.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] A Static Evaluation of Code Completion by Large Language Models',\n",
       "   'tldr': 'Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform ...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79773,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/79773-a-static-evaluation-of-code-completion-by-large-language-models',\n",
       "   'video_url': None},\n",
       "  'I96': {'abstract': 'Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-industry.35',\n",
       "   'authors': ['Sarthak Ahuja',\n",
       "    'Mohammad Kachuee',\n",
       "    'Fatemeh Sheikholeslami',\n",
       "    'Weiqing Liu',\n",
       "    'Jaeyoung Do'],\n",
       "   'category': 'Industry',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Industry',\n",
       "   'event_ids': ['session-4_-industry-(virtual-poster)'],\n",
       "   'id': 'I96',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-industry.35.pdf',\n",
       "   'paper_type': 'industry',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Industry',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '[Industry] Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems',\n",
       "   'tldr': 'Off-Policy reinforcement learning has been the driving force for the state-of-the-art conversational AIs leading to more natural human-agent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance betw...',\n",
       "   'track': 'Industry',\n",
       "   'underline_id': 79805,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/79805-scalable-and-safe-remediation-of-defective-actions-in-self-learning-conversational-systems',\n",
       "   'video_url': None},\n",
       "  'IWSLT_1': {'abstract': 'This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sweta Agrawal',\n",
       "    'Antonios Anastasopoulos',\n",
       "    'Luisa Bentivogli',\n",
       "    'Ondej Bojar',\n",
       "    'Claudia Borg',\n",
       "    'Marine Carpuat',\n",
       "    'Roldano Cattoni',\n",
       "    'Mauro Cettolo',\n",
       "    'Mingda Chen',\n",
       "    'William Chen',\n",
       "    'Khalid Choukri',\n",
       "    'Alexandra Chronopoulou',\n",
       "    'Anna Currey',\n",
       "    'Thierry Declerck',\n",
       "    'Qianqian Dong',\n",
       "    'Kevin Duh',\n",
       "    'Yannick Estve',\n",
       "    'Marcello Federico',\n",
       "    'Souhir Gahbiche',\n",
       "    'Barry Haddow',\n",
       "    'Benjamin Hsu',\n",
       "    'Phu Mon Htut',\n",
       "    'Hirofumi Inaguma',\n",
       "    'Dvid Javorsk',\n",
       "    'John Judge',\n",
       "    'Yasumasa Kano',\n",
       "    'Tom Ko',\n",
       "    'Rishu Kumar',\n",
       "    'Pengwei Li',\n",
       "    'Xutai Ma',\n",
       "    'Prashant Mathur',\n",
       "    'Evgeny Matusov',\n",
       "    'Paul McNamee',\n",
       "    'John P. McCrae',\n",
       "    'Kenton Murray',\n",
       "    'Maria Nadejde',\n",
       "    'Satoshi Nakamura',\n",
       "    'Matteo Negri',\n",
       "    'Ha Nguyen',\n",
       "    'Jan Niehues',\n",
       "    'Xing Niu',\n",
       "    'Atul Kr. Ojha',\n",
       "    'John E. Ortega',\n",
       "    'Proyag Pal',\n",
       "    'Juan Pino',\n",
       "    'Lonneke van der Plas',\n",
       "    'Peter Polk',\n",
       "    'Elijah Rippeth',\n",
       "    'Elizabeth Salesky',\n",
       "    'Jiatong Shi',\n",
       "    'Matthias Sperber',\n",
       "    'Sebastian Stker',\n",
       "    'Katsuhito Sudoh',\n",
       "    'Yun Tang',\n",
       "    'Brian Thompson',\n",
       "    'Kevin Tran',\n",
       "    'Marco Turchi',\n",
       "    'Alex Waibel',\n",
       "    'Mingxuan Wang',\n",
       "    'Shinji Watanabe',\n",
       "    'Rodolfo Zevallos'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'other',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN',\n",
       "   'tldr': 'This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_10': {'abstract': 'We present a simple yet efficient method to enhance the quality of machine translation models trained on multimodal corpora by augmenting the training text with labels of detected objects in the corresponding video segments. We then test the effects of label augmentation in both baseline and two automatic speech recognition (ASR) conditions. In contrast with multimodal techniques that merge visual and textual features, our modular method is easy to implement and the results are more interpretable.  Comparisons are made with Transformer translation architectures trained with baseline and augmented labels, showing improvements of up to +1.0 BLEU on the How2 dataset.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jeremy Gwinnup',\n",
       "    'Tim Anderson',\n",
       "    'Brian Ore',\n",
       "    'Eric Hansen',\n",
       "    'Kevin Duh'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_10',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Video Translation Context with Object Labels',\n",
       "   'tldr': 'We present a simple yet efficient method to enhance the quality of machine translation models trained on multimodal corpora by augmenting the training text with labels of detected objects in the corresponding video segments. We then test the effects of label augmentation in both baseline and two aut',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_11': {'abstract': 'This paper presents the submission of Huawei Translation Services Center for the IWSLT 2023 dubbing task in the unconstrained setting. The proposed solution consists of a Transformer-based machine translation model and a phoneme duration predictor. The Transformer is deep and multiple target-to-source length-ratio class labels are used to control target lengths. The variation predictor in FastSpeech2 is utilized to predict phoneme durations. To optimize the isochrony in dubbing, re-ranking and scaling are performed. The source audio duration is used as a reference to re-rank the translations of different length-ratio labels, and the one with minimum time deviation is preferred. Additionally, the phoneme duration outputs are scaled within a defined threshold to narrow the duration gap with the source audio.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhiqiang Rao',\n",
       "    'Hengchao Shang',\n",
       "    'Jinlong Yang',\n",
       "    'Daimeng Wei',\n",
       "    'Zongyao Li',\n",
       "    'Jiaxin GUO',\n",
       "    'Shaojun Li',\n",
       "    'Zhengzhe Yu',\n",
       "    'Zhanglin Wu',\n",
       "    'Yuhao Xie',\n",
       "    'Bin Wei',\n",
       "    'Jiawei Zheng',\n",
       "    'Lizhi Lei',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Length-Aware NMT and Adaptive Duration for Automatic Dubbing',\n",
       "   'tldr': 'This paper presents the submission of Huawei Translation Services Center for the IWSLT 2023 dubbing task in the unconstrained setting. The proposed solution consists of a Transformer-based machine translation model and a phoneme duration predictor. The Transformer is deep and multiple target-to-sour',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_12': {'abstract': \"This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-trained models. Our primary submission for Tamasheq outperforms the previous state of the art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU on this year's test set, outperforming the second best participant by 7.7 points. For Quechua, we also rank first and achieve 17.7 BLEU, despite having only two hours of translation data. Finally, we show that our proposed multilingual architecture is also competitive for high-resource languages, outperforming the best unconstrained submission to the IWSLT 2021 Multilingual track, despite using much less training data and compute.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Edward Gow-Smith',\n",
       "    'Alexandre Berard',\n",
       "    'Marcely Zanon Boito',\n",
       "    'Ioan Calapodescu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"NAVER LABS Europe's Multilingual Speech Translation Systems for the IWSLT 2023 Low-Resource Track\",\n",
       "   'tldr': \"This paper presents NAVER LABS Europe's systems for Tamasheq-French and Quechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our work attempts to maximize translation quality in low-resource settings using multilingual parameter-efficient solutions that leverage strong pre-traine\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_13': {'abstract': \"This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired by offline-trained models and directly applied a policy to obtain the real-time inference; for the subtitling one, we adapted the direct ST model to produce well-formed subtitles and exploited the same architecture to produce timestamps needed for the subtitle synchronization with audiovisual content. Our English-German SimulST system shows a reduced computational-aware latency compared to the one achieved by the top-ranked systems in the 2021 and 2022 rounds of the task, with gains of up to 3.5 BLEU. Our automatic subtitling system outperforms the only-existing solution based on a direct system by 3.7 and 1.7 SubER in English-German and English-Spanish respectively.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sara Papi', 'Marco Gaido', 'Matteo Negri'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Direct Models for Simultaneous Translation and Automatic Subtitling: FBK@IWSLT2023',\n",
       "   'tldr': \"This paper describes the FBK's participation in the Simultaneous Translation and Automatic Subtitling tracks of the IWSLT 2023 Evaluation Campaign. Our submission focused on the use of direct architectures to perform both tasks: for the simultaneous one, we leveraged the knowledge already acquired b\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_14': {'abstract': 'There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2,  BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human ratings of SST, which has been recently collected as Continuous Ratings (CR), are unclear. In this paper, we leverage the evaluations of candidate systems submitted to the English-German SST task at IWSLT 2022 and conduct an extensive correlation analysis of CR and the aforementioned metrics. Our study reveals that the offline metrics are well correlated with CR and can be reliably used for evaluating machine translation in simultaneous mode, with some limitations on the test set size. We conclude that given the current quality levels of SST, these metrics can be used as proxies for CR, alleviating the need for large scale human evaluation. Additionally, we observe that correlations of the metrics with translation as a reference is significantly higher than with simultaneous interpreting, and thus we recommend the former for reliable evaluation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Dominik Machek', 'Ondej Bojar', 'Raj Dabre'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MT Metrics Correlate with Human Ratings of Simultaneous Speech Translation',\n",
       "   'tldr': 'There have been several meta-evaluation studies on the correlation between human ratings and offline machine translation (MT) evaluation metrics such as BLEU, chrF2,  BertScore and COMET. These metrics have been used to evaluate simultaneous speech translation (SST) but their correlations with human',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_16': {'abstract': \"This paper presents Huawei Translation Service Center (HW-TSC)'s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets  constrained and unconstrained conditions. We train the formality control models for these four language pairs under these two conditions respectively, and submit the corresponding translation results. Our efforts are divided into two fronts: enhancing general translation quality and improving formality control capability. According to the different requirements of the formality control task, we use a multi-stage pre-training method to train a bilingual or multilingual neural machine translation (NMT) model as the basic model, which can improve the general translation quality of the base model to a relatively high level. Then, under the premise of affecting the general translation quality of the basic model as little as possible, we adopt domain adaptation and reranking-based transductive learning methods to improve the formality control capability of the model.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhanglin Wu',\n",
       "    'Zongyao Li',\n",
       "    'Daimeng Wei',\n",
       "    'Hengchao Shang',\n",
       "    'Jiaxin Guo',\n",
       "    'Xiaoyu Chen',\n",
       "    'Zhiqiang Rao',\n",
       "    'Zhengzhe YU',\n",
       "    'Jinlong Yang',\n",
       "    'Shaojun Li',\n",
       "    'Yuhao Xie',\n",
       "    'Bin Wei',\n",
       "    'Jiawei Zheng',\n",
       "    'Ming Zhu',\n",
       "    'Lizhi Lei',\n",
       "    'Hao Yang',\n",
       "    'Yanfei Jiang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Neural Machine Translation Formality Control with Domain Adaptation and Reranking-based Transductive Learning',\n",
       "   'tldr': \"This paper presents Huawei Translation Service Center (HW-TSC)'s submission on the IWSLT 2023 formality control task, which provides two training scenarios: supervised and zero-shot, each containing two language pairs, and sets  constrained and unconstrained conditions. We train the formality contro\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_17': {'abstract': \"This paper presents HW-TSC's submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training, and unconstrained training) with models of cascaded architectures. We use data enhancement, pre-training models and other means to improve the ASR quality, and R-Drop, deep model, domain data selection, etc. to improve the translation quality. Compared with last year's best results, we achieve 2.1 BLEU improvement on the MuST-C English-German test set.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zongyao Li',\n",
       "    'Zhanglin Wu',\n",
       "    'Zhiqiang Rao',\n",
       "    'Xie YuHao',\n",
       "    'Guo JiaXin',\n",
       "    'Daimeng Wei',\n",
       "    'Hengchao Shang',\n",
       "    'Wang Minghan',\n",
       "    'Xiaoyu Chen',\n",
       "    'Zhengzhe YU',\n",
       "    'Li ShaoJun',\n",
       "    'Lei LiZhi',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HW-TSC at IWSLT2023: Break the Quality Ceiling of Offline Track via Pre-Training and Domain Adaptation',\n",
       "   'tldr': \"This paper presents HW-TSC's submissions to the IWSLT 2023 Offline Speech Translation task, including speech translation of talks from English to German, Chinese, and Japanese, respectively. We participate in all three conditions (constrained training, constrained with large language models training\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_2': {'abstract': 'We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Elizabeth Salesky',\n",
       "    'Kareem Darwish',\n",
       "    'Mohamed Al-Badrashiny',\n",
       "    'Mona Diab',\n",
       "    'Jan Niehues'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology',\n",
       "   'tldr': 'We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_21': {'abstract': 'This paper describes the submissions of the research group USTC-NELSLIP to the 2023 IWSLT Offline Speech Translation competition, which involves translating spoken English into written Chinese. We utilize both cascaded models and end-to-end models for this task. To improve the performance of the cascaded models, we introduce Whisper to reduce errors in the intermediate source language text, achieving a significant improvement in ASR recognition performance. For end-to-end models, we propose Stacked Acoustic-and-Textual En- coding extension (SATE-ex), which feeds the output of the acoustic decoder into the textual decoder for information fusion and to prevent error propagation. Additionally, we improve the performance of the end-to-end system in translating speech by combining the SATE-ex model with the encoder-decoder model through ensembling.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xinyuan Zhou',\n",
       "    'Jianwei Cui',\n",
       "    'Zhongyi Ye',\n",
       "    'Yichi Wang',\n",
       "    'Luzhen Xu',\n",
       "    'Hanyi Zhang',\n",
       "    'Weitai Zhang',\n",
       "    'Lirong Dai'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_21',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Submission of USTC's System for the IWSLT 2023 - Offline Speech Translation Track\",\n",
       "   'tldr': 'This paper describes the submissions of the research group USTC-NELSLIP to the 2023 IWSLT Offline Speech Translation competition, which involves translating spoken English into written Chinese. We utilize both cascaded models and end-to-end models for this task. To improve the performance of the cas',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_22': {'abstract': \"This paper describes I2R's submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year's edition. The I2R system leverages on pretrained models that have been exposed to large-scale audio and text data for our base model. We introduce several stages of additional pretraining followed by fine-tuning to adapt the system for the downstream speech translation task. The strategy is supplemented by other techniques such as data augmentation, domain tagging, knowledge distillation, and model ensemble, among others. We evaluate the system on several publicly available test sets for comparison.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Muhammad Huzaifah', 'Kye Min Tan', 'Richeng Duan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"I2R's End-to-End Speech Translation System for IWSLT 2023 Offline Shared Task\",\n",
       "   'tldr': \"This paper describes I2R's submission to the offline speech translation track for IWSLT 2023. We focus on an end-to-end approach for translation from English audio to German text, one of the three available language directions in this year's edition. The I2R system leverages on pretrained models tha\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_23': {'abstract': \"This paper describes the NiuTrans end-to-end speech translation system submitted for the IWSLT 2023 English-to-Chinese offline task. Our speech translation models are composed of pre-trained ASR and MT models under the SATE framework. Several pre-trained models with diverse architectures and input representations (e.g., log Mel-filterbank and waveform) were utilized. We proposed an IDA method to iteratively improve the performance of the MT models and generate the pseudo ST data through MT systems. We then trained ST models with different structures and data settings to enhance ensemble performance. Experimental results demonstrate that our NiuTrans system achieved a BLEU score of 29.22 on the MuST-C En-Zh tst-COMMON set, outperforming the previous year's submission by 0.12 BLEU despite using less MT training data.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuchen Han',\n",
       "    'Xiaoqian Liu',\n",
       "    'Hao Chen',\n",
       "    'Yuhao Zhang',\n",
       "    'Chen Xu',\n",
       "    'Tong Xiao',\n",
       "    'Jingbo Zhu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_23',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The NiuTrans End-to-End Speech Translation System for IWSLT23 English-to-Chinese Offline Task',\n",
       "   'tldr': 'This paper describes the NiuTrans end-to-end speech translation system submitted for the IWSLT 2023 English-to-Chinese offline task. Our speech translation models are composed of pre-trained ASR and MT models under the SATE framework. Several pre-trained models with diverse architectures and input r',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_24': {'abstract': 'This paper describes the ON-TRAC consortium speech translation systems developed for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation tracks featured in the low-resource and dialect speech translation shared tasks, namely; i) spoken Tamasheq to written French, ii) spoken Pashto to written French, and  iii) spoken Tunisian to written English. All our primary submissions are based on the end-to-end speech-to-text neural architecture using a pretrained SAMU-XLSR model as a speech encoder and a mbart model as a decoder. The SAMU-XLSR model is built from the XLS-R~128 in order to generate language agnostic sentence-level embeddings. This building is driven by the LaBSE model trained on multilingual text dataset. This architecture allows us to improve the input speech representations and achieve significant improvements compared to conventional end-to-end speech translation systems.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Antoine Laurent',\n",
       "    'Souhir Gahbiche',\n",
       "    'Ha Nguyen',\n",
       "    'Haroun Elleuch',\n",
       "    'Fethi Bougares',\n",
       "    'Antoine Thiol',\n",
       "    'Hugo Riguidel',\n",
       "    'Salima Mdhaffar',\n",
       "    'Galle Laperrire',\n",
       "    'Lucas Maison',\n",
       "    'Sameer Khurana',\n",
       "    'Yannick Estve'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_24',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ON-TRAC Consortium Systems for the IWSLT 2023 Dialectal and Low-resource Speech Translation Tasks',\n",
       "   'tldr': 'This paper describes the ON-TRAC consortium speech translation systems developed for IWSLT 2023 evaluation campaign. Overall, we participated in three speech translation tracks featured in the low-resource and dialect speech translation shared tasks, namely; i) spoken Tamasheq to written French, ii)',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_25': {'abstract': 'This paper describes the systems submitted for Marathi to Hindi low-resource speech translation task. Our primary submission is based on an end-to-end direct speech translation system, whereas the contrastive one is a cascaded system. The backbone of both the systems is a Hindi-Marathi bilingual ASR system trained on 2790 hours of imperfect transcribed speech. The end-to-end speech translation system was directly initialized from the ASR, and then fine-tuned for direct speech translation with an auxiliary CTC loss for translation. The MT model for the cascaded system is initialized from a cross-lingual language model, which was then fine-tuned using 1.6 M parallel sentences. All our systems were trained from scratch on publicly available datasets. In the end, we use a language model to re-score the n-best hypotheses. Our primary submission achieved 30.5 and 39.6 BLEU whereas the contrastive system obtained 21.7 and 28.6 BLEU on official dev and test sets respectively. The paper also presents the analysis on several experiments that were conducted and outlines the strategies for improving speech translation in low-resource scenarios.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Santosh Kesiraju',\n",
       "    'Karel Bene',\n",
       "    'Maksim Tikhonov',\n",
       "    'Jan ernock'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_25',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BUT Systems for IWSLT 2023 Marathi - Hindi Low Resource Speech Translation Task',\n",
       "   'tldr': 'This paper describes the systems submitted for Marathi to Hindi low-resource speech translation task. Our primary submission is based on an end-to-end direct speech translation system, whereas the contrastive one is a cascaded system. The backbone of both the systems is a Hindi-Marathi bilingual ASR',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_26': {'abstract': \"This paper describes CMU's submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Brian Yan',\n",
       "    'Jiatong Shi',\n",
       "    'Soumi Maiti',\n",
       "    'William Chen',\n",
       "    'Xinjian Li',\n",
       "    'Yifan Peng',\n",
       "    'Siddhant Arora',\n",
       "    'Shinji Watanabe'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"CMU's IWSLT 2023 Simultaneous Speech Translation System\",\n",
       "   'tldr': \"This paper describes CMU's submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use W\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_28': {'abstract': 'This paper describes the speech translation system submitted as part of the IWSLT 2023 shared task on low resource speech translation. The low resource task aids in building models for language pairs where the training corpus is limited. In this paper, we focus on two language pairs, namely, Tamasheq-French (TmhFra) and Marathi-Hindi (MrHi) and implement a speech translation system that is unconstrained. We evaluate three strategies in our system: (a) Data augmentation where we perform different operations on audio as well as text samples, (b) an ensemble model that integrates a set of models trained using a combination of augmentation strategies, and (c) post-processing techniques where we explore the use of large language models (LLMs) to improve the quality of sentences that are generated. Experiments show how data augmentation can relatively improve the BLEU score by 5.2% over the baseline system for TmhFra while an ensemble model further improves performance by 17% for TmhFra and 23% for MrHi task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Akshaya Vishnu Kudlu Shanbhogue',\n",
       "    'Ran Xue',\n",
       "    'Soumya Saha',\n",
       "    'Daniel Zhang',\n",
       "    'Ashwinkumar Ganesan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Low Resource Speech Translation with Data Augmentation and Ensemble Strategies',\n",
       "   'tldr': 'This paper describes the speech translation system submitted as part of the IWSLT 2023 shared task on low resource speech translation. The low resource task aids in building models for language pairs where the training corpus is limited. In this paper, we focus on two language pairs, namely, Tamashe',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_29': {'abstract': 'AppTek participated in the subtitling and formality tracks of the IWSLT 2023 evaluation. This paper describes the details of our subtitling pipeline - speech segmentation, speech recognition, punctuation prediction and inverse text normalization, text machine translation and direct speech-to-text translation, intelligent line segmentation - and how we make use of the provided subtitling-specific data in training and fine-tuning. The evaluation results show that our final submissions are competitive, in particular outperforming the submissions by other participants by 5% absolute as measured by the SubER subtitle quality metric. For the formality track, we participate with our En-Ru and En-Pt production models, which support formality control via prefix tokens. Except for informal Portuguese, we achieve near perfect formality level accuracy while at the same time offering high general translation quality.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Parnia Bahar',\n",
       "    'Patrick Wilken',\n",
       "    'Javier Iranzo-Snchez',\n",
       "    'Mattia Di Gangi',\n",
       "    'Evgeny Matusov',\n",
       "    'Zoltn Tske'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_29',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"Speech Translation with Style: AppTek's Submissions to the IWSLT Subtitling and Formality Tracks in 2023\",\n",
       "   'tldr': 'AppTek participated in the subtitling and formality tracks of the IWSLT 2023 evaluation. This paper describes the details of our subtitling pipeline - speech segmentation, speech recognition, punctuation prediction and inverse text normalization, text machine translation and direct speech-to-text tr',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_3': {'abstract': \"This paper presents the \\textsc{MineTrans} English-to-Chinese speech translation systems developed for two challenge tracks of IWSLT 2023, i.e., Offline Speech Translation (S2T) and Speech-to-Speech Translation (S2ST).  For the S2T track, \\textsc{MineTrans} employs a practical cascaded system to explore the limits of translation performance in both constrained and unconstrained settings, where the whole system consists of automatic speech recognition (ASR), punctuation recognition (PC), and machine translation (MT) modules.  We also investigate the effectiveness of multiple ASR architectures and explore two MT strategies: supervised in-domain fine-tuning and prompt-guided translation using a large language model.  For the S2ST track, we explore a speech-to-unit (S2U) framework to build an end-to-end S2ST system. This system encodes the target speech as discrete units via our trained HuBERT. Then it leverages the standard sequence-to-sequence model to directly learn the mapping between source speech and discrete units without any auxiliary recognition tasks (i.e., ASR and MT tasks). Various efforts are made to improve the \\textsc{MineTrans}'s performance, such as acoustic model pre-training on large-scale data, data filtering, data augmentation, speech segmentation, knowledge distillation, consistency training, model ensembles, etc.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yichao Du',\n",
       "    'Guo Zhengsheng',\n",
       "    'Jinchuan Tian',\n",
       "    'Zhirui Zhang',\n",
       "    'Xing Wang',\n",
       "    'Jianwei Yu',\n",
       "    'Zhaopeng Tu',\n",
       "    'Tong Xu',\n",
       "    'Enhong Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The MineTrans Systems for IWSLT 2023 Offline Speech Translation and Speech-to-Speech Translation Tasks',\n",
       "   'tldr': 'This paper presents the \\textsc{MineTrans} English-to-Chinese speech translation systems developed for two challenge tracks of IWSLT 2023, i.e., Offline Speech Translation (S2T) and Speech-to-Speech Translation (S2ST).  For the S2T track, \\textsc{MineTrans} employs a practical cascaded system to explo',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_30': {'abstract': 'This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE--SPA) track featured in the Evaluation Campaign of IWSLT 2023: low-resource and dialect speech translation. Two main submission types were supported in the campaign: constrained and unconstrained. We submitted six total systems of which our best (primary) constrained system consisted of an ST model based on the Fairseq S2T framework where the audio representations were created using log mel-scale filter banks as features and the translations were performed using a transformer. The best (primary) unconstrained system used a pipeline approach which combined automatic speech recognition (ASR) with machine translation (MT). The ASR transcriptions for the best unconstrained system were computed using a pre-trained XLS-R-based model along with a fine-tuned language model. Transcriptions were translated using a MT system based on a fine-tuned, pre-trained language model (PLM). The four other submissions are presented in this article (2 constrained and 2 unconstrained) for comparison because they consist of various architectures. Our results show that direct ST (ASR and MT combined together) can be more effective than a PLM in a low-resource (constrained) setting for Quechua to Spanish. On the other hand, we show that fine-tuning of any type on both the ASR and MT system is worthwhile, resulting in nearly 16 BLEU for the unconstrained task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['John E. Ortega', 'Rodolfo Zevallos', 'William Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_30',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'QUESPA Submission for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks',\n",
       "   'tldr': 'This article describes the QUESPA team speech translation (ST) submissions for the Quechua to Spanish (QUE--SPA) track featured in the Evaluation Campaign of IWSLT 2023: low-resource and dialect speech translation. Two main submission types were supported in the campaign: constrained and unconstrain',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_31': {'abstract': 'This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks. We submitted systems for five low-resource tasks and the dialectal task. In this work, we explored self-supervised pre-trained speech models and finetuned them on speech translation downstream tasks. We use the Wav2vec 2.0, XLSR-53, and Hubert as self-supervised models. Unlike Hubert, Wav2vec 2.0 and XLSR-53 achieve the best results when we remove the top three layers. Our results show that Wav2vec 2.0 and Hubert perform similarly with their relative best configuration. In addition, we found that Wav2vec 2.0 pre-trained on audio data of the same language as the source language of a speech translation model achieves better results. For the low-resource setting, the best results are achieved using either the Wav2vec 2.0 or Hubert models, while XLSR-53 achieves the best results for the dialectal transfer task. We find that XLSR-53 does not perform well for low-resource tasks. Using Wav2vec 2.0, we report close to~2 BLEU point improvements on the test set for the Tamasheq-French compared to the baseline system at the IWSLT 2022.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jonathan Mbuya', 'Antonios Anastasopoulos'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_31',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks',\n",
       "   'tldr': 'This paper describes the GMU Systems for the IWSLT 2023 Dialect and Low-resource Speech Translation Tasks. We submitted systems for five low-resource tasks and the dialectal task. In this work, we explored self-supervised pre-trained speech models and finetuned them on speech translation downstream ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_32': {'abstract': 'This paper describes our work on the IWSLT2023 Speech-to-Speech task. Our proposed cascaded system consists of an ensemble of Conformer and S2T-Transformer-based ASR models, a Transformer-based MT model, and a Diffusion-based TTS model. Our primary focus in this competition was to investigate the modeling ability of the Diffusion model for TTS tasks in high-resource scenarios and the role of TTS in the overall S2S task. To this end, we proposed DTS, an end-to-end diffusion-based TTS model that takes raw text as input and generates waveform by iteratively denoising on pure Gaussian noise. Compared to previous TTS models, the speech generated by DTS is more natural and performs better in code-switching scenarios. As the training process is end-to-end, it is relatively straightforward. Our experiments demonstrate that DTS outperforms other TTS models on the GigaS2S benchmark, and also brings positive gains for the entire S2S system.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Minghan Wang',\n",
       "    'Yinglu Li',\n",
       "    'Jiaxin GUO',\n",
       "    'Zongyao Li',\n",
       "    'Hengchao Shang',\n",
       "    'Daimeng Wei',\n",
       "    'Min Zhang',\n",
       "    'Shimin Tao',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_32',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"The HW-TSC's Speech-to-Speech Translation System for IWSLT 2023\",\n",
       "   'tldr': 'This paper describes our work on the IWSLT2023 Speech-to-Speech task. Our proposed cascaded system consists of an ensemble of Conformer and S2T-Transformer-based ASR models, a Transformer-based MT model, and a Diffusion-based TTS model. Our primary focus in this competition was to investigate the mo',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_33': {'abstract': \"This paper presents JHU's submissions to the IWSLT 2023 dialectal and low-resource track of Tunisian Arabic to English speech translation. The Tunisian dialect lacks formal orthography and abundant training data, making it challenging to develop effective speech translation (ST) systems. To address these challenges, we explore the integration of large pre-trained machine translation (MT) models, such as mBART and NLLB-200 in both end-to-end (E2E) and cascaded speech translation (ST) systems. We also improve the performance of automatic speech recognition (ASR) through the use of pseudo-labeling data augmentation and channel matching on telephone data. Finally, we combine our E2E and cascaded ST systems with Minimum Bayes-Risk decoding. Our combined system achieves a BLEU score of 21.6 and 19.1 on test2 and test3, respectively.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Amir Hussein',\n",
       "    'Cihan Xiao',\n",
       "    'Neha Verma',\n",
       "    'Thomas Thebaud',\n",
       "    'Matthew Wiesner',\n",
       "    'Sanjeev Khudanpur'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'JHU IWSLT 2023 Dialect Speech Translation System Description',\n",
       "   'tldr': \"This paper presents JHU's submissions to the IWSLT 2023 dialectal and low-resource track of Tunisian Arabic to English speech translation. The Tunisian dialect lacks formal orthography and abundant training data, making it challenging to develop effective speech translation (ST) systems. To address \",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_34': {'abstract': 'Multilingual neural translation models exploit cross-lingual transfer to perform zero-shot translation between unseen language pairs. Past efforts to improve cross-lingual transfer have focused on aligning contextual sentence-level representations. This paper introduces three novel contributions to allow exploiting nearest neighbours at the token level during training, including: (i) an efficient, gradient-friendly way to share representations between neighboring tokens; (ii) an attentional semantic layer which extracts latent features from shared embeddings; and (iii) an agreement loss to harmonize predictions across different sentence representations. Experiments on two multilingual datasets demonstrate consistent gains in zero shot translation over strong baselines.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nishant Kambhatla', 'Logan Born', 'Anoop Sarkar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_34',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning Nearest Neighbour Informed Latent Word Embeddings to Improve Zero-Shot Machine Translation',\n",
       "   'tldr': 'Multilingual neural translation models exploit cross-lingual transfer to perform zero-shot translation between unseen language pairs. Past efforts to improve cross-lingual transfer have focused on aligning contextual sentence-level representations. This paper introduces three novel contributions to ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_35': {'abstract': 'We describe the Johns Hopkins ACL 60-60 Speech Translation systems submitted to the IWSLT 2023 Multilingual track, where we were tasked to translate ACL presentations from English into 10 languages. We developed cascaded speech translation systems for both the constrained and unconstrained subtracks. Our systems make use of pre-trained models as well as domain-specific corpora for this highly technical evaluation-only task. We find that the specific technical domain which ACL presentations fall into presents a unique challenge for both ASR and MT, and we present an error analysis and an ACL-specific corpus we produced to enable further work in this area.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Henry Li Xinyuan',\n",
       "    'Neha Verma',\n",
       "    'Bismarck Bamfo Odoom',\n",
       "    'Ujvala Pradeep',\n",
       "    'Matthew Wiesner',\n",
       "    'Sanjeev Khudanpur'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'JHU IWSLT 2023 Multilingual Speech Translation System Description',\n",
       "   'tldr': 'We describe the Johns Hopkins ACL 60-60 Speech Translation systems submitted to the IWSLT 2023 Multilingual track, where we were tasked to translate ACL presentations from English into 10 languages. We developed cascaded speech translation systems for both the constrained and unconstrained subtracks',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_36': {'abstract': 'This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and text-to-speech (TTS). We make tremendous efforts to handle the challenging multi-source input. Specifically, to improve the robustness to multi-source speech input, we adopt various data augmentation strategies and a ROVER-based score fusion on multiple ASR model outputs. To better handle the noisy ASR transcripts, we introduce a three-stage fine-tuning strategy to improve translation accuracy. Finally, we build a TTS model with high naturalness and sound quality, which leverages a two-stage framework, using network bottleneck features as a robust intermediate representation for speaker timbre and linguistic content disentanglement. Based on the two-stage framework, pre-trained speaker embedding is leveraged as a condition to transfer the speaker timbre in the source English speech to the translated Chinese speech. Experimental results show that our system has high translation accuracy, speech naturalness, sound quality, and speaker similarity. Moreover, it shows good robustness to multi-source data.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kun Song',\n",
       "    'Yi Lei',\n",
       "    'Peikun Chen',\n",
       "    'Yiqing Cao',\n",
       "    'Kun Wei',\n",
       "    'Yongmao Zhang',\n",
       "    'Lei Xie',\n",
       "    'Ning Jiang',\n",
       "    'Guoqing Zhao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_36',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task',\n",
       "   'tldr': 'This paper describes the NPU-MSXF system for the IWSLT 2023 speech-to-speech translation (S2ST) task which aims to translate from English speech of multi-source to Chinese speech. The system is built in a cascaded manner consisting of automatic speech recognition (ASR), machine translation (MT), and',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_37': {'abstract': \"This paper describes the UCSC's submission to the shared task on formality control for spoken language translation at IWSLT 2023. For this task, we explored the use of 'additive style intervention' using a pre-trained multilingual translation model, namely mBART. Compared to prior approaches where a single style-vector was added to all tokens in the encoder output, we explored an alternative approach in which we learn a unique style-vector for each input token. We believe this approach, which we call 'style embedding intervention,' is better suited for formality control as it can potentially learn which specific input tokens to modify during decoding. While the proposed approach obtained similar performance to 'additive style intervention' for the supervised English-to-Vietnamese task, it performed significantly better for English-to-Korean, in which it achieved an average matched accuracy of 90.6 compared to 85.2 for the baseline. When we constrained the model further to only perform style intervention on the <bos> (beginning of sentence) token, the average matched accuracy improved further to 92.0, indicating that the model could learn to control the formality of the translation output based solely on the embedding of the <bos> token.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Priyesh Vakharia', 'Shree Vignesh S', 'Pranjali Basmatkar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_37',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Low-Resource Formality Controlled NMT Using Pre-trained LM',\n",
       "   'tldr': \"This paper describes the UCSC's submission to the shared task on formality control for spoken language translation at IWSLT 2023. For this task, we explored the use of 'additive style intervention' using a pre-trained multilingual translation model, namely mBART. Compared to prior approaches where a\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_38': {'abstract': \"This paper describes NAIST's submission to the IWSLT 2023 Simultaneous Speech Translation task: English-to-{German, Japanese, Chinese} speech-to-text translation and English-to-Japanese speech-to-speech translation. Our speech-to-text system uses an end-to-end multilingual speech translation model based on large-scale pre-trained speech and text models. We add Inter-connections into the model to incorporate the outputs from intermediate layers of the pre-trained speech model and augment prefix-to-prefix text data using Bilingual Prefix Alignment to enhance the simultaneity of the offline speech translation model. Our speech-to-speech system employs an incremental text-to-speech module that consists of a Japanese pronunciation estimation model, an acoustic model, and a neural vocoder.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ryo Fukuda',\n",
       "    'Yuta Nishikawa',\n",
       "    'Yasumasa Kano',\n",
       "    'Yuka Ko',\n",
       "    'Tomoya Yanagita',\n",
       "    'Kosuke Doi',\n",
       "    'Mana Makinae',\n",
       "    'Sakriani Sakti',\n",
       "    'Katsuhito Sudoh',\n",
       "    'Satoshi Nakamura'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_38',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NAIST Simultaneous Speech-to-speech Translation System for IWSLT 2023',\n",
       "   'tldr': \"This paper describes NAIST's submission to the IWSLT 2023 Simultaneous Speech Translation task: English-to-{German, Japanese, Chinese} speech-to-text translation and English-to-Japanese speech-to-speech translation. Our speech-to-text system uses an end-to-end multilingual speech translation model b\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_39': {'abstract': 'The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Under this notion of target adaptive training, generating rare or difficult tokens is rewarded which improves the translation quality while reducing latency. The predictions made by a language model in the decoder are combined with the traditional cross entropy loss which frees up the focus on the source side context. Our experimental results over multiple language pairs show that compared to previous state of the art methods in simultaneous translation, we can use an augmented target side context to improve BLEU scores significantly. We show improvements over the state of the art in the low latency range with lower average lagging values (faster output).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aditi Jain', 'Nishant Kambhatla', 'Anoop Sarkar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_39',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Language Model Based Target Token Importance Rescaling for Simultaneous Neural Machine Translation',\n",
       "   'tldr': 'The decoder in simultaneous neural machine translation receives limited information from the source while having to balance the opposing requirements of latency versus translation quality. In this paper, we use an auxiliary target-side language model to augment the training of the decoder model. Und',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_4': {'abstract': 'End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AST student model. While KD allows using larger pretrained models, the reliance of previous KD approaches on manual audio transcripts in the data pipeline restricts the applicability of this framework to AST. We present an imitation learning approach where a teacher NMT system corrects the errors of an AST student without relying on manual transcripts. We show that the NMT teacher can recover from errors in automatic transcriptions and is able to correct erroneous translations of the AST student, leading to improvements of about 4 BLEU points over the standard AST end-to-end baseline on the English-German CoVoST-2 and MuST-C datasets, respectively. Code and data are publicly available: https://github.com/HubReb/imitkd_ast/releases/tag/v1.1',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Rebekka Hubert', 'Artem Sokolov', 'Stefan Riezler'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving End-to-End Speech Translation by Imitation-Based Knowledge Distillation with Synthetic Transcripts',\n",
       "   'tldr': 'End-to-end automatic speech translation (AST) relies on data that combines audio inputs with text translation outputs. Previous work used existing large parallel corpora of transcriptions and translations in a knowledge distillation (KD) setup to distill a neural machine translation (NMT) into an AS',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_40': {'abstract': 'This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took a cascade approach of an acoustic model and a vocoder.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhengdong Yang',\n",
       "    'Shuichiro Shimizu',\n",
       "    'Wangjin Zhou',\n",
       "    'Sheng Li',\n",
       "    'Chenhui Chu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Kyoto Speech-to-Speech Translation System for IWSLT 2023',\n",
       "   'tldr': 'This paper describes the Kyoto speech-to-speech translation system for IWSLT 2023. Our system is a combination of speech-to-text translation and text-to-speech synthesis. For the speech-to-text translation model, we used the dual-decoderTransformer model. For text-to-speech synthesis model, we took ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_41': {'abstract': 'Simultaneous speech translation (SimulST) translates partial speech inputs incrementally. Although the monotonic correspondence between input and output is preferable for smaller latency, it is not the case for distant language pairs such as English and Japanese. A prospective approach to this problem is to mimic simultaneous interpretation (SI) using SI data to train a SimulST model. However, the size of such SI data is limited, so the SI data should be used together with ordinary bilingual data whose translations are given in offline. In this paper, we propose an effective way to train a SimulST model using mixed data of SI and offline. The proposed method trains a single model using the mixed data with style tags that tell the model to generate SI- or offline-style outputs. Experiment results show improvements of BLEURT in different latency ranges, and our analyses revealed the proposed model generates SI-style outputs more than the baseline.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yuka Ko',\n",
       "    'Ryo Fukuda',\n",
       "    'Yuta Nishikawa',\n",
       "    'Yasumasa Kano',\n",
       "    'Katsuhito Sudoh',\n",
       "    'Satoshi Nakamura'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_41',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Tagged End-to-End Simultaneous Speech Translation Training Using Simultaneous Interpretation Data',\n",
       "   'tldr': 'Simultaneous speech translation (SimulST) translates partial speech inputs incrementally. Although the monotonic correspondence between input and output is preferable for smaller latency, it is not the case for distant language pairs such as English and Japanese. A prospective approach to this probl',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_42': {'abstract': 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Text Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our proposed solution is a cascaded incremental decoding system that comprises an ASR model and an MT model. The ASR model is based on the U2++ architecture and can handle both streaming and offline speech scenarios with ease. Meanwhile, the MT model adopts the Deep-Transformer architecture. To improve performance, we explore methods to generate a confident partial target text output that guides the next MT incremental decoding process. In our experiments, we demonstrate that our simultaneous strategies achieve low latency while maintaining a loss of no more than 2 BLEU points when compared to offline systems.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiaxin GUO',\n",
       "    'Daimeng Wei',\n",
       "    'Zhanglin Wu',\n",
       "    'Zongyao Li',\n",
       "    'Zhiqiang Rao',\n",
       "    'Minghan Wang',\n",
       "    'Hengchao Shang',\n",
       "    'Xiaoyu Chen',\n",
       "    'Zhengzhe Yu',\n",
       "    'Shaojun Li',\n",
       "    'Yuhao Xie',\n",
       "    'Lizhi Lei',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"The HW-TSC's Simultaneous Speech-to-Text Translation System for IWSLT 2023 Evaluation\",\n",
       "   'tldr': 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Text Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our proposed solution is a cascaded incremental decoding system that comprises ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_43': {'abstract': 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Speech Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our solution is a cascaded incremental decoding system, consisting of an ASR model, an MT model, and a TTS model. By adopting the strategies used in the Speech-to-Text track, we have managed to generate a more confident target text for each audio segment input, which can guide the next MT incremental decoding process. Additionally, we have integrated the TTS model to seamlessly reproduce audio files from the translation hypothesis. To enhance the effectiveness of our experiment, we have utilized a range of methods to reduce error conditions in the TTS input text and improve the smoothness of the TTS output audio.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hengchao Shang',\n",
       "    'Zhiqiang Rao',\n",
       "    'Zongyao Li',\n",
       "    'Zhanglin Wu',\n",
       "    'Jiaxin GUO',\n",
       "    'Minghan Wang',\n",
       "    'Daimeng Wei',\n",
       "    'Shaojun Li',\n",
       "    'Zhengzhe YU',\n",
       "    'Xiaoyu Chen',\n",
       "    'Lizhi Lei',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_43',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"The HW-TSC's Simultaneous Speech-to-Speech Translation System for IWSLT 2023 Evaluation\",\n",
       "   'tldr': 'In this paper, we present our submission to the IWSLT 2023 Simultaneous Speech-to-Speech Translation competition. Our participation involves three language directions: English-German, English-Chinese, and English-Japanese. Our solution is a cascaded incremental decoding system, consisting of an ASR ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_44': {'abstract': 'In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Peter Polak',\n",
       "    'Danni Liu',\n",
       "    'Ngoc-Quan Pham',\n",
       "    'Jan Niehues',\n",
       "    'Alexander Waibel',\n",
       "    'Ondej Bojar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023',\n",
       "   'tldr': 'In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional en',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_45': {'abstract': 'This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders with CTC and Optimal Transport, to adapt the speech representations to the space of the text model, thus maximizing transfer learning from MT. After this pretraining, we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart from the available ST corpora, we create synthetic data with SegAugment to better adapt our models to the custom segmentations of the IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released IWSLT.ACLdev2023.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ioannis Tsiamas',\n",
       "    'Gerard I. Gllego',\n",
       "    'Jose Fonollosa',\n",
       "    'Marta R. Costa-juss'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_45',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23',\n",
       "   'tldr': 'This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_46': {'abstract': \"This system description paper introduces the systems submitted by Xiaomi AI Lab to the three tracks of the IWSLT 2023 Evaluation Campaign, namely the offline speech translation (Offline-ST) track, the offline speech-to-speech translation (Offline-S2ST) track, and the simultaneous speech translation (Simul-ST) track. All our submissions for these three tracks only involve the English-Chinese language direction. Our English-Chinese speech translation systems are constructed using large-scale pre-trained models as the foundation. Specifically, we fine-tune these models' corresponding components for various downstream speech translation tasks. Moreover, we implement several popular techniques, such as data filtering, data augmentation, speech segmentation, and model ensemble, to improve the system's overall performance. Extensive experiments show that our systems achieve a significant improvement over the strong baseline systems in terms of the automatic evaluation metric.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Wuwei Huang',\n",
       "    'Mengge Liu',\n",
       "    'Xiang Li',\n",
       "    'Yanzhi Tian',\n",
       "    'Fengyu Yang',\n",
       "    'Wen Zhang',\n",
       "    'Jian Luan',\n",
       "    'Bin Wang',\n",
       "    'Yuhang Guo',\n",
       "    'Jinsong Su'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_46',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"The Xiaomi AI Lab's Speech Translation Systems for IWSLT 2023 Offline Task, Simultaneous Task and Speech-to-Speech Task\",\n",
       "   'tldr': 'This system description paper introduces the systems submitted by Xiaomi AI Lab to the three tracks of the IWSLT 2023 Evaluation Campaign, namely the offline speech translation (Offline-ST) track, the offline speech-to-speech translation (Offline-S2ST) track, and the simultaneous speech translation ',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_47': {'abstract': \"In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-specific data-driven approach, and 2) the generation of synthetic data through the employment of large-scale language models and empirically-grounded prompt engineering. By adapting methodologies and models to accommodate the unique linguistic properties of each language, we observe a notable enhancement in performance relative to the baseline, substantiating the heightened efficacy of data-driven approaches. Moreover, our devised prompt engineering strategy yields superior synthetic translation instances.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Seugnjun Lee',\n",
       "    'Hyeonseok Moon',\n",
       "    'Chanjun Park',\n",
       "    'Heuiseok Lim'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Formality-Sensitive Machine Translation Using Data-Centric Approaches and Prompt Engineering',\n",
       "   'tldr': \"In this paper, we present the KU x Upstage team's submission for the Special Task on Formality Control on Spoken Language Translation, which involves translating English into four languages with diverse grammatical formality markers.  Our methodology comprises two primary components: 1) a language-s\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_48': {'abstract': 'For the 2023 IWSLT Maltese Speech Translation Task, UM-DFKI jointly presents a cascade solution which achieves 0.6 BLEU. While this is the first time that a Maltese speech translation task has been released by IWSLT, this paper explores previous solutions for other speech translation tasks, focusing primarily on low-resource scenarios. Moreover, we present our method of fine-tuning XLS-R models for Maltese ASR using a collection of multi-lingual speech corpora as well as the fine-tuning of the mBART model for Maltese to English machine translation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Aiden Williams',\n",
       "    'Kurt Abela',\n",
       "    'Rishu Kumar',\n",
       "    'Martin Br',\n",
       "    'Hannah Billinghurst',\n",
       "    'Kurt Micallef',\n",
       "    'Ahnaf Mozib Samin',\n",
       "    'Andrea DeMarco',\n",
       "    'Lonneke van der Plas',\n",
       "    'Claudia Borg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UM-DFKI Maltese Speech Translation',\n",
       "   'tldr': 'For the 2023 IWSLT Maltese Speech Translation Task, UM-DFKI jointly presents a cascade solution which achieves 0.6 BLEU. While this is the first time that a Maltese speech translation task has been released by IWSLT, this paper explores previous solutions for other speech translation tasks, focusing',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_5': {'abstract': \"This paper presents the USTC system for the IWSLT 2023 Dialectal and Low-resource shared task, which involves translation from Tunisian Arabic to English. We aim to investigate the mutual transfer between Tunisian Arabic and Modern Standard Arabic (MSA) to enhance the performance of speech translation (ST) by following standard pre-training and fine-tuning pipelines. We synthesize a substantial amount of pseudo Tunisian-English paired data using a multi-step pre-training approach. Integrating a Tunisian-MSA translation module into the end-to-end ST model enables the transfer from Tunisian to MSA and facilitates linguistic normalization of the dialect. To increase the robustness of the ST system, we optimize the model's ability to adapt to ASR errors and propose a model ensemble method. Results indicate that applying the dialect transfer method can increase the BLEU score of dialectal ST. It is shown that the optimal system ensembles both cascaded and end-to-end ST models, achieving BLEU improvements of 2.4 and 2.8 in test1 and test2 sets, respectively, compared to the best published system.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Pan Deng',\n",
       "    'Shihao Chen',\n",
       "    'Weitai Zhang',\n",
       "    'Jie Zhang',\n",
       "    'Lirong Dai'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"The USTC's Dialect Speech Translation System for IWSLT 2023\",\n",
       "   'tldr': 'This paper presents the USTC system for the IWSLT 2023 Dialectal and Low-resource shared task, which involves translation from Tunisian Arabic to English. We aim to investigate the mutual transfer between Tunisian Arabic and Modern Standard Arabic (MSA) to enhance the performance of speech translati',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_50': {'abstract': \"This paper provides an overview of NVIDIA NeMo's speech translation systems for the IWSLT 2023 Offline Speech Translation Task. This year, we focused on end-to-end system which capitalizes on pre-trained models and synthetic data to mitigate the problem of direct speech translation data scarcity. When trained on IWSLT 2022 constrained data, our best En->De end-to-end model achieves the average score of 31 BLEU on 7 test sets from IWSLT 2010-2020 which improves over our last year cascade (28.4) and end-to-end (25.7) submissions. When trained on IWSLT 2023 constrained data, the average score drops to 29.5 BLEU.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Oleksii Hrinchuk',\n",
       "    'Vladimir Bataev',\n",
       "    'Evelina Bakhturina',\n",
       "    'Boris Ginsburg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2023',\n",
       "   'tldr': \"This paper provides an overview of NVIDIA NeMo's speech translation systems for the IWSLT 2023 Offline Speech Translation Task. This year, we focused on end-to-end system which capitalizes on pre-trained models and synthetic data to mitigate the problem of direct speech translation data scarcity. Wh\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_51': {'abstract': 'This paper describes the speech translation systems SRI-B developed for the IWSLT 2023 Evaluation Campaign Dialectal and Low-resource track: Marathi-Hindi Speech Translation. We propose systems for both the constrained (systems are trained only on the datasets provided by the organizers) and the unconstrained conditions (systems can be trained with any resource). For both the conditions, we build end-to-end speech translation networks comprising of a conformer encoder and a transformer decoder. Under both the conditions, we leverage Marathi Automatic Speech Recognition (ASR) data to pre-train the encoder and subsequently train the entire model on the speech translation data. Our results demonstrate that pre-training the encoder with ASR data is a key step in significantly improving the speech translation performance. We also show that conformer encoders are inherently superior to its transformer counterparts for speech translation tasks. Our primary submissions achieved a BLEU% score of 31.2 on the constrained condition and 32.4 on the unconstrained condition. We secured the top position in the constrained condition and second position in the unconstrained condition.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Balaji Radhakrishnan',\n",
       "    'Saurabh Agrawal',\n",
       "    'Raj Prakash Gohil',\n",
       "    'Kiran Praveen',\n",
       "    'Advait Vinay Dhopeshwarkar',\n",
       "    'Abhishek Pandey'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_51',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"SRI-B's Systems for IWSLT 2023 Dialectal and Low-resource Track: Marathi-Hindi Speech Translation\",\n",
       "   'tldr': 'This paper describes the speech translation systems SRI-B developed for the IWSLT 2023 Evaluation Campaign Dialectal and Low-resource track: Marathi-Hindi Speech Translation. We propose systems for both the constrained (systems are trained only on the datasets provided by the organizers) and the unc',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_52': {'abstract': 'This paper describes the system we submitted to the IWSLT 2023 multilingual speech translation track, with input being English speech and output being text in 10 target languages. Our system consists of CNN and Transformer, convolutional neural networks downsample speech features and extract local information, while transformer extract global features and output the final results. In our system, we use speech recognition tasks to pre-train encoder parameters, and then use speech translation corpus to train the multilingual speech translation model. We have also adopted other methods to optimize the model, such as data augmentation, model ensemble, etc. Our system can obtain satisfactory results on test sets of 10 languages in the MUST-C corpus.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhipeng Wang', 'Yuhang Guo', 'Shuoying Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_52',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"BIT's System for Multilingual Track\",\n",
       "   'tldr': 'This paper describes the system we submitted to the IWSLT 2023 multilingual speech translation track, with input being English speech and output being text in 10 target languages. Our system consists of CNN and Transformer, convolutional neural networks downsample speech features and extract local i',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_53': {'abstract': 'This paper briefly describes Matesub, the subtitling tool Translated used to participate in the Subtitling shared task at IWSLT 2023. Matesub is a professional web-based tool that combines state-of-the-art AI with a WYSIWYG editor. The automatic generation of subtitles in Matesub is based on a cascade architecture, composed of ASR, text segmenter and MT neural models, which allows covering any pair from about 70 languages and their variants.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Simone Perone'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_53',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Matesub: The Translated Subtitling Tool at the IWSLT2023 Subtitling Task',\n",
       "   'tldr': 'This paper briefly describes Matesub, the subtitling tool Translated used to participate in the Subtitling shared task at IWSLT 2023. Matesub is a professional web-based tool that combines state-of-the-art AI with a WYSIWYG editor. The automatic generation of subtitles in Matesub is based on a casca',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_54': {'abstract': 'Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Although such units show impressive modeling results, their robustness capabilities have not been extensively investigated. This work focuses on improving the robustness of discrete input representations for generative spoken language modeling. First, we formally define how to measure the robustness of such representations to various signal variations that do not alter the spoken information (e.g., time-stretch). Next, we empirically demonstrate how current state-of-the-art representation models lack robustness to such variations. To overcome this, we propose an effective and efficient method to learn robust discrete speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method significantly improves over the evaluated baselines when considering encoding and modeling metrics. We additionally evaluate our method on the speech-to-speech translation task, considering Spanish-English and French-English translations, and show the proposed approach outperforms the evaluated baselines.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Itai Gat',\n",
       "    'Felix Kreuk',\n",
       "    'Tu Anh Nguyen',\n",
       "    'Ann Lee',\n",
       "    'Jade Copet',\n",
       "    'Gabriel Synnaeve',\n",
       "    'Emmanuel Dupoux',\n",
       "    'Yossi Adi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_54',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling',\n",
       "   'tldr': 'Generative Spoken Language Modeling research focuses on optimizing speech Language Models (LMs) using raw audio recordings without accessing any textual supervision. Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models. Altho',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_55': {'abstract': 'Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-art fully NAT models on widely used WMT and IWSLT benchmarks by up to 1.88 BLEU gain, while maintaining the inference latency comparable to other fully NAT models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiaao Zhan',\n",
       "    'Qian Chen',\n",
       "    'Boxing Chen',\n",
       "    'Wen Wang',\n",
       "    'Yu Bai',\n",
       "    'Yang Gao'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_55',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DePA: Improving Non-autoregressive Translation with Dependency-Aware Decoder',\n",
       "   'tldr': 'Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependenc',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_56': {'abstract': 'Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find this issue is closely related to an unexpected copying behavior during online back-translation (BT). In this work, we propose a simple but effective training schedule that incorporates a language discriminator loss. The loss imposes constraints on the intermediate translation so that the translation is in the desired language. By conducting extensive experiments on different language pairs, including similar and distant, high and low-resource languages, we find that our method alleviates the copying problem, thus improving the translation performance on low-resource languages.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yihong Liu',\n",
       "    'Alexandra Chronopoulou',\n",
       "    'Hinrich Schtze',\n",
       "    'Alexander Fraser'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_56',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss',\n",
       "   'tldr': 'Although unsupervised neural machine translation (UNMT) has achieved success in many language pairs, the copying problem, i.e., directly copying some parts of the input sentence as the translation, is common among distant language pairs, especially when low-resource languages are involved. We find t',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_7': {'abstract': 'Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach ($k$NN-MT) for effective adaptation ($+0.8$ BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Danni Liu',\n",
       "    'Thai Binh Nguyen',\n",
       "    'Sai Koneru',\n",
       "    'Enes Yavuz Ugan',\n",
       "    'Ngoc-Quan Pham',\n",
       "    'Tuan Nam Nguyen',\n",
       "    'Tu Anh Dinh',\n",
       "    'Carlos Mullov',\n",
       "    'Alexander Waibel',\n",
       "    'Jan Niehues'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': \"KIT's Multilingual Speech Translation System for IWSLT 2023\",\n",
       "   'tldr': 'Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the tra',\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'IWSLT_8': {'abstract': \"This paper describes the BIGAI's submission to IWSLT 2023 Offline Speech Translation task on three language tracks from English to Chinese, German and Japanese. The end-to-end systems are built upon a Wav2Vec2 model for speech recognition and mBART50 models for machine translation. An adapter module is applied to bridge the speech module and the translation module. The CTC loss between speech features and source token sequence is incorporated during training. Experiments show that the systems can generate reasonable translations on three languages. The proposed models achieve BLEU scores of 22.3 for ende, 10.7 for enja and 33.0 for enzh on tst2023 TED datasets. However, the performance is decreased by a significant margin on complex scenarios like persentations and interview.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhihang Xie'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['IWSLT'],\n",
       "   'id': 'IWSLT_8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The BIGAI Offline Speech Translation Systems for IWSLT 2023 Evaluation',\n",
       "   'tldr': \"This paper describes the BIGAI's submission to IWSLT 2023 Offline Speech Translation task on three language tracks from English to Chinese, German and Japanese. The end-to-end systems are built upon a Wav2Vec2 model for speech recognition and mBART50 models for machine translation. An adapter module\",\n",
       "   'track': 'The 20th International Conference on Spoken Language Translation',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_11': {'abstract': 'We present a novel dataset for physical and abstract plausibility of events in English. Based on naturally occurring sentences extracted from Wikipedia, we infiltrate degrees of abstractness, and automatically generate perturbed pseudo-implausible events. We annotate a filtered and balanced subset for plausibility using crowd-sourcing, and perform extensive cleansing to ensure annotation quality. In-depth quantitative analyses indicate that annotators favor plausibility over implausibility and disagree more on implausible events. Furthermore, our plausibility dataset is the first to capture abstractness in events to the same extent as concreteness, and we find that event abstractness has an impact on plausibility ratings: more concrete event participants trigger a perception of implausibility.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Annerose Eichel', 'Sabine Schulte Im Walde'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Dataset for Physical and Abstract Plausibility and Sources of Human Disagreement',\n",
       "   'tldr': 'We present a novel dataset for physical and abstract plausibility of events in English. Based on naturally occurring sentences extracted from Wikipedia, we infiltrate degrees of abstractness, and automatically generate perturbed pseudo-implausible events. We annotate a filtered and balanced subset f',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_12': {'abstract': 'The Turkish particle dA is a focus-associated enclitic, and it can act as a discourse connective conveying multiple senses, like additive, contrastive, causal etc. Like many other linguistic expressions, it is subject to usage ambiguity and creates a challenge in natural language automatization tasks. For the first time, we annotate the discourse and non-discourse connnective occurrences of dA in Turkish with the PDTB principles. Using a minimal set of linguistic features, we develop binary classifiers to distinguish its discourse connective usage from its other usages. We show that despite its ability to cliticize to any syntactic type, variable position in the sentence and having a wide argument span, its discourse/non-discourse connective usage can be annotated reliably and its discourse usage can be disambiguated by exploiting local cues.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ebru Ersyleyen', 'Deniz Zeyrek', 'Frat ter'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Annotating and Disambiguating the Discourse Usage of the Enclitic dA in Turkish',\n",
       "   'tldr': 'The Turkish particle dA is a focus-associated enclitic, and it can act as a discourse connective conveying multiple senses, like additive, contrastive, causal etc. Like many other linguistic expressions, it is subject to usage ambiguity and creates a challenge in natural language automatization task',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_13': {'abstract': 'High-quality labeled data is paramount to the performance of modern machine learning models. However, annotating data is a time-consuming and costly process that requires human experts to examine large collections of raw data. For conversational agents in production settings with access to large amounts of user-agent conversations, the challenge is to decide what data should be annotated first. We consider the Natural Language Understanding (NLU) component of a conversational agent deployed in a real-world setup with limited resources. We present an active learning pipeline for offline detection of classification errors that leverages two strong classifiers. Then, we perform topic modeling on the potentially mis-classified samples to ease data analysis and to reveal error patterns. In our experiments, we show on a real-world dataset that by using our method to prioritize data annotation we reach 100\\\\% of the performance annotating only 36\\\\% of the data. Finally, we present an analysis of some of the error patterns revealed and argue that our pipeline is a valuable tool to detect critical errors and reduce the workload of annotators.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Damian Pascual',\n",
       "    'Aritz Bercher',\n",
       "    'Akansha Bhardwaj',\n",
       "    'Mingbo Cui',\n",
       "    'Dominic Kohler',\n",
       "    'Liam Van Der Poel',\n",
       "    'Paolo Rosso'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_13',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Active Learning Pipeline for NLU Error Detection in Conversational Agents',\n",
       "   'tldr': 'High-quality labeled data is paramount to the performance of modern machine learning models. However, annotating data is a time-consuming and costly process that requires human experts to examine large collections of raw data. For conversational agents in production settings with access to large amo',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_18': {'abstract': 'This work presents two corpora based on excerpts from two novels with an informal narration style in German. We performed fine-grained multi-layer annotations of animate referents, assigning local and global prominence-lending features to the annotated referring expressions. In addition, our corpora include annotations of intra-sentential segments, which can serve as a more reliable unit of length measurement. Furthermore, we present two exemplary studies demonstrating how to use these corpora.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Magdalena Repp', 'Petra B. Schumacher', 'Fahime Same'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_18',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi-layered Annotation of Conversation-like Narratives in German',\n",
       "   'tldr': 'This work presents two corpora based on excerpts from two novels with an informal narration style in German. We performed fine-grained multi-layer annotations of animate referents, assigning local and global prominence-lending features to the annotated referring expressions. In addition, our corpora',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_20': {'abstract': 'Most tasks in NLP require labeled data. Data labeling is often done on crowdsourcing platforms due to scalability reasons. However, publishing data on public platforms can only be done if no privacy-relevant information is included. Textual data often contains sensitive information like person names or locations. In this work, we investigate how removing personally identifiable information (PII) as well as applying differential privacy (DP) rewriting can enable text with privacy-relevant information to be used for crowdsourcing. We find that DP-rewriting before crowdsourcing can preserve privacy while still leading to good label quality for certain tasks and data. PII-removal led to good label quality in all examined tasks, however, there are no privacy guarantees given.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nina Mouhammad',\n",
       "    'Johannes Daxenberger',\n",
       "    'Benjamin Schiller',\n",
       "    'Ivan Habernal'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_20',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Crowdsourcing on Sensitive Data with Privacy-Preserving Text Rewriting',\n",
       "   'tldr': 'Most tasks in NLP require labeled data. Data labeling is often done on crowdsourcing platforms due to scalability reasons. However, publishing data on public platforms can only be done if no privacy-relevant information is included. Textual data often contains sensitive information like person names',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_22': {'abstract': 'In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several research questions have been focused on, from the investigation of a possible heuristics to provide at least hints to annotators which verbs to include and which are outside the current version of the ontology, to the possible use of the automatic scores to help the annotators to be more efficient in finding a threshold for identifying verbs that cannot be assigned to any existing class and therefore they are to be used as seeds for a new class. We have also carefully examined the correlation of the automatic scores with the human annotation. While the correlation turned out to be strong, its influence on the annotation proper is modest due to its near linearity, even though the mere fact of such pre-annotation leads to relatively short annotation times.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jana Strakov', 'Eva Fukov', 'Jan Haji', 'Zdeka Ureov'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Extending an Event-type Ontology: Adding Verbs and Classes Using Fine-tuned LLMs Suggestions',\n",
       "   'tldr': 'In this project, we have investigated the use of advanced machine learning methods, specifically fine-tuned large language models, for pre-annotating data for a lexical extension task, namely adding descriptive words (verbs) to an existing (but incomplete, as of yet) ontology of event types. Several',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_26': {'abstract': \"Much work in natural language processing (NLP) relies on human annotation. The majority of this implicitly assumes that annotator's labels are temporally stable, although the reality is that human judgements are rarely consistent over time. As a subjective annotation task, hate speech labels depend on annotator's emotional and moral reactions to the language used to convey the message. Studies in Cognitive Science reveal a `foreign language effect', whereby people take differing moral positions and perceive offensive phrases to be weaker in their second languages. Does this affect annotations as well? We conduct an experiment to investigate the impacts of (1) time and (2) different language conditions (English and German) on measurements of intra-annotator agreement in a hate speech labelling task. While we do not observe the expected lower stability in the different language condition, we find that overall agreement is significantly lower than is implicitly assumed in annotation tasks, which has important implications for dataset reproducibility in NLP.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gavin Abercrombie', 'Dirk Hovy', 'Vinodkumar Prabhakaran'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_26',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Temporal and Second Language Influence on Intra-Annotator Agreement and Stability in Hate Speech Labelling',\n",
       "   'tldr': \"Much work in natural language processing (NLP) relies on human annotation. The majority of this implicitly assumes that annotator's labels are temporally stable, although the reality is that human judgements are rarely consistent over time. As a subjective annotation task, hate speech labels depend \",\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_27': {'abstract': 'Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morphological richness compared to English. In this article, we introduce a new dataset, BenCoref, comprising coreference annotations for Bengali texts gathered from four distinct domains. This relatively small dataset contains 5200 mention annotations forming 502 mention clusters within 48,569 tokens. We describe the process of creating this dataset and report performance of multiple models trained using BenCoref. We anticipate that our work sheds some light on the variations in coreference phenomena across multiple domains in Bengali and encourages the development of additional resources for Bengali. Furthermore, we found poor crosslingual performance at zero-shot  setting from English, highlighting the need for more language-specific resources for this task.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shadman Rohan',\n",
       "    'Mojammel Hossain',\n",
       "    'Mohammad Rashid',\n",
       "    'Nabeel Mohammed'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_27',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations',\n",
       "   'tldr': 'Coreference Resolution is a well studied problem in NLP. While widely studied for English and other resource-rich languages, research on coreference resolution in Bengali largely remains unexplored due to the absence of relevant datasets. Bengali, being a low-resource language, exhibits greater morp',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_28': {'abstract': 'The availability of annotated legal corpora is crucial for a number of tasks, such as legal search, legal information retrieval, and predictive justice. Annotation is mostly assumed to be a straightforward task: as long as the annotation scheme is well defined and the guidelines are clear, annotators are expected to agree on the labels. This is not always the case, especially in legal annotation, which can be extremely difficult even for expert annotators. We propose a legal annotation procedure that takes into account annotator certainty and improves it through negotiation. We also collect annotator feedback and show that our approach contributes to a positive annotation environment. Our work invites reflection on often neglected ethical concerns regarding legal annotation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Emma Zanoli',\n",
       "    'Matilde Barbini',\n",
       "    'Davide Riva',\n",
       "    'Sergio Picascia',\n",
       "    'Emanuela Furiosi',\n",
       "    \"Stefano D'Ancona\",\n",
       "    'Cristiano Chesi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_28',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Annotators-in-the-loop: Testing a Novel Annotation Procedure on Italian Case Law',\n",
       "   'tldr': 'The availability of annotated legal corpora is crucial for a number of tasks, such as legal search, legal information retrieval, and predictive justice. Annotation is mostly assumed to be a straightforward task: as long as the annotation scheme is well defined and the guidelines are clear, annotator',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_33': {'abstract': 'This submission reports on a three-part series of original methods geared towards producing semantic annotations for the decompositional marker \"again\". The three methods are (i) exhaustive expert annotation based on a comprehensive set of guidelines, (ii) extension of expert annotation by predicting presuppositions with a Multinomial Nave Bayes classifier in the context of a meta-analysis to optimize feature selection and (iii) quality-controlled crowdsourcing with ensuing evaluation and KMeans clustering of annotation vectors.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Martin Kopf', 'Remus Gergel'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_33',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Annotating Decomposition in Time: Three Approaches for Again',\n",
       "   'tldr': 'This submission reports on a three-part series of original methods geared towards producing semantic annotations for the decompositional marker \"again\". The three methods are (i) exhaustive expert annotation based on a comprehensive set of guidelines, (ii) extension of expert annotation by predictin',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_34': {'abstract': 'Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests likely corefering event pairs only. We evaluate the effectiveness of this approach by first simulating the annotation process and then, using a novel annotator-centric Recall-Annotation effort trade-off metric, we compare the results of various underlying models and datasets. We finally present a method for obtaining 97\\\\% recall while substantially reducing the workload required by a fully manual annotation process.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shafiuddin Rehan Ahmed',\n",
       "    'Abhijnan Nath',\n",
       "    'Michael Regan',\n",
       "    'Adam Pollins',\n",
       "    'Nikhil Krishnaswamy',\n",
       "    'James H. Martin'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_34',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'How Good Is the Model in Model-in-the-loop Event Coreference Resolution Annotation?',\n",
       "   'tldr': 'Annotating cross-document event coreference links is a time-consuming and cognitively demanding task that can compromise annotation quality and efficiency. To address this, we propose a model-in-the-loop annotation approach for event coreference resolution, where a machine learning model suggests li',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_38': {'abstract': 'The annotation task we elaborated aims at describing the contextual factors that influence the appearance and interpretation of moral predicates, in newspaper articles on police brutality, in French and in English. The paper provides a brief review of the literature on moral predicates and their relation with context. The paper also describes the elaboration of the corpus and the ontology. Our hypothesis is that the use of moral adjectives and their appearance in context could change depending on the political orientation of the journal. We elaborated an annotation task to investigate the precise contexts discussed in articles on police brutality. The paper concludes by describing the study and the annotation task in details.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tess Feyen', 'Alda Mari', 'Paul Portner'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_38',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pragmatic Annotation of Articles Related to Police Brutality',\n",
       "   'tldr': 'The annotation task we elaborated aims at describing the contextual factors that influence the appearance and interpretation of moral predicates, in newspaper articles on police brutality, in French and in English. The paper provides a brief review of the literature on moral predicates and their rel',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_40': {'abstract': \"We present the RST Continuity Corpus (RST-CC), a corpus of discourse relations annotated for continuity dimensions. Continuity or discontinuity (maintaining or shifting deictic centres across discourse segments) is an important property of discourse relations, but the two are correlated in greatly varying ways. To analyse this correlation, the relations in the RST-CC are annotated using operationalised versions of Givn's (1993) continuity dimensions. We also report on the inter-annotator agreement, and discuss recurrent annotation issues. First results show substantial variation of continuity dimensions within and across relation types.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Debopam Das', 'Markus Egg'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The RST Continuity Corpus',\n",
       "   'tldr': 'We present the RST Continuity Corpus (RST-CC), a corpus of discourse relations annotated for continuity dimensions. Continuity or discontinuity (maintaining or shifting deictic centres across discourse segments) is an important property of discourse relations, but the two are correlated in greatly v',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_41': {'abstract': \"We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manually annotated for a variety of popular NLP tasks, including syntactic dependency parsing, entity recognition, coreference resolution, and discourse parsing. We evaluate state-of-the-art NLP systems on GENTLE and find severe degradation for at least some genres in their performance on all tasks, which indicates GENTLE's utility as an evaluation dataset for NLP systems.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tatsuya Aoyama',\n",
       "    'Shabnam Behzad',\n",
       "    'Luke Gessler',\n",
       "    'Lauren Levine',\n",
       "    'Jessica Lin',\n",
       "    'Yang Janet Liu',\n",
       "    'Siyao Peng',\n",
       "    'Yilun Zhu',\n",
       "    'Amir Zeldes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_41',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GENTLE: A Genre-Diverse Multilayer Challenge Set for English NLP and Linguistic Evaluation',\n",
       "   'tldr': 'We present GENTLE, a new mixed-genre English challenge corpus totaling 17K tokens and consisting of 8 unusual text types for out-of-domain evaluation: dictionary entries, esports commentaries, legal documents, medical notes, poetry, mathematical proofs, syllabuses, and threat letters. GENTLE is manu',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_42': {'abstract': 'The task of summarisation is notoriously difficult to evaluate, with agreement even between expert raters unlikely to be perfect. One technique for summary evaluation relies on collecting comparison data by presenting annotators with generated summaries and tasking them with selecting the best one. This paradigm is currently being exploited in reinforcement learning using human feedback, whereby a reward function is trained using pairwise choice data. Comparisons are an easier way to elicit human feedback for summarisation, however, such decisions can be bottle necked by the usability of the annotator interface. In this paper, we present the results of a pilot study exploring how the user interface impacts annotator agreement when judging summary quality.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sian Gooding', 'Lucas Werner', 'Victor Crbune'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_42',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Study on Annotation Interfaces for Summary Comparison',\n",
       "   'tldr': 'The task of summarisation is notoriously difficult to evaluate, with agreement even between expert raters unlikely to be perfect. One technique for summary evaluation relies on collecting comparison data by presenting annotators with generated summaries and tasking them with selecting the best one. ',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_44': {'abstract': 'Within the research presented in this article, we created a new question answering benchmark database for Hungarian called MILQA. When creating the dataset, we basically followed the principles of the English SQuAD 2.0, however, like in some more recent English question answering datasets, we introduced a number of innovations beyond SQuAD: e.g., yes/no-questions, list-like answers consisting of several text spans, long answers, questions requiring calculation and other question types where you cannot simply copy the answer from the text. For all these non-extractive question types, the pragmatically adequate form of the answer was also added to make the training of generative models possible. We implemented and evaluated a set of baseline retrieval and answer span extraction models on the dataset. BM25 performed better than any vector-based solution for retrieval. Cross-lingual transfer from English significantly improved span extraction models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Attila Novk',\n",
       "    'Borbla Novk',\n",
       "    'Tams Zombori',\n",
       "    'Gerg Szab',\n",
       "    'Zsolt Sznt',\n",
       "    'Richrd Farkas'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_44',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Question Answering Benchmark Database for Hungarian',\n",
       "   'tldr': 'Within the research presented in this article, we created a new question answering benchmark database for Hungarian called MILQA. When creating the dataset, we basically followed the principles of the English SQuAD 2.0, however, like in some more recent English question answering datasets, we introd',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_47': {'abstract': \"Natural Language Inference (NLI) has been a cornerstone task in evaluating language models' inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models' ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP community.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Animesh Nighojkar', 'Antonio Laverghetta Jr.', 'John Licato'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_47',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference',\n",
       "   'tldr': \"Natural Language Inference (NLI) has been a cornerstone task in evaluating language models' inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models' ability to capture the nuances of natural human reasonin\",\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_48': {'abstract': 'UMR-Writer is a web-based tool for annotating semantic graphs with the Uniform Meaning Representation (UMR) scheme. UMR is a graph-based semantic representation that can be applied cross-linguistically for deep semantic analysis of texts. In this work, we implemented a new keyboard interface in UMR-Writer 2.0, which is a powerful addition to the original mouse interface, supporting faster annotation for more experienced annotators. The new interface also addresses issues with the original mouse interface. Additionally, we demonstrate an efficient workflow for annotation project management in UMR-Writer 2.0, which has been applied to many projects.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sijia Ge',\n",
       "    'Jin Zhao',\n",
       "    'Kristin Wright-bettner',\n",
       "    'Skatje Myers',\n",
       "    'Nianwen Xue',\n",
       "    'Martha Palmer'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_48',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UMR-Writer 2.0: Incorporating a New Keyboard Interface and Workflow into UMR-Writer',\n",
       "   'tldr': 'UMR-Writer is a web-based tool for annotating semantic graphs with the Uniform Meaning Representation (UMR) scheme. UMR is a graph-based semantic representation that can be applied cross-linguistically for deep semantic analysis of texts. In this work, we implemented a new keyboard interface in UMR-',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_50': {'abstract': 'We investigate whether the Cambridge Grammar of the English Language (2002) and its extensive descriptions work well as a corpus annotation scheme. We develop annotation guidelines and in the process outline some interesting linguistic uncertainties that we had to resolve. To test the applicability of CGEL to real-world corpora, we conduct an interannotator study on sentences from the English Web Treebank, showing that consistent annotation of even complex syntactic phenomena like gapping using the CGEL formalism is feasible. Why introduce yet another formalism for English syntax? We argue that CGEL is attractive due to its exhaustive analysis of English syntactic phenomena, its labeling of both constituents and functions, and its accessibility. We look towards expanding CGELBank and augmenting it with automatic conversions from existing treebanks in the future.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Brett Reynolds', 'Aryaman Arora', 'Nathan Schneider'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Unified Syntactic Annotation of English in the CGEL Framework',\n",
       "   'tldr': 'We investigate whether the Cambridge Grammar of the English Language (2002) and its extensive descriptions work well as a corpus annotation scheme. We develop annotation guidelines and in the process outline some interesting linguistic uncertainties that we had to resolve. To test the applicability ',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_51': {'abstract': 'Patent descriptions are a crucial component of patent applications, as they are key to understanding the invention and play a significant role in securing patent grants. While discursive analyses have been undertaken for scientific articles, they have not been as thoroughly explored for patent descriptions, despite the increasing importance of Intellectual Property and the constant rise of the number of patent applications. In this study, we propose an annotation scheme containing 16 classes that allows categorizing each sentence in patent descriptions according to their discursive roles. We publish an experimental human-annotated corpus of 16 patent descriptions and analyze challenges that may be encountered in such work. This work can be base for an automated annotation and thus contribute to enriching linguistic resources in the patent domain.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Lufei Liu', 'Xu Sun', 'Franois Veltz', 'Kim Gerdes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_51',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Annotating Discursive Roles of Sentences in Patent Descriptions',\n",
       "   'tldr': 'Patent descriptions are a crucial component of patent applications, as they are key to understanding the invention and play a significant role in securing patent grants. While discursive analyses have been undertaken for scientific articles, they have not been as thoroughly explored for patent descr',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_52': {'abstract': 'Cross-lingual annotation projection is a practical method for improving performance on low resource structured prediction tasks. An important step in annotation projection is obtaining alignments between the source and target texts, which enables the mapping of annotations across the texts. By manually correcting automatically generated alignments, we examine the impact of alignment quality---automatic, manual, and mixed---on downstream performance for two information extraction tasks and quantify the trade-off between annotation effort and model performance.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shabnam Behzad',\n",
       "    'Seth Ebner',\n",
       "    'Marc Marone',\n",
       "    'Benjamin Van Durme',\n",
       "    'Mahsa Yarmohammadi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_52',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short paper (4 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Effect of Alignment Correction on Cross-Lingual Annotation Projection',\n",
       "   'tldr': 'Cross-lingual annotation projection is a practical method for improving performance on low resource structured prediction tasks. An important step in annotation projection is obtaining alignments between the source and target texts, which enables the mapping of annotations across the texts. By manua',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_54': {'abstract': \"Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the Potato-Prolific dataset for Question-Answering, Offensiveness, text Rewriting and politeness rating with demographic Nuance). POPQUORN contains 45,000 annotations from 1,484 annotators, drawn from a representative sample regarding sex, age, and race as the US population. Through a series of analyses, we show that annotators' background plays a significant role in their judgments. Further, our work shows that backgrounds not previously considered in NLP (e.g., education), are meaningful and should be considered. Our study suggests that understanding the background of annotators and collecting labels from a demographically balanced pool of crowd workers is important to reduce the bias of datasets. The dataset, annotator background, and annotation interface are available at https://github.com/Jiaxin-Pei/potato-prolific-dataset.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiaxin Pei', 'David Jurgens'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_54',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'When Do Annotator Demographics Matter? Measuring the Influence of Annotator Demographics with the POPQUORN Dataset',\n",
       "   'tldr': 'Annotators are not fungible. Their demographics, life experiences, and backgrounds all contribute to how they label data. However, NLP has only recently considered how annotator identity might influence their decisions. Here, we present POPQUORN (the Potato-Prolific dataset for Question-Answering, O',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_58': {'abstract': 'In this paper we address the scarcity of annotated data for NArabizi, a Romanized form of North African Arabic used mostly on  social media, which poses challenges for Natural Language Processing  (NLP). We introduce an enriched version of NArabizi Treebank (Seddah et al., 2020) with three main contributions: the addition of two novel annotation layers (named entity recognition and offensive language detection) and a re-annotation of the tokenization, morpho-syntactic and syntactic layers that ensure annotation consistency. Our experimental results, using different tokenization schemes, showcase the value of our contributions and highlight the impact of working with non-gold tokenization for NER and dependency parsing. To facilitate future research, we make these annotations publicly available. Our enhanced NArabizi Treebank paves the way for creating sophisticated language models and NLP tools for this under-represented language.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Arij Riabi', 'Menel Mahamdi', 'Djam Seddah'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_58',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enriching the NArabizi Treebank: A Multifaceted Approach to Supporting an Under-Resourced Language',\n",
       "   'tldr': 'In this paper we address the scarcity of annotated data for NArabizi, a Romanized form of North African Arabic used mostly on  social media, which poses challenges for Natural Language Processing  (NLP). We introduce an enriched version of NArabizi Treebank (Seddah et al., 2020) with three main cont',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_7': {'abstract': 'In this paper, we present the interim results of a transformer-based annotation pipeline for Ancient and Medieval Greek. As the texts in the Database of Byzantine Book Epigrams have not been normalised, they pose more challenges for manual and automatic annotation than Ancient Greek, normalised texts do. As a result, the existing annotation tools perform poorly. We compiled three data sets for the development of an automatic annotation tool and carried out an inter-annotator agreement study, with a promising agreement score. The experimental results show that our part-of-speech tagger yields accuracy scores that are almost 50 percentage points higher than the widely used rule-based system Morpheus. In addition, error analysis revealed problems related to phenomena also occurring in current social media language.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Colin Swaelens', 'Ilse De Vos', 'Els Lefever'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Medieval Social Media: Manual and Automatic Annotation of Byzantine Greek Marginal Writing',\n",
       "   'tldr': 'In this paper, we present the interim results of a transformer-based annotation pipeline for Ancient and Medieval Greek. As the texts in the Database of Byzantine Book Epigrams have not been normalised, they pose more challenges for manual and automatic annotation than Ancient Greek, normalised text',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_8': {'abstract': 'The mythological domain has various ways of expressing events and background knowledge. Using data extracted according to the hylistic approach (Zgoll, 2019), we annotated a data set of 6315 sentences from various mythological contexts and geographical origins, like Ancient Greece and Rome or Mesopotamia, into four categories: single-point events (e.g. actions), durative-constant (background knowledge, continuous states), durative-initial, and durative-resultativ. This data is used to train a classifier, which is able to reliably distinguish event types.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Franziska Pannach'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': '\"Orpheus Came to His End by Being Struck by a Thunderbolt\": Annotating Events in Mythological Sequences',\n",
       "   'tldr': 'The mythological domain has various ways of expressing events and background knowledge. Using data extracted according to the hylistic approach (Zgoll, 2019), we annotated a data set of 6315 sentences from various mythological contexts and geographical origins, like Ancient Greece and Rome or Mesopo',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_9': {'abstract': 'In this paper, we give a brief survey of the difficulties in handling the syntax of mathematical expressions in Universal Dependencies, focusing on examples from English language corpora. We first examine the prevalence and current handling of mathematical expressions in UD corpora. We then examine several strategies for how to approach the handling of syntactic dependencies for such expressions: as multi-word expressions, as a domain appropriate for code-switching, or as approximate to other types of natural language. Ultimately, we argue that  mathematical expressions should primarily be analyzed as natural language, and we offer recommendations for the treatment of basic mathematical expressions as analogous to English natural language.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Lauren Levine'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long paper (8 pages)',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Difficulties in Handling Mathematical Expressions in Universal Dependencies',\n",
       "   'tldr': 'In this paper, we give a brief survey of the difficulties in handling the syntax of mathematical expressions in Universal Dependencies, focusing on examples from English language corpora. We first examine the prevalence and current handling of mathematical expressions in UD corpora. We then examine ',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F1': {'abstract': 'Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use. While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators. This norm assumes that multiple annotators are available, which is not the case for highly specialized tasks or low-resource languages. In this paper, we ask: Can we evaluate the quality of a dataset constructed by a single human annotator? To address this question, we propose four weak verifiers to help estimate dataset quality, and outline when each may be employed. We instantiate these strategies for the task of semantic analysis of adpositions in Gujarati, a low-resource language, and show that our weak verifiers concur with a double-annotation study. As an added contribution, we also release the first dataset with semantic annotations in Gujarati along with several model baselines.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Maitrey Mehta', 'Vivek Srikumar'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Verifying Annotation Agreement without Multiple Experts: A Case Study with Gujarati SNACS',\n",
       "   'tldr': 'Good datasets are a foundation of NLP research, and form the basis for training and evaluating models of language use. While creating datasets, the standard practice is to verify the annotation consistency using a committee of human annotators. This norm assumes that multiple annotators are availabl',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F2': {'abstract': 'The task of textual geolocation retrieving the coordinates of a place based on a free-form language description calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currently based on open-source data (Wikipedia and Twitter), where the location of the described place is mostly implicit, such that the location retrieval resolution is limited. Furthermore, there are no datasets available for addressing the problem of textual geolocation in morphologically rich and resource-poor languages, such as Hebrew. In this paper, we present the Hebrew Geo-Location (HeGeL) corpus, designed to collect literal place descriptions and analyze lingual geospatial reasoning. We crowdsourced 5,649 literal Hebrew place descriptions of various place types in three cities in Israel. Qualitative and empirical analysis show that the data exhibits abundant use of geospatial reasoning and requires a novel environmental representation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Tzuf Paz-Argaman',\n",
       "    'Tal Bauman',\n",
       "    'Itai Mondshine',\n",
       "    'Itzhak Omer',\n",
       "    'Sagi Dalyot',\n",
       "    'Reut Tsarfaty'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HeGeL: A Novel Dataset for Geo-Location from Hebrew Text',\n",
       "   'tldr': 'The task of textual geolocation retrieving the coordinates of a place based on a free-form language description calls for not only grounding but also natural language understanding and geospatial reasoning. Even though there are quite a few datasets in English used for geolocation, they are currentl',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F3': {'abstract': 'This paper presents HaVQA, the first multi-modal dataset for visual question-answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, the dataset provides 12,044 gold standard English-Hausa parallel sentences that were translated in a fashion that guarantees their semantic match with the corresponding visual information. We conducted several baseline experiments on the dataset, including visual question answering, visual question elicitation, text-only and multi-modal machine translation.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Shantipriya Parida',\n",
       "    'Idris Abdulmumin',\n",
       "    'Shamsuddeen Muhammad',\n",
       "    'Aneesh Bose',\n",
       "    'Guneet Kohli',\n",
       "    'Ibrahim Ahmad',\n",
       "    'Ketan Kotwal',\n",
       "    'Sayan Deb Sarkar',\n",
       "    'Ondej Bojar',\n",
       "    'Habeebah Kakudi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language',\n",
       "   'tldr': 'This paper presents HaVQA, the first multi-modal dataset for visual question-answering (VQA) tasks in the Hausa language. The dataset was created by manually translating 6,022 English question-answer pairs, which are associated with 1,555 unique images from the Visual Genome dataset. As a result, th',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F4': {'abstract': \"With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morphological paradigm completion and its impact on morphological inflection systems: First, we propose an error taxonomy and annotation pipeline for inflection training data. Then, we compare the effect of different types of noise on multiple state-of-the-art inflection models. Finally, we propose a novel character-level masked language modeling (CMLM) pretraining objective and explore its impact on the models' resistance to noise.  Our experiments show that various architectures are impacted differently by separate types of noise, but encoder-decoders tend to be more robust to noise than models trained with a copy bias. CMLM pretraining helps transformers, but has lower impact on LSTMs.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Adam Wiemerslage',\n",
       "    'Changbing Yang',\n",
       "    'Garrett Nicolai',\n",
       "    'Miikka Silfverberg',\n",
       "    'Katharina Kann'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Investigation of Noise in Morphological Inflection',\n",
       "   'tldr': 'With a growing focus on morphological inflection systems for languages where high-quality data is scarce, training data noise is a serious but so far largely ignored concern. We aim at closing this gap by investigating the types of noise encountered within a pipeline for truly unsupervised morpholog',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F5': {'abstract': \"Metrics for Inter-Annotator Agreement (IAA), like Cohen's Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure validity or reproducibility. While researchers are encouraged to increase annotator agreement, this can lead to specific and tailored annotation guidelines. We hypothesize that this may result in diverging annotations from different groups. To study this, we first propose the Lee et al. Protocol (LEAP), a standardized and codified annotation protocol. LEAP strictly enforces transparency in the annotation process, which ensures reproducibility of annotation guidelines. Using LEAP to annotate a dialog dataset, we empirically show that while research groups may create reliable guidelines by raising agreement, this can cause divergent annotations across different research groups, thus questioning the validity of the annotations. Therefore, we caution NLP researchers against using reliability as a proxy for reproducibility and validity.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Seunggun Lee',\n",
       "    'Alexandra DeLucia',\n",
       "    'Nikita Nangia',\n",
       "    'Praneeth Ganedi',\n",
       "    'Ryan Guan',\n",
       "    'Rubing Li',\n",
       "    'Britney Ngaw',\n",
       "    'Aditya Singhal',\n",
       "    'Shalaka Vaidya',\n",
       "    'Zijun Yuan',\n",
       "    'Lining Zhang',\n",
       "    'Joo Sedoc'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Common Law Annotations: Investigating the Stability of Dialog System Output Annotations',\n",
       "   'tldr': \"Metrics for Inter-Annotator Agreement (IAA), like Cohen's Kappa, are crucial for validating annotated datasets. Although high agreement is often used to show the reliability of annotation procedures, it is insufficient to ensure validity or reproducibility. While researchers are encouraged to increa\",\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F6': {'abstract': \"Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, MABL, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Anubha Kabra',\n",
       "    'Emmy Liu',\n",
       "    'Simran Khanuja',\n",
       "    'Alham Fikri Aji',\n",
       "    'Genta Winata',\n",
       "    'Samuel Cahyawijaya',\n",
       "    'Anuoluwapo Aremu',\n",
       "    'Perez Ogayo',\n",
       "    'Graham Neubig'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi-lingual and Multi-cultural Figurative Language Understanding',\n",
       "   'tldr': 'Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is ',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F7': {'abstract': 'Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and automated evaluation. In addition, we also find a validation gap: only few automated metrics have been validated using human experiments. To this end, we thoroughly scrutinize both the standardization and validation gap and reveal the resulting pitfalls. This work also paves the way to close the standardization and validation gap in TST evaluation by calling out requirements to be met by future research.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Phil Ostheimer',\n",
       "    'Mayank Kumar Nagda',\n",
       "    'Marius Kloft',\n",
       "    'Sophie Fellenz'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Call for Standardization and Validation of Text Style Transfer Evaluation',\n",
       "   'tldr': 'Text Style Transfer (TST) evaluation is, in practice, inconsistent. Therefore, we conduct a meta-analysis on human and automated TST evaluation and experimentation that thoroughly examines existing literature in the field. The meta-analysis reveals a substantial standardization gap in human and auto',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F8': {'abstract': \"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to `hallucinations', low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023's 'Reality Check' theme, we present GUMSum, a small but carefully crafted dataset of English summaries in 12 written and spoken genres for evaluation of abstractive summarization. Summaries are highly constrained, focusing on substitutive potential, factuality, and faithfulness. We present guidelines and evaluate human agreement as well as subjective judgments on recent system outputs, comparing general-domain untuned approaches, a fine-tuned one, and a prompt-based approach, to human performance. Results show that while GPT3 achieves impressive scores, it still underperforms humans, with varying quality across genres. Human judgments reveal different types of errors in supervised, prompted, and human-generated summaries, shedding light on the challenges of producing a good summary.\",\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yang Janet Liu', 'Amir Zeldes'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GUMSum: Multi-Genre Data and Evaluation for English Abstractive Summarization',\n",
       "   'tldr': \"Automatic summarization with pre-trained language models has led to impressively fluent results, but is prone to `hallucinations', low performance on non-news genres, and outputs which are not exactly summaries. Targeting ACL 2023's 'Reality Check' theme, we present GUMSum, a small but carefully cra\",\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'LAW_F9': {'abstract': 'We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accurately link entities in anaphoric and coreference relations without an understanding of the transformations those entities undergo. We show how adding event semantics helps to better model entity coreference. We argue that all transformation predicates, not just creation verbs, introduce a new entity into the discourse, as a kind of generalized Result Role, which is typically not textually mentioned. This allows us to model procedural texts as process graphs and to compute the coreference type for any two entities in the recipe. We present our annotation methodology and the corpus generated as well as describe experiments on coreference resolution of entity mentions under a process-oriented model of events.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kyeongmin Rim',\n",
       "    'Jingxuan Tu',\n",
       "    'Bingyang Ye',\n",
       "    'Marc Verhagen',\n",
       "    'Eben Holderness',\n",
       "    'James Pustejovsky'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['LAW'],\n",
       "   'id': 'LAW_F9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': '',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Coreference under Transformation Labeling Dataset: Entity Tracking in Procedural Texts Using Event Models',\n",
       "   'tldr': 'We demonstrate that coreference resolution in procedural texts is significantly improved when performing transformation-based entity linking prior to coreference relation identification. When events in the text introduce changes to the state of participating entities, it is often impossible to accur',\n",
       "   'track': 'The 17th Linguistic Annotation Workshop (LAW-XVII) \\\\\\\\ @ ACL 2023',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_F1': {'abstract': 'Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which contain time-dependent event knowledge with complex temporal dynamics. Specifically, existing enhancement approaches often assume knowledge embedding is time-independent. In contrast, the entity embedding in tKG models usually evolves, which poses the challenge of aligning temporally relevant texts with entities. To this end, we propose to study enhancing temporal knowledge embedding with textual data in this paper. As an approach to this task, we propose Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations (ECOLA), which takes the temporal aspect into account and injects textual information into temporal knowledge embedding. To evaluate ECOLA, we introduce three new datasets for training and evaluating ECOLA. Extensive experiments show that ECOLA significantly enhances temporal KG embedding models with up to 287% relative improvements regarding Hits@1 on the link prediction task. The code and models are publicly available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Zhen Han',\n",
       "    'Ruotong Liao',\n",
       "    'Jindong Gu',\n",
       "    'Yao Zhang',\n",
       "    'Zifeng Ding',\n",
       "    'Yujia Gu',\n",
       "    'Heinz Koeppl',\n",
       "    'Hinrich Schtze',\n",
       "    'Volker Tresp'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_F1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ECOLA: Enhancing Temporal Knowledge Embeddings with Contextualized Language Representations',\n",
       "   'tldr': 'Since conventional knowledge embedding models cannot take full advantage of the abundant textual information, there have been extensive research efforts in enhancing knowledge embedding using texts. However, existing enhancement approaches cannot apply to temporal knowledge graphs (tKGs), which cont',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_F3': {'abstract': 'Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human learning process. In this paper, we propose a unified easy-to-hard learning framework consisting of three stages, i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking the human learning process. By breaking down the learning process into multiple stages, our framework facilitates the model to acquire general IE task knowledge and improve its generalization ability. Extensive experiments across four IE tasks demonstrate the effectiveness of our framework. We achieve new state-of-the-art results on 13 out of 17 datasets. Our code is available at https://github.com/DAMO-NLP-SG/IE-E2H.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Chang Gao', 'Wenxuan Zhang', 'Wai Lam', 'Lidong Bing'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_F3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Easy-to-Hard Learning for Information Extraction',\n",
       "   'tldr': 'Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_F4': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Elisa Bassignana',\n",
       "    'Filip Ginter',\n",
       "    'Sampo Pyysalo',\n",
       "    'Rob van der Goot',\n",
       "    'Barbara Plank'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_F4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Silver Syntax Pre-training for Cross-Domain Relation Extraction',\n",
       "   'tldr': '',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_F5': {'abstract': 'Information Synchronization of semi-structured data across languages is challenging. For instance, Wikipedia tables in one language should be synchronized across languages. To address this problem, we introduce a new dataset INFOSYNC and a two-step method for tabular synchronization. INFOSYNC contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages, of which a subset (3.5K pairs) are manually annotated. The proposed method includes 1) Information Alignment to map rows and 2) Information Update for updating missing/outdated information for aligned tables across multilingual tables. When evaluated on INFOSYNC, information alignment achieves an F1 score of 87.91 (en  non-en). To evaluate information updation, we perform human-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach obtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of the proposed method.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Siddharth Khincha',\n",
       "    'Chelsi Jain',\n",
       "    'Vivek Gupta',\n",
       "    'Tushar Kataria',\n",
       "    'Shuo Zhang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_F5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'INFOSYNC: Information Synchronization across Multilingual Semi-structured Tables',\n",
       "   'tldr': 'Information Synchronization of semi-structured data across languages is challenging. For instance, Wikipedia tables in one language should be synchronized across languages. To address this problem, we introduce a new dataset INFOSYNC and a two-step method for tabular synchronization. INFOSYNC contai',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_F6': {'abstract': 'Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to REs low incidence in instruction-tuning datasets, making up less than 1% of all tasks (Wang et al., 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Kai Zhang', 'Bernal Gutirrez', 'Yu Su'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_F6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors',\n",
       "   'tldr': 'Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small L',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M1': {'abstract': 'In contrast to large text corpora, knowledge graphs (KG) provide dense and structured representations of factual information. This makes them attractive for systems that supplement or ground the knowledge found in pre-trained language models with an external knowledge source. This has especially been the case for classification tasks, where recent work has focused on creating pipeline models that retrieve information from KGs like ConceptNet as additional context. Many of these models consist of multiple components, and although they differ in the number and nature of these parts, they all have in common that for some given text query, they attempt to identify and retrieve a relevant subgraph from the KG. Due to the noise and idiosyncrasies often found in KGs, it is not known how current methods compare to a scenario where the aligned subgraph is completely relevant to the query. In this work, we try to bridge this knowledge gap by reviewing current approaches to text-to-KG alignment and evaluating them on two datasets where manually created graphs are available, providing insights into the effectiveness of current methods. We release our code for reproducibility.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Sondre Wold', 'Lilja vrelid', 'Erik Velldal'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Text-To-KG Alignment: Comparing Current Methods on Classification Tasks',\n",
       "   'tldr': 'In contrast to large text corpora, knowledge graphs (KG) provide dense and structured representations of factual information. This makes them attractive for systems that supplement or ground the knowledge found in pre-trained language models with an external knowledge source. This has especially bee',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M11': {'abstract': 'Relation extraction is a crucial language processing task for various downstream applications, including knowledge base completion, question answering, and summarization. Traditional relation-extraction techniques, however, rely on a predefined set of relations and model the extraction as a classification task. Consequently, such closed-world extraction methods are insufficient for inducing novel relations from a corpus. Unsupervised techniques like OpenIE, which extract <head, relation, tail> triples, generate relations that are too general for practical information extraction applications. In this work, we contribute the following: 1) We motivate and introduce a new task, corpus-based task-specific relation discovery. 2) We adapt existing data sources to create Wiki-Art, a novel dataset for task-specific relation discovery. 3) We develop a novel framework for relation discovery using zero-shot entity linking, prompting, and type-specific clustering. Our approach effectively connects unstructured text spans to their shared underlying relations, bridging the data-representation gap and significantly outperforming baselines on both quantitative and qualitative metrics. Our code and data are available in our GitHub repository.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Karthik Ramanan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M11',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Corpus-Based Task-Specific Relation Discovery',\n",
       "   'tldr': 'Relation extraction is a crucial language processing task for various downstream applications, including knowledge base completion, question answering, and summarization. Traditional relation-extraction techniques, however, rely on a predefined set of relations and model the extraction as a classifi',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M12': {'abstract': 'Fifteen years of work on entity linking has established the importance of different information sources in making linking decisions: mention and entity name similarity, contextual relevance, and features of the knowledge base. Modern state-of-the-art systems build on these features, including through neural representations (Wu et al., 2020). In contrast to this trend, the autoregressive language model GENRE (De Cao et al., 2021) generates normalized entity names for mentions and beats many other entity linking systems, despite making no use of knowledge base (KB) information. How is this possible? We analyze the behavior of GENRE on several entity linking datasets and demonstrate that its performance stems from memorization of name patterns. In contrast, it fails in cases that might benefit from using the KB. We experiment with a modification to the model to enable it to utilize KB information, highlighting challenges to incorporating traditional entity linking information sources into autoregressive models.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Elliot Schumacher', 'James Mayfield', 'Mark Dredze'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'On the Surprising Effectiveness of Name Matching Alone in Autoregressive Entity Linking',\n",
       "   'tldr': 'Fifteen years of work on entity linking has established the importance of different information sources in making linking decisions: mention and entity name similarity, contextual relevance, and features of the knowledge base. Modern state-of-the-art systems build on these features, including throug',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M14': {'abstract': 'Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wrong answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of LLMs. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to LLMs to generate the answer. Our framework, Knowledge-Augmented language model PromptING (KAPING), requires no model training, thus completely zero-shot. We validate the performance of our KAPING framework on the knowledge graph question answering task, that aims to answer the users question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48% in average, across multiple LLMs of various sizes.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jinheon Baek', 'Alham Aji', 'Amir Saffari'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering',\n",
       "   'tldr': 'Large Language Models (LLMs) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead LLMs to generate factually wro',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M15': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Lihu Chen', 'Simon Razniewski', 'Gerhard Weikum'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M15',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Knowledge Base Completion for Long-Tail Entities',\n",
       "   'tldr': '',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M16': {'abstract': 'Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes to generalization across domains where labeled data is scarce. Previous research mostly focuses on developing models either heavily relying on context, or dedicated solely to a specific domain. In contrast, we propose CoSiNES, a generic and adaptable framework with Contrastive Siamese Network for Entity Standardization that effectively adapts a pretrained language model to capture the syntax and semantics of the entities in a new domain. We construct a new dataset in the technology domain, which contains 640 technical stack entities and 6,412 mentions collected from industrial content management systems. We demonstrate that CoSiNES yields higher accuracy and faster runtime than baselines derived from leading methods in this domain. CoSiNES also achieves competitive performance in four standard datasets from the chemistry, medicine, and biomedical domains, demonstrating its cross-domain applicability. Code and data is available at https://github.com/konveyor/tackle-container-advisor/tree/main/entity_standardizer/cosines',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Jiaqing Yuan',\n",
       "    'Michele Merler',\n",
       "    'Mihir Choudhury',\n",
       "    'Raju Pavuluri',\n",
       "    'Munindar Singh',\n",
       "    'Maja Vukovic'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M16',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CoSiNES: Contrastive Siamese Network for Entity Standardization',\n",
       "   'tldr': 'Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes t',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M6': {'abstract': 'Humans often describe complex quantitative data using trend-based patterns. Trend-based patterns can be interpreted as higher order functions and relations over numerical data such as extreme values, rates of change, or cyclical repetition. One application where trends abound are descriptions of numerical tabular data. Therefore, the alignment of numerical tables and textual description of trends enables easier interpretations of tables. Most existing approaches can align quantities in text with tabular data but are unable to detect and align trend-based patterns about data. In this paper, we introduce the initial steps for aligning trend-based patterns about the data, i.e. the detection of textual description of trends and the alignment of trends with a relevant table. We introduce the problem of identifying quantifiably verifiable statements (QVS) in the text and aligning them with tables and datasets. We define the structure of these statements and implement a structured based detection. In our experiments, we demonstrate our method can detect and align these statements from several domains and compare favorably with traditional sequence labeling methods.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Pegah Jandaghi', 'Jay Pujara'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Identifying Quantifiably Verifiable Statements from Text',\n",
       "   'tldr': 'Humans often describe complex quantitative data using trend-based patterns. Trend-based patterns can be interpreted as higher order functions and relations over numerical data such as extreme values, rates of change, or cyclical repetition. One application where trends abound are descriptions of num',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M8': {'abstract': 'Event-event temporal relation extraction aims to extract the temporal order between a pair of event mentions, which is usually used to construct temporal event graphs. However, event graphs generated by existing methods are usually globally inconsistent (event graphs containing cycles), semantically irrelevant (two unrelated events having temporal links), and context unaware (neglecting neighborhood information of an event node). In this paper, we propose a novel event-event temporal relation extraction method to address these limitations. Our model combines a pretrained language model and a graph neural network to output event embeddings, which captures the contextual information of event graphs. Moreover, to achieve global consistency and semantic relevance, (1) event temporal order should be in accordance with the norm of their embeddings, and (2) two events have temporal relation only if their embeddings are close enough. Experimental results on a real-world event dataset demonstrate that our method achieves state-of-the-art performance and generates high-quality event graphs.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Xiaomeng Jin', 'Haoyang Wen', 'Xinya Du', 'Heng Ji'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M8',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Toward Consistent and Informative Event-Event Temporal Relation Extraction',\n",
       "   'tldr': 'Event-event temporal relation extraction aims to extract the temporal order between a pair of event mentions, which is usually used to construct temporal event graphs. However, event graphs generated by existing methods are usually globally inconsistent (event graphs containing cycles), semantically',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'MATCHING_M9': {'abstract': 'Event extraction is a complex task that involves extracting events from unstructured text. Prior classification-based methods require comprehensive entity annotations for joint training, while newer generation-based methods rely on heuristic templates containing oracle information such as event type, which is often unavailable in real-world scenarios. In this study, we consider a more realistic task setting, namely the Oracle-Free Event Extraction (OFEE) task, where only the input context is given, without any oracle information including event type, event ontology, or trigger word. To address this task, we propose a new framework, COFFEE. This framework extracts events solely based on the document context, without referring to any oracle information. In particular, COFFEE introduces a contrastive selection model to refine the generated triggers and handle multi-event instances. Our proposed COFFEE outperforms state-of-the-art approaches in the oracle-free setting of the event extraction task, as evaluated on two public variants of the ACE05 benchmark. The code used in our study has been made publicly available.',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Meiru Zhang',\n",
       "    'Yixuan Su',\n",
       "    'Zaiqiao Meng',\n",
       "    'Zihao Fu',\n",
       "    'Nigel Collier'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['MATCHING'],\n",
       "   'id': 'MATCHING_M9',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'COFFEE: A Contrastive Oracle-Free Framework for Event Extraction',\n",
       "   'tldr': 'Event extraction is a complex task that involves extracting events from unstructured text. Prior classification-based methods require comprehensive entity annotations for joint training, while newer generation-based methods rely on heuristic templates containing oracle information such as event type',\n",
       "   'track': 'The First Workshop on Matching From Unstructured and Structured Data (MATCHING 2023)',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_12': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Naoki Otani', 'Jun Araki', 'HyeongSik Kim', 'Eduard Hovy'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_12',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'On the Underspecification of Situations in Open-domain Conversational Datasets',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_14': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Parker Glenn', 'Parag Pravin Dakle', 'Preethi Raghavan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_14',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Correcting Semantic Parses with Natural Language through Dynamic Schema Encoding',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_17': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Longfei Yang', 'Jiyi Li', 'Sheng Li', 'Takahiro Shinozaki'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_17',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Dialogue State Tracking with Sparse Local Slot Attention',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_19': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yen-Ting Lin', 'Yun-Nung Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_19',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_21': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Anirudh S. Sundar', 'Larry Heck'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_21',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'cTBLS: Augmenting Large Language Models with Conversational Tables',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_22': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Maarten De Raedt',\n",
       "    \"Fr\\\\'{e}deric Godin\",\n",
       "    'Thomas Demeester',\n",
       "    'Chris Develder'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_22',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'IDAS: Intent Discovery with Abstractive Summarization',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_23': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Qiusi Zhan', 'Xiaojie Guo', 'Heng Ji', 'Lingfei Wu'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_23',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'User Simulator Assisted Open-ended Conversational Recommendation System',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_35': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Divyanshu Aggarwal', 'Vivek Gupta', 'Anoop Kunchukuttan'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_35',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Evaluating Inter-Bilingual Semantic Parsing for Indian Languages',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_37': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ze-Song Xu', 'Yun-Nung Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_37',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Zero-Shot Dialogue Relation Extraction by Relating Explainable Triggers and Relation Names',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_40': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Gaetan Lopez Latouche', 'Laurence Marcotte', 'Ben Swanson'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_40',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generating Video Game Scripts with Style',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_50': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Ananya Ganesh', 'Martha Palmer', 'Katharina Kann'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_50',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Survey of Challenges and Methods in the Computational Modeling of Multi-Party Dialog',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_53': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Raghav Gupta',\n",
       "    'Renat Aksitov',\n",
       "    'Samrat Phatale',\n",
       "    'Simral Chaudhary',\n",
       "    'Harrison Lee',\n",
       "    'Abhinav Rastogi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_53',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Conversational Recommendation as Retrieval: A Simple, Strong Baseline',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_7': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Seyed Mahed Mousavi',\n",
       "    'Simone Caldarella',\n",
       "    'Giuseppe Riccardi'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F1': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Francesco Cazzaro',\n",
       "    'Davide Locatelli',\n",
       "    'Ariadna Julieta Quattoni',\n",
       "    'Xavier Carreras'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F1',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Translate First Reorder Later: Leveraging Monotonicity in Semantic Parsing',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F2': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Hui-Chi Kuo', 'Yun-Nung Chen'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F2',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F3': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Nikita Moghe',\n",
       "    'Evgeniia Razumovskaia',\n",
       "    'Liane K Guillou',\n",
       "    \"Ivan Vuli\\\\'{c}\",\n",
       "    'Anna Korhonen',\n",
       "    'Alexandra Birch'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F3',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F4': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Vishal Vivek Saley',\n",
       "    'Rocktim Jyoti Das',\n",
       "    'Dinesh Raghu',\n",
       "    'Mausam None'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F4',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DKAF: KB Arbitration for Learning Task-Oriented Dialog Systems with Dialog-KB Inconsistencies',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F5': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Yi-Lin Tuan',\n",
       "    'Alon Albalak',\n",
       "    'Wenda Xu',\n",
       "    'Michael S Saxon',\n",
       "    'Connor F Pryor',\n",
       "    'Lise Getoor',\n",
       "    'William Yang Wang'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F5',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CausalDialogue: Modeling Utterance-level Causality in Conversations',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F6': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Duong Minh Le', 'Ruohao Guo', 'Wei Xu', 'Alan Ritter'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F6',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'long',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improved Instruction Ordering in Recipe-Grounded Conversation',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'NLP4ConvAI_F7': {'abstract': '',\n",
       "   'anthology_url': None,\n",
       "   'authors': ['Swaroop Mishra', 'Elnaz Nouri'],\n",
       "   'category': 'Workshop',\n",
       "   'demo_url': None,\n",
       "   'display_track': None,\n",
       "   'event_ids': ['NLP4ConvAI'],\n",
       "   'id': 'NLP4ConvAI_F7',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': None,\n",
       "   'paper_type': 'short',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Workshop',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models',\n",
       "   'tldr': '',\n",
       "   'track': 'The 5th Workshop on NLP for Conversational AI',\n",
       "   'underline_id': None,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'P1012': {'abstract': 'Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8\\\\% point on average on four datasets, three corruption types, and two corruption ratios',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.117',\n",
       "   'authors': ['KiYoon Yoo', 'Wonhyuk Ahn', 'Jiho Jang', 'Nojun Kwak'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-7_-nlp-applications-(poster)'],\n",
       "   'id': 'P1012',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['security/privacy'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.117.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76518/poster_document/3a6ff71ae74141123912362267cb603d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76518/poster/2c9f4be6bc19d9ab1bc8002b9c7c0a2b.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76518/slideshow/96628ad5604720105794087bed259a68.key',\n",
       "   'title': 'Robust Multi-bit Natural Language Watermarking through Invariant Features',\n",
       "   'tldr': 'Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security meas...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76518,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76518-robust-multi-bit-natural-language-watermarking-through-invariant-features',\n",
       "   'video_url': None},\n",
       "  'P1015': {'abstract': 'Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.128',\n",
       "   'authors': ['Jinghui Lu',\n",
       "    'dongsheng zhu',\n",
       "    'weidong Han',\n",
       "    'Rui Zhao',\n",
       "    'Brian Mac Namee',\n",
       "    'Fei Tan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1015',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.128.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76519/poster_document/a91cb5b87f466e41bfa938d86444ab3a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76519/poster/696f8fa641de2d5556b2c47da77bbae8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76519/slideshow/2b901eb762f595914c13d0aca8bacf47.pptx',\n",
       "   'title': 'What Makes Pre-trained Language Models Better Zero-shot Learners?',\n",
       "   'tldr': 'Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is availa...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76519,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76519-what-makes-pre-trained-language-models-better-zero-shot-learnersquestion',\n",
       "   'video_url': None},\n",
       "  'P1018': {'abstract': 'Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality with pros and cons. In this work, we propose a novel critic decoding method for controlled language generation (CriticControl) that combines the strengths of reinforcement learning and weighted decoding. Specifically, we adopt the actor-critic framework and train an LM-steering critic from reward models. Similar to weighted decoding, our method freezes the language model and manipulates the output token distribution using a critic to improve training efficiency and stability. Evaluation of our method on three controlled generation tasks, topic control, sentiment control, and detoxification, shows that our approach generates more coherent and well-controlled texts than previous methods. In addition, CriticControl demonstrates superior generalization ability in zero-shot settings. Human evaluation studies also corroborate our findings.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.281',\n",
       "   'authors': ['Minbeom Kim',\n",
       "    'Hwanhee Lee',\n",
       "    'Kang Min Yoo',\n",
       "    'Joonsuk Park',\n",
       "    'Hwaran Lee',\n",
       "    'Kyomin Jung'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1018',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['inference methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.281.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77394/poster_document/1d6b88cc8c54269cd28c6c9cd39d3f54.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77394/poster/54e9b070b0f90ccb76ca3e76d853d61a.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77394/slideshow/68471414e4c7da83c1a107815800a518.pdf',\n",
       "   'title': 'Critic-Guided Decoding for Controlled Text Generation',\n",
       "   'tldr': 'Steering language generation towards objectives or away from undesired content has been a long-standing goal in utilizing language models (LM). Recent work has demonstrated reinforcement learning and weighted decoding as effective approaches to achieve a higher level of language control and quality ...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77394,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77394-lida-a-tool-for-automatic-generation-of-grammar-agnostic-visualizations-and-infographics-using-large-language-models',\n",
       "   'video_url': None},\n",
       "  'P1022': {'abstract': 'Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only under conservative shifts in data distribution and typically focus on a single component (i.e., retriever or reader) rather than an end-to-end system. This work proposes a more realistic end-to-end domain shift evaluation setting covering five diverse domains. We not only find that end-to-end models fail to generalize but that high retrieval scores often still yield poor answer prediction accuracy. To address these failures, we investigate several interventions, in the form of data augmentations, for improving model adaption and use our evaluation set to elucidate the relationship between the efficacy of an intervention scheme and the particular type of dataset shifts we consider. We propose a generalizability test that estimates the type of shift in a target dataset without training a model in the target domain and that the type of shift is predictive of which data augmentation schemes will be effective for domain adaption. Overall, we find that these interventions increase end-to-end performance by up to ~24 points.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.807',\n",
       "   'authors': ['Dheeru Dua', 'Emma Strubell', 'Sameer Singh', 'Pat Verga'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-1_-question-answering-(oral)'],\n",
       "   'id': 'P1022',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['open-domain qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.807.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76217/poster_document/42950ede03137c1c85aca00d334d25dd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76217/poster/e8ab469e0e837ac11054f9e604efbff1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'To Adapt or to Annotate: Challenges and Interventions for Domain Adaptation in Open-Domain Question Answering',\n",
       "   'tldr': 'Recent advances in open-domain question answering (ODQA) have demonstrated impressive accuracy on general-purpose domains like Wikipedia. While some work has been investigating how well ODQA models perform when tested for out-of-domain (OOD) generalization, these studies have been conducted only und...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76217,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15196/lecture/76217-to-adapt-or-to-annotate-challenges-and-interventions-for-domain-adaptation-in-open-domain-question-answering',\n",
       "   'video_url': None},\n",
       "  'P1027': {'abstract': \"Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.826',\n",
       "   'authors': ['Mo Yu',\n",
       "    'Jiangnan Li',\n",
       "    'Shunyu Yao',\n",
       "    'Wenjie Pang',\n",
       "    'Xiaochen Zhou',\n",
       "    'Zhou Xiao',\n",
       "    'Fandong Meng',\n",
       "    'Jie Zhou'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-4_-resources-and-evaluation-(virtual-poster)'],\n",
       "   'id': 'P1027',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.826.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76520/poster/ce52cc33a7dc4614483673fdc41bdfd9.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Personality Understanding of Fictional Characters during Book Reading',\n",
       "   'tldr': \"Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situat...\",\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76520,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76520-personality-understanding-of-fictional-characters-during-book-reading',\n",
       "   'video_url': None},\n",
       "  'P1033': {'abstract': 'Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation.\\nWe instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.82',\n",
       "   'authors': ['Maximillian Chen',\n",
       "    'Xiao Yu',\n",
       "    'Weiyan Shi',\n",
       "    'Urvi Awasthi',\n",
       "    'Zhou Yu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-4_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1033',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications', 'conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.82.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76521/poster_document/6a362b135a0be77a8774629ae0a0ec97.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76521/poster/6d2df2c6a60e301609d61d533c87125e.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76521/slideshow/a8804a4526e404f48652f0a6d71bef00.pdf',\n",
       "   'title': 'Controllable Mixed-Initiative Dialogue Generation through Prompting',\n",
       "   'tldr': 'Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-train...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76521,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76521-controllable-mixed-initiative-dialogue-generation-through-prompting',\n",
       "   'video_url': None},\n",
       "  'P1040': {'abstract': 'Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require real-time or on-device processing capabilities. In this paper, we perform a focused study on the feasibility of applying recent model compression techniques to sketch-based and sequence-to-sequence Text-to-SQL models. Our results reveal that sketch-based Text-to-SQL models generally have higher inference efficiency and respond better to model compression than sequence-to-sequence models, making them ideal for real-world deployments, especially in use cases with simple SQL statements.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.740',\n",
       "   'authors': ['Shuo Sun',\n",
       "    'Yuze Gao',\n",
       "    'Yuchen Zhang',\n",
       "    'Jian Su',\n",
       "    'Bin Chen',\n",
       "    'Yingzhan Lin',\n",
       "    'Shuqi Sun'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-1_-theme_-reality-check-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1040',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['lessons from deployment'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.740.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'An Exploratory Study on Model Compression for Text-to-SQL',\n",
       "   'tldr': 'Text-to-SQL translates user queries into SQL statements that can retrieve relevant answers from relational databases. Recent approaches to Text-to-SQL rely on pre-trained language models that are computationally expensive and technically challenging to deploy in real-world applications that require ...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 77399,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77399-an-exploratory-study-on-model-compression-for-text-to-sql',\n",
       "   'video_url': None},\n",
       "  'P1047': {'abstract': 'The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually with multimodal information. Nevertheless, news videos from different perspectives regarding the same event are commonly posted together, which contain complementary or contradictory information and thus can be used to evaluate each other mutually. To this end, we introduce a new and practical paradigm, i.e., cross-sample fake news video detection, and propose a novel framework, Neighbor-Enhanced fakE news video Detection (NEED), which integrates the neighborhood relationship of new videos belonging to the same event. NEED can be readily combined with existing single-sample detectors and further enhance their performances with the proposed graph aggregation (GA) and debunking rectification (DR) modules. Specifically, given the feature representations obtained from single-sample detectors, GA aggregates the neighborhood information with the dynamic graph to enrich the features of independent samples. After that, DR explicitly leverages the relationship between debunking videos and fake news videos to refute the candidate videos via textual and visual consistency. Extensive experiments on the public benchmark demonstrate that NEED greatly improves the performance of both single-modal (up to 8.34\\\\% in accuracy) and multimodal (up to 4.97\\\\% in accuracy) base detectors.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.756',\n",
       "   'authors': ['Peng Qi',\n",
       "    'Yuyang Zhao',\n",
       "    'Yufeng Shen',\n",
       "    'Wei Ji',\n",
       "    'Juan Cao',\n",
       "    'Tat-Seng Chua'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P1047',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['misinformation detection and analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.756.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77401/poster_document/83336aa8ad7777df39d6eb784b2bff68.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Two Heads Are Better Than One: Improving Fake News Video Detection by Correlating with Neighbors',\n",
       "   'tldr': 'The prevalence of short video platforms has spawned a lot of fake news videos, which have stronger propagation ability than textual fake news. Thus, automatically detecting fake news videos has been an important countermeasure in practice. Previous works commonly verify each news video individually ...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 77401,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77401-two-heads-are-better-than-one-improving-fake-news-video-detection-by-correlating-with-neighbors',\n",
       "   'video_url': None},\n",
       "  'P1056': {'abstract': 'Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can hardly incorporate long-range information. To tackle the problem, we propose a new task, named unsupervised long-document question answering (ULQA), aiming to generate high-quality long-document QA instances in an unsupervised manner. Besides, we propose AttenWalker, a novel unsupervised method to aggregate and generate answers with long-range dependency so as to construct long-document QA pairs. Specifically, AttenWalker is composed of three modules, i.e. span collector, span linker and answer aggregator. Firstly, the span collector takes advantage of constituent parsing and reconstruction loss to select informative candidate spans for constructing answers. Secondly, with the help of the attention graph of a pre-trained long-document model, potentially interrelated text spans (that might be far apart) could be linked together via an attention-walking algorithm. Thirdly, in the answer aggregator, linked spans are aggregated into the final answer via the mask-filling ability of a pre-trained model. Extensive experiments show that AttenWalker outperforms previous methods on NarrativeQA and Qasper. In addition, AttenWalker also shows strong performance in the few-shot learning setting.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.862',\n",
       "   'authors': ['Yuxiang Nie', 'Heyan Huang', 'Wei Wei', 'Xian-Ling Mao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1056',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.862.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77402/poster_document/aa711b498244436ae10cfd9c4eb83007.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77402/poster/8d5cbc773b46c97f5a1e5ed35e93dea4.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'AttenWalker: Unsupervised Long-Document Question Answering via Attention-based Graph Walking',\n",
       "   'tldr': 'Annotating long-document question answering (long-document QA) pairs is time-consuming and expensive. To alleviate the problem, it might be possible to generate long-document QA pairs via unsupervised question answering (UQA) methods. However, existing UQA tasks are based on short documents, and can...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77402,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77402-attenwalker-unsupervised-long-document-question-answering-via-attention-based-graph-walking',\n",
       "   'video_url': None},\n",
       "  'P1059': {'abstract': 'In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. \\nWe identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO, are blind to this issue due to oversimplified constraint settings. To solve it, we create a new challenging test set of English-German, increasing the average constraint count per sentence from 1.1$\\\\sim$1.7 to 6.1 and the length per target constraint from 1.1$\\\\sim$1.2 words to 3.4 words. Then we find that PH and CS methods degrade as the number of constraints increases, but they have complementary strengths. Specifically, PH is better at retaining high constraint accuracy but lower translation quality as measured by BLEU and COMET scores. In contrast, CS has the opposite results. Based on these observations, we propose a simple but effective method combining the advantages of PH and CS. This approach involves training a model like PH to predict the term labels, and then during inference replacing those labels with target terminology text like CS, so that the subsequent generation is aware of the target term content. Extensive experimental results show that this approach can achieve high constraint accuracy and translation quality simultaneously, regardless of the number or length of constraints.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.332',\n",
       "   'authors': ['Huaao Zhang',\n",
       "    'Qiang Wang',\n",
       "    'Bo - Qin',\n",
       "    'Zelin Shi',\n",
       "    'haibo wang',\n",
       "    'MING CHEN'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-1_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P1059',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['online adaptation for mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.332.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76522/poster_document/5218ec7f33e3fbb9605100a249fea63c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76522/poster/c3a630e791a0c82de4e7d270616d9c01.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76522/slideshow/625b3a075e957b942481ca966e75fb28.pptx',\n",
       "   'title': 'Understanding and Improving the Robustness of Terminology Constraints in Neural Machine Translation',\n",
       "   'tldr': 'In this work, we study the robustness of two typical terminology translation methods: Placeholder (PH) and Code-Switch (CS), concerning (1) the number of constraints and (2) the target constraint length. \\nWe identify that existing terminology constraint test sets, such as IATE, Wiktionary, and TICO,...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76522,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76522-understanding-and-improving-the-robustness-of-terminology-constraints-in-neural-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P106': {'abstract': 'Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (\\\\emph{i.e.,} textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of contribution to emotion recognition. More seriously, there might be inconsistencies between the emotion of individual modality and the video. The challenges mentioned above are caused by the inherent uncertainty of emotion. Inspired by the recent advances of quantum theory in modeling uncertainty, we make an initial attempt to design a quantum-inspired adaptive-priority-learning model (QAP) to address the challenges. Specifically, the quantum state is introduced to model modal features, which allows each modality to retain all emotional tendencies until the final classification. Additionally, we design Q-attention to orderly integrate three modalities, and then QAP learns modal priority adaptively so that modalities can provide different amounts of information based on priority. Experimental results on the IEMOCAP and MOSEI datasets show that QAP establishes new state-of-the-art results.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.772',\n",
       "   'authors': ['Ziming Li',\n",
       "    'Yan Zhou',\n",
       "    'Yaxin Liu',\n",
       "    'Fuqing Zhu',\n",
       "    'Chuanpeng Yang',\n",
       "    'Songlin Hu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P106',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['style analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.772.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77264/poster_document/b06b9473ecfc45646630814c29ec80e1.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'QAP: A Quantum-Inspired Adaptive-Priority-Learning Model for Multimodal Emotion Recognition',\n",
       "   'tldr': 'Multimodal emotion recognition for video has gained considerable attention in recent years, in which three modalities (\\\\emph{i.e.,} textual, visual and acoustic) are involved. Due to the diverse levels of informational content related to emotion, three modalities typically possess varying degrees of...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77264,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77264-qap-a-quantum-inspired-adaptive-priority-learning-model-for-multimodal-emotion-recognition',\n",
       "   'video_url': None},\n",
       "  'P1065': {'abstract': 'Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models.\\nIn this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. \\nThis is in contrast to the common belief that LayerNorm\\'s only role is to normalize the activations during the forward pass, and their gradients during the backward pass.\\n\\nWe consider a geometric interpretation of LayerNorm and show that it consists of two components: \\n(a) projection of the input vectors to a d-1 space that is orthogonal to the [1,1,...,1] vector, and\\n(b) scaling of all vectors to the same norm of \\\\sqrt{d}. \\nWe show that each of these components is important for the attention layer that follows it in Transformers:\\n(a) projection allows the attention mechanism to create an attention query that attends to all keys equally, offloading the need to learn this operation in the attention; and\\n(b) scaling allows each key to potentially receive the highest attention, and prevents keys from being \"un-select-able\\'\\'.\\nWe show empirically that Transformers do indeed benefit from these properties of LayeNorm in general language modeling and even in computing simple functions such as \"majority\\'\\'. Our code is available at https://github.com/tech-srl/layer\\\\_norm\\\\_expressivity\\\\_role .',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.895',\n",
       "   'authors': ['Shaked Brody', 'Uri Alon', 'Eran Yahav'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1065',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generalization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.895.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77407/poster_document/3ef6abb27f8ce5bfd562135f7a368179.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77407/poster/9799352a4185fdf9a5c6dfdd13bc6100.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77407/slideshow/0f3cbe59439c48490dbdcd3c2509605d.pdf',\n",
       "   'title': \"On the Expressivity Role of LayerNorm in Transformers' Attention\",\n",
       "   'tldr': \"Layer Normalization (LayerNorm) is an inherent component in all Transformer-based models.\\nIn this paper, we show that LayerNorm is crucial to the expressivity of the multi-head attention layer that follows it. \\nThis is in contrast to the common belief that LayerNorm's only role is to normalize the a...\",\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77407,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77407-on-the-expressivity-role-of-layernorm-in-transformers-attention',\n",
       "   'video_url': None},\n",
       "  'P1067': {'abstract': \"The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, these methods suffer from low knowledge coverage caused by PLM bias -- the tendency to generate certain tokens over other tokens regardless of prompt changes, and high dependency on the PLM quality -- only models using GPT-3 can achieve the best result. \\n\\nTo address the aforementioned challenges, we propose RASO: a new VQA pipeline that deploys a generate-then-select strategy guided by world knowledge for the first time. Rather than following the de facto standard to train a multi-modal model that directly generates the VQA answer, \\\\{pasted macro `MODEL'\\\\}name first adopts PLM to generate all the possible answers, and then trains a lightweight answer selection model for the correct answer. As proved in our analysis, RASO expands the knowledge coverage from in-domain training data by a large margin. We provide extensive experimentation and show the effectiveness of our pipeline by advancing the state-of-the-art by 4.1\\\\% on OK-VQA, without additional computation cost.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.147',\n",
       "   'authors': ['Xingyu Fu',\n",
       "    'Sheng Zhang',\n",
       "    'Gukyeong Kwon',\n",
       "    'Pramuditha Perera',\n",
       "    'Henghui Zhu',\n",
       "    'Yuhao Zhang',\n",
       "    'Alexander Hanbo Li',\n",
       "    'William Yang Wang',\n",
       "    'Zhiguo Wang',\n",
       "    'Vittorio Castelli',\n",
       "    'Patrick Ng',\n",
       "    'Dan Roth',\n",
       "    'Bing Xiang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1067',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['vision question answering'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.147.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77408/poster_document/3621ca1f66a6bf2b6f670d9eb15e1de6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77408/poster/89e46fab33a9e70e7dd917cb915df391.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77408/slideshow/f62dff9c63fbea88821e759ac1d5058a.pdf',\n",
       "   'title': 'Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge',\n",
       "   'tldr': 'The open-ended Visual Question Answering (VQA) task requires AI models to jointly reason over visual and natural language inputs using world knowledge. Recently, pre-trained Language Models (PLM) such as GPT-3 have been applied to the task and shown to be powerful world knowledge sources. However, t...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 77408,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77408-indicmt-eval-a-dataset-to-meta-evaluate-machine-translation-metrics-for-indian-languages',\n",
       "   'video_url': None},\n",
       "  'P1069': {'abstract': 'Various datasets have been proposed to promote the development of Table Question Answering (TQA) technique. However, the problem setting of existing TQA benchmarks suffers from two limitations. First, they directly provide models with explicit table structures where row headers and column headers of the table are explicitly annotated and treated as model input during inference. Second, they only consider tables of limited types and ignore other tables especially complex tables with flexible header locations. Such simplified problem setting cannot cover practical scenarios where models need to process tables without header annotations in the inference phase or tables of different types. To address above issues, we construct a new TQA dataset with implicit and multi-type table structures, named IM-TQA, which not only requires the model to understand tables without directly available header annotations but also to handle multi-type tables including previously neglected complex tables. We investigate the performance of recent methods on our dataset and find that existing methods struggle in processing implicit and multi-type table structures. Correspondingly, we propose an RGCN-RCI framework outperforming recent baselines. We will release our dataset to facilitate future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.278',\n",
       "   'authors': ['Mingyu Zheng',\n",
       "    'Yang Hao',\n",
       "    'Wenbin Jiang',\n",
       "    'Zheng Lin',\n",
       "    'Yajuan Lyu',\n",
       "    'QiaoQiao She',\n",
       "    'Weiping Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1069',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['table qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.278.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76523/poster_document/ed794af2196f2215d618ddcb255a1014.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76523/poster/3849876b683c5183eec06b23adb58a0a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'IM-TQA: A Chinese Table Question Answering Dataset with Implicit and Multi-type Table Structures',\n",
       "   'tldr': 'Various datasets have been proposed to promote the development of Table Question Answering (TQA) technique. However, the problem setting of existing TQA benchmarks suffers from two limitations. First, they directly provide models with explicit table structures where row headers and column headers of...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76523,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76523-layoutmask-enhance-text-layout-interaction-in-multi-modal-pre-training-for-document-understanding',\n",
       "   'video_url': None},\n",
       "  'P1071': {'abstract': 'Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (PLMs), which has exhibited effectiveness in the few-shot flat text classification tasks. However, limited work has studied the paradigm of prompt-based learning in the HTC problem when the training data is extremely scarce. In this work, we define a path-based few-shot setting and establish a strict path-based evaluation metric to further explore few-shot HTC tasks. To address the issue, we propose the hierarchical verbalizer (\"HierVerb\"), a multi-verbalizer framework treating HTC as a single- or multi-label classification problem at multiple layers and learning vectors as verbalizers constrained by hierarchical structure and hierarchical contrastive learning. In this manner, HierVerb fuses label hierarchy knowledge into verbalizers and remarkably outperforms those who inject hierarchy through graph encoders, maximizing the benefits of PLMs. Extensive experiments on three popular HTC datasets under the few-shot settings demonstrate that prompt with HierVerb significantly boosts the HTC performance, meanwhile indicating an elegant way to bridge the gap between the large pre-trained model and downstream hierarchical classification tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.164',\n",
       "   'authors': ['Ke Ji', 'Yixin Lian', 'Jingsheng Gao', 'Baoyuan Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1071',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['fine-tuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.164.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76524/poster_document/ecedaee0ff67396f1d6ccff179da5916.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76524/poster/ad5c072ff86a7cdf02e3b84866a1a738.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76524/slideshow/a9a0e26162e64fcef49342a4765b3e6b.pptx',\n",
       "   'title': 'Hierarchical Verbalizer for Few-Shot Hierarchical Text Classification',\n",
       "   'tldr': 'Due to the complex label hierarchy and intensive labeling cost in practice, the hierarchical text classification (HTC) suffers a poor performance especially when low-resource or few-shot settings are considered. Recently, there is a growing trend of applying prompts on pre-trained language models (P...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76524,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76524-hierarchical-verbalizer-for-few-shot-hierarchical-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1072': {'abstract': \"Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model's ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.830',\n",
       "   'authors': ['Namgyu Ho', 'Laura Schmid', 'Se-Young Yun'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-3_-large-language-models-(poster)'],\n",
       "   'id': 'P1072',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'fine-tuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.830.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76525/poster_document/cc0a529d96e7997980dcbe312c1f47ab.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76525/poster/4d577cc47baa10ade7ca9e14d8896f98.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Large Language Models Are Reasoning Teachers',\n",
       "   'tldr': 'Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76525,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76525-large-language-models-are-reasoning-teachers',\n",
       "   'video_url': None},\n",
       "  'P1074': {'abstract': 'Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowledge. Under such observation, we tend to investigate the feasibility of a memory-friendly model with strong generalization capability that could boost the performance of HTC without prior statistics or label semantics. In this paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance the text representations with only syntactic information of the label hierarchy. Specifically, we convert the label hierarchy into an unweighted tree structure, termed coding tree, with the guidance of structural entropy. Then we design a structure encoder to incorporate hierarchy-aware information in the coding tree into text representations. Besides the text encoder, HiTIN only contains a few multi-layer perceptions and linear transformations, which greatly saves memory. We conduct experiments on three commonly used datasets and the results demonstrate that HiTIN could achieve better test performance and less memory consumption than state-of-the-art (SOTA) methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.432',\n",
       "   'authors': ['He Zhu', 'Chong Zhang', 'Junjie Huang', 'Junran Wu', 'Ke Xu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1074',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['graph-based methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.432.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76526/poster_document/00737ee74c536706ec05928d0c533c1d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76526/poster/d95658ed377df01dea6859d8108f30ab.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76526/slideshow/7cb2831ab40475bc52a192f8e7fa911e.pdf',\n",
       "   'title': 'HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text Classification',\n",
       "   'tldr': 'Hierarchical text classification (HTC) is a challenging subtask of multi-label classification as the labels form a complex hierarchical structure. Existing dual-encoder methods in HTC achieve weak performance gains with huge memory overheads and their structure encoders heavily rely on domain knowle...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76526,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76526-hitin-hierarchy-aware-tree-isomorphism-network-for-hierarchical-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1075': {'abstract': \"Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent  performance in the target language.\\nHowever, if no labeled data in the target language are available, they need to be created through human annotations.\\nIn this study, we devise a metric to select annotation candidates from an unlabeled data pool that efficiently enhance accuracy for few-shot cross-lingual transfer.\\nIt is known that training a model with hard examples is important to improve the model's performance.\\nTherefore, we first identify examples that MMLM cannot solve in a zero-shot cross-lingual transfer setting and demonstrate that it is hard to predict peculiar examples in the target language, i.e., the examples distant from the source language examples in cross-lingual semantic space of the MMLM.\\nWe then choose high peculiarity examples as annotation candidates and perform few-shot cross-lingual transfer.\\nIn comprehensive experiments with 20 languages and 6 tasks, we demonstrate that the high peculiarity examples improve the target language accuracy compared to other candidate selection methods proposed in previous studies.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.47',\n",
       "   'authors': ['Hwichan Kim', 'Mamoru Komachi'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P1075',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.47.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77409/poster_document/85f1d6316ddb7031a780dcc84f43203a.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Few-shot Cross-lingual Transfer with Target Language Peculiar Examples',\n",
       "   'tldr': 'Few-shot cross-lingual transfer, fine-tuning Multilingual Masked Language Model (MMLM) with source language labeled data and a small amount of target language labeled data, provides excellent  performance in the target language.\\nHowever, if no labeled data in the target language are available, they ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77409,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77409-enhancing-few-shot-cross-lingual-transfer-with-target-language-peculiar-examples',\n",
       "   'video_url': None},\n",
       "  'P1078': {'abstract': 'Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive experiments on three sentence-level cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https://github.com/ChillingDream/DAP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.191',\n",
       "   'authors': ['Ziheng Li',\n",
       "    'Shaohan Huang',\n",
       "    'Zihan Zhang',\n",
       "    'Zhi-Hong Deng',\n",
       "    'Qiang Lou',\n",
       "    'Haizhen Huang',\n",
       "    'Jian Jiao',\n",
       "    'Furu Wei',\n",
       "    'Weiwei Deng',\n",
       "    'Qi Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P1078',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mutlilingual representations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.191.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76527/poster_document/00b2b86acf6bd811809a6544eb3e2e17.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76527/poster/8714ddb2ba5e9426aead614769ae0576.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76527/slideshow/67befc487a95b7922fc086ea5d8d5dec.pdf',\n",
       "   'title': 'Dual-Alignment Pre-training for Cross-lingual Sentence Embedding',\n",
       "   'tldr': 'Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explo...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76527,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76527-dual-alignment-pre-training-for-cross-lingual-sentence-embedding',\n",
       "   'video_url': None},\n",
       "  'P1080': {'abstract': 'Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational solutions to model semantic features and infer the corresponding implications from a literal statement. Moreover, the limited amount of available training data leads to subpar performance in out-of-domain and cross-target scenarios, as data-driven approaches are prone to rely on superficial and domain-specific features.\\nIn this work, we decompose the stance detection task from a linguistic perspective, and investigate key components and inference paths in this task. The stance triangle is a generic linguistic framework previously proposed to describe the fundamental ways people express their stance. We further expand it by characterizing the relationship between explicit and implicit objects. We then use the framework to extend one single training corpus with additional annotation. Experimental results show that strategically-enriched data can significantly improve the performance on out-of-domain and cross-target evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.220',\n",
       "   'authors': ['Zhengyuan Liu',\n",
       "    'Yong Keong Yap',\n",
       "    'Hai Leong Chieu',\n",
       "    'Nancy Chen'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)'],\n",
       "   'id': 'P1080',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.220.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76218/poster_document/b20ad277679402ffd016ec184e9ec2f9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76218/poster/ca9b749e7aadb03e98bee68021c02314.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Guiding Computational Stance Detection with Expanded Stance Triangle Framework',\n",
       "   'tldr': 'Stance detection determines whether the author of a piece of text is in favor of, against, or neutral towards a specified target, and can be used to gain valuable insights into social media. The ubiquitous indirect referral of targets makes this task challenging, as it requires computational solutio...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76218,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15209/lecture/76218-guiding-computational-stance-detection-with-expanded-stance-triangle-framework',\n",
       "   'video_url': None},\n",
       "  'P1084': {'abstract': 'Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.723',\n",
       "   'authors': ['Yujia Qin',\n",
       "    'Cheng Qian',\n",
       "    'Xu Han',\n",
       "    'Yankai Lin',\n",
       "    'Huadong Wang',\n",
       "    'Ruobing Xie',\n",
       "    'Zhiyuan Liu',\n",
       "    'Maosong Sun',\n",
       "    'Jie Zhou'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1084',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['continual learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.723.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77410/poster_document/c4ef5eee9fb2d123e0b0105d0a38d537.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77410/poster/45d3e720762aba034738de1bdcdf91cc.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77410/slideshow/19fcf4201201873ced30ea7cff722f5d.pdf',\n",
       "   'title': 'Recyclable Tuning for Continual Pre-training',\n",
       "   'tldr': 'Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tun...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77410,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77410-recyclable-tuning-for-continual-pre-training',\n",
       "   'video_url': None},\n",
       "  'P1087': {'abstract': 'Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 15,372 supporting facts and 125,954 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5\\\\% and 47.5\\\\% of the cases on our dataset and DuReader, respectively. The interface, dataset, and codes are publicly available at https://github.com/thunlp/WebCPM.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.499',\n",
       "   'authors': ['Yujia Qin',\n",
       "    'Zihan Cai',\n",
       "    'Dian Jin',\n",
       "    'Lan Yan',\n",
       "    'Shihao Liang',\n",
       "    'Kunlun Zhu',\n",
       "    'Yankai Lin',\n",
       "    'Xu Han',\n",
       "    'Ning Ding',\n",
       "    'Huadong Wang',\n",
       "    'Ruobing Xie',\n",
       "    'Fanchao Qi',\n",
       "    'Zhiyuan Liu',\n",
       "    'Maosong Sun',\n",
       "    'Jie Zhou'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-7_-large-language-models-(poster)'],\n",
       "   'id': 'P1087',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.499.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76528/poster_document/0d3703e551b09e03d162c16001e06da9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76528/poster/f0a234e1364f716657515b96db57be62.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76528/slideshow/8369df72249c99c9a0edffc662e55525.pptx',\n",
       "   'title': 'WebCPM: Interactive Web Search for Chinese Long-form Question Answering',\n",
       "   'tldr': 'Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates t...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76528,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76528-webcpm-interactive-web-search-for-chinese-long-form-question-answering',\n",
       "   'video_url': None},\n",
       "  'P1090': {'abstract': \"The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model's superiority in personalization.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.299',\n",
       "   'authors': ['Yihong Tang',\n",
       "    'Bo Wang',\n",
       "    'Miao Fang',\n",
       "    'Dongming Zhao',\n",
       "    'Kun Huang',\n",
       "    'Ruifang He',\n",
       "    'Yuexian Hou'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-7_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1090',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge augmented',\n",
       "    'applications',\n",
       "    'conversational modeling'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.299.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76529/poster_document/60af9314e49f8634a72d11b2716ddfc4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76529/poster/1828da69db1c2fabf99b6fa90ace2294.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona',\n",
       "   'tldr': 'The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes ar...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76529,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76529-chain-of-skills-a-configurable-model-for-open-domain-question-answering',\n",
       "   'video_url': None},\n",
       "  'P1099': {'abstract': 'Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work -- sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works. Previous studies show that MTL performance can be improved by algorithmic tricks. However, what tasks and skills should be included is less well explored. In this work, we conduct a case study in Financial NLP where multiple datasets exist for skills relevant to the domain, such as numeric reasoning and sentiment analysis. Due to the task difficulty and data scarcity in the Financial NLP domain, we explore when aggregating such diverse skills from multiple datasets with MTL can work. Our findings suggest that the key to MTL success lies in skill diversity, relatedness between tasks, and choice of aggregation size and shared capacity. Specifically, MTL works well when tasks are diverse but related, and when the size of the task aggregation and the shared capacity of the model are balanced to avoid overwhelming certain tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.412',\n",
       "   'authors': ['Jingwei Ni',\n",
       "    'Zhijing Jin',\n",
       "    'QIAN WANG',\n",
       "    'Mrinmaya Sachan',\n",
       "    'Markus Leippold'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-4_-nlp-applications-(poster)'],\n",
       "   'id': 'P1099',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['financial/business nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.412.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76530/poster_document/8d2aaca8475009ba3e7bdb594492aa24.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76530/poster/c0cb23e4a0c98ea61301b99c8573e3f3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76530/slideshow/89b1375850502a5fea8261738f856ab0.pptx',\n",
       "   'title': 'When Does Aggregating Multiple Skills with Multi-Task Learning Work? A Case Study in Financial NLP',\n",
       "   'tldr': 'Multi-task learning (MTL) aims at achieving a better model by leveraging data and knowledge from multiple tasks. However, MTL does not always work -- sometimes negative transfer occurs between tasks, especially when aggregating loosely related skills, leaving it an open question when MTL works. Prev...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76530,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76530-when-does-aggregating-multiple-skills-with-multi-task-learning-workquestion-a-case-study-in-financial-nlp',\n",
       "   'video_url': None},\n",
       "  'P110': {'abstract': 'Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e. \"supervised pre-training\") showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model\\'s capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3\\\\% and Flan-T5 by 5.8\\\\%.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.558',\n",
       "   'authors': ['Tianyi Tang', 'Junyi Li', 'Wayne Xin Zhao', 'Ji-Rong Wen'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-7_-generation-(virtual-poster)'],\n",
       "   'id': 'P110',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation', 'model architectures'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.558.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77265/poster_document/8f8cff628a409b202c7ea9163c3b1ec9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77265/poster/f1e604f45bac1df589b05c9a133e0dad.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77265/slideshow/846c31c843142d32746e7deee54b0d7b.pdf',\n",
       "   'title': 'MVP: Multi-task Supervised Pre-training for Natural Language Generation',\n",
       "   'tldr': 'Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled ...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77265,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77265-mvp-multi-task-supervised-pre-training-for-natural-language-generation',\n",
       "   'video_url': None},\n",
       "  'P1101': {'abstract': \"Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix all the semantic information together in fixed-length vectors, which are difficult to recover and lack explainability. This paper presents a token-level matching inference algorithm, which can be applied on top of any language model to improve its performance on STS task. Our method calculates pairwise token-level similarity and token matching scores, and then aggregates them with pretrained token weights to produce sentence similarity. Experimental results on seven STS datasets show that our method improves the performance of almost all language models, with up to 12.7\\\\% gain in Spearman's correlation. We also demonstrate that our method is highly explainable and computationally efficient.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.49',\n",
       "   'authors': ['Hongwei Wang', 'Dong Yu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P1101',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['semantic textual similarity'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.49.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76531/poster_document/5278624e0593c6f540e5da4a79b827f1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76531/poster/8804c192e0f90b338275c052951f43d4.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Going Beyond Sentence Embeddings: A Token-Level Matching Algorithm for Calculating Semantic Textual Similarity',\n",
       "   'tldr': 'Semantic Textual Similarity (STS) measures the degree to which the underlying semantics of paired sentences are equivalent. State-of-the-art methods for STS task use language models to encode sentences into embeddings. However, these embeddings are limited in representing semantics because they mix ...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76531,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76531-going-beyond-sentence-embeddings-a-token-level-matching-algorithm-for-calculating-semantic-textual-similarity',\n",
       "   'video_url': None},\n",
       "  'P1102': {'abstract': 'Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generation, which takes advantage of the few-shot learning ability and generation capacity of large language models (LLM). Specifically, we formulate the conversation generation problem as a language modeling task, then finetune an LLM with a few human conversations to capture the characteristics of the information-seeking process and use it for generating synthetic conversations with high quality. Experimental results on two frequently-used datasets verify that AutoConv has substantial improvements over strong baselines and alleviates the dependence on human annotation. In addition, we also provide several analysis studies to promote future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.149',\n",
       "   'authors': ['Siheng Li',\n",
       "    'Cheng Yang',\n",
       "    'Yichun Yin',\n",
       "    'Xinyu Zhu',\n",
       "    'Zesen Cheng',\n",
       "    'Lifeng Shang',\n",
       "    'Xin Jiang',\n",
       "    'Qun Liu',\n",
       "    'Yujiu Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1102',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge augmented', 'grounded dialog'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.149.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76532/poster_document/f5d022295fda6c538a8b64112fcc1ed8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76532/poster/aef5a420ae89c7907c30b66dc77fe0f7.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76532/slideshow/909e43be4118b1411e72efa203e0d126.pdf',\n",
       "   'title': 'AutoConv: Automatically Generating Information-seeking Conversations with Large Language Models',\n",
       "   'tldr': 'Information-seeking conversation, which aims to help users gather information through conversation, has achieved great progress in recent years. However, the research is still stymied by the scarcity of training data. To alleviate this problem, we propose AutoConv for synthetic conversation generati...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76532,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76532-autoconv-automatically-generating-information-seeking-conversations-with-large-language-models',\n",
       "   'video_url': None},\n",
       "  'P1126': {'abstract': \"Automatic machine translation (MT) metrics are widely used to distinguish the \\nquality of  machine translation systems across relatively large test sets (system-level evaluation). \\nHowever, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the segment-level quality by correlating metrics with how useful the translations are for downstream task.\\nWe evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model and a translation model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the machine translated test sentences. Our experiments demonstrate that all metrics exhibit negligible correlation with the extrinsic evaluation of the downstream outcomes. We also find that the scores provided by neural metrics are not interpretable, in large part due to having undefined ranges. We synthesise our analysis into recommendations for future MT metrics to produce labels rather than scores for more informative interaction between machine translation and multilingual language understanding.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.730',\n",
       "   'authors': ['Nikita Moghe',\n",
       "    'Tom Sherborne',\n",
       "    'Mark Steedman',\n",
       "    'Alexandra Birch'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-2_-machine-translation-(oral)'],\n",
       "   'id': 'P1126',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.730.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76219/poster_document/6c2921f79c8c2c97b4ed3f8eca06b715.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76219/poster/5e3efd3e68ef3668190f8ca0e73ba210.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76219/slideshow/6eaba39496f646e75e348be69a299086.pdf',\n",
       "   'title': 'Extrinsic Evaluation of Machine Translation Metrics',\n",
       "   'tldr': 'Automatic machine translation (MT) metrics are widely used to distinguish the \\nquality of  machine translation systems across relatively large test sets (system-level evaluation). \\nHowever, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at t...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76219,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15206/lecture/76219-extrinsic-evaluation-of-machine-translation-metrics',\n",
       "   'video_url': None},\n",
       "  'P1132': {'abstract': 'Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying argumentative claims in need of specific revisions. By learning from collaborative editing behaviors in online debates, we seek to capture implicit revision patterns in order to develop approaches aimed at guiding writers in how to further improve their arguments. We systematically compare the ability of common word embedding models to capture the differences between different versions of the same text, and we analyze their impact on various types of writing issues. To deal with the noisy nature of revision-based corpora, we propose a new sampling strategy based on revision distance. Opposed to approaches from prior work, such sampling can be done without employing additional annotations and judgments. Moreover, we provide evidence that using contextual information and domain knowledge can further improve prediction results. How useful a certain type of context is, depends on the issue the claim is suffering from, though.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.880',\n",
       "   'authors': ['Gabriella Skitalinskaya', 'Henning Wachsmuth'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['poster-session-6_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)'],\n",
       "   'id': 'P1132',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument quality assessment'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.880.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76533/poster_document/06819c881f4dbc530b49a5a2cfcfb42a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76533/poster/cabf2929a63021d9c428695f4e960f1f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'To Revise or Not to Revise: Learning to Detect Improvable Claims for Argumentative Writing Support',\n",
       "   'tldr': 'Optimizing the phrasing of argumentative text is crucial in higher education and professional development. However, assessing whether and how the different claims in a text should be revised is a hard task, especially for novice writers. In this work, we explore the main challenges to identifying ar...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76533,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76533-to-revise-or-not-to-revise-learning-to-detect-improvable-claims-for-argumentative-writing-support',\n",
       "   'video_url': None},\n",
       "  'P1133': {'abstract': 'Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to poor performance in new domains. In this work, we propose DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain. To pre-train DIONYSUS, we create two pseudo summaries for each dialogue example: one from a fine-tuned summarization model and the other from important dialogue turns. We then choose one of these pseudo summaries based on information distribution differences in different types of dialogues. This selected pseudo summary serves as the objective for pre-training DIONYSUS using a self-supervised approach on a large dialogue corpus. Our experiments show that DIONYSUS outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.76',\n",
       "   'authors': ['Yu Li',\n",
       "    'Baolin Peng',\n",
       "    'Pengcheng He',\n",
       "    'Michel Galley',\n",
       "    'Zhou Yu',\n",
       "    'Jianfeng Gao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-1_-summarization-(poster)'],\n",
       "   'id': 'P1133',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational summarization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.76.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76534/poster_document/92ce69313e530dee8a2a7787e8d3cea6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76534/poster/bb97ad47deafe31a34cec60be9d66628.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization',\n",
       "   'tldr': 'Dialogue summarization has recently garnered significant attention due to its wide range of applications. However, existing methods for summarizing dialogues have limitations because they do not take into account the inherent structure of dialogue and rely heavily on labeled data, which can lead to ...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76534,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76534-dionysus-a-pre-trained-model-for-low-resource-dialogue-summarization',\n",
       "   'video_url': None},\n",
       "  'P1139': {'abstract': 'Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on the cases with complex visual spatial relations. Meanwhile, our method can produce more spatially-diversified generation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.442',\n",
       "   'authors': ['Yu Zhao',\n",
       "    'Hao Fei',\n",
       "    'Wei Ji',\n",
       "    'Jianguo Wei',\n",
       "    'Meishan Zhang',\n",
       "    'Min Zhang',\n",
       "    'Tat-Seng Chua'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['poster-session-3_-language-grounding-to-vision,-robotics,-and-beyond-(poster)'],\n",
       "   'id': 'P1139',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal content generation', 'cross-modal application'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.442.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76535/poster_document/46c87e6826efad0d608c56412aa1accb.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76535/poster/e9ba887d8931311ee61451e062999961.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generating Visual Spatial Description via Holistic 3D Scene Understanding',\n",
       "   'tldr': 'Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this w...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76535,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76535-a-causal-framework-to-quantify-the-robustness-of-mathematical-reasoning-with-language-models',\n",
       "   'video_url': None},\n",
       "  'P1142': {'abstract': 'Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit semantic grounding in MLM and propose Descriptive Masked Language Modeling (DMLM), a knowledge-enhanced reading comprehension objective, where the model is required to predict the most likely word in a context, being provided with the word\\'s definition. For instance, given the sentence \"I was going to the \\\\_\\'\\', if we provided as definition \"financial institution\\'\\', the model would have to predict the word \"bank\\'\\'; if, instead, we provided \"sandy seashore\\'\\', the model should predict \"beach\\'\\'. Our evaluation highlights the effectiveness of DMLM in comparison with standard MLM, showing improvements on a number of well-established NLU benchmarks, as well as other semantics-focused tasks, e.g., Semantic Role Labeling. Furthermore, we demonstrate how it is possible to take full advantage of DMLM to embed explicit semantics in downstream tasks, explore several properties of DMLM-based contextual representations and suggest a number of future directions to investigate.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.808',\n",
       "   'authors': ['Edoardo Barba', 'Niccol Campolungo', 'Roberto Navigli'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-7_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1142',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['polysemy', 'word embeddings'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.808.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77420/poster_document/e8c5693df2d730de094fe1a2dce532d3.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77420/slideshow/4f4c7f120fd743abaa0fc952e81cc7d5.pdf',\n",
       "   'title': 'DMLM: Descriptive Masked Language Modeling',\n",
       "   'tldr': 'Over the last few years, Masked Language Modeling (MLM) pre-training has resulted in remarkable advancements in many Natural Language Understanding (NLU) tasks, which sparked an interest in researching alternatives and extensions to the MLM objective. In this paper, we tackle the absence of explicit...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77420,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77420-dmlm-descriptive-masked-language-modeling',\n",
       "   'video_url': None},\n",
       "  'P1143': {'abstract': 'Adapters have emerged as a parameter-efficient Transformer-based framework for cross-lingual transfer by inserting lightweight language-specific modules (language adapters) and task-specific modules (task adapters) within pretrained multilingual models. Zero-shot transfer is enabled by pairing the language adapter in the target language with an appropriate task adapter in a source language. If our target languages are known apriori, we explore how zero-shot transfer can be further improved within the adapter framework by utilizing unlabeled text during task-specific finetuning. We construct language-specific subspaces using standard linear algebra constructs and selectively project source-language representations into the target language subspace during task-specific finetuning using two schemes. Our experiments on three cross-lingual tasks, Named Entity Recognition (NER), Question Answering (QA) and Natural Language Inference (NLI) yield consistent benefits compared to adapter baselines over a wide variety of target languages with up to 11\\\\% relative improvement in NER, 2\\\\% relative improvement in QA and 5\\\\% relative improvement in NLI.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.39',\n",
       "   'authors': ['Ujan Deb', 'Ridayesh Ramesh Parab', 'Preethi Jyothi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P1143',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.39.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76536/poster_document/4daff938fabd9a7ea3d7fcc1a1893e42.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76536/poster/e430349a35c2f26a1adf7d30ffb8a4c8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76536/slideshow/af8d5de2f2df45016b399001c39baeae.pdf',\n",
       "   'title': 'Zero-shot Cross-lingual Transfer With Learned Projections Using Unlabeled Target-Language Data',\n",
       "   'tldr': 'Adapters have emerged as a parameter-efficient Transformer-based framework for cross-lingual transfer by inserting lightweight language-specific modules (language adapters) and task-specific modules (task adapters) within pretrained multilingual models. Zero-shot transfer is enabled by pairing the l...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76536,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76536-zero-shot-cross-lingual-transfer-with-learned-projections-using-unlabeled-target-language-data',\n",
       "   'video_url': None},\n",
       "  'P1148': {'abstract': 'Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that GIFT can significantly improve the performance of three PLMs on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-the-art performance on MPC understanding.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.651',\n",
       "   'authors': ['Jia-Chen Gu',\n",
       "    'Zhenhua Ling',\n",
       "    'Quan Liu',\n",
       "    'Cong Liu',\n",
       "    'Guoping Hu'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-3_-dialogue-and-interactive-systems-(oral)'],\n",
       "   'id': 'P1148',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.651.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76220/poster_document/51cc2afc36a2af097c5072b02e2576f1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76220/poster/b6c2bb1993d4852574834c4ca2b3ee90.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding',\n",
       "   'tldr': 'Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of i...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76220,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15224/lecture/76220-gift-graph-induced-fine-tuning-for-multi-party-conversation-understanding',\n",
       "   'video_url': None},\n",
       "  'P1149': {'abstract': 'Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-specific classifiers or sampling one from relevant discrete samples. However, they cannot effectively model a complex space with diverse attributes, high dimensionality, and asymmetric structure, leaving subsequent controls unsatisfying. In this work, we propose a novel control framework using probability density estimation in the latent space. Our method utilizes an invertible transformation function, the Normalizing Flow, that maps the complex distributions in the latent space to simple Gaussian distributions in the prior space. Thus, we can perform sophisticated and flexible controls in the prior space and feed the control effects back into the latent space owing to the \\nbijection property of invertible transformations. Experiments on single-attribute and multi-attribute control reveal that our method outperforms several strong baselines on attribute relevance and text quality, achieving a new SOTA. Further analysis of control strength adjustment demonstrates the flexibility of our control strategy.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.704',\n",
       "   'authors': ['Yuxuan Gu',\n",
       "    'Xiaocheng Feng',\n",
       "    'Sicheng Ma',\n",
       "    'Lingyuan Lingyuan Zhang',\n",
       "    'Heng Gong',\n",
       "    'Weihong Zhong',\n",
       "    'Bing Qin'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)'],\n",
       "   'id': 'P1149',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.704.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76537/poster_document/0e464dbdafd822c082b4a53f7b9ff00e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76537/poster/feb98f3b14bcbbb5c491abaa762ad4c4.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Controllable Text Generation via Probability Density Estimation in the Latent Space',\n",
       "   'tldr': 'Previous work on controllable text generation has explored the idea of control from the latent space, such as optimizing a representation with attribute-specific classifiers or sampling one from relevant discrete samples. However, they cannot effectively model a complex space with diverse attributes...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76537,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76537-controllable-text-generation-via-probability-density-estimation-in-the-latent-space',\n",
       "   'video_url': None},\n",
       "  'P115': {'abstract': 'Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. \\nNonetheless, in real-world scenarios of UIE applications, new data of different IE tasks and domains usually come in a stream over time. \\nA desirable UIE system should be capable of continually learning new tasks without forgetting old ones, thereby allowing knowledge and functionalities expansion without re-training the whole system. \\nIn this paper, we study the UIE system under a more challenging yet practical scenario, i.e., \"lifelong learning settings, to evaluate its abilities in three aspects, including knowledge sharing and expansion, catastrophic forgetting prevention, and rapid generalization on few-shot and unseen tasks.\\nTo achieve these three goals, we present a novel parameter- and deployment-efficient prompt tuning method namely Lottery Prompt Tuning (LPT).\\nLPT freezes the PLM\\'s parameters and sequentially learns compact pruned prompt vectors for each task leveraging a binary prompt mask, while keeping the prompt parameters selected by the previous tasks insusceptible.\\nFurthermore, we use a simple yet effective method to perform mask selection and show the powerful transferability of Lottery Prompts to novel tasks.\\nExtensive experiments demonstrate that LPT consistently sets state-of-the-art performance on multiple lifelong learning settings of UIE, including task-incremental setting on seen tasks, few-shot adaptation, and zero-shot generalization on novel tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.16',\n",
       "   'authors': ['Zujie Liang',\n",
       "    'feng wei',\n",
       "    'Yin Jie',\n",
       "    'YUXI QIAN',\n",
       "    'Zhenghong Hao',\n",
       "    'Bing Han'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['poster-session-4_-information-extraction-(poster)'],\n",
       "   'id': 'P115',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.16.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76371/poster_document/3ebfdd5049957a2e430686b590762259.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76371/poster/2e4bdf7e3a00e07b5dc1663979ed20f7.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76371/slideshow/031be86f1bfe6c321778ecb88c740c29.pdf',\n",
       "   'title': 'Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning',\n",
       "   'tldr': 'Thanks to the recent success of Pre-trained Language Models (PLMs), it has become a promising research direction to develop a universal model (UIE) that can solve all typical information extraction tasks within one generative framework. \\nNonetheless, in real-world scenarios of UIE applications, new ...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76371,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76371-is-continuous-prompt-a-combination-of-discrete-promptsquestion-towards-a-novel-view-for-interpreting-continuous-prompts',\n",
       "   'video_url': None},\n",
       "  'P1150': {'abstract': 'Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effective preventive strategies to guarantee their proper functioning. In this paper, we address the problem of hallucination detection in NMT by following a simple intuition: as hallucinations are detached from the source content, they exhibit encoder-decoder attention patterns that are statistically different from those of good quality translations. We frame this problem with an optimal transport formulation and propose a fully unsupervised, plug-in detector that can be used with any attention-based NMT model. Experimental results show that our detector not only outperforms all previous model-based detectors, but is also competitive with detectors that employ external models trained on millions of samples for related tasks such as quality estimation and cross-lingual sentence similarity.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.770',\n",
       "   'authors': ['Nuno M. Guerreiro',\n",
       "    'Pierre Colombo',\n",
       "    'Pablo Piantanida',\n",
       "    'Andr Martins'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-6_-machine-translation-(poster)'],\n",
       "   'id': 'P1150',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation', 'mt deployment and maintainence'],\n",
       "   'languages': ['german'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.770.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76538/poster_document/5a88d1eca6925ba956733bde170ee89f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76538/poster/35e1da48be1cc3b6e0abafa4783c7cf1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Optimal Transport for Unsupervised Hallucination Detection in Neural Machine Translation',\n",
       "   'tldr': 'Neural machine translation (NMT) has become the de-facto standard in real-world machine translation applications. However, NMT models can unpredictably produce severely pathological translations, known as hallucinations, that seriously undermine user trust. It becomes thus crucial to implement effec...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76538,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76538-optimal-transport-for-unsupervised-hallucination-detection-in-neural-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1154': {'abstract': \"Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a data augmentation framework using knowledge distillation. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to augment training data, which is then used for the next step. We incorporate this framework into the state-of-the-art  language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in sequence-generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.823',\n",
       "   'authors': ['Hung Quoc To',\n",
       "    'Nghi D. Q. Bui',\n",
       "    'Jin L.C. Guo',\n",
       "    'Tien N Nguyen'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1154',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code generation and understanding'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.823.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77421/poster_document/5150c4a9928ac4789de14deb7a705d6e.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77421/slideshow/870a81a1ef30c1118779e2b2a96fef1d.pdf',\n",
       "   'title': 'Better Language Models of Code through Self-Improvement',\n",
       "   'tldr': 'Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve thi...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77421,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77421-better-language-models-of-code-through-self-improvement',\n",
       "   'video_url': None},\n",
       "  'P116': {'abstract': 'Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristic of both methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map OoD examples outside the ID data manifold with the regularization inherited from pre-training. Besides, the student model sees only ID examples during parameter learning, further promoting more distinguishable features for OoD detection. We conduct extensive experiments over multiple benchmark datasets, i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that the proposed method yields new state-of-the-art performance. We also explore its application as an AIGC detector to distinguish answers generated by ChatGPT and human experts. It is observed that our model exceeds human evaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.403',\n",
       "   'authors': ['Qianhui Wu',\n",
       "    'Huiqiang Jiang',\n",
       "    'Haonan Yin',\n",
       "    'Brje F. Karlsson',\n",
       "    'Chin-Yew Lin'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)'],\n",
       "   'id': 'P116',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['natural language inference'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.403.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76372/poster_document/8139ebfb1679279da7a3e3a378b87bd7.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76372/slideshow/3a55ac923e951ddb0fa248456b61b248.pdf',\n",
       "   'title': 'Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text',\n",
       "   'tldr': 'Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and the...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76372,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76372-multi-level-knowledge-distillation-for-out-of-distribution-detection-in-text',\n",
       "   'video_url': None},\n",
       "  'P1160': {'abstract': 'There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the KGs given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval (DiFaR). Specifically, we first embed all facts in KGs onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our DiFaR framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.558',\n",
       "   'authors': ['Jinheon Baek',\n",
       "    'Alham Fikri Aji',\n",
       "    'Jens Lehmann',\n",
       "    'Sung Ju Hwang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['poster-session-1_-information-retrieval-and-text-mining-(poster)'],\n",
       "   'id': 'P1160',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['dense retrieval', 're-ranking'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.558.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76539/poster_document/7ebf174ce88d0f8ba0e19553cd24d71b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76539/poster/0eb9a278e6d00373503b4b39cf0f3c18.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Direct Fact Retrieval from Knowledge Graphs without Entity Linking',\n",
       "   'tldr': 'There has been a surge of interest in utilizing Knowledge Graphs (KGs) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in KGs usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, th...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 76539,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76539-end-to-end-task-oriented-dialogue-systems-based-on-schema',\n",
       "   'video_url': None},\n",
       "  'P1162': {'abstract': 'Automatic summarization generates concise summaries that contain key ideas of source documents.\\nAs the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the \"Lasswell Communication Model proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMs\\' zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.482',\n",
       "   'authors': ['Yiming Wang', 'Zhuosheng Zhang', 'Rui Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-7_-summarization-(poster)'],\n",
       "   'id': 'P1162',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.482.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76540/poster_document/e5972694a58456a0a4ae80dc37f061b7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76540/poster/2da71110609637e818906495ab1927d2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76540/slideshow/6911656b256a7988fd003146c42ee3a8.pdf',\n",
       "   'title': 'Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method',\n",
       "   'tldr': 'Automatic summarization generates concise summaries that contain key ideas of source documents.\\nAs the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be no...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76540,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76540-element-aware-summarization-with-large-language-models-expert-aligned-evaluation-and-chain-of-thought-method',\n",
       "   'video_url': None},\n",
       "  'P1166': {'abstract': 'Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a category hierarchy also have some latent relevancy among labels in the same level of the hierarchy. We refer to these labels as peer labels, from which the peer effects are originally utilized in our work to improve the classification performance. To fully explore the peer-label relationship, we develop a PeerHTC method. This method innovatively measures the latent relevancy of peer labels through several metrics and then encodes the relevancy with a Graph Convolutional Neural Network. We also propose a sample importance learning method to ameliorate the side effects raised by modelling the peer label relevancy. Our experiments on several standard datasets demonstrate the evidence of peer labels and the superiority of PeerHTC over other state-of-the-art HTC methods in terms of classification accuracy.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.207',\n",
       "   'authors': ['Junru Song', 'Feifei Wang', 'Yang Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1166',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge-augmented methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.207.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76541/poster_document/814e847028cf4df34fcb02748fd6dcec.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76541/poster/158dba318ae35b48584cfac24ed6413c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76541/slideshow/95b23756538d1e7db7acd50dfef7ada9.pdf',\n",
       "   'title': 'Peer-Label Assisted Hierarchical Text Classification',\n",
       "   'tldr': 'Hierarchical text classification (HTC) is a challenging task, in which the labels of texts can be organized into a category hierarchy. To deal with the HTC problem, many existing works focus on utilizing the parent-child relationships that are explicitly shown in the hierarchy. However, texts with a...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76541,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76541-peer-label-assisted-hierarchical-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1168': {'abstract': 'Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide future designs of sentence representation learning methods by taking a closer look at contrastive SRL through the lens of isotropy, contextualization and learning dynamics. We interpret its successes through the geometry of the representation shifts and show that contrastive learning brings isotropy, and drives high intra-sentence similarity: when in the same sentence, tokens converge to similar positions in the semantic space. We also find that what we formalize as \"spurious contextualization\" is mitigated for semantically meaningful tokens, while augmented for functional ones. We find that the embedding space is directed towards the origin during training, with more areas now better defined. We ablate these findings by observing the learning dynamics with different training temperatures, batch sizes and pooling methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.778',\n",
       "   'authors': ['Chenghao Xiao', 'Yang Long', 'Noura Al Moubayed'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1168',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['phrase/sentence embedding'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.778.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77423/poster_document/f6bb8ffec07c33905e87e4e94f35d4dc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77423/poster/669860ea77b6569293703d7039f32cc8.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77423/slideshow/7ead86acd03db3d2258da8177c0967ce.pdf',\n",
       "   'title': 'On Isotropy, Contextualization and Learning Dynamics of Contrastive-based Sentence Representation Learning',\n",
       "   'tldr': 'Incorporating contrastive learning objectives in sentence representation learning (SRL) has yielded significant improvements on many sentence-level NLP tasks. However, it is not well understood why contrastive learning works for learning sentence-level semantics. In this paper, we aim to help guide ...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 77423,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77423-on-isotropy-contextualization-and-learning-dynamics-of-contrastive-based-sentence-representation-learning',\n",
       "   'video_url': None},\n",
       "  'P1170': {'abstract': \"Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their versatility for tasks of interest for Classical languages: we explore (i) encoder-only and encoder-decoder architectures using RoBERTa and T5 as strong model types, and create for each of them (ii) a monolingual Ancient Greek and a multilingual instance that includes Latin and English. We evaluate all models on morphological and syntactic tasks, including lemmatization, which demonstrates the added value of T5's decoding abilities. We further define two probing tasks to investigate the knowledge acquired by models pre-trained on Classical texts. Our experiments provide the first benchmarking analysis of existing models of Ancient Greek. Results show that our models provide significant improvements over the SoTA. The systematic analysis of model types can inform future research in designing language models for Classical languages, including the development of novel generative tasks. We make all our models available as community resources, along with a large curated pre-training corpus for Ancient Greek, to support the creation of a larger, comparable model zoo for Classical Philology.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.846',\n",
       "   'authors': ['Frederick Riemenschneider', 'Anette Frank'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-2_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1170',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['corpus creation',\n",
       "    'benchmarking',\n",
       "    'automatic creation and evaluation of language resources',\n",
       "    'datasets for low resource languages'],\n",
       "   'languages': ['ancient greek', 'latin'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.846.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76542/poster_document/d1c9f649a10311b8ff64b4a3104bcd68.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76542/poster/15b7546ecc4c7e57b3b2e60c778cdf3c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76542/slideshow/c59f6349bdb13530ef84d59bfcf558b5.pdf',\n",
       "   'title': 'Exploring Large Language Models for Classical Philology',\n",
       "   'tldr': 'Recent advances in NLP have led to the creation of powerful language models for many languages including Ancient Greek and Latin. While prior work on Classical languages unanimously uses BERT, in this work we create four language models for Ancient Greek that vary along two dimensions to study their...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76542,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76542-exploring-large-language-models-for-classical-philology',\n",
       "   'video_url': None},\n",
       "  'P1173': {'abstract': 'With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to stimulate models for correct answers. However, we argue that input-side adaptation could be arduous due to the lack of gradient signals and they usually require thousands of API queries, resulting in high computation and time costs. Specifically, DecT first extracts prompt-stimulated output scores for initial predictions. On top of that, we train an additional decoder network on the output representations to incorporate posterior data knowledge. By gradient-based optimization, DecT can be trained within several seconds and requires only one PTM query per sample. Empirically, we conduct extensive natural language understanding experiments and show that DecT significantly outperforms state-of-the-art algorithms with a 200x speed-up. Our code is available at https://github.com/thunlp/DecT.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.840',\n",
       "   'authors': ['Ganqu CUI',\n",
       "    'Wentao Li',\n",
       "    'Ning Ding',\n",
       "    'Longtao Huang',\n",
       "    'Zhiyuan Liu',\n",
       "    'Maosong Sun'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1173',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['parameter-efficient finetuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.840.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76543/poster_document/bd8e6811643343ec6cc6b02acb08b10c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76543/poster/d17e60e1009e4fe74bb9e21ff082790a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76543/slideshow/ab17c2b4a669eba892e6cb55e5a6c762.pptx',\n",
       "   'title': 'Decoder Tuning: Efficient Language Understanding as Decoding',\n",
       "   'tldr': 'With the evergrowing sizes of pre-trained models (PTMs), it has been an emerging practice to only provide the inference APIs for users, namely model-as-a-service (MaaS) setting. To adapt PTMs with model parameters frozen, most current approaches focus on the input side, seeking powerful prompts to s...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76543,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76543-a-simple-yet-strong-domain-agnostic-de-bias-method-for-zero-shot-sentiment-classification',\n",
       "   'video_url': None},\n",
       "  'P1178': {'abstract': \"Emotion-Cause Pair Extraction (ECPE) aims to identify the document's emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model the multiplex relations between clauses by constructing different edges. However, the data of emotions, causes, and pairs are extremely unbalanced, and current methods get their representation using the same graph structure. In this paper, we propose a **J**oint **C**onstrained Learning framework with **B**oundary-adjusting for Emotion-Cause Pair Extraction (**JCB**). Specifically, through constrained learning, we summarize the prior rules existing in the data and force the model to take them into consideration in optimization, which helps the model learn a better representation from unbalanced data. Furthermore, we adjust the decision boundary of classifiers according to the relations between subtasks, which have always been ignored. No longer working independently as in the previous framework, the classifiers corresponding to three subtasks cooperate under the relation constraints. Experimental results show that **JCB** obtains competitive results compared with state-of-the-art methods and prove its robustness on unbalanced data.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.62',\n",
       "   'authors': ['Huawen Feng',\n",
       "    'Junlong Liu',\n",
       "    'Junhao Zheng',\n",
       "    'Haibin Chen',\n",
       "    'Xichen Shang',\n",
       "    'Qianli Ma'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-1_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P1178',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['document-level extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.62.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76544/poster_document/f5d4e482ce6bee16ea6c82c5ff2e843f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76544/poster/685c81799d1cb5f1fb74233912455fa1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Joint Constrained Learning with Boundary-adjusting for Emotion-Cause Pair Extraction',\n",
       "   'tldr': \"Emotion-Cause Pair Extraction (ECPE) aims to identify the document's emotion clauses and corresponding cause clauses. Like other relation extraction tasks, ECPE is closely associated with the relationship between sentences. Recent methods based on Graph Convolutional Networks focus on how to model t...\",\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76544,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76544-joint-constrained-learning-with-boundary-adjusting-for-emotion-cause-pair-extraction',\n",
       "   'video_url': None},\n",
       "  'P118': {'abstract': 'Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely learn similar features and can be pruned without harming performance. Inspired by the minimum-redundancy feature selection, we assume that focusing on the most representative and distinctive features with minimum resources can mitigate the above issues and lead to more effective and efficient MHAs. In particular, we propose Grouped Head Attention, trained with a self-supervised group constraint that group attention heads, where each group focuses on an essential but distinctive feature subset. We additionally propose a Voting-to-Stay procedure to remove redundant heads, thus achieving a transformer with lighter weights. Extensive experiments are consistent with our hypothesis. Moreover, our method achieves significant performance gains on three well-established tasks while considerably compressing parameters.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.812',\n",
       "   'authors': ['Jinjie Ni',\n",
       "    'Rui Mao',\n",
       "    'Zonglin Yang',\n",
       "    'Han Lei',\n",
       "    'Erik Cambria'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-1_-machine-translation-(poster)'],\n",
       "   'id': 'P118',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['efficient inference for mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.812.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76373/poster_document/a927406ba1c7ced6921f752284c89419.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76373/slideshow/4642b35ea51e32e427a468d5866d7c26.pdf',\n",
       "   'title': 'Finding the Pillars of Strength for Multi-Head Attention',\n",
       "   'tldr': 'Recent studies have revealed some issues of Multi-Head Attention (MHA), e.g., redundancy and over-parameterization. Specifically, the heads of MHA were originally designed to attend to information from different representation subspaces, whereas prior studies found that some attention heads likely l...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76373,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76373-finding-the-pillars-of-strength-for-multi-head-attention',\n",
       "   'video_url': None},\n",
       "  'P1180': {'abstract': \"Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science journalists' work. We analyze the performance of possible existing solutions as baselines for the CSJ task. Based on these findings, we propose to combine the three components - SELECT, SIMPLIFY and REWRITE (SSR) to produce cross-lingual simplified science summaries for non-expert readers. Our empirical evaluation on the Wikipedia dataset shows that SSR significantly outperforms the baselines for the CSJ task and can serve as a strong baseline for future work. We also perform an ablation study investigating the impact of individual components of SSR. Further, we analyze the performance of SSR on a high-quality, real-world CSJ dataset with human evaluation and in-depth analysis, demonstrating the superior performance of SSR for CSJ.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.103',\n",
       "   'authors': ['Mehwish Fatima', 'Michael Strube'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-1_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1180',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': ['german'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.103.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76545/poster_document/37d5bee15cd86d183c3ae147cb446be7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76545/poster/625a7ce79d49fa415737ef4ef722db34.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76545/slideshow/2463808192912794c2d407ff479e1d9b.pdf',\n",
       "   'title': 'Cross-lingual Science Journalism: Select, Simplify and Rewrite Summaries for Non-expert Readers',\n",
       "   'tldr': 'Automating Cross-lingual Science Journalism (CSJ) aims to generate popular science summaries from English scientific texts for non-expert readers in their local language. We introduce CSJ as a downstream task of text simplification and cross-lingual scientific summarization to facilitate science jou...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76545,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76545-cross-lingual-science-journalism-select-simplify-and-rewrite-summaries-for-non-expert-readers',\n",
       "   'video_url': None},\n",
       "  'P1183': {'abstract': 'Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range semantic dependencies in conversation history while maintaining low computational cost. The framework incorporates a context-aware encoder, which employs a dynamic memory decay mechanism and models context at different levels of granularity. We evaluate HSGE on a widely used benchmark dataset for complex sequential question answering. Experimental results demonstrate that it outperforms existing baselines averaged on all question types.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.195',\n",
       "   'authors': ['Hao Sun',\n",
       "    'Yang Li',\n",
       "    'Liwei Deng',\n",
       "    'Bowen Li',\n",
       "    'Binyuan Hui',\n",
       "    'Binhua Li',\n",
       "    'Yunshi Lan',\n",
       "    'Yan Zhang',\n",
       "    'Yongbin Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-5_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1183',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.195.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76546/poster_document/5a57719ec634b605e306053b9ac47844.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76546/poster/05f2bf50fd0ddb1f2bb83985c077bf74.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76546/slideshow/38875b9f7b8734e39784bc00b915de53.pdf',\n",
       "   'title': 'History Semantic Graph Enhanced Conversational KBQA with Temporal Information Modeling',\n",
       "   'tldr': 'Context information modeling is an important task in conversational KBQA. However, existing methods usually assume the independence of utterances and model them in isolation. In this paper, we propose a History Semantic Graph Enhanced KBQA model (HSGE) that is able to effectively model long-range se...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76546,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/76546-history-semantic-graph-enhanced-conversational-kbqa-with-temporal-information-modeling',\n",
       "   'video_url': None},\n",
       "  'P1185': {'abstract': 'Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhance the robustness of evaluation metrics, which in turn leads to an improvement in the performance of machine translation systems. Codes are available at {https://github.com/powerpuffpomelo/fairseq\\\\_mrt}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.297',\n",
       "   'authors': ['Yiming Yan',\n",
       "    'Tao Wang',\n",
       "    'Chengqi Zhao',\n",
       "    'Shujian Huang',\n",
       "    'Jiajun CHEN',\n",
       "    'Mingxuan Wang'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-2_-machine-translation-(oral)'],\n",
       "   'id': 'P1185',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.297.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76221/poster_document/aefee2112a7c86744d85408dad5c93b2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76221/poster/e4210989716dbf3196ef7dd77d280802.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training',\n",
       "   'tldr': 'Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher corr...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76221,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15206/lecture/76221-bleurt-has-universal-translations-an-analysis-of-automatic-metrics-by-minimum-risk-training',\n",
       "   'video_url': None},\n",
       "  'P1190': {'abstract': 'In this paper, we aim to adapt the idea of retrieval-based neural approaches \\nto the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicting the sentiment polarity and it is usually aspect-dependent. Semantic similar neighbors with different polarities will be infeasible even counterproductive. To tackle this issue, we propose a  retrieval-based neural ASTE approach, named RLI (Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation), which exploits the label information of neighbors. Given an aspect-opinion term pair, we retrieve semantic similar triplets from the training corpus and interpolate their label information into the augmented representation of the target pair. The retriever is jointly trained with the whole ASTE framework, and neighbors with both similar semantics and sentiments can be recalled with the aid of this distant supervision. In addition, we design a simple yet effective pre-train method for the retriever that implicitly encodes the label similarities. Extensive experiments and analysis on two widely-used benchmarks show that the proposed model establishes a new state-of-the-art on ASTE.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.303',\n",
       "   'authors': ['Guoxin Yu',\n",
       "    'Lemao Liu',\n",
       "    'Haiyun Jiang',\n",
       "    'Shuming Shi',\n",
       "    'Xiang Ao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1190',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument mining'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.303.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77425/poster_document/5269d25a78796e1773162e7a71291efd.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77425/slideshow/232ffb460c6c68d071825b2aa4fa02ee.pdf',\n",
       "   'title': 'Making Better Use of Training Corpus: Retrieval-based Aspect Sentiment Triplet Extraction via Label Interpolation',\n",
       "   'tldr': 'In this paper, we aim to adapt the idea of retrieval-based neural approaches \\nto the Aspect Sentiment Triplet Extraction (ASTE) task. Different from previous studies retrieving semantic similar neighbors, the ASTE task has its specialized challenges when adapting, i.e., the purpose includes predicti...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77425,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77425-making-better-use-of-training-corpus-retrieval-based-aspect-sentiment-triplet-extraction-via-label-interpolation',\n",
       "   'video_url': None},\n",
       "  'P1191': {'abstract': \"Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years.\\n    Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web applications such as email thread summarization and meeting minutes draft. Dialogue summarization is a challenging task in that dialogue has dynamic interaction nature and presumably inconsistent information flow among various speakers. Many researchers address this task by modeling dialogue with pre-computed static graph structure using external linguistic toolkits. \\n    However, such methods heavily depend on the reliability of external tools and the static graph construction is disjoint with the graph representation learning phase, which makes the graph can't be dynamically adapted for the downstream summarization task. \\n    In this paper, we propose a Static-Dynamic graph-based Dialogue Summarization model (SDDS), which fuses prior knowledge from human expertise and adaptively learns the graph structure in an end-to-end learning fashion.\\n    To verify the effectiveness of SDDS, we conduct experiments on three benchmark datasets (SAMSum, MediaSum, and DialogSum) and the results verify the superiority of SDDS.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.775',\n",
       "   'authors': ['Shen Gao',\n",
       "    'Xin Cheng',\n",
       "    'Mingzhe Li',\n",
       "    'Xiuying Chen',\n",
       "    'Jinpeng Li',\n",
       "    'Dongyan Zhao',\n",
       "    'Rui Yan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['session-4_-summarization-(virtual-poster)'],\n",
       "   'id': 'P1191',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational summarization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.775.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76547/poster_document/b04184bb96f5560f6510e536d742ba7b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76547/poster/367232d883b5f6225dcc73481d976526.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Dialogue Summarization with Static-Dynamic Structure Fusion Graph',\n",
       "   'tldr': 'Dialogue, the most fundamental and specially privileged arena of language, gains increasing ubiquity across the Web in recent years.\\n    Quickly going through the long dialogue context and capturing salient information scattered over the whole dialogue session benefit users in many real-world Web ap...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76547,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76547-dialogue-summarization-with-static-dynamic-structure-fusion-graph',\n",
       "   'video_url': None},\n",
       "  'P1195': {'abstract': \"Adversarial training is one of the best-performing methods in improving the robustness of deep language models. \\nHowever, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training.\\nTo address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70\\\\% compared to current best-performing adversarial training methods.\\nExperiments demonstrate that DSRM considerably improves BERT's resistance to textual adversarial attacks and achieves state-of-the-art robust accuracy on various benchmarks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.680',\n",
       "   'authors': ['SongYang Gao',\n",
       "    'Shihan Dou',\n",
       "    'Yan Liu',\n",
       "    'Xiao Wang',\n",
       "    'Qi Zhang',\n",
       "    'Zhongyu Wei',\n",
       "    'Jin Ma',\n",
       "    'Ying Shan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1195',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['adversarial attacks/examples/training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.680.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76548/poster_document/dbcec40311989bf603ed36746298863a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76548/poster/38153d2bb8c6a9d57c32014bc88eab14.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization',\n",
       "   'tldr': 'Adversarial training is one of the best-performing methods in improving the robustness of deep language models. \\nHowever, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these gener...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76548,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76548-dsrm-boost-textual-adversarial-training-with-distribution-shift-risk-minimization',\n",
       "   'video_url': None},\n",
       "  'P1200': {'abstract': 'In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene hallucination mechanism that dynamically generates pseudo visual SG from the given textual SG. Several SG-pivoting based learning objectives are introduced for unsupervised translation training. On the benchmark Multi30K data, our SG-based method outperforms the best-performing baseline by significant BLEU scores on the task and setup, helping yield translations with better completeness, relevance and fluency without relying on paired images. Further in-depth analyses reveal how our model advances in the task setting.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.329',\n",
       "   'authors': ['Hao Fei',\n",
       "    'Qian Liu',\n",
       "    'Meishan Zhang',\n",
       "    'Min Zhang',\n",
       "    'Tat-Seng Chua'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-7_-machine-translation-(poster)'],\n",
       "   'id': 'P1200',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot/zero-shot mt', 'multilingual mt', 'multimodality'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.329.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76549/poster_document/0374197fb92059f4b7ae53d4c4d2236f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76549/poster/df2a636eb6df003617eadf1f578b7128.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination',\n",
       "   'tldr': 'In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76549,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76549-scene-graph-as-pivoting-inference-time-image-free-unsupervised-multimodal-machine-translation-with-visual-scene-hallucination',\n",
       "   'video_url': None},\n",
       "  'P1205': {'abstract': 'We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. \\nOur benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, named entity recognition, sentiment analysis, question answering, etc.) and machine learning tasks (sequence labeling, document-level classification, and regression). We run the first systematic evaluation of pre-trained language models for Bulgarian, comparing and contrasting results across the nine tasks in the benchmark. The evaluation results show strong performance on sequence labeling tasks, but there is a lot of room for improvement for tasks that require more complex reasoning. We make bgGLUE publicly available together with the fine-tuning and the evaluation code, as well as a public leaderboard at https://bgglue.github.io, and we hope that it will enable further advancements in developing NLU models for Bulgarian.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.487',\n",
       "   'authors': ['Momchil Hardalov',\n",
       "    'Pepa Atanasova',\n",
       "    'Todor Mihaylov',\n",
       "    'Galia Angelova',\n",
       "    'Kiril Simov',\n",
       "    'Petya Osenova',\n",
       "    'Veselin Stoyanov',\n",
       "    'Ivan K. Koychev',\n",
       "    'Preslav Nakov',\n",
       "    'Dragomir Radev'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-6_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1205',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['benchmarking',\n",
       "    'language resources',\n",
       "    'multilingual corpora',\n",
       "    'nlp datasets'],\n",
       "   'languages': ['bulgarian'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.487.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76550/poster_document/20d819a862869da626f2266f559f5725.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76550/poster/7bfc1b19b1cc2cdc14784c871b3b1ee2.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76550/slideshow/2ae532455ab8d4bec5241b6c4b4dd157.pdf',\n",
       "   'title': 'bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark',\n",
       "   'tldr': 'We present bgGLUE (Bulgarian General Language Understanding Evaluation), a benchmark for evaluating language models on Natural Language Understanding (NLU) tasks in Bulgarian. \\nOur benchmark includes NLU tasks targeting a variety of NLP problems (e.g., natural language inference, fact-checking, name...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76550,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76550-pairspanbert-an-enhanced-language-model-for-bridging-resolution',\n",
       "   'video_url': None},\n",
       "  'P1206': {'abstract': 'Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does not know.This paper investigates \"overthinking\" in the open-world scenario and its impact on OOD intent classification. Inspired by this, we propose a two-birds-one-stone method, which allows the model to decide whether to make a decision on OOD classification early during inference and can ensure accuracy and accelerate inference. At the same time, to adapt to the behavior of dynamic inference, we also propose a training method based on ensemble methods. In addition to bringing certain theoretical insights, we also conduct detailed experiments on three real-world intent datasets. Compared with the previous baselines, our method can not only improve inference speed, but also achieve significant performance improvements.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.595',\n",
       "   'authors': ['Yunhua Zhou', 'Jianqiang Yang', 'Pengyu Wang', 'Xipeng Qiu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-7_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1206',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented', 'factuality', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.595.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76551/poster_document/02a27c51d335ef05a24dbc39063d978a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76551/poster/344b8e52f614e55229b4a4fa5de0c453.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76551/slideshow/e6789c36b81dde0866999835af55d4df.pptx',\n",
       "   'title': 'Two Birds One Stone: Dynamic Ensemble for OOD Intent Classification',\n",
       "   'tldr': 'Out-of-domain (OOD) intent classification is an active field of natural language understanding, which is of great practical significance for intelligent devices such as the Task-Oriented Dialogue System. It mainly contains two challenges: it requires the model to know what it knows and what it does ...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76551,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76551-two-birds-one-stone-dynamic-ensemble-for-ood-intent-classification',\n",
       "   'video_url': None},\n",
       "  'P1207': {'abstract': \"Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90\\\\% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs' capability to learn to reason in context.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.153',\n",
       "   'authors': ['Boshi Wang',\n",
       "    'Sewon Min',\n",
       "    'Xiang Deng',\n",
       "    'Jiaming Shen',\n",
       "    'You Wu',\n",
       "    'Luke Zettlemoyer',\n",
       "    'Huan Sun'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-6_-large-language-models-(poster)'],\n",
       "   'id': 'P1207',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.153.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76552/poster_document/aaca91a6e6f6f6807017a0a849cb7f79.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76552/poster/8e975b6f257161e030bc1d78a8135dd8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters',\n",
       "   'tldr': 'Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its succe...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76552,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76552-towards-understanding-chain-of-thought-prompting-an-empirical-study-of-what-matters',\n",
       "   'video_url': None},\n",
       "  'P1212': {'abstract': 'Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proactively lead the conversation based on some key topics of the news. In addition, both information-seeking and chit-chat scenarios are included realistically, where the user may ask a series of questions about the news details or express their opinions and be eager to chat. To further develop this novel task, we collect a human-to-human Chinese dialogue dataset NewsDialogues, which includes 1K conversations with a total of 14.6K utterances and detailed annotations for target topics and knowledge spans. Furthermore, we propose a method named Predict-Generate-Rank, consisting of a \\ngenerator for grounded knowledge prediction and response generation, and a ranker for the ranking of multiple responses to alleviate the exposure bias. We conduct comprehensive experiments to demonstrate the effectiveness of the proposed method and further present several key findings and challenges to prompt future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.224',\n",
       "   'authors': ['Siheng Li',\n",
       "    'Yichun Yin',\n",
       "    'Cheng Yang',\n",
       "    'Wangjie Jiang',\n",
       "    'Yiwei Li',\n",
       "    'Zesen Cheng',\n",
       "    'Lifeng Shang',\n",
       "    'Xin Jiang',\n",
       "    'Qun Liu',\n",
       "    'Yujiu Yang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-1_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1212',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge augmented', 'grounded dialog'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.224.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77427/poster_document/9fea4b8ade0af790b73a9aef85d00827.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77427/poster/5cafdc4a460d917645862872d3de2053.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77427/slideshow/97bc4967ee30e1276e1a874fe5c1ef79.pdf',\n",
       "   'title': 'NewsDialogues: Towards Proactive News Grounded Conversation',\n",
       "   'tldr': 'Hot news is one of the most popular topics in daily conversations. However, news grounded conversation has long been stymied by the lack of well-designed task definition and scarce data. In this paper, we propose a novel task, Proactive News Grounded Conversation, in which a dialogue system can proa...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77427,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77427-newsdialogues-towards-proactive-news-grounded-conversation',\n",
       "   'video_url': None},\n",
       "  'P1213': {'abstract': 'Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not deeply explore the intrinsic structure of unlabeled data, and as a result, cannot seek out the characteristics that define an intent in general. In this paper, starting from the intuition that discovering intents could be beneficial for identifying known intents, we propose a probabilistic framework for discovering intents where intent assignments are treated as latent variables. We adopt the Expectation Maximization framework for optimization. Specifically, In the E-step, we conduct intent discovery and explore the intrinsic structure of unlabeled data by the posterior of intent assignments. In the M-step, we alleviate the forgetting of prior knowledge transferred from known intents by optimizing the discrimination of labeled data. Extensive experiments conducted on three challenging real-world datasets demonstrate the generality and effectiveness of the proposed framework and implementation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.209',\n",
       "   'authors': ['Yunhua Zhou', 'Guofeng Quan', 'Xipeng Qiu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1213',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.209.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76553/poster_document/c16e3e48de41dbf97a9858fcd4090c17.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76553/poster/e44f0388827a092228841d274e2a0e7f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76553/slideshow/ceabec0482ed2d0363716f27e50e3979.pptx',\n",
       "   'title': 'A Probabilistic Framework for Discovering New Intents',\n",
       "   'tldr': 'Discovering new intents is of great significance for establishing the Task-Oriented Dialogue System. Most existing methods either cannot transfer prior knowledge contained in known intents or fall into the dilemma of forgetting prior knowledge in the follow-up. Furthermore, these methods do not deep...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76553,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76553-a-probabilistic-framework-for-discovering-new-intents',\n",
       "   'video_url': None},\n",
       "  'P1223': {'abstract': 'Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not further differentiate detected OOD intents in fine granularity. Discovering focuses on how to cluster unlabeled samples according to their semantic representation, which relies heavily on prior knowledge and can not provide label information for the formed clusters. To be closer to the real user-facing scenarios, we introduce a task paradigm to extend Classification with Discovering referred as Open Environment Intent Prediction, which is to make a further fine-grained discovery of OOD based on OOD Intent Classification. Using various widely-used generative models as an archetype, we propose a general scheme for Open Environment Intent Prediction. In a nutshell, we first perform intent detection to identify the In-domain (IND) samples and then generate labels for those identified as OOD. With these generated labels, we can discover new general intents and provide label information for them. We develop a suite of benchmarks on the existing intent datasets and present a simple yet effective implementation. Extensive experiments demonstrate that our method establishes substantial improvement compared to the baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.140',\n",
       "   'authors': ['Yunhua Zhou', 'Jiawei Hong', 'Xipeng Qiu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-1_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1223',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented', 'factuality', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.140.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77430/poster_document/de0b841c92acb5d4070fba18e6a34be2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77430/poster/82e52b17611665291c4b1c44ba0ef351.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77430/slideshow/4c8d7593a50df62ba7c88f5666939e67.pptx',\n",
       "   'title': 'Towards Open Environment Intent Prediction',\n",
       "   'tldr': 'Out-of-Domain (OOD) Intent Classification and New Intent Discovering are two basic and critical tasks in the Task-Oriented Dialogue System, which are typically treated two independent tasks. Classification focuses on identifying intents beyond the predefined set of the dialog system, but it will not...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77430,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77430-towards-open-environment-intent-prediction',\n",
       "   'video_url': None},\n",
       "  'P1227': {'abstract': 'Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we propose a novel non-autoregressive approach to GEC that decouples the architecture into a permutation network that outputs a self-attention weight matrix that can be used in beam search to find the best permutation of input tokens (with auxiliary <ins> tokens) and a decoder network based on a step-unrolled denoising autoencoder that fills in specific tokens. This allows us to find the token permutation after only one forward pass of the permutation network, avoiding autoregressive constructions. We show that the resulting network improves over previously known non-autoregressive methods for GEC and reaches the level of autoregressive methods that do not use language-specific synthetic data generation methods. Our results are supported by a comprehensive experimental validation on the ConLL-2014 and BEA datasets and an extensive ablation study that supports our architectural and algorithmic choices.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.86',\n",
       "   'authors': ['Konstantin Yakovlev',\n",
       "    'Alexander Podolskiy',\n",
       "    'Andrey Bout',\n",
       "    'Sergey I Nikolenko',\n",
       "    'Irina Piontkovskaya'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1227',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.86.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76554/poster_document/18a9376df9b9137a5a04984f230cb9eb.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76554/poster/470a5a2e1db400724650a3fef4b691af.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76554/slideshow/8df820a8693d0ab61819f9ae6475c072.pdf',\n",
       "   'title': 'GEC-DePenD: Non-Autoregressive Grammatical Error Correction with Decoupled Permutation and Decoding',\n",
       "   'tldr': 'Grammatical error correction (GEC) is an important NLP task that is currently usually solved with autoregressive sequence-to-sequence models. However, approaches of this class are inherently slow due to one-by-one token generation, so non-autoregressive alternatives are needed. In this work, we prop...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76554,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76554-gec-depend-non-autoregressive-grammatical-error-correction-with-decoupled-permutation-and-decoding',\n",
       "   'video_url': None},\n",
       "  'P1228': {'abstract': 'To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output quality. Recently, strategies from controllable simplification have been leveraged to achieve state-of-the-art results on document simplification by first generating a document-level plan (a sequence of sentence-level simplification operations) and using this plan to guide sentence-level simplification downstream. However, this is still limited in that the simplification model has no direct access to the local inter-sentence document context, likely having a negative impact on surface realisation. We explore various systems that use document context within the simplification process itself, either by iterating over larger text units or by extending the system architecture to attend over a high-level representation of document context. In doing so, we achieve state-of-the-art performance on the document simplification task, even when not relying on plan-guidance. Further, we investigate the performance and efficiency tradeoffs of system variants and make suggestions of when each should be preferred.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.834',\n",
       "   'authors': ['Liam Cripwell', 'Jol Legrand', 'Claire Gardent'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-4_-generation-(virtual-poster)'],\n",
       "   'id': 'P1228',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.834.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77432/poster_document/7ae844174e21e7933b07ba16d04b5572.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77432/poster/d9b55b964cc44bc7a810bd19f5ddf49d.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Context-Aware Document Simplification',\n",
       "   'tldr': 'To date, most work on text simplification has focused on sentence-level inputs. Early attempts at document simplification merely applied these approaches iteratively over the sentences of a document. However, this fails to coherently preserve the discourse structure, leading to suboptimal output qua...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77432,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77432-a-synthetic-data-generation-framework-for-grounded-dialogues',\n",
       "   'video_url': None},\n",
       "  'P1236': {'abstract': 'State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring large amounts of labeled speech for specific target accents is challenging. Choosing an informative subset of speech samples that are most representative of the target accents becomes important for effective ASR finetuning. To address this problem, we propose DITTO (Data-efficient and faIr Targeted subseT selectiOn that uses Submodular Mutual Information (SMI) functions as acquisition functions to find the most informative set of utterances matching a target accent within a fixed budget. An important feature of DITTO is that it supports fair targeting for multiple accents, i.e. it can automatically select representative data points from multiple accents when the ASR model needs to perform well on more than one accent. We show that compared to other speech selection methods, DITTO is 3-5 times as label-efficient for its improvements on the Indic-TTS and L2 datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.319',\n",
       "   'authors': ['Suraj N Kothawade',\n",
       "    'Anmol Reddy Mekala',\n",
       "    'D.Chandra Sekhara SS Hetha Havya',\n",
       "    'Mayank Kothyari',\n",
       "    'Rishabh K Iyer',\n",
       "    'Ganesh Ramakrishnan',\n",
       "    'Preethi Jyothi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-4_-theme_-reality-check-(virtual-poster)'],\n",
       "   'id': 'P1236',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['(non-)generalizability', 'evaluation', 'methodology'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.319.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76555/poster_document/4766e7da18e90df8f421e2ac7316a84f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76555/poster/c50869648044d5678cb3efaefaae1444.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76555/slideshow/989f1b25be5981633f5760213ae67548.pdf',\n",
       "   'title': 'DITTO: Data-efficient and Fair Targeted Subset Selection for ASR Accent Adaptation',\n",
       "   'tldr': 'State-of-the-art Automatic Speech Recognition (ASR) systems are known to exhibit disparate performance on varying speech accents. To improve performance on a specific target accent, a commonly adopted solution is to finetune the ASR model using accent-specific labeled speech. However, acquiring larg...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 76555,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76555-how-does-the-brain-process-syntactic-structure-while-listeningquestion',\n",
       "   'video_url': None},\n",
       "  'P1239': {'abstract': 'Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi\\\\_nlp',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.282',\n",
       "   'authors': ['Han Cheol Moon',\n",
       "    'Shafiq Joty',\n",
       "    'Ruochen Zhao',\n",
       "    'Megh Thakkar',\n",
       "    'Chi Xu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)'],\n",
       "   'id': 'P1239',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['adversarial attacks/examples/training', 'robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.282.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76556/poster_document/c192a9886bdcc4ea5d45b2bf4820ad98.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76556/poster/416fb1593248de2cf8fc5077a625a250.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76556/slideshow/ca68f7a5dc4e234c4f5fffaa8829a3c1.pdf',\n",
       "   'title': 'Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications',\n",
       "   'tldr': 'Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We in...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76556,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76556-randomized-smoothing-with-masked-inference-for-adversarially-robust-text-classifications',\n",
       "   'video_url': None},\n",
       "  'P1242': {'abstract': \"Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained in a model's pretrained parameters, and instance-specific information that is supplied at inference time. However, the integration and reasoning abilities of NLU models in the presence of multiple knowledge sources have been largely understudied. In this work, we propose a test suite of coreference resolution subtasks that require reasoning over multiple facts. These subtasks differ in terms of which knowledge sources contain the relevant facts. We also introduce subtasks where knowledge is present only at inference time using fictional knowledge. We evaluate state-of-the-art coreference resolution models on our dataset. Our results indicate that several models struggle to reason on-the-fly over knowledge observed both at pretrain time and at inference time. However, with task-specific training, a subset of models demonstrates the ability to integrate certain knowledge types from multiple sources. Still, even the best performing models seem to have difficulties with reliably integrating knowledge presented only at inference time.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.841',\n",
       "   'authors': ['Akshatha Arodi',\n",
       "    'Martin Pmsl',\n",
       "    'Kaheer Suleman',\n",
       "    'Adam Trischler',\n",
       "    'Alexandra Olteanu',\n",
       "    'Jackie Chi Kit Cheung'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-2_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1242',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.841.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76557/poster_document/5fca22498b727a08ac8dc9d746b7e616.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76557/poster/a8ffcc157e06d0d2008a3d6be7f6caa6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76557/slideshow/099f1332dd3ea8bcca01a080cc67734d.pdf',\n",
       "   'title': 'The KITMUS Test: Evaluating Knowledge Integration from Multiple Sources',\n",
       "   'tldr': 'Many state-of-the-art natural language understanding (NLU) models are based on pretrained neural language models. These models often make inferences using information from multiple sources. An important class of such inferences are those that require both background knowledge, presumably contained i...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76557,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76557-the-kitmus-test-evaluating-knowledge-integration-from-multiple-sources',\n",
       "   'video_url': None},\n",
       "  'P1243': {'abstract': 'In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unseen classes. To remedy this overconfidence, we introduce Contrastive Novelty-Augmented Learning (CoNAL), a two-step method that generates OOD examples representative of novel classes, then trains to decrease confidence on them. First, we generate OOD examples by prompting a large language model twice: we prompt it to enumerate relevant novel classes, then generate examples from each novel class matching the task format. Second, we train a classifier with a novel contrastive objective that encourages lower confidence on generated OOD examples than training examples. When trained with CoNAL, classifiers improve in their ability to detect and abstain on novel class examples over prior methods by an average of 2.3\\\\% in terms of accuracy under the accuracy-coverage curve (AUAC) and 5.5\\\\% AUROC across 4 NLP datasets, with no cost to in-distribution accuracy.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.658',\n",
       "   'authors': ['Albert Xu', 'Xiang Ren', 'Robin Jia'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1243',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.658.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76558/poster_document/55cf47e61e686bad94e3ecfeca70b766.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76558/poster/25b770cbb79fb3f30899a338abb6024b.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76558/slideshow/ecf2aa1d28c5bf86afdf767b341294a8.pdf',\n",
       "   'title': 'Contrastive Novelty-Augmented Learning: Anticipating Outliers with Large Language Models',\n",
       "   'tldr': 'In many task settings, text classification models are likely to encounter examples from novel classes on which they cannot predict correctly. Selective prediction, in which models abstain on low-confidence examples, provides a possible solution, but existing models are often overly confident on unse...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76558,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76558-contrastive-novelty-augmented-learning-anticipating-outliers-with-large-language-models',\n",
       "   'video_url': None},\n",
       "  'P1245': {'abstract': \"Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs.\\nWhile these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself.\\nIn this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. \\nThis observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. \\nWe implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject--verb number agreement task (SVA). \\nWith this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, \\nidentify which pathways of the self-attention mechanism are most important.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.669',\n",
       "   'authors': ['Kevin Du',\n",
       "    'Lucas Torroba Hennigen',\n",
       "    'Niklas Stoehr',\n",
       "    'Alex Warstadt',\n",
       "    'Ryan Cotterell'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-3_-interpretability-and-analysis-of-models-for-nlp-(oral)'],\n",
       "   'id': 'P1245',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data influence', 'feature attribution'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.669.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76222/poster_document/e83bb4b7b8a4a345fd59286e64122540.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76222/poster/7f267ba061d898e4b53a76e290f3de01.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Generalizing Backpropagation for Gradient-Based Interpretability',\n",
       "   'tldr': \"Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs.\\nWhile these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner working...\",\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76222,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15222/lecture/76222-generalizing-backpropagation-for-gradient-based-interpretability',\n",
       "   'video_url': None},\n",
       "  'P1254': {'abstract': 'One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of building effective cross-modal representations, but also by the lack of specific evaluation and training data. We present a new MMT approach based on a strong text-only MT model, which uses neural adapters, a novel guided self-attention mechanism and which is jointly trained on both visually-conditioned masking and MMT. We also introduce CoMMuTE, a Contrastive Multilingual Multimodal Translation Evaluation set of ambiguous sentences and their possible translations, accompanied by disambiguating images corresponding to each translation. Our approach obtains competitive results compared to strong text-only models on standard EnglishFrench, EnglishGerman and EnglishCzech benchmarks and outperforms baselines and state-of-the-art MMT systems by a large margin on our contrastive test set. Our code and CoMMuTE are freely available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.295',\n",
       "   'authors': ['Matthieu Futeral',\n",
       "    'Cordelia Schmid',\n",
       "    'Ivan Laptev',\n",
       "    'Benot Sagot',\n",
       "    'Rachel Bawden'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['poster-session-7_-language-grounding-to-vision,-robotics,-and-beyond-(poster)'],\n",
       "   'id': 'P1254',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal matchine translation'],\n",
       "   'languages': ['french', 'german', 'czech'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.295.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76559/poster_document/076ad8e05897a12849528ac35b4b7325.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76559/poster/825f086202e97547b9e92c9e4dce0ef6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Tackling Ambiguity with Images: Improved Multimodal Machine Translation and Contrastive Evaluation',\n",
       "   'tldr': 'One of the major challenges of machine translation (MT) is ambiguity, which can in some cases be resolved by accompanying context such as images. However, recent work in multimodal MT (MMT) has shown that obtaining improvements from images is challenging, limited not only by the difficulty of buildi...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76559,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76559-tackling-ambiguity-with-images-improved-multimodal-machine-translation-and-contrastive-evaluation',\n",
       "   'video_url': None},\n",
       "  'P1258': {'abstract': 'The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate a compact embedding without requiring any changes in its architecture or training procedure. Specifically, we use a correlation penalty added to the weighted reconstruction loss that better captures the informative features in the text embeddings, which improves the efficiency of the language models. We evaluated EmbedTextNet on three different downstream tasks: text similarity, language modelling, and text retrieval. Empirical results on diverse benchmark datasets demonstrate the effectiveness and superiority of EmbedTextNet compared to state-of-art methodologies in recent works, especially in extremely low dimensional embedding sizes. The developed code for reproducibility is included in the supplementary material.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.625',\n",
       "   'authors': ['Dae Yon Hwang', 'Bilal Taha', 'Yaroslav Nechaev'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1258',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['representation learning', 'model compression methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.625.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77434/poster_document/280fbeb68b32fcf5c1475fea7712157c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77434/poster/968641458a34e92b5096e7593aefb875.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77434/slideshow/606333221cf40a7592eb718693d11af0.pdf',\n",
       "   'title': 'EmbedTextNet: Dimension Reduction with Weighted Reconstruction and Correlation Losses for Efficient Text Embedding',\n",
       "   'tldr': 'The size of embeddings generated by large language models can negatively affect system latency and model size in certain downstream practical applications (e.g. KNN search). In this work, we propose EmbedTextNet, a light add-on network that can be appended to an arbitrary language model to generate ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77434,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77434-propsegment-a-large-scale-corpus-for-proposition-level-segmentation-and-entailment-recognition',\n",
       "   'video_url': None},\n",
       "  'P1265': {'abstract': \"Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks. However, most of these explainability methods have been shown to be brittle in the face of adversarial perturbations of their inputs in the image and generic textual domain. In this work we show that this phenomenon extends to specific and important high stakes domains like biomedical datasets. In particular, we observe that the robustness of explanations should be characterized in terms of the accuracy of the explanation in linking a model's inputs and its decisions - faithfulness - and its relevance from the perspective of domain experts - plausibility. This is crucial to prevent explanations that are inaccurate but still look convincing in the context of the domain at hand. To this end, we show how to adapt current attribution robustness estimation methods to a given domain, so as to take into account domain-specific plausibility. This results in our DomainAdaptiveAREstimator (DARE) attribution robustness estimator, allowing us to properly characterize the domain-specific robustness of faithful explanations. Next, we provide two methods, adversarial training and FAR training, to mitigate the brittleness characterized by DARE, allowing us to train networks that display robust attributions. Finally, we empirically validate our methods with extensive experiments on three established biomedical benchmarks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.644',\n",
       "   'authors': ['Adam Daniel Ivankay', 'Mattia Rigotti', 'Pascal Frossard'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-6_-nlp-applications-(oral)'],\n",
       "   'id': 'P1265',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['healthcare applications, clincial nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.644.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76223/poster_document/1fe185023e5c1bab289a90324f7719fd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76223/poster/1adb2726f877c4495f18f3e1a245db88.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76223/slideshow/d2ddc38cc5e07411ae12b2774cca8659.pdf',\n",
       "   'title': 'DARE: Towards Robust Text Explanations in Biomedical and Healthcare Applications',\n",
       "   'tldr': 'Along with the successful deployment of deep neural networks in several application domains, the need to unravel the black-box nature of these networks has seen a significant increase recently. Several methods have been introduced to provide insight into the inference process of deep neural networks...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76223,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15259/lecture/76223-dare-towards-robust-text-explanations-in-biomedical-and-healthcare-applications',\n",
       "   'video_url': None},\n",
       "  'P1266': {'abstract': 'In this work, we study the problem of unsupervised open-domain keyphrase generation, where the objective is a keyphrase generation model that can be built without using human-labeled data and can perform consistently across domains. To solve this problem, we propose a seq2seq model that consists of two modules, namely phraseness and informativeness module, both of which can be built in an unsupervised and open-domain fashion. The phraseness module generates phrases, while the informativeness module guides the generation towards those that represent the core concepts of the text. We thoroughly evaluate our proposed method using eight benchmark datasets from different domains. Results on in-domain datasets show that our approach achieves state-of-the-art results compared with existing unsupervised models, and overall narrows the gap between supervised and unsupervised methods down to about 16\\\\%. Furthermore, we demonstrate that our model performs consistently across domains, as it surpasses the baselines on out-of-domain datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.592',\n",
       "   'authors': ['Lam Thanh Do', 'Pritom Saha Akash', 'Kevin Chen-Chuan Chang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1266',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generative models'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.592.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76560/poster_document/7ddc5add2d5ec388508dd16ad2e96662.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76560/poster/cc996a2da97cb82b2137df704d771a4c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76560/slideshow/abfa37925bbe8d0dde41d161984ceef0.pdf',\n",
       "   'title': 'Unsupervised Open-domain Keyphrase Generation',\n",
       "   'tldr': 'In this work, we study the problem of unsupervised open-domain keyphrase generation, where the objective is a keyphrase generation model that can be built without using human-labeled data and can perform consistently across domains. To solve this problem, we propose a seq2seq model that consists of ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76560,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76560-unsupervised-open-domain-keyphrase-generation',\n",
       "   'video_url': None},\n",
       "  'P1270': {'abstract': 'Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We propose speech2sign-U, a neural network-based approach capable of both character-level and word-level SSR-U. Our approach significantly outperforms baselines directly adapted from unsupervised speech recognition (ASR-U) models by as much as 50\\\\% recall@10 on several challenging American sign language corpora with various levels of sample sizes, vocabulary sizes, and audio and visual variability. The code is available at {https://github.com/cactuswiththoughts/UnsupSpeech2Sign.git}{cactuswiththoughts/UnsupSpeech2Sign.git}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.424',\n",
       "   'authors': ['Liming Wang',\n",
       "    'Junrui Ni',\n",
       "    'Heting Gao',\n",
       "    'Jialu Li',\n",
       "    'Kai Chieh Chang',\n",
       "    'Xulin Fan',\n",
       "    'Junkai Wu',\n",
       "    'Mark Hasegawa-Johnson',\n",
       "    'Chang D. Yoo'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['session-7_-speech-and-multimodality-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1270',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['speech and vision', 'speech technologies', 'multimodality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.424.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77437/poster_document/9168bfd85dae94702d6477fdc9b56e46.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77437/poster/82af5b6eb511616c4796e49c036b5f63.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77437/slideshow/65b1ce823a196ccd0ff0c25fe2d5598d.pdf',\n",
       "   'title': 'Listen, Decipher and Sign: Toward Unsupervised Speech-to-Sign Language Recognition',\n",
       "   'tldr': 'Existing supervised sign language recognition systems rely on an abundance of well-annotated data. Instead, an unsupervised speech-to-sign language recognition (SSR-U) system learns to translate between spoken and sign languages by observing only non-parallel speech and sign-language corpora. We pro...',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 77437,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77437-listen-decipher-and-sign-toward-unsupervised-speech-to-sign-language-recognition',\n",
       "   'video_url': None},\n",
       "  'P1275': {'abstract': \"Unsupervised speech recognition (\\\\{pasted macro `ASRU'\\\\}/) is the problem of learning automatic speech recognition (ASR) systems from \\\\emph{unpaired} speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties and address such issues as sensitivity to hyperparameters and training instability. In this paper, we proposed a general theoretical framework to study the properties of \\\\{pasted macro `ASRU'\\\\}/ systems based on random matrix theory and the theory of neural tangent kernels. Such a framework allows us to prove various learnability conditions and sample complexity bounds of \\\\{pasted macro `ASRU'\\\\}/. Extensive \\\\{pasted macro `ASRU'\\\\}/ experiments on synthetic languages with three classes of transition graphs provide strong empirical evidence for our theory (code available at {https://github.com/cactuswiththoughts/UnsupASRTheory.git}{cactuswiththoughts/UnsupASRTheory.git}).\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.67',\n",
       "   'authors': ['Liming Wang', 'Mark Hasegawa-Johnson', 'Chang D. Yoo'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['poster-session-6_-speech-and-multimodality-(poster)'],\n",
       "   'id': 'P1275',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic speech recognition'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.67.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76561/poster_document/48cc91547ac8bf5cd85bccfca6da7213.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76561/poster/2b056106aa044c0bc0283a2fb6ae71ea.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76561/slideshow/90d86efc5f75f1a9bae863e8c228dfd1.pdf',\n",
       "   'title': 'A Theory of Unsupervised Speech Recognition',\n",
       "   'tldr': \"Unsupervised speech recognition (\\\\{pasted macro `ASRU'\\\\}/) is the problem of learning automatic speech recognition (ASR) systems from \\\\emph{unpaired} speech-only and text-only corpora. While various algorithms exist to solve this problem, a theoretical framework is missing to study their properties ...\",\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 76561,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76561-generating-structured-pseudo-labels-for-noise-resistant-zero-shot-video-sentence-localization',\n",
       "   'video_url': None},\n",
       "  'P1276': {'abstract': 'Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict their entity types or constructing type-specific prompts to locate entities. However, these methods not only require a multi-round prompting manner with a high time overhead and computational cost, but also require elaborate prompt templates, that are difficult to apply in practical scenarios. In this paper, we unify entity locating and entity typing into prompt learning, and design a dual-slot multi-prompt template with the position slot and type slot to prompt locating and typing respectively. Multiple prompts can be input to the model simultaneously, and then the model extracts all entities by parallel predictions on the slots. To assign labels for the slots during training, we design a dynamic template filling mechanism that uses the extended bipartite graph matching between prompts and the ground-truth entities. We conduct experiments in various settings, including resource-rich flat and nested NER datasets and low-resource in-domain and cross-domain datasets. Experimental results show that the proposed model achieves a significant performance improvement, especially in the cross-domain few-shot setting, which outperforms the state-of-the-art model by +7.7\\\\% on average.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.698',\n",
       "   'authors': ['Yongliang Shen',\n",
       "    'Zeqi Tan',\n",
       "    'Shuhui Wu',\n",
       "    'Wenqi Zhang',\n",
       "    'Rongsheng Zhang',\n",
       "    'Yadong Xi',\n",
       "    'Weiming Lu',\n",
       "    'Yueting Zhuang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['poster-session-5_-information-extraction-(poster)'],\n",
       "   'id': 'P1276',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.698.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76562/poster_document/ba2f19ea434190d5723bdb4ee18fbed2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76562/poster/c972837117805b232f394d094a7558a7.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'PromptNER: Prompt Locating and Typing for Named Entity Recognition',\n",
       "   'tldr': 'Prompt learning is a new paradigm for utilizing pre-trained language models and has achieved great success in many tasks. To adopt prompt learning in the NER task, two kinds of methods have been explored from a pair of symmetric perspectives, populating the template by enumerating spans to predict t...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76562,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/76562-promptner-prompt-locating-and-typing-for-named-entity-recognition',\n",
       "   'video_url': None},\n",
       "  'P1277': {'abstract': \"Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms' importance for memory models.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.830',\n",
       "   'authors': ['Vladimir Araujo', 'Alvaro M Soto', 'Marie-Francine Moens'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1277',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['self-supervised learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.830.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77439/slideshow/03d1e72457b0a1defff3fec76bf302d5.pdf',\n",
       "   'title': 'A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information',\n",
       "   'tldr': 'Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity me...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77439,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77439-a-memory-model-for-question-answering-from-streaming-data-supported-by-rehearsal-and-anticipation-of-coreference-information',\n",
       "   'video_url': None},\n",
       "  'P1278': {'abstract': 'Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.\\nTypical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.19',\n",
       "   'authors': ['Antonio Valerio Miceli Barone',\n",
       "    'Fazl Barez',\n",
       "    'Shay B. Cohen',\n",
       "    'Ioannis Konstas'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1278',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['scaling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.19.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77440/poster_document/4b40c88c89c8f17b7a85b113c28aaf2a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77440/poster/4a094764843a5b2507d29aa838ea5d24.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77440/slideshow/91600b73a216de457928d5883999fd68.pdf',\n",
       "   'title': 'The Larger they are, the Harder they Fail: Language Models do not Recognize Identifier Swaps in Python',\n",
       "   'tldr': 'Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming.\\nTypical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, s...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77440,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77440-the-larger-they-are-the-harder-they-fail-language-models-do-not-recognize-identifier-swaps-in-python',\n",
       "   'video_url': None},\n",
       "  'P1284': {'abstract': 'In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the ReLU-activated Transformer can be considered as a linear model on a single input. We further leverage the linearity of the model and propose a linear decomposition of the model output to generate local explanations. Our evaluation of sentiment classification and machine translation shows that our method achieves competitive performance in efficiency and fidelity of explanation. In addition, we demonstrate the potential of our approach in applications with examples of error analysis on multiple tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.572',\n",
       "   'authors': ['Sen Yang',\n",
       "    'Shujian Huang',\n",
       "    'wei zou',\n",
       "    'Jianbing Zhang',\n",
       "    'Xinyu Dai',\n",
       "    'Jiajun CHEN'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1284',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['feature attribution'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.572.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76563/poster_document/48b138abb13ee65ed0fbe766997b63c5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76563/poster/23c41d7defa89d7bc7c5988076adc4fa.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Local Interpretation of Transformer Based on Linear Decomposition',\n",
       "   'tldr': 'In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on a wide range of tasks. However, limitations in interpretability have hindered their applications in the real world. This work proposes to interpret neural networks by linear decomposition and finds that the Re...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76563,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76563-local-interpretation-of-transformer-based-on-linear-decomposition',\n",
       "   'video_url': None},\n",
       "  'P1286': {'abstract': \"Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real world entities. For example, given the choice `Should we make a Simnel cake or a Pandan cake?` a natural response from a non-expert may be indirect: `let's make the green one`. Reference resolution has been little studied with natural expressions, thus robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities (Alternative Entities), a new public dataset of entity pairs and utterances, and develop models for the disambiguation problem. Consisting of 42K indirect referring expressions across three domains, it enables for the first time the study of how large language models can be adapted to this task. We find they achieve 82\\\\%-87\\\\% accuracy in realistic settings, which while reasonable also invites further advances.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.688',\n",
       "   'authors': ['Mohammad Javad Hosseini',\n",
       "    'Filip Radlinski',\n",
       "    'Silvia Pareti',\n",
       "    'Annie Louis'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Discourse and Pragmatics',\n",
       "   'event_ids': ['poster-session-2_-discourse-and-pragmatics-(poster)'],\n",
       "   'id': 'P1286',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['bridging resolution', 'dialogue'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.688.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76564/poster_document/66e5de90a3a7ae622e344cab9511abbf.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76564/poster/bb80f7c856bc469780ddadf41dc34dba.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76564/slideshow/082e06c2485ad1c6b292753caa5cdcca.pdf',\n",
       "   'title': 'Resolving Indirect Referring Expressions for Entity Selection',\n",
       "   'tldr': 'Recent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address the problem of reference resolution, when people use natural expressions to choose between real wo...',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'underline_id': 76564,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76564-resolving-indirect-referring-expressions-for-entity-selection',\n",
       "   'video_url': None},\n",
       "  'P129': {'abstract': 'Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. \\nHowever, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since different tasks might conflict with each other.\\nExisting MTL methods for alleviating this issue is to leverage heuristics or gradient-based algorithm to achieve an arbitrary Pareto optimal trade-off among different tasks. \\nIn this paper, we present a novel gradient trade-off approach to mitigate the task conflict problem, dubbed GetMTL, which can achieve a specific trade-off among different tasks nearby the main objective of multi-task text classification (MTC), so as to improve the performance of each task simultaneously.\\nThe results of extensive experiments on two benchmark datasets back up our theoretical analysis and validate the superiority of our proposed GetMTL.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.144',\n",
       "   'authors': ['Heyan Chai',\n",
       "    'Jinhao Cui',\n",
       "    'Ye Wang',\n",
       "    'Min Zhang',\n",
       "    'Binxing Fang',\n",
       "    'Qing Liao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P129',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-task learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.144.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76374/poster_document/60230036f941324c3d58187d5b0eee77.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Gradient Trade-offs between Tasks in Multi-task Text Classification',\n",
       "   'tldr': 'Multi-task learning (MTL) has emerged as a promising approach for sharing inductive bias across multiple tasks to enable more efficient learning in text classification. \\nHowever, training all tasks simultaneously often yields degraded performance of each task than learning them independently, since ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76374,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76374-improving-gradient-trade-offs-between-tasks-in-multi-task-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1291': {'abstract': 'Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on our taxonomy, we present a framework for constructing benchmarks and documenting their limitations.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.202',\n",
       "   'authors': ['Arjun Subramonian',\n",
       "    'Xingdi Yuan',\n",
       "    'Hal Daum III',\n",
       "    'Su Lin Blodgett'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-4_-theme_-reality-check-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1291',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.202.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77441/poster_document/e497a137f96f0a7ccf2ce9afb9e80eca.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77441/poster/ea097c0389640d7c983d1c91289c1da1.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77441/slideshow/47f6a315f8ad000b454128218ba95835.pdf',\n",
       "   'title': 'It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance',\n",
       "   'tldr': 'Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 77441,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77441-it-takes-two-to-tango-navigating-conceptualizations-of-nlp-tasks-and-measurements-of-performance',\n",
       "   'video_url': None},\n",
       "  'P1292': {'abstract': 'Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit that utilizes execution results of the generated code from LLMs to improve the code quality on the competitive programming task. \\nWe execute the generated code on the example test case provided in the question and wrap execution results into a supplementary comment. Utilizing this comment as guidance, our fault-aware code editor is employed to correct errors in the generated code.\\nWe perform extensive evaluations across two competitive programming datasets with nine different LLMs. Compared to directly generating from LLMs, our approach can improve the average of pass@1 by 89\\\\% on APPS-dev, 31\\\\% on APPS-test, and 48\\\\% on HumanEval over nine popular code generation LLMs with parameter sizes ranging from 110M to 175B. \\nCompared to other post-processing methods, our method demonstrates superior accuracy and efficiency.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.45',\n",
       "   'authors': ['Kechi Zhang', 'Zhuo Li', 'Jia Li', 'Ge Li', 'Zhi Jin'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-3_-nlp-applications-(poster)'],\n",
       "   'id': 'P1292',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code generation and understanding'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.45.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76565/poster_document/ee6e5921423b9b1a2d58b8f45b0d6840.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76565/poster/199c5eacd632270215ede9fbad3599c6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76565/slideshow/3cfa43f5a9482c0924ccf8eda8dbf57f.pptx',\n",
       "   'title': 'Self-Edit: Fault-Aware Code Editor for Code Generation',\n",
       "   'tldr': 'Large language models (LLMs) have demonstrated an impressive ability to generate codes on competitive programming tasks. However, with limited sample numbers, LLMs still suffer from poor accuracy. Inspired by the process of human programming, we propose a generate-and-edit approach named Self-Edit t...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76565,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76565-self-edit-fault-aware-code-editor-for-code-generation',\n",
       "   'video_url': None},\n",
       "  'P1297': {'abstract': 'Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are usually created ad-hoc and whose data distribution often differs from the one in real applications, showing significant performance degradation in real-world scenarios. We perform an extensive assessment of the out-of-distribution performances of supervised models for classification in the emotion and hate-speech detection tasks and show that NLI-based zero-shot models often outperform them, making task-specific annotation useless when the characteristics of final-user data are not known in advance. To benefit from both supervised and zero-shot approaches, we propose to fine-tune an NLI-based model on the task-specific dataset. The resulting model often outperforms all available supervised models both in distribution and out of distribution, with only a few thousand training samples.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.524',\n",
       "   'authors': ['Luana Bulla', 'Aldo Gangemi', \"Misael Mongiovi'\"],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1297',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['transfer learning / domain adaptation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.524.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77442/poster_document/911c65a1155a191cf87fd1680c412881.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77442/poster/071d52858cbee9a3eedf0b6a823aeb7b.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77442/slideshow/5ae5ff02cce16e6d98d1a1c8438a3239.pdf',\n",
       "   'title': 'Towards Distribution-shift Robust Text Classification of Emotional Content',\n",
       "   'tldr': 'Supervised models based on Transformers have been shown to achieve impressive performances in many natural language processing tasks. However, besides requiring a large amount of costly manually annotated data, supervised models tend to adapt to the characteristics of the training dataset, which are...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77442,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77442-leveraging-explicit-procedural-instructions-for-data-efficient-action-prediction',\n",
       "   'video_url': None},\n",
       "  'P1299': {'abstract': \"Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.140',\n",
       "   'authors': ['Ori Ram',\n",
       "    'Liat Bezalel',\n",
       "    'Adi Zicher',\n",
       "    'Yonatan Belinkov',\n",
       "    'Jonathan Berant',\n",
       "    'Amir Globerson'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-7_-information-retrieval-and-text-mining-(oral)'],\n",
       "   'id': 'P1299',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['passage retrieval',\n",
       "    'dense retrieval',\n",
       "    'contrastive learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.140.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76224/poster_document/25cc7a6203dd6faf42cd8e5b1476d237.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76224/poster/fcddba04d31c817f846f51db55fcc968.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76224/slideshow/b1e0405eda464b4b30bc9dfced1f5743.pdf',\n",
       "   'title': 'What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary',\n",
       "   'tldr': 'Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representa...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 76224,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15276/lecture/76224-what-are-you-token-aboutquestion-dense-retrieval-as-distributions-over-the-vocabulary',\n",
       "   'video_url': None},\n",
       "  'P1302': {'abstract': 'Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token sequences that represent any given string. To truly compute the probability of a string one should marginalize over all tokenizations, which is typically intractable. Here, we analyze whether the practice of ignoring the marginalization is justified. To this end, we devise an importance-sampling-based algorithm that allows us to compute estimates of the marginal probabilities and compare them to the default procedure in a range of state-of-the-art models and datasets. Our results show that the gap in log-likelihood is no larger than 0.5\\\\% in most cases, but that it becomes more pronounced for data with long complex words.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.1',\n",
       "   'authors': ['Nadezhda Chirkova',\n",
       "    'Germn Kruszewski',\n",
       "    'Jos Rozen',\n",
       "    'Marc Dymetman'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-7_-large-language-models-(poster)'],\n",
       "   'id': 'P1302',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.1.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76566/poster_document/ee348f0298f0e2c2fde7b00cb52ff6df.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76566/poster/f96a26fba31e458a2db1053e4440715d.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76566/slideshow/dc76843ce4a56bca1fd3b88c7cb5f807.pdf',\n",
       "   'title': 'Should you marginalize over possible tokenizations?',\n",
       "   'tldr': 'Autoregressive language models (LMs) map token sequences to probabilities. The usual practice for computing the probability of any character string (e.g. English sentences) is to first transform it into a sequence of tokens that is scored by the model. However, there are exponentially many token seq...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76566,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76566-should-you-marginalize-over-possible-tokenizationsquestion',\n",
       "   'video_url': None},\n",
       "  'P1303': {'abstract': 'Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. \\nTo reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language.\\nX-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language, and unlike most multilingual prior work, is an end-to-end dataset for building fully-functioning agents. \\n\\nThe many difficulties we encountered in creating X-RiSAWOZ led us to develop a toolset to accelerate the post-editing of a new language dataset after translation. This toolset improves machine translation with a hybrid entity alignment technique that combines neural with dictionary-based methods, along with many automated and semi-automated validation checks. \\n\\nWe establish strong baselines for X-RiSAWOZ by training dialogue agents in the zero- and few-shot settings where limited gold data is available in the target language. Our results suggest that our translation and post-editing methodology and toolset can be used to create new high-quality multilingual dialogue agents cost-effectively. Our dataset, code, and toolkit are released open-source.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.174',\n",
       "   'authors': ['Mehrad Moradshahi',\n",
       "    'Tianhao Shen',\n",
       "    'Kalika Bali',\n",
       "    'Monojit Choudhury',\n",
       "    'Gael de Chalendar',\n",
       "    'Anmol Goel',\n",
       "    'Sungkyun Kim',\n",
       "    'Prashant Kodali',\n",
       "    'Ponnurangam Kumaraguru',\n",
       "    'Nasredine Semmar',\n",
       "    'Sina Semnani',\n",
       "    'Jiwon Seo',\n",
       "    'Vivek Seshadri',\n",
       "    'Manish Shrivastava',\n",
       "    'Michael Sun',\n",
       "    'Aditya Yadavalli',\n",
       "    'Chaobin You',\n",
       "    'Deyi Xiong',\n",
       "    'Monica S Lam'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1303',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code-switching',\n",
       "    'mixed language',\n",
       "    'multilingualism',\n",
       "    'cross-lingual transfer',\n",
       "    'multilingual pre-training',\n",
       "    'multilingual benchmarks',\n",
       "    'multilingual evaluation'],\n",
       "   'languages': ['chinese', 'french', 'hindi'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.174.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77443/poster_document/55ed3b6827fba1d72753d135a0276776.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'X-RiSAWOZ: High-Quality End-to-End Multilingual Dialogue Datasets and Few-shot Agents',\n",
       "   'tldr': 'Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. \\nTo reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by trans...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77443,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77443-x-risawoz-high-quality-end-to-end-multilingual-dialogue-datasets-and-few-shot-agents',\n",
       "   'video_url': None},\n",
       "  'P1306': {'abstract': 'We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism:   hierarchical structure information strengthening and a final specific task. \\nThe model used is better than existing ones at learning the contextual representations of utterances embedded within its hierarchical semantic structure and thereby improves system performance. In addition, we created a mechanism using inductive grammar to dynamically prune the unpromising directions in the semantic structure parsing process. \\nFinally, through experiments{Our code will be published when this paper is accepted.} on the TOP and TOPv2 (low-resource setting) datasets, we achieved state-of-the-art (SOTA)  performance, confirming the effectiveness of our proposed model.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.648',\n",
       "   'authors': ['Truong Dinh Do', 'Phuong Minh Nguyen', 'Minh Le Nguyen'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1306',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.648.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77444/poster_document/ab2163b24bc4e66af4f92aa7bd60448b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77444/poster/370f94b94b4534d95c2ee3ab8be63a36.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77444/slideshow/dedc8ce522a80f3ad1ece1107df25bf5.pdf',\n",
       "   'title': 'StructSP: Efficient Fine-tuning of Task-Oriented Dialog System by Using Structure-aware Boosting and Grammar Constraints',\n",
       "   'tldr': 'We have investigated methods utilizing hierarchical structure information representation in the semantic parsing task and have devised a method that reinforces the semantic awareness of a pre-trained language model via a two-step fine-tuning mechanism:   hierarchical structure information strengthen...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77444,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77444-end-to-end-knowledge-retrieval-with-multi-modal-queries',\n",
       "   'video_url': None},\n",
       "  'P1309': {'abstract': 'In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion process and learns a reverse diffusion process to recover the entity boundaries. In inference, DiffusionNER first randomly samples some noisy spans from a standard Gaussian distribution and then generates the named entities by denoising them with the learned reverse diffusion process. The proposed boundary-denoising diffusion process allows progressive refinement and dynamic sampling of entities, empowering DiffusionNER with efficient and flexible entity generation capability. Experiments on multiple flat and nested NER datasets demonstrate that DiffusionNER achieves comparable or even better performance than previous state-of-the-art models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.215',\n",
       "   'authors': ['Yongliang Shen',\n",
       "    'Kaitao Song',\n",
       "    'Xu Tan',\n",
       "    'Dongsheng Li',\n",
       "    'Weiming Lu',\n",
       "    'Yueting Zhuang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['poster-session-1_-information-extraction-(poster)'],\n",
       "   'id': 'P1309',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.215.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76567/poster_document/8dc12f600a8144cafdca7cb35c3bb11a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76567/poster/832333181053e5ac992f99370ecb20eb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DiffusionNER: Boundary Diffusion for Named Entity Recognition',\n",
       "   'tldr': 'In this paper, we propose DiffusionNER, which formulates the named entity recognition task as a boundary-denoising diffusion process and thus generates named entities from noisy spans. During training, DiffusionNER gradually adds noises to the golden entity boundaries by a fixed forward diffusion pr...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76567,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76567-diffusionner-boundary-diffusion-for-named-entity-recognition',\n",
       "   'video_url': None},\n",
       "  'P1312': {'abstract': 'Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both questions are no. This is because the softmax bottleneck sometimes prevents the LMs from predicting the desired distribution and the pointer networks can be used to break the bottleneck efficiently. Based on the finding, we propose several softmax alternatives by simplifying the pointer networks and accelerating the word-by-word rerankers. In GPT-2, our proposals are significantly better and more efficient than mixture of softmax, a state-of-the-art softmax alternative. In summarization experiments, without very significantly decreasing its training/testing speed, our best method based on T5-Small improves factCC score by 2 points in CNN/DM and XSUM dataset, and improves MAUVE scores by 30\\\\% in BookSum paragraph-level dataset.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.805',\n",
       "   'authors': ['Haw-Shiuan Chang',\n",
       "    'Zonghai Yao',\n",
       "    'Alolika Gon',\n",
       "    'hong yu',\n",
       "    'Andrew McCallum'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-4_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1312',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model architectures'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.805.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77445/poster_document/ca3c3eba1d2c3c8811f773babf0287f8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77445/poster/f693371637f01badd41e0752674cdafc.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77445/slideshow/7fdf0937ea83c77c8139d9d5e73ebee7.pdf',\n",
       "   'title': 'Revisiting the Architectures like Pointer Networks to Efficiently Improve the Next Word Distribution, Summarization Factuality, and Beyond',\n",
       "   'tldr': 'Is the output softmax layer, which is adopted by most language models (LMs), always the best way to compute the next word probability? Given so many attention layers in a modern transformer-based LM, are the pointer networks redundant nowadays? In this study, we discover that the answers to both que...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77445,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77445-revisiting-the-architectures-like-pointer-networks-to-efficiently-improve-the-next-word-distribution-summarization-factuality-and-beyond',\n",
       "   'video_url': None},\n",
       "  'P1315': {'abstract': \"Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world's languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages interspersed within a sentence) is much more scarce. Given this resource constraint, the question of how pretraining using limited amounts of code-switched text could be altered to improve performance for code-switched NLP becomes important to tackle. In this paper, we explore different masked language modeling (MLM) pretraining techniques for code-switched text that are cognizant of language boundaries prior to masking. The language identity of the tokens can either come from human annotators, trained language classifiers, or simple relative frequency-based estimates. We also present an MLM variant by introducing a residual connection from an earlier layer in the pretrained model that uniformly boosts performance on downstream tasks. Experiments on two downstream tasks, Question Answering (QA) and Sentiment Analysis (SA), involving four code-switched language pairs (Hindi-English, Spanish-English, Tamil-English, Malayalam-English) yield relative improvements of up to 5.8 and 2.7 F1 scores on QA (Hindi-English) and SA (Tamil-English), respectively, compared to standard pretraining techniques. To understand our task improvements better, we use a series of probes to study what additional information is encoded by our pretraining techniques and also introduce an auxiliary loss function that explicitly models language identification to further aid the residual MLM variants.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.66',\n",
       "   'authors': ['Richeek Das',\n",
       "    'Sahasra Ranjan',\n",
       "    'Shreya Pathak',\n",
       "    'Preethi Jyothi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P1315',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code-switching', 'multilingual pre-training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.66.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76568/poster_document/8b0bf008ac38dd28542caa9b124ba547.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76568/poster/00a07aa1f155a88fe6206102279d5003.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76568/slideshow/320721d4bffd2c2beedf74225d0e5710.pdf',\n",
       "   'title': 'Improving Pretraining Techniques for Code-Switched NLP',\n",
       "   'tldr': \"Pretrained models are a mainstay in modern NLP applications. Pretraining requires access to large volumes of unlabeled text. While monolingual text is readily available for many of the world's languages, access to large quantities of code-switched text (i.e., text with tokens of multiple languages i...\",\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76568,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76568-improving-pretraining-techniques-for-code-switched-nlp',\n",
       "   'video_url': None},\n",
       "  'P1316': {'abstract': 'Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT uses multiple CLS tokens with a parameterization and objective that encourages their diversity. Thus instead of fine-tuning each BERT model in an ensemble (and running them all at test time), we need only fine-tune our single Multi-CLS BERT model (and run the one model at test time, ensembling just the multiple final CLS embeddings). To test its effectiveness, we build Multi-CLS BERT on top of a state-of-the-art pretraining method for BERT (Aroca-Ouellette and Rudzicz, 2020). In experiments on GLUE and SuperGLUE we show that our Multi-CLS BERT reliably improves both overall accuracy and confidence estimation. When only 100 training samples are available in GLUE, the Multi-CLS BERT\\\\_Base model can even outperform the corresponding BERT\\\\_Large model. We analyze the behavior of our Multi-CLS BERT, showing that it has many of the same characteristics and behavior as a typical BERT 5-way ensemble, but with nearly 4-times less computation and memory.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.48',\n",
       "   'authors': ['Haw-Shiuan Chang',\n",
       "    'Ruei-Yao Sun',\n",
       "    'Kathryn Ricci',\n",
       "    'Andrew McCallum'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-3_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P1316',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['self-supervised learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.48.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76569/poster_document/755b584297777b5f555565da8221fde0.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76569/poster/5561d726d0cf2e27e0ede2a0cbbd4723.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76569/slideshow/11e397da963b4b874f52e131759001d5.pdf',\n",
       "   'title': 'Multi-CLS BERT: An Efficient Alternative to Traditional Ensembling',\n",
       "   'tldr': 'Ensembling BERT models often significantly improves accuracy, but at the cost of significantly more computation and memory footprint. In this work, we propose Multi-CLS BERT, a novel ensembling method for CLS-based prediction tasks that is almost as efficient as a single BERT model. Multi-CLS BERT u...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76569,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76569-multi-cls-bert-an-efficient-alternative-to-traditional-ensembling',\n",
       "   'video_url': None},\n",
       "  'P1322': {'abstract': 'Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs.  Experimental results show that none of these techniques can solve the simplest addition induction problem completely.  In the end, we introduce LMs with tutor, which demonstrates every single step of teaching.  LMs with tutor is able to deliver 100\\\\% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.516',\n",
       "   'authors': ['Jing Qian',\n",
       "    'Hong Wang',\n",
       "    'Zekun Li',\n",
       "    'Shiyang Li',\n",
       "    'Xifeng Yan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1322',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.516.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76570/poster_document/9c41752eacf62fc77849655d7116e2e4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76570/poster/583379491ffa6fe6c639e81fbd484641.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76570/slideshow/b68c1f88cf0e2d492071b7a30ea13abb.pptx',\n",
       "   'title': 'Limitations of Language Models in Arithmetic and Symbolic Induction',\n",
       "   'tldr': 'Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76570,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76570-limitations-of-language-models-in-arithmetic-and-symbolic-induction',\n",
       "   'video_url': None},\n",
       "  'P1323': {'abstract': 'We focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous works like DialoGPT, PersonaGPT and other dialog generation models encode two-party dialogues and do not account for the visual information. To the best of our knowledge we are the first to propose the paradigm of multimodal persona based dialogue generation. We contribute a novel dataset, ComSet, consisting of 54K strips, harvested from 13 popular comics available online. Further, we propose a multimodal persona-based architecture, MPDialog, to generate dialogues for the next panel in the strip which decreases the perplexity score by ~10 points over strong dialogue generation baseline models. We demonstrate that there is still ample opportunity for improvement, highlighting the importance of building stronger dialogue systems that are able to generate persona-consistent dialogues and understand the context through various modalities.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.791',\n",
       "   'authors': ['Harsh Agrawal',\n",
       "    'Aditya M. Mishra',\n",
       "    'Manish Gupta',\n",
       "    'Mausam -'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-1_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1323',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-modal dialogue systems', 'conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.791.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76571/poster_document/9a2e62988bbbbdc19c00a5f86b431108.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76571/slideshow/205c2365cd10bf6b22897a14156f1898.pdf',\n",
       "   'title': 'Multimodal Persona Based Generation of Comic Dialogs',\n",
       "   'tldr': 'We focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous wor...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76571,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76571-dubbing-in-practice-a-large-scale-study-of-human-localization-with-insights-for-automatic-dubbing',\n",
       "   'video_url': None},\n",
       "  'P1325': {'abstract': \"There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).\\nHowever, existing methods typically encode task information with a simple dataset name as a prefix to the encoder.\\nThis not only limits the effectiveness of multi-task learning, but also hinders the model's ability to generalize to new domains or tasks that were not seen during training, which is crucial for real-world applications.\\nIn this paper, we propose compositional task configurations, a set of prompts prepended to the encoder to improve cross-task generalization of unified models.\\nWe design the task configurations to explicitly specify the task type, as well as its input and output types.\\nWe show that this not only allows the model to better learn shared knowledge across different tasks at training, but also allows us to control the model by composing new configurations that apply novel input-output combinations in a zero-shot manner.\\nWe demonstrate via experiments over ten table-to-text tasks that our method outperforms the UnifiedSKG baseline by noticeable margins in both in-domain and zero-shot settings, with average improvements of +0.5 and +12.6 from using a T5-large backbone, respectively.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.341',\n",
       "   'authors': ['Jifan Chen',\n",
       "    'Yuhao Zhang',\n",
       "    'Lan Liu',\n",
       "    'Rui Dong',\n",
       "    'Xinchi Chen',\n",
       "    'Patrick Ng',\n",
       "    'William Yang Wang',\n",
       "    'zhiheng huang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1325',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generalization', 'few-shot qa', 'table qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.341.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77447/poster_document/73f9cf3df21fb941f56230f09fea5250.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77447/slideshow/af5fbd986d047381ab302bd95a1f6876.pdf',\n",
       "   'title': 'Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations',\n",
       "   'tldr': 'There has been great progress in unifying various table-to-text tasks using a single encoder-decoder model trained via multi-task learning (Xie et al., 2022).\\nHowever, existing methods typically encode task information with a simple dataset name as a prefix to the encoder.\\nThis not only limits the e...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77447,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77447-improving-cross-task-generalization-of-unified-table-to-text-models-with-compositional-task-configurations',\n",
       "   'video_url': None},\n",
       "  'P1328': {'abstract': 'Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-ofdistribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks, three broadly-applicable modeling interventions (increasing model size, using a different adaptation method, and pre-training on more data), and 14 diverse datasets to investigate the relationship between sample efficiency (amount of data needed to reach a given ID accuracy) and robustness (how models fare on OOD evaluation). We find that higher sample efficiency is only correlated with better average OOD robustness on some modeling interventions and tasks, but not others. On individual datasets, models with lower sample efficiency can even be more robust. These results suggest that general-purpose methods for improving sample efficiency are unlikely to yield universal OOD robustness improvements, since such improvements are highly dataset- and task-dependent. Even in an era of large, multi-purpose pre-trained models, task-specific decisions may often be necessary for OOD generalization.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.144',\n",
       "   'authors': ['Nelson F. Liu', 'Ananya Kumar', 'Percy Liang', 'Robin Jia'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['poster-session-2_-theme_-reality-check-(poster)'],\n",
       "   'id': 'P1328',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['(non-)generalizability'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.144.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76572/poster/0b1cea037f7178c3eaba76bfb8dab2f8.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Are Sample-Efficient NLP Models More Robust?',\n",
       "   'tldr': 'Recent results in image classification and extractive question answering have observed that pre-trained models trained on less in-distribution data have better out-ofdistribution performance. However, it is unclear how broadly these trends hold. We conduct a large empirical study across three tasks,...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 76572,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76572-are-sample-efficient-nlp-models-more-robustquestion',\n",
       "   'video_url': None},\n",
       "  'P1329': {'abstract': 'The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.102',\n",
       "   'authors': ['Ta-Chung Chi',\n",
       "    'Ting-Han Fan',\n",
       "    'Li-Wei Chen',\n",
       "    'alexander rudnicky',\n",
       "    'Peter J Ramadge'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-2_-large-language-models-(poster)'],\n",
       "   'id': 'P1329',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.102.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76573/poster/9be23164ebc2909f6ed24f896f6d2288.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings',\n",
       "   'tldr': 'The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positio...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76573,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76573-latent-positional-information-is-in-the-self-attention-variance-of-transformer-language-models-without-positional-embeddings',\n",
       "   'video_url': None},\n",
       "  'P133': {'abstract': \"Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively decouple semantics of samples, limiting the zero-shot performance of DST. In this paper, we present a simple and effective ``divide, conquer and combine'' solution, which explicitly disentangles the semantics of seen data, and leverages the performance and robustness with the mixture-of-experts mechanism. Specifically, we divide the seen data into semantically independent subsets and train corresponding experts, the newly unseen samples are mapped and inferred with mixture-of-experts with our designed ensemble inference.\\nExtensive experiments on MultiWOZ2.1 upon T5-Adapter show our schema significantly and consistently improves the zero-shot performance, achieving the SOTA on settings without external knowledge, with only 10M trainable parameters.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.114',\n",
       "   'authors': ['Qingyue Wang',\n",
       "    'Liang Ding',\n",
       "    'Yanan Cao',\n",
       "    'Yibing Zhan',\n",
       "    'Zheng Lin',\n",
       "    'Shi Wang',\n",
       "    'Dacheng Tao',\n",
       "    'Li Guo'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-3_-dialogue-and-interactive-systems-(oral)'],\n",
       "   'id': 'P133',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented',\n",
       "    'multilingual / low resource',\n",
       "    'dialogue state tracking'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.114.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76178/poster_document/13945b6521d767f5809704f59f5a3b9b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76178/poster/1d92769ee041ee153545b58c58d98f89.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Divide, Conquer, and Combine: Mixture of Semantic-Independent Experts for Zero-Shot Dialogue State Tracking',\n",
       "   'tldr': 'Zero-shot transfer learning for Dialogue State Tracking (DST) helps to handle a variety of task-oriented dialogue domains without the cost of collecting in-domain data. Existing works mainly study common data- or model-level augmentation methods to enhance the generalization but fail to effectively ...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76178,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15224/lecture/76178-divide-conquer-and-combine-mixture-of-semantic-independent-experts-for-zero-shot-dialogue-state-tracking',\n",
       "   'video_url': None},\n",
       "  'P1333': {'abstract': \"Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore,  these methods hurt the PLMs' performance on downstream tasks. In this study, we propose Gender-tuning, which debiases the PLMs through fine-tuning on downstream tasks' datasets. For this aim, Gender-tuning integrates Masked Language Modeling (MLM) training objectives into fine-tuning's training process. Comprehensive experiments show that Gender-tuning outperforms the state-of-the-art baselines in terms of average gender bias scores in PLMs while improving PLMs' performance on downstream tasks solely using the downstream tasks' dataset. Also, Gender-tuning is a deployable debiasing tool for any PLM that works with original fine-tuning.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.336',\n",
       "   'authors': ['Somayeh Ghanbarzadeh',\n",
       "    'Yan Huang',\n",
       "    'Hamid Palangi',\n",
       "    'Radames Saul Cruz Moreno',\n",
       "    'Hamed Khanpour'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-7_-ethics-and-nlp-(virtual-poster)'],\n",
       "   'id': 'P1333',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/unfairness mitigation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.336.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77448/poster_document/28e1264f034916d23782954078565334.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models',\n",
       "   'tldr': 'Recent studies have revealed that the widely-used Pre-trained Language Models (PLMs) propagate societal biases from the large unmoderated pre-training corpora. Existing solutions require debiasing training processes and datasets for debiasing, which are resource-intensive and costly. Furthermore,  t...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77448,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77448-gender-tuning-empowering-fine-tuning-for-debiasing-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P1340': {'abstract': \"\\\\textit{\\\\textbf{\\\\textcolor{red}{Warning}:} This paper contains content that may be offensive or upsetting.}\\n\\nPretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.\\nApart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models' conversational capabilities.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.757',\n",
       "   'authors': ['Jiaxu Zhao',\n",
       "    'Meng Fang',\n",
       "    'Zijing Shi',\n",
       "    'Yitong Li',\n",
       "    'Ling Chen',\n",
       "    'Mykola Pechenizkiy'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-6_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1340',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['bias/toxicity'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.757.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76574/poster_document/7efd39fd452295354e43ac5fa192edfa.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76574/poster/4aa910f889d274267f13c36f4863e790.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76574/slideshow/8f1f5d54666bb7d1db3a5ff977db1b9b.pptx',\n",
       "   'title': 'CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models',\n",
       "   'tldr': '\\\\textit{\\\\textbf{\\\\textcolor{red}{Warning}:} This paper contains content that may be offensive or upsetting.}\\n\\nPretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in ...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76574,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76574-chbias-bias-evaluation-and-mitigation-of-chinese-conversational-language-models',\n",
       "   'video_url': None},\n",
       "  'P1342': {'abstract': 'Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation history, neglecting the focus can limit performance in inquiry conversation where the order of the focuses plays there a key role. In this paper, we investigate the problem of response generation in inquiry conversation by taking the focus into consideration. We propose a novel Focus-aware Response Generation (FRG) method by jointly optimizing a multi-level encoder and a set of focal decoders to generate several candidate responses that correspond to different focuses. Additionally, a focus ranking module is proposed to predict the next focus and rank the candidate responses. Experiments on two orthogonal inquiry conversation datasets (judicial, medical domain) demonstrate that our method generates results significantly better in automatic metrics and human evaluation compared to the state-of-the-art approaches.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.797',\n",
       "   'authors': ['Yiquan Wu',\n",
       "    'Weiming Lu',\n",
       "    'Yating Zhang',\n",
       "    'Adam Jatowt',\n",
       "    'Jun Feng',\n",
       "    'Changlong Sun',\n",
       "    'Fei Wu',\n",
       "    'Kun Kuang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-4_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1342',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.797.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77449/poster_document/c8f257b2a23af2b8ac4483198d3c2c14.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77449/poster/0082d921a94079d8d39ee6585c839d15.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77449/slideshow/b7e60a6af37a984ff3b26ac3dd7b40f4.pdf',\n",
       "   'title': 'Focus-aware Response Generation in Inquiry Conversation',\n",
       "   'tldr': 'Inquiry conversation is a common form of conversation that aims to complete the investigation (e.g., court hearing, medical consultation and police interrogation) during which a series of focus shifts occurs. While many models have been proposed to generate a smooth response to a given conversation ...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77449,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77449-focus-aware-response-generation-in-inquiry-conversation',\n",
       "   'video_url': None},\n",
       "  'P1345': {'abstract': 'Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines. In this work, we study ambiguities that arise in text-to-image generative models. We curate the Text-to-image Ambiguity Benchmark (TAB) dataset to study different types of ambiguities in text-to-image generative models. We then propose the Text-to-ImagE Disambiguation (TIED) framework to disambiguate the prompts given to the text-to-image generative models by soliciting clarifications from the end user. Through automatic and human evaluations, we show the effectiveness of our framework in generating more faithful images aligned with end user intention in the presence of ambiguities.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.804',\n",
       "   'authors': ['Ninareh Mehrabi',\n",
       "    'Palash Goyal',\n",
       "    'Apurv Verma',\n",
       "    'Jwala Dhamala',\n",
       "    'Varun Kumar',\n",
       "    'Qian Hu',\n",
       "    'Kai-Wei Chang',\n",
       "    'Richard Zemel',\n",
       "    'Aram Galstyan',\n",
       "    'Rahul Gupta'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['poster-session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)'],\n",
       "   'id': 'P1345',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal content generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.804.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76575/poster_document/1b3b9df7e8eb17b7646f0627068d0aea.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76575/poster/ef2702f9f1eff7499b7373ecb5b81f79.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76575/slideshow/dcbe5adc33506960259e417efe4164be.pdf',\n",
       "   'title': 'Resolving Ambiguities in Text-to-Image Generative Models',\n",
       "   'tldr': 'Natural language often contains ambiguities that can lead to misinterpretation and miscommunication. While humans can handle ambiguities effectively by asking clarifying questions and/or relying on contextual cues and common-sense knowledge, resolving ambiguities can be notoriously hard for machines...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76575,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76575-resolving-ambiguities-in-text-to-image-generative-models',\n",
       "   'video_url': None},\n",
       "  'P1348': {'abstract': 'Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.\\nBPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down.\\nWe formalize BPE as a combinatorial optimization problem.\\nVia submodular functions, we prove that the iterative greedy version is a 1/sigma*(1-e\\\\^(-sigma))-approximation of an optimal merge sequence, where sigma is the total backward curvature with respect to the optimal merge sequence.\\nEmpirically the lower bound of the approximation is approx0.37.\\n\\nWe provide a faster implementation of BPE which improves the runtime complexity from O(NM) to O(N log M), where N is the sequence length and M is the merge count.\\nFinally, we optimize the brute-force algorithm for optimal BPE using memoization.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.38',\n",
       "   'authors': ['Vilm Zouhar',\n",
       "    'Clara Meister',\n",
       "    'Juan Luis Gastaldi',\n",
       "    'Li Du',\n",
       "    'Tim Vieira',\n",
       "    'Mrinmaya Sachan',\n",
       "    'Ryan Cotterell'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-4_-machine-translation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1348',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['vocabulary learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.38.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77450/poster_document/08e73809b853942ce77257a52be27bbf.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77450/poster/63c9bc7e69f245c449e5107f8539bede.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77450/slideshow/d04e0ce8204ec1af37cbd73160344b2d.pdf',\n",
       "   'title': 'A Formal Perspective on Byte-Pair Encoding',\n",
       "   'tldr': 'Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method.\\nBPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down.\\nWe formalize BPE a...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77450,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77450-a-formal-perspective-on-byte-pair-encoding',\n",
       "   'video_url': None},\n",
       "  'P1349': {'abstract': 'Subword tokenization is a key part of most NLP pipelines.\\nHowever, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. \\nWe propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.\\nNevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.\\nDefining efficiency in terms of Rnyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.\\nWe posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance.\\nIn machine translation,  we find that across multiple tokenizers, the Rnyi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.284',\n",
       "   'authors': ['Vilm Zouhar',\n",
       "    'Clara Meister',\n",
       "    'Juan Luis Gastaldi',\n",
       "    'Li Du',\n",
       "    'Mrinmaya Sachan',\n",
       "    'Ryan Cotterell'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-7_-machine-translation-(poster)'],\n",
       "   'id': 'P1349',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['vocabulary learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.284.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76576/poster_document/be7cf2a5996186ed2323c6c6d10e4a00.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76576/poster/6aa25f7f6a88c7a3c3c274b7aa0376e3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76576/slideshow/00029f7855e40f162a8db7cc4f21ce78.pdf',\n",
       "   'title': 'Tokenization and the Noiseless Channel',\n",
       "   'tldr': 'Subword tokenization is a key part of most NLP pipelines.\\nHowever, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. \\nWe propose that good tokenizers lead to efficient channel usage, where the channel is the means by w...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76576,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76576-task-optimized-adapters-for-an-end-to-end-task-oriented-dialogue-system',\n",
       "   'video_url': None},\n",
       "  'P1355': {'abstract': \"Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the distribution of generated text only work with quantified distributions, which require pre-defined categories, proportions of the distribution, or an existing corpus following the desired distributions. However, many important distributions, such as personal preferences, are unquantified. In this work, we tackle the problem of generating text following arbitrary distributions (quantified and unquantified) by proposing NANO, a few-shot human-in-the-loop training algorithm that continuously learns from human feedback. NANO achieves state-of-the-art results on single topic/attribute as well as quantified distribution control compared to previous works. We also show that NANO is able to learn unquantified distributions, achieves personalization, and captures differences between different individuals' personal preferences with high sample efficiency.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.758',\n",
       "   'authors': ['Xiang Fan',\n",
       "    'Yiwei Lyu',\n",
       "    'Paul Pu Liang',\n",
       "    'Ruslan Salakhutdinov',\n",
       "    'Louis-Philippe Morency'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-7_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1355',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot generation',\n",
       "    'interactive and collaborative generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.758.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77451/poster_document/0b40e9723cb7b18696cad9e27f235936.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77451/poster/d54db28460aa04ea65c992b4913560be.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77451/slideshow/b1dd32cd7e8c8d9c6aa965cf20237708.pdf',\n",
       "   'title': 'Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control',\n",
       "   'tldr': 'Pretrained language models have demonstrated extraordinary capabilities in language generation. However, real-world tasks often require controlling the distribution of generated text in order to mitigate bias, promote fairness, and achieve personalization. Existing techniques for controlling the dis...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77451,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77451-nano-nested-human-in-the-loop-reward-learning-for-few-shot-language-model-control',\n",
       "   'video_url': None},\n",
       "  'P1362': {'abstract': 'Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with gradient descent. Our theoretical analysis reveals that TOUR can be viewed as a generalization of the classical Rocchio algorithm for pseudo relevance feedback, and we present two variants that leverage pseudo-labels as hard binary or soft continuous labels. We first apply TOUR on phrase retrieval with our proposed phrase re-ranker, and also evaluate its effectiveness on passage retrieval with an off-the-shelf re-ranker. TOUR greatly improves end-to-end open-domain question answering accuracy, as well as passage retrieval performance. TOUR also consistently improves direct re-ranking by up to 2.0\\\\% while running 1.32.4x faster with an efficient implementation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.354',\n",
       "   'authors': ['Mujeen Sung',\n",
       "    'Jungsoo Park',\n",
       "    'Jaewoo Kang',\n",
       "    'Danqi Chen',\n",
       "    'Jinhyuk Lee'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1362',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['open-domain qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.354.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77452/poster_document/ece65ef96f4330317ede438c04668a26.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77452/poster/e4a1905ddc987330bd67d6943323d770.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Optimizing Test-Time Query Representations for Dense Retrieval',\n",
       "   'tldr': 'Recent developments of dense retrieval rely on quality representations of queries and contexts from pre-trained query and context encoders. In this paper, we introduce TOUR (Test-Time Optimization of Query Representations), which further optimizes instance-level query representations guided by signa...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77452,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77452-optimizing-test-time-query-representations-for-dense-retrieval',\n",
       "   'video_url': None},\n",
       "  'P1365': {'abstract': 'Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on various topics. The dataset construction method is discussed, which is applicable to constructing similar resources for related languages. We present comprehensive experiments and analyses of several resource-efficient approaches to QA, including monolingual, cross-lingual, and multilingual setups, along with comparisons against machine-translated silver data. Our strong baseline models reach 76\\\\% in the F1 score, while the estimated human performance is 92\\\\%, indicating that the benchmark presents a good challenge for future work. We make the dataset, models, and leaderboard publicly available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.661',\n",
       "   'authors': ['Fitsum Gaim', 'Wonsuk Yang', 'Hancheol Park', 'Jong Park'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Diversity',\n",
       "   'event_ids': ['session-3_-linguistic-diversity-(oral)'],\n",
       "   'id': 'P1365',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['less-resourced languages'],\n",
       "   'languages': ['tigrinya', 'east african', 'afro-asiatic', 'semitic'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.661.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76225/poster_document/a3f955d0ee2ef736e9fe49afa6a1284b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76225/poster/dff5e35f3d7864963f55f37fbaa4c19f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya',\n",
       "   'tldr': 'Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated data...',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'underline_id': 76225,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15225/lecture/76225-question-answering-in-a-low-resourced-language-benchmark-dataset-and-models-for-tigrinya',\n",
       "   'video_url': None},\n",
       "  'P1368': {'abstract': 'The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.653',\n",
       "   'authors': ['Zheng Xin Yong',\n",
       "    'Hailey Schoelkopf',\n",
       "    'Niklas Muennighoff',\n",
       "    'Alham Fikri Aji',\n",
       "    'David Ifeoluwa Adelani',\n",
       "    'KHALID ALMUBARAK',\n",
       "    'M Saiful Bari',\n",
       "    'Lintang Sutawika',\n",
       "    'Jungo Kasai',\n",
       "    'Ahmed Baruwa',\n",
       "    'Genta Indra Winata',\n",
       "    'Stella Biderman',\n",
       "    'Edward Raff',\n",
       "    'Dragomir Radev',\n",
       "    'Vassilina Nikoulina'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P1368',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.653.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76577/poster_document/e23cd0480a5b11aacb3b9b8a572b7c56.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76577/poster/547e498720f6a579565742f976cdd356.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76577/slideshow/8a9657ed351a0894aa80aa89236a0014.pdf',\n",
       "   'title': 'BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting',\n",
       "   'tldr': 'The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76577,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76577-faithdial-a-faithful-benchmark-for-information-seeking-dialogue',\n",
       "   'video_url': None},\n",
       "  'P1370': {'abstract': 'This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding.\\nThe proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentence embeddings with miCSE entails enforcing the structural consistency across augmented views for every sentence, making contrastive self-supervised learning more sample efficient. As a result, the proposed approach shows strong performance in the few-shot learning domain. While it achieves superior results compared to state-of-the-art methods on multiple benchmarks in few-shot learning, it is comparable in the full-shot scenario. This study opens up avenues for efficient self-supervised learning methods that are more robust than current contrastive methods for sentence embedding.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.339',\n",
       "   'authors': ['Tassilo Klein', 'Moin Nabi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-1_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P1370',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['self-supervised learning',\n",
       "    'representation learning',\n",
       "    'few-shot learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.339.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76578/poster_document/30ce451940054d3ac4f11a4b4c18b14f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76578/poster/f917657572dd32ec4cd389b39b00d4c8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76578/slideshow/f192fe81a5722f08f334461b4d6d8139.pdf',\n",
       "   'title': 'miCSE: Mutual Information Contrastive Learning for Low-shot Sentence Embeddings',\n",
       "   'tldr': 'This paper presents miCSE, a mutual information-based contrastive learning framework that significantly advances the state-of-the-art in few-shot sentence embedding.\\nThe proposed approach imposes alignment between the attention pattern of different views during contrastive learning. Learning sentenc...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76578,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76578-controlling-learned-effects-to-reduce-spurious-correlations-in-text-classifiers',\n",
       "   'video_url': None},\n",
       "  'P1378': {'abstract': 'We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from CITATION and CITATION for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three different Korean word forms: letters, syllables and morphemes. Finally, we discuss and describe future perspectives on Korean morphological paradigms and the dataset.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.414',\n",
       "   'authors': ['Eunkyul Leah Jo',\n",
       "    'Kim Kyuwon',\n",
       "    'Xihan Wu',\n",
       "    'KyungTae Lim',\n",
       "    'Jungyeul Park',\n",
       "    'Chulwoo Park'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-7_-resources-and-evaluation-(virtual-poster)'],\n",
       "   'id': 'P1378',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['corpus creation'],\n",
       "   'languages': ['korean'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.414.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77454/poster_document/aa501cd84c59a94c9c2cc79abeb325e0.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'K-UniMorph: Korean Universal Morphology and its Feature Schema',\n",
       "   'tldr': 'We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that ...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 77454,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77454-k-unimorph-korean-universal-morphology-and-its-feature-schema',\n",
       "   'video_url': None},\n",
       "  'P138': {'abstract': 'Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets.\\nHowever, they are unstable when applied to deep networks.\\nIn this paper, we provide an explanation for the instability of IFs and develop a solution to this problem.\\nWe show that IFs are unreliable when the two data points belong to two different classes.\\nOur solution leverages class information to improve the stability of IFs.\\nExtensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.104',\n",
       "   'authors': ['Thang Nguyen-Duc',\n",
       "    'Hoang Thanh-Tung',\n",
       "    'Quan Hung Tran',\n",
       "    'Dang Huu-Tien',\n",
       "    'Hieu Ngoc Nguyen',\n",
       "    'Anh T. V. Dau',\n",
       "    'Nghi D. Q. Bui'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-4_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P138',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['representation learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.104.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76375/poster_document/e20f72356296d3b2fa6d6411c92c70f9.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76375/slideshow/71841d2d78b0c80cf98c5a93d21a851e.pdf',\n",
       "   'title': 'Class based Influence Functions for Error Detection',\n",
       "   'tldr': 'Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets.\\nHowever, they are unstable when applied to deep networks.\\nIn this paper, we provide an explanation for the instability of IFs and develop a solution to this problem.\\nWe show that IFs are unreliabl...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76375,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76375-class-based-influence-functions-for-error-detection',\n",
       "   'video_url': None},\n",
       "  'P1381': {'abstract': 'Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. \\nHowever, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms.\\nRecently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided.\\nIn this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation.\\nWe present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation.\\nWe use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks.\\nWe show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.\\nWe also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer.\\nWe are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.870',\n",
       "   'authors': ['Cheng-Han Chiang', 'Hung-yi Lee'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-6_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1381',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation of datasets',\n",
       "    'evaluation methodologies',\n",
       "    'evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.870.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76579/poster_document/7084c8cb6419c106ba7e5635982a351d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76579/poster/fbd75257d4d4ee6ab2e55155ac22d460.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76579/slideshow/f0e7b0b65f152835b265c13c1216cb4d.pdf',\n",
       "   'title': 'Can Large Language Models Be an Alternative to Human Evaluations?',\n",
       "   'tldr': 'Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. \\nHowever, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural languag...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76579,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76579-can-large-language-models-be-an-alternative-to-human-evaluationsquestion',\n",
       "   'video_url': None},\n",
       "  'P1388': {'abstract': 'In this paper, we address the challenge of discovering financial signals in narrative financial reports. As these documents are often lengthy and tend to blend routine information with new information, it is challenging for professionals to discern critical financial signals. To this end, we leverage the inherent nature of the year-to-year structure of reports to define a novel signal-highlighting task; more importantly, we propose a compare-and-contrast multistage pipeline that recognizes different relationships between the reports and locates relevant rationales for these relationships. We also create and publicly release a human-annotated dataset for our task. Our experiments on the dataset validate the effectiveness of our pipeline, and we provide detailed analyses and ablation studies to support our findings.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.800',\n",
       "   'authors': ['Jia-Huei Ju',\n",
       "    'Yu-Shiang Huang',\n",
       "    'Cheng-Wei Lin',\n",
       "    'Che Lin',\n",
       "    'Chuan-Ju Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-7_-nlp-applications-(poster)'],\n",
       "   'id': 'P1388',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['financial/business nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.800.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76580/poster_document/471946dd18a87586ce3213a85926f0e7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76580/poster/151874cede8cf5e64f058d2ed0041923.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76580/slideshow/fc7ff1aade3e7c3f349713086a0c7026.pdf',\n",
       "   'title': 'A Compare-and-contrast Multistage Pipeline for Uncovering Financial Signals in Financial Reports',\n",
       "   'tldr': 'In this paper, we address the challenge of discovering financial signals in narrative financial reports. As these documents are often lengthy and tend to blend routine information with new information, it is challenging for professionals to discern critical financial signals. To this end, we leverag...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76580,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76580-a-compare-and-contrast-multistage-pipeline-for-uncovering-financial-signals-in-financial-reports',\n",
       "   'video_url': None},\n",
       "  'P1391': {'abstract': 'We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.382',\n",
       "   'authors': ['Wang-Chiew Tan',\n",
       "    'Yuliang Li',\n",
       "    'Pedro Rodriguez',\n",
       "    'Richard James',\n",
       "    'Xi Victoria Lin',\n",
       "    'Alon Halevy',\n",
       "    'Wen-tau Yih'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-4_-theme_-reality-check-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1391',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['methodology'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.382.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77458/poster_document/4ac0bb9c320f68d92f1ce0bbeb02b185.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Reimagining Retrieval Augmented Language Models for Answering Queries',\n",
       "   'tldr': 'We present a reality check on large language models and inspect the promise of retrieval-augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the pa...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 77458,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77458-reimagining-retrieval-augmented-language-models-for-answering-queries',\n",
       "   'video_url': None},\n",
       "  'P1398': {'abstract': 'Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context.  However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  \\n\\nWe create and publicly release TimelineQA, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA with several state-of-the-art QA models. Our experiments reveal that for atomic queries, an extractive QA system significantly out-performs a state-of-the-art retrieval-augmented QA system. For multi-hop queries involving aggregates, we show that the best result is obtained with a state-of-the-art table QA technique, assuming the ground truth set of episodes for deriving the answer is available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.6',\n",
       "   'authors': ['Wang-Chiew Tan',\n",
       "    'Jane Dwivedi-Yu',\n",
       "    'Yuliang Li',\n",
       "    'Lambert Mathias',\n",
       "    'Marzieh Saeidi',\n",
       "    'Jing Nathan Yan',\n",
       "    'Alon Y. Halevy'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1398',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['commonsense qa', 'multihop qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.6.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TimelineQA: A Benchmark for Question Answering over Timelines',\n",
       "   'tldr': 'Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical re...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 78155,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/78155-timelineqa-a-benchmark-for-question-answering-over-timelines',\n",
       "   'video_url': None},\n",
       "  'P140': {'abstract': 'The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.411',\n",
       "   'authors': ['Daoguang Zan',\n",
       "    'Bei Chen',\n",
       "    'Fengji Zhang',\n",
       "    'Dianjie Lu',\n",
       "    'Bingchao Wu',\n",
       "    'Bei Guan',\n",
       "    'Wang Yongji',\n",
       "    'Jian-Guang LOU'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-1_-theme_-reality-check-(virtual-poster)'],\n",
       "   'id': 'P140',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.411.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76376/poster_document/498cc80fbf0c54659993e57b3ebcc5eb.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76376/slideshow/408c7bfd807af12fa2cab57138b4aded.pdf',\n",
       "   'title': 'Large Language Models Meet NL2Code: A Survey',\n",
       "   'tldr': 'The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Cod...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 76376,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76376-cats-a-pragmatic-chinese-answer-to-sequence-dataset-with-large-scale-and-high-quality',\n",
       "   'video_url': None},\n",
       "  'P1402': {'abstract': \"While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of \\\\{pasted macro `BENCHMARK'\\\\}, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.322',\n",
       "   'authors': ['Derek Tam',\n",
       "    'Anisha Mascarenhas',\n",
       "    'Shiyue Zhang',\n",
       "    'Sarah Kwan',\n",
       "    'Mohit Bansal',\n",
       "    'Colin Raffel'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1402',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.322.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77460/poster_document/1940608b78cfcaf7455cb928d44725a9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77460/poster/44e7086176cd43d083187f9eb2faf945.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77460/slideshow/fd403f4c7f6f85da1f11e2dfe41c1579.pdf',\n",
       "   'title': 'Evaluating the Factual Consistency of Large Language Models Through News Summarization',\n",
       "   'tldr': 'While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB (Factual Inconsistency Benchmark) that ...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77460,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77460-diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-models',\n",
       "   'video_url': None},\n",
       "  'P1411': {'abstract': \"Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tailed label distribution. Recent automated ICD coding efforts improve performance by encoding medical notes and codes with additional data and knowledge bases. However, most of them do not reflect how human coders generate the code: first, the coders select general code categories and then look for specific subcategories that are relevant to a patient's condition. Inspired by this, we propose a two-stage decoding mechanism to predict ICD codes. Our model uses the hierarchical properties of the codes to split the prediction into two steps: At first, we predict the parent code and then predict the child code based on the previous prediction. Experiments on the public MIMIC-III data set have shown that our model performs well in single-model settings without external data or knowledge.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.285',\n",
       "   'authors': ['Thanh-Tung Nguyen',\n",
       "    'Viktor Schlegel',\n",
       "    'Abhinav Ramesh Kashyap',\n",
       "    'Stefan Winkler'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-1_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1411',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['healthcare applications, clincial nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.285.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77463/poster_document/6f055ce7bc934d57a13ca067ebb143b2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77463/poster/507f5b7b44176d2e5827c6b6b55da3af.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77463/slideshow/30fdc6a942a1bd757ae35a554b4eea0c.pdf',\n",
       "   'title': 'A Two-Stage Decoder for Efficient ICD Coding',\n",
       "   'tldr': 'Clinical notes in healthcare facilities are tagged with the International Classification of Diseases (ICD) code; a list of classification codes for medical diagnoses and procedures. ICD coding is a challenging multilabel text classification problem due to noisy clinical document inputs and long-tail...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77463,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77463-a-two-stage-decoder-for-efficient-icd-coding',\n",
       "   'video_url': None},\n",
       "  'P1424': {'abstract': \"Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems' generalizability due to existing datasets' limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates whether systems can correctly understand the effect of incremental changes. Specifically, TODAY introduces slight contextual changes for given event pairs, and systems are asked to tell how this subtle contextual change would affect relevant temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3.5, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY's supervision style and explanation annotations can be used in joint learning, encouraging models to use more appropriate signals during training and thus outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3.5, thus moving us more toward the goal of generic temporal reasoning systems.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.671',\n",
       "   'authors': ['Yu Feng', 'Ben Zhou', 'Haoyu Wang', 'Helen Jin', 'Dan Roth'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P1424',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reasoning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.671.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76581/poster_document/b95996c364f7d4a66b77d795ef65b377.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76581/poster/33286a9e98a517c196fad9aad31926eb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76581/slideshow/b1d9a1bc157009ca4913eb5f243e37ae.pdf',\n",
       "   'title': 'Generic Temporal Reasoning with Differential Analysis and Explanation',\n",
       "   'tldr': \"Temporal reasoning is the task of predicting temporal relations of event pairs. While temporal reasoning models can perform reasonably well on in-domain benchmarks, we have little idea of these systems' generalizability due to existing datasets' limitations. In this work, we introduce a novel task n...\",\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76581,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76581-on-text-based-personality-computing-challenges-and-future-directions',\n",
       "   'video_url': None},\n",
       "  'P1430': {'abstract': 'Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence.\\nThe AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization.\\nIn this paper, we present CoFe, a test suite to investigate in-context compositional generalization.\\nWe find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization.\\nWe study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.\\nFurthermore, two strong limitations are observed: in-context compositional generalization on fictional words is much weaker than that on commonly used ones; it is still critical that the in-context examples should cover required linguistic structures, even though the backbone model has been pre-trained on large corpus.\\nWe hope our analysis would facilitate the understanding and utilization of in-context learning paradigm.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.618',\n",
       "   'authors': ['Shengnan An',\n",
       "    'Zeqi Lin',\n",
       "    'Qiang Fu',\n",
       "    'Bei Chen',\n",
       "    'Nanning Zheng',\n",
       "    'Jian-Guang LOU',\n",
       "    'Dongmei Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-7_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1430',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.618.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76582/poster_document/e90bebc8cd0b96323c7ab08b1cd78f33.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76582/poster/5c5edee106f863728bfd1b065d28eec2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'How Do In-Context Examples Affect Compositional Generalization?',\n",
       "   'tldr': 'Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence.\\nThe AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-con...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76582,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76582-how-do-in-context-examples-affect-compositional-generalizationquestion',\n",
       "   'video_url': None},\n",
       "  'P1433': {'abstract': 'State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation performance. Our method is based on the contrastive learning between clean and noisy sentence pairs. We first introduce noise to the target side of the parallel sentence pair, forming the negative samples. With the original parallel pairs as the positive sample, the QE model is contrastively trained to distinguish the positive samples from the negative ones. This objective is jointly trained with the regression-style quality estimation, so as to prevent the QE model from overfitting to monolingual features. Experiments on WMT QE evaluation datasets demonstrate that our method improves the estimation performance by a large margin while mitigating the bias.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.121',\n",
       "   'authors': ['Hui Huang',\n",
       "    'Shuangzhi Wu',\n",
       "    'Kehai Chen',\n",
       "    'Hui Di',\n",
       "    'Muyun Yang',\n",
       "    'Tiejun Zhao'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-6_-machine-translation-(oral)'],\n",
       "   'id': 'P1433',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.121.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76226/poster/acdd140b88f9d9effb123fabcf9a8b94.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Translation Quality Estimation with Bias Mitigation',\n",
       "   'tldr': 'State-of-the-art translation Quality Estimation (QE) models are proven to be biased. More specifically, they over-rely on monolingual features while ignoring the bilingual semantic alignment. In this work, we propose a novel method to mitigate the bias of the QE model and improve estimation performa...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76226,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15261/lecture/76226-improving-translation-quality-estimation-with-bias-mitigation',\n",
       "   'video_url': None},\n",
       "  'P1436': {'abstract': 'Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty estimation (UE) in the real-world setting, where the data is always noisy and out of distribution. This paper extensively studies UE on the benchmark designed for WSD. Specifically, we first compare four uncertainty scores for a state-of-the-art WSD model and verify that the conventional predictive probabilities obtained at the end of the model are inadequate to quantify uncertainty. Then, we examine the capability of capturing data and model uncertainties by the model with the selected UE score on well-designed test scenarios and discover that the model reflects data uncertainty satisfactorily but underestimates model uncertainty. Furthermore, we explore numerous lexical properties that intrinsically affect data uncertainty and provide a detailed analysis of four critical aspects: the syntactic category, morphology, sense granularity, and semantic relations.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.245',\n",
       "   'authors': ['Zhu Liu', 'Ying Liu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-1_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1436',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['polysemy'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.245.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Ambiguity Meets Uncertainty: Investigating Uncertainty Estimation for Word Sense Disambiguation',\n",
       "   'tldr': 'Word sense disambiguation (WSD), which aims to determine an appropriate sense for a target word given its context, is crucial for natural language understanding. Existing supervised methods treat WSD as a classification task and have achieved remarkable performance. However, they ignore uncertainty ...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77467,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77467-ambiguity-meets-uncertainty-investigating-uncertainty-estimation-for-word-sense-disambiguation',\n",
       "   'video_url': None},\n",
       "  'P1448': {'abstract': 'Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using a raw text corpus. Concretely, pseudo-demonstrations are constructed by (1) finding the nearest neighbors to the test input from the corpus and pairing them with random task labels, and (2) applying a set of techniques to reduce the amount of direct copying the model does from the resulting demonstrations. Evaluation on nine classification datasets shows that Z-ICL outperforms previous zero-shot methods by a significant margin, and is on par with in-context learning with labeled training data in the few-shot setting. Overall, Z-ICL provides a significantly higher estimate of the zero-shot performance levels of a model, and supports future efforts to develop better pseudo-demonstrations that further improve zero-shot results.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.129',\n",
       "   'authors': ['Xinxi Lyu',\n",
       "    'Sewon Min',\n",
       "    'Iz Beltagy',\n",
       "    'Luke Zettlemoyer',\n",
       "    'Hannaneh Hajishirzi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-6_-large-language-models-(poster)'],\n",
       "   'id': 'P1448',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.129.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76583/poster_document/bfdce4acdf0a6a42bf897c14c05f8f0c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76583/poster/eb62cfb0ce8651fe5c36bb0453fd9d3c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76583/slideshow/e13a0ffb4a939d3268752d988cdd005d.pptx',\n",
       "   'title': 'Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations',\n",
       "   'tldr': 'Although large language models can be prompted for both zero- and few-shot learning, performance drops significantly when no demonstrations are available. In this paper, we introduce Z-ICL, a new zero-shot method that closes the gap by constructing pseudo-demonstrations for a given test input using ...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76583,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76583-z-icl-zero-shot-in-context-learning-with-pseudo-demonstrations',\n",
       "   'video_url': None},\n",
       "  'P1449': {'abstract': 'Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA) model to generate many potential translations for each source document. Learning on these wider range translations, an MT model can learn a smoothed distribution, thereby reducing the risk of data sparsity. We demonstrate that the DA model, which estimates the posterior distribution, largely improves the MT performance, outperforming the previous best system by 2.30 s-BLEU on News and achieving new state-of-the-art on News and Europarl benchmarks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.599',\n",
       "   'authors': ['Guangsheng Bao', 'ZHIYANG TENG', 'Yue Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-1_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P1449',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mt theory'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.599.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76584/poster_document/dad497730130fa83f34504d21ac5ee48.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76584/poster/c5663731fcdd5f4d2b9a4e2992e799b4.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76584/slideshow/7f57d7919eb20f7c820ea96d66e62434.pdf',\n",
       "   'title': 'Target-Side Augmentation for Document-Level Machine Translation',\n",
       "   'tldr': 'Document-level machine translation faces the challenge of data sparsity due to its long input length and a small amount of training data, increasing the risk of learning spurious patterns. To address this challenge, we propose a target-side augmentation method, introducing a data augmentation (DA) m...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76584,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76584-target-side-augmentation-for-document-level-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1452': {'abstract': 'Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label generation problem, CF is drastically reduced and the generalizable representations of pre-trained models can be better retained. We thus propose a new CIL method (VAG) that also leverages the sparsity of vocabulary to focus the generation and creates pseudo-replay samples by using label semantics. Experimental results show that VAG outperforms baselines by a large margin.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.109',\n",
       "   'authors': ['Yijia Shao', 'Yiduo Guo', 'Dongyan Zhao', 'Bing Liu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1452',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['continual learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.109.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76585/poster_document/c95cc975f2d788497a04a1e46a5c2712.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76585/poster/3f7b0d8e324eab2a44becdadd3ad939c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Class-Incremental Learning based on Label Generation',\n",
       "   'tldr': 'Despite the great success of pre-trained language models, it is still a challenge to use these models for continual learning, especially for the class-incremental learning (CIL) setting due to catastrophic forgetting (CF). This paper reports our finding that if we formulate CIL as a continual label ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76585,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76585-class-incremental-learning-based-on-label-generation',\n",
       "   'video_url': None},\n",
       "  'P1453': {'abstract': 'In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding opinion expressions, in this paper, we propose Table Filling BERT~(TF-BERT), which considers the consistency of multi-word opinion expressions at the span-level. Specially, we learn the span representations with a table filling method, by constructing an upper triangular table for each sentiment polarity, of which the elements represent the sentiment intensity of the specific sentiment polarity for all spans in the sentence. Two methods are then proposed, including table-decoding and table-aggregation, to filter out target spans or aggregate each table for sentiment polarity classification. In addition, we design a sentiment consistency regularizer to guarantee the sentiment consistency of each span for different sentiment polarities. Experimental results on three benchmarks demonstrate the effectiveness of our proposed model.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.515',\n",
       "   'authors': ['Mao Zhang',\n",
       "    'Yongxin Zhu',\n",
       "    'Zhen Liu',\n",
       "    'Zhimin Bao',\n",
       "    'Yunfei Wu',\n",
       "    'Xing Sun',\n",
       "    'Linli Xu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1453',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['style analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.515.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76586/poster_document/aa5de1520bf7b789089c40e16e1de22e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76586/poster/28b8991b03524af688a892754c9e12c7.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76586/slideshow/778f37fbd15a1fe7088c89a611355909.pdf',\n",
       "   'title': 'Span-level Aspect-based Sentiment Analysis via Table Filling',\n",
       "   'tldr': 'In this paper, we propose a novel span-level model for Aspect-Based Sentiment Analysis (ABSA), which aims at identifying the sentiment polarity of the given aspect. In contrast to conventional ABSA models that focus on modeling the word-level dependencies between an aspect and its corresponding opin...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76586,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76586-span-level-aspect-based-sentiment-analysis-via-table-filling',\n",
       "   'video_url': None},\n",
       "  'P1455': {'abstract': 'A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains.\\nPangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.270',\n",
       "   'authors': ['Yu Gu', 'Xiang Deng', 'Yu Su'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-6_-question-answering-(poster)'],\n",
       "   'id': 'P1455',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge base qa', 'semantic parsing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.270.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76587/poster_document/7f7dc2bfb5d794a5ecc4015e64132430.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76587/poster/df2423ee41a762d278d86514bac6552e.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76587/slideshow/80c66c28499015eb8288a3e6d0833fab.pdf',\n",
       "   'title': \"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments\",\n",
       "   'tldr': 'A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring ...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76587,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76587-don-t-generate-discriminate-a-proposal-for-grounding-language-models-to-real-world-environments',\n",
       "   'video_url': None},\n",
       "  'P1459': {'abstract': 'Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. Our idea is to group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. We theoretically investigate this problem and demonstrate that its upper bound can be reduced to the classic {\\\\it graph k-cut problem}. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the computational cost while preserving performance. The efficiency improvement is more pronounced on larger datasets and on devices with more memory capacity, attesting to its practical utility for large-scale applications.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.208',\n",
       "   'authors': ['Wanyun Cui', 'Xingran Chen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1459',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge-augmented methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.208.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76588/poster_document/ed18229b495904693d164fc87287cc9a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76588/poster/5a009b7922155118a345c8b65b78eb97.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76588/slideshow/a0bd5e718ce61718c0527240e2198bce.pdf',\n",
       "   'title': 'Free Lunch for Efficient Textual Commonsense Integration in Language Models',\n",
       "   'tldr': 'Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP task...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76588,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76588-free-lunch-for-efficient-textual-commonsense-integration-in-language-models',\n",
       "   'video_url': None},\n",
       "  'P1463': {'abstract': 'Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This assumption limits their application to real-world scenarios where a source lexicon may have multiple target constraints, and disambiguation is needed to select the most suitable one. In this paper, we propose disambiguated LCNMT (D-LCNMT) to solve the problem. D-LCNMT is a robust and effective two-stage framework that disambiguates the constraints based on contexts at first, then integrates the disambiguated constraints into LCNMT. Experimental results show that our approach outperforms strong baselines including existing data argumentation based approaches on benchmark datasets, and comprehensive experiments in scenarios where a source lexicon corresponds to multiple target constraints demonstrate the constraint disambiguation superiority of our approach.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.673',\n",
       "   'authors': ['Jinpeng Zhang',\n",
       "    'Nini Xiao',\n",
       "    'Ke Wang',\n",
       "    'Chuanqi Dong',\n",
       "    'Xiangyu Duan',\n",
       "    'Yuqi Zhang',\n",
       "    'Min Zhang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-7_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P1463',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['switch-code translation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.673.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77473/poster_document/fb4b1c1bd9de67953276b3a4febb771d.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Disambiguated Lexically Constrained Neural Machine Translation',\n",
       "   'tldr': 'Lexically constrained neural machine translation (LCNMT), which controls the translation generation with pre-specified constraints, is important in many practical applications. Current approaches to LCNMT typically assume that the pre-specified lexicon constraints are contextually appropriate. This ...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77473,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77473-disambiguated-lexically-constrained-neural-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1464': {'abstract': 'Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, however, either solely focus on language branch, or learn vision-language interaction in a shallow mechanism. In this context, we propose a Deeply coupled Cross-modal Prompt learning (DCP) method based on CLIP. DCP flexibly accommodates the interplay between vision and language with a Cross-Modal Prompt Attention (CMPA) mechanism, which enables the mutual exchange of respective representation through a well-connected multi-head attention progressively and strongly. We then conduct comprehensive few-shot learning experiments on 11 image classification datasets and analyze the robustness to domain shift as well. Thorough experimental analysis evidently demonstrates the superb few-shot generalization and compelling domain adaption capacity of a well-executed DCP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.504',\n",
       "   'authors': ['Xuejing Liu',\n",
       "    'Wei Tang',\n",
       "    'Jinghui Lu',\n",
       "    'Rui Zhao',\n",
       "    'Zhaojun Guo',\n",
       "    'Fei Tan'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['session-1_-speech-and-multimodality-(virtual-poster)'],\n",
       "   'id': 'P1464',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.504.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77474/poster_document/f58ad0d9efec7295a3621e26748f70e3.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77474/poster/06ca68dbecb79c70162bfb6ae8139b6d.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77474/slideshow/76e56204ea828744b3507e287ef470dc.pptx',\n",
       "   'title': 'Deeply Coupled Cross-Modal Prompt Learning',\n",
       "   'tldr': 'Recent advancements in multimodal foundation models (e.g., CLIP) have excelled in zero-shot generalization. Prompt tuning involved in the knowledge transfer from foundation models to downstream tasks has gained significant attention recently. Existing prompt-tuning methods in cross-modal learning, h...',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 77474,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77474-deeply-coupled-cross-modal-prompt-learning',\n",
       "   'video_url': None},\n",
       "  'P1465': {'abstract': 'Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multimodal dataset for modeling persuasion behaviors. Our dataset includes 199 dialogue transcriptions and videos captured in a multi-player social deduction game setting, 26,647 utterance level annotations of persuasion strategy, and game level annotations of deduction game outcomes. We provide extensive experiments to show how dialogue context and visual signals benefit persuasion strategy prediction. We also explore the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes. Our dataset can be found at https://persuasion-deductiongame. socialai-data.org. The codes and models are available at https://github.com/ SALT-NLP/PersuationGames.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.411',\n",
       "   'authors': ['Bolin Lai',\n",
       "    'Hongxin Zhang',\n",
       "    'Miao Liu',\n",
       "    'Aryan J Pariani',\n",
       "    'Fiona Ryan',\n",
       "    'Wenqi Jia',\n",
       "    'Shirley Anugrah Hayati',\n",
       "    'James M Rehg',\n",
       "    'Diyi Yang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['session-1_-speech-and-multimodality-(virtual-poster)'],\n",
       "   'id': 'P1465',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.411.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Werewolf Among Us: Multimodal Resources for Modeling Persuasion Behaviors in Social Deduction Games',\n",
       "   'tldr': 'Persuasion modeling is a key building block for conversational agents. Existing works in this direction are limited to analyzing textual dialogue corpus. We argue that visual signals also play an important role in understanding human persuasive behaviors. In this paper, we introduce the first multim...',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 77475,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77475-werewolf-among-us-multimodal-resources-for-modeling-persuasion-behaviors-in-social-deduction-games',\n",
       "   'video_url': None},\n",
       "  'P1466': {'abstract': 'Incorporating conversational context and knowledge into dialogue generation models has been essential for improving the quality of the generated responses. The context, comprising utterances from previous dialogue exchanges, is used as a source of content for response generation and as a means of selecting external knowledge. However, to avoid introducing irrelevant content, it is key to enable fine-grained scoring of context and knowledge. In this paper, we present a novel approach to context and knowledge weighting as an integral part of model training.\\nWe guide the model training through a Contextual Knowledge Learning (CKL) process which involves Latent Vectors for context and knowledge, respectively. CKL Latent Vectors capture the relationship between context, knowledge, and responses through weak supervision and enable differential weighting of context utterances and knowledge sentences during the training process. Experiments with two standard datasets and human evaluation demonstrate that CKL leads to a significant improvement compared with the performance of six strong baseline models and shows robustness with regard to reduced sizes of training sets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.433',\n",
       "   'authors': ['Wen Zheng', 'Natasa Milic-Frayling', 'Ke Zhou'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-1_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1466',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge augmented', 'conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.433.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76589/poster_document/44d6fbc1b4fb31ce47d53d8e6512cb51.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76589/poster/6f8eabe98ae4946f5e18764991aa1f03.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Contextual Knowledge Learning for Dialogue Generation',\n",
       "   'tldr': 'Incorporating conversational context and knowledge into dialogue generation models has been essential for improving the quality of the generated responses. The context, comprising utterances from previous dialogue exchanges, is used as a source of content for response generation and as a means of se...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76589,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76589-contextual-knowledge-learning-for-dialogue-generation',\n",
       "   'video_url': None},\n",
       "  'P1471': {'abstract': 'Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.827',\n",
       "   'authors': ['Xuekai Zhu', 'Jian Guan', 'Minlie Huang', 'Juan Liu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1471',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['style generation', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.827.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76590/poster_document/c0cbedba19ebd4d3922b5f3ec07352fe.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76590/poster/d9ad6b6317056332797f6ad9a5198388.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing',\n",
       "   'tldr': 'Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more compl...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76590,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76590-storytrans-non-parallel-story-author-style-transfer-with-discourse-representations-and-content-enhancing',\n",
       "   'video_url': None},\n",
       "  'P1475': {'abstract': 'Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated labels present many issues, including unspecific relations and inconsistent annotations. We fix such issues by leveraging more reliable text-label matching criteria to generate high-quality data for persona attribute extraction. We also propose a contrastive learning- and generation-based model with a novel hard negative sampling strategy for generalized zero-shot persona attribute extraction. We benchmark our model with state-of-the-art baselines on our dataset and a public dataset, showing outstanding accuracy gains. Our sampling strategy also exceeds others by a large margin in persona attribute extraction.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.544',\n",
       "   'authors': ['Luyao Zhu',\n",
       "    'Wei Li',\n",
       "    'Rui Mao',\n",
       "    'Vlad Pandelea',\n",
       "    'Erik Cambria'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1475',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument mining', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.544.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76591/poster_document/6deb2c12c082c45e020bfeeee9255c74.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76591/poster/4b3406c9fda03aebe4fd07836772152c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76591/slideshow/19609693e3dfde8bc625c4c051952f32.pdf',\n",
       "   'title': 'PAED: Zero-Shot Persona Attribute Extraction in Dialogues',\n",
       "   'tldr': 'Persona attribute extraction is critical for personalized human-computer interaction. Dialogue is an important medium that communicates and delivers persona information. Although there is a public dataset for triplet-based persona attribute extraction from conversations, its automatically generated ...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76591,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76591-paed-zero-shot-persona-attribute-extraction-in-dialogues',\n",
       "   'video_url': None},\n",
       "  'P1476': {'abstract': 'Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity pair embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two document-level RE datasets, BioRED and Re-DocRED, demonstrate the effectiveness of our method. Particularly, when using 1\\\\% end-task training data, our method outperforms PLM-based RE classifier by 10.5\\\\% and 6.1\\\\% on the two datasets, respectively.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.739',\n",
       "   'authors': ['Wenxuan Zhou',\n",
       "    'Sheng Zhang',\n",
       "    'Tristan Naumann',\n",
       "    'Muhao Chen',\n",
       "    'Hoifung Poon'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['poster-session-1_-information-extraction-(poster)'],\n",
       "   'id': 'P1476',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['document-level extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.739.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76592/poster_document/b7ebd7fba2ebea7db44f80825c5d8f34.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76592/poster/ba3314e07ce5ba8c768d1ab961207436.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76592/slideshow/af6a96317e4c9226fa80e5d748935c95.pdf',\n",
       "   'title': 'Continual Contrastive Finetuning Improves Low-Resource Relation Extraction',\n",
       "   'tldr': 'Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the entity pair...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76592,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76592-continual-contrastive-finetuning-improves-low-resource-relation-extraction',\n",
       "   'video_url': None},\n",
       "  'P1477': {'abstract': \"Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling. We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20\\\\%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.714',\n",
       "   'authors': ['Fangyu Liu',\n",
       "    'Francesco Piccinno',\n",
       "    'Syrine Krichene',\n",
       "    'Chenxi Pang',\n",
       "    'Kenton Lee',\n",
       "    'Mandar Joshi',\n",
       "    'Yasemin Altun',\n",
       "    'Nigel Collier',\n",
       "    'Julian Martin Eisenschlos'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-2_-question-answering-(poster)'],\n",
       "   'id': 'P1477',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodal qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.714.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76593/poster_document/d785934d0c918ef7e6ab89ec8568bd43.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76593/poster/5e75c496f52ad9a06f6bbf9a0476df7b.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76593/slideshow/91ab9bd7c099540170c9e6fc88b1bcae.pdf',\n",
       "   'title': 'MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering',\n",
       "   'tldr': \"Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in ...\",\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76593,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76593-enhancing-unsupervised-semantic-parsing-with-distributed-contextual-representations',\n",
       "   'video_url': None},\n",
       "  'P1479': {'abstract': 'Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. However, prior works often conduct constant computation efforts for each proving state while ignoring that the hard states often need more exploration than easy states. Moreover, they evaluate and guide the proof search solely depending on the current proof state instead of considering the whole proof trajectory as human reasoning does. Here, to accommodate general theorems, we propose a novel Dynamic-Tree Driven Theorem Solver (DT-Solver) by guiding the search procedure with state confidence and proof-level values. Specifically, DT-Solver introduces a dynamic-tree Monte-Carlo search algorithm, which dynamically allocates computing budgets for different state confidences, guided by a new proof-level value function to discover proof states that require substantial exploration.\\nExperiments on two popular theorem-proving datasets, PISA and Mathlib, show significant performance gains by our DT-Solver over the state-of-the-art approaches, with a 6.65\\\\% improvement on average in terms of success rate. And especially under low computing resource settings (11.03\\\\% improvement on average).',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.706',\n",
       "   'authors': ['Haiming Wang',\n",
       "    'Ye Yuan',\n",
       "    'Zhengying Liu',\n",
       "    'Jianhao Shen',\n",
       "    'Yichun Yin',\n",
       "    'Jing Xiong',\n",
       "    'Enze Xie',\n",
       "    'Han Shi',\n",
       "    'Yujun Li',\n",
       "    'lin li',\n",
       "    'Jian Yin',\n",
       "    'Zhenguo Li',\n",
       "    'Xiaodan Liang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-7_-nlp-applications-(poster)'],\n",
       "   'id': 'P1479',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mathematical nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.706.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76594/poster_document/c2caf989f02a9c7f03340ad5504c4ff4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76594/poster/2afb53500edaf4fe7027357be901a7ee.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76594/slideshow/aaa045bfb69630f870782997b9e578e4.pptx',\n",
       "   'title': 'DT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function',\n",
       "   'tldr': 'Recent advances in neural theorem-proving resort to large language models and tree searches. When proving a theorem, a language model advises single-step actions based on the current proving state and the tree search finds a sequence of correct steps using actions given by the language model. Howeve...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76594,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76594-dt-solver-automated-theorem-proving-with-dynamic-tree-sampling-guided-by-proof-level-value-function',\n",
       "   'video_url': None},\n",
       "  'P1482': {'abstract': \"Audio-visual text generation aims to understand multi-modality contents and translate them into texts. Although various transfer learning techniques of text generation have been proposed, they focused on uni-modal analysis (e.g. text-to-text, visual-to-text) and lack consideration of multi-modal content and cross-modal relation. Motivated by the fact that humans can recognize the timbre of the same low-level concepts (e.g., footstep, rainfall, and laughing), even in different visual conditions, we aim to mitigate the domain discrepancies by audio-visual correlation.\\nIn this paper, we propose a novel Transferable Audio-Visual Text Generation framework, named TAVT, which consists of two key components: Audio-Visual Meta-Mapper (AVMM) and Dual Counterfactual Contrastive Learning (DCCL). (1) AVMM first introduces a universal auditory semantic space and drifts the domain-invariant low-level concepts into visual prefixes. Then the reconstruct-based learning encourages the AVMM to learn ``which pixels belong to the same sound'' and achieve audio-enhanced visual prefix. The well-trained AVMM can be further applied to uni-modal setting. (2) Furthermore, DCCL leverages the destructive counterfactual transformations to provide cross-modal constraints for AVMM from the perspective of feature distribution and text generation. (3) The experimental results show that TAVT outperforms the state-of-the-art methods across multiple domains (cross-datasets, cross-categories) and various modal settings (uni-modal, multi-modal).\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.836',\n",
       "   'authors': ['Wang Lin',\n",
       "    'Tao Jin',\n",
       "    'Wenwen Pan',\n",
       "    'Linjun Li',\n",
       "    'Xize Cheng',\n",
       "    'Ye Wang',\n",
       "    'Zhou Zhao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)'],\n",
       "   'id': 'P1482',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.836.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76595/poster_document/f8d5819c4ad05e708e6ef55d962b0837.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76595/poster/283c9556b8c0f2a2f4e4494adbb2f21a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TAVT: Towards Transferable Audio-Visual Text Generation',\n",
       "   'tldr': 'Audio-visual text generation aims to understand multi-modality contents and translate them into texts. Although various transfer learning techniques of text generation have been proposed, they focused on uni-modal analysis (e.g. text-to-text, visual-to-text) and lack consideration of multi-modal con...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76595,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76595-tavt-towards-transferable-audio-visual-text-generation',\n",
       "   'video_url': None},\n",
       "  'P1495': {'abstract': 'In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the desired attribute. Experimental results show that FPT can achieve better control accuracy and text fluency than baseline models in single-attribute control tasks. In multi-attribute control tasks, FPT achieves comparable control accuracy with the state-of-the-art approach while keeping the flexibility to control new attributes without retraining existing models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.96',\n",
       "   'authors': ['Congda Ma',\n",
       "    'Tianyu Zhao',\n",
       "    'Makoto Shing',\n",
       "    'Kei Sawada',\n",
       "    'Manabu Okumura'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)'],\n",
       "   'id': 'P1495',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['efficient models'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.96.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76596/poster_document/615056ca739bcd3c77765a0a2b82a444.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76596/poster/7b515a46237b904c7ef5fd6fb146d4bb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Focused Prefix Tuning for Controllable Text Generation',\n",
       "   'tldr': 'In a controllable text generation dataset, there exist unannotated attributes that could provide irrelevant learning signals to models that use it for training and thus degrade their performance. We propose focused prefix tuning (FPT) to mitigate the problem and to enable the control to focus on the...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76596,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76596-focused-prefix-tuning-for-controllable-text-generation',\n",
       "   'video_url': None},\n",
       "  'P1498': {'abstract': 'Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Considering that such tasks suffer from high annotation costs, we investigate zero-shot and few-shot scenarios. Based on an existing syntactic treebank, we adopt a signal-based method to transform seen syntactic dependencies into unseen ones between elementary discourse units (EDUs), where the signals are detected by masked language modeling. Besides, we apply single-view and multi-view data selection to access reliable pseudo-labeled instances. Experimental results show the effectiveness of these baselines. Moreover, we discuss several crucial points about our dataset and approach.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.607',\n",
       "   'authors': ['Gongyao Jiang', 'Shuang Liu', 'Meishan Zhang', 'Min Zhang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'event_ids': ['session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)'],\n",
       "   'id': 'P1498',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['dependency parsing'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.607.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77480/poster_document/f43ed1f947363d9ff71d2119c810efac.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Pilot Study on Dialogue-Level Dependency Parsing for Chinese',\n",
       "   'tldr': 'Dialogue-level dependency parsing has received insufficient attention, especially for Chinese. To this end, we draw on ideas from syntactic dependency and rhetorical structure theory (RST), developing a high-quality human-annotated corpus, which contains 850 dialogues and 199,803 dependencies. Consi...',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'underline_id': 77480,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77480-a-pilot-study-on-dialogue-level-dependency-parsing-for-chinese',\n",
       "   'video_url': None},\n",
       "  'P1500': {'abstract': 'Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of selecting the best output from these stochastic samples. As a case study framed in the context of question generation, we propose two prompt-based approaches, namely round-trip and prompt-based score, to selecting high-quality questions from a set of LLM-generated candidates. Our method works without the need to modify the underlying model,  nor does it rely on human-annotated references --- both of which are realistic constraints for real-world deployment of LLMs. With automatic as well as human evaluations, we empirically demonstrate that our approach can effectively select questions of higher qualities than greedy generation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.820',\n",
       "   'authors': ['Xingdi Yuan',\n",
       "    'Tong Wang',\n",
       "    'Yen-Hsiang Wang',\n",
       "    'Emery Fine',\n",
       "    'Rania Abdelghani',\n",
       "    'Hlne Sauzon',\n",
       "    'Pierre-Yves Oudeyer'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1500',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.820.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77481/poster_document/ec5346bde36a033a793256fa873a9880.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77481/poster/844bbda65770611bec001be461d21233.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77481/slideshow/2ed632a068029e987781d9ab1f4530cf.pdf',\n",
       "   'title': 'Selecting Better Samples from Pre-trained LLMs: A Case Study on Question Generation',\n",
       "   'tldr': 'Large Language Models (LLMs) have in recent years demonstrated impressive prowess in natural language generation. A common practice to improve generation diversity is to sample multiple outputs from the model. However, partly due to the inaccessibility of LLMs, there lacks a simple and robust way of...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77481,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77481-selecting-better-samples-from-pre-trained-llms-a-case-study-on-question-generation',\n",
       "   'video_url': None},\n",
       "  'P1504': {'abstract': \"Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., \\\\textit{question}), which ignore the effect on ES and lack explicit goals to guide emotional positive transition. To this end, we introduce a new paradigm to formalize multi-turn ESC as a process of positive emotion elicitation. Addressing this task requires finely adjusting the elicitation intensity in ES as the conversation progresses while maintaining conversational goals like coherence. In this paper, we propose \\\\textsc{Supporter}, a mixture-of-expert-based reinforcement learning model, and well design ES and dialogue coherence rewards to guide policy's learning for responding. Experiments verify the superiority of \\\\textsc{Supporter} in achieving positive emotion elicitation during responding while maintaining conversational goals including coherence.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.96',\n",
       "   'authors': ['Jinfeng Zhou', 'Zhuang Chen', 'Bo Wang', 'Minlie Huang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1504',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['human-in-the-loop',\n",
       "    'grounded dialog',\n",
       "    'conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.96.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76597/poster_document/1c07bb009e5a83b9b05d9ad0e6e6a711.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76597/poster/2e4baf5420cf5413a7d89ae4dc1e6900.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76597/slideshow/250bbf427e0d85b78f0b6f8ab6926dc8.pdf',\n",
       "   'title': 'Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach',\n",
       "   'tldr': \"Emotional support conversation (ESC) aims to provide emotional support (ES) to improve one's mental state. Existing works stay at fitting grounded responses and responding strategies (e.g., \\\\textit{question}), which ignore the effect on ES and lack explicit goals to guide emotional positive transiti...\",\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76597,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76597-facilitating-multi-turn-emotional-support-conversation-with-positive-emotion-elicitation-a-reinforcement-learning-approach',\n",
       "   'video_url': None},\n",
       "  'P1509': {'abstract': 'Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data and identifying adversarial ones as outliers. However, these methods suffer from significant performance degradation when the adversarial samples lie close to the non-adversarial data manifold. To address this limitation, we propose a score-based generative method to implicitly model the data distribution. Our approach utilizes the gradient of the log-density data distribution and calculates the distribution gap between adversarial and normal samples through multi-step iterations using Langevin dynamics. In addition, we use supervised contrastive learning to guide the gradient estimation using label information, which avoids collapsing to a single data manifold and better preserves the anisotropy of the different labeled data distributions. Experimental results on three text classification tasks upon four advanced attack algorithms show that our approach is a significant improvement (average +15.2 F1 score against previous SOTA) over previous detection methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.40',\n",
       "   'authors': ['Rong Bao',\n",
       "    'Rui Zheng',\n",
       "    'Liang Ding',\n",
       "    'Qi Zhang',\n",
       "    'Dacheng Tao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1509',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['adversarial attacks/examples/training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.40.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76598/poster_document/ff3d94e693b5a823ee9045c9da787845.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76598/poster/a24bde79c15ba967a9e333d64345e609.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CASN:Class-Aware Score Network for Textual Adversarial Detection',\n",
       "   'tldr': 'Adversarial detection aims to detect adversarial samples that threaten the security of deep neural networks, which is an essential step toward building robust AI systems. Density-based estimation is widely considered as an effective technique by explicitly modeling the distribution of normal data an...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76598,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76598-casnclass-aware-score-network-for-textual-adversarial-detection',\n",
       "   'video_url': None},\n",
       "  'P1513': {'abstract': 'Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curve that has lacked at the adapter learning and enabling the natural and consistent response generation that is appropriate for the goal. Our method is a model-agnostic approach and does not require prompt-tuning as only input data without a prompt. As results of the experiment, our method shows competitive performance on the MultiWOZ benchmark compared to the existing end-to-end models. In particular, we attain state-of-the-art performance on the DST task of 2.2 dataset.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.464',\n",
       "   'authors': ['Namo Bang', 'Jeehyun Lee', 'Myoung-Wan Koo'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1513',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.464.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77483/poster/9854eba389cfb341aab3ff10f13f4d99.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77483/slideshow/054c5367cd9c6919c5086f4cb3cfd448.pdf',\n",
       "   'title': 'Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System',\n",
       "   'tldr': 'Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the convers...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77483,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77483-detoxifying-text-with-marco-controllable-revision-with-experts-and-anti-experts',\n",
       "   'video_url': None},\n",
       "  'P1516': {'abstract': 'A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we\\nevaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.377',\n",
       "   'authors': ['Rongxin Zhu', 'Jianzhong Qi', 'Jey Han Lau'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-2_-summarization-(poster)'],\n",
       "   'id': 'P1516',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation',\n",
       "    'conversational summarization',\n",
       "    'factuality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.377.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76599/poster_document/eebe2604a48a838226046efd58dd623b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76599/poster/3c3e7c9544234a1bcd26dfd72bebf2dd.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76599/slideshow/cf5c72d409f7affd53ff6410713d84a3.pdf',\n",
       "   'title': 'Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization',\n",
       "   'tldr': 'A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fi...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76599,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76599-annotating-and-detecting-fine-grained-factual-errors-for-dialogue-summarization',\n",
       "   'video_url': None},\n",
       "  'P1518': {'abstract': 'Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-translation), usually suffer from the lack of syntactic diversity -- the generated paraphrase sentences are very similar to the source sentences in terms of syntax. In this work, we present ParaAMR, a large-scale syntactically diverse paraphrase dataset created by abstract meaning representation back-translation. Our quantitative analysis, qualitative examples, and human evaluation demonstrate that the paraphrases of ParaAMR are syntactically more diverse compared to existing large-scale paraphrase datasets while preserving good semantic similarity. In addition, we show that ParaAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled paraphrase generation, and data augmentation for few-shot learning. Our results thus showcase the potential of ParaAMR for improving various NLP applications.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.447',\n",
       "   'authors': ['Kuan-Hao Huang',\n",
       "    'Varun Iyer',\n",
       "    'I-Hung Hsu',\n",
       "    'Anoop Kumar',\n",
       "    'Kai-Wei Chang',\n",
       "    'Aram Galstyan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-3_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P1518',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['paraphrase generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.447.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76600/poster_document/fc599e0a521d6403f15b7a2b5838cb7f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76600/poster/f79ef9b9518a150b06b2165c8dbf3ce8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ParaAMR: A Large-Scale Syntactically Diverse Paraphrase Dataset by AMR Back-Translation',\n",
       "   'tldr': 'Paraphrase generation is a long-standing task in natural language processing (NLP). Supervised paraphrase generation models, which rely on human-annotated paraphrase pairs, are cost-inefficient and hard to scale up. On the other hand, automatically annotated paraphrase pairs (e.g., by machine back-t...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76600,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76600-paraamr-a-large-scale-syntactically-diverse-paraphrase-dataset-by-amr-back-translation',\n",
       "   'video_url': None},\n",
       "  'P1520': {'abstract': 'This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies.\\nOur model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism.\\nAdditionally, we introduce citation regularization, which induces attention weights to represent citation relations by modeling text and citations jointly.\\nOur model outperforms a prior dynamic embedded topic model regarding perplexity and coherence, while maintaining sufficient diversity across topics.\\nFurthermore, we confirm that our model can potentially predict emerging topics from academic literature.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.366',\n",
       "   'authors': ['Nozomu Miyamoto',\n",
       "    'Masaru Isonuma',\n",
       "    'Sho Takase',\n",
       "    'Junichiro Mori',\n",
       "    'Ichiro Sakata'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-1_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1520',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.366.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77484/poster_document/3fccb1d729afcfc619645f1f1fdb5546.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77484/slideshow/893a66f11f83065eb006145ade9f936d.pdf',\n",
       "   'title': 'Dynamic Structured Neural Topic Model with Self-Attention Mechanism',\n",
       "   'tldr': 'This study presents a dynamic structured neural topic model, which can handle the time-series development of topics while capturing their dependencies.\\nOur model captures the topic branching and merging processes by modeling topic dependencies based on a self-attention mechanism.\\nAdditionally, we in...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 77484,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77484-dynamic-structured-neural-topic-model-with-self-attention-mechanism',\n",
       "   'video_url': None},\n",
       "  'P1530': {'abstract': \"Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. However, we identify issues with the dataset quality and evaluation metric. For the dataset, we categorize 3 types of problems on the procedure level, step level and state change level respectively, and build a clean dataset OpenPI-C using multiple rounds of human judgment. For the evaluation metric, we propose a cluster-based metric to fix the original metric's preference for repetition.\\n\\nModel-wise, we enhance the seq2seq generation baseline by reinstating two key properties for state tracking: temporal dependency and entity awareness. The state of the world after an action is inherently dependent on the previous state. We model this dependency through a dynamic memory bank and allow the model to attend to the memory slots during decoding. On the other hand, the state of the world is naturally a union of the states of involved entities. Since the entities are unknown in the open-vocabulary setting, we propose a two-stage model that refines the state change prediction conditioned on entities predicted from the first stage. Empirical results show the effectiveness of our proposed model, especially on the cleaned dataset and the cluster-based metric. The code and data are released at https://github.com/shirley-wu/openpi-c\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.452',\n",
       "   'authors': ['Xueqing Wu', 'Sha Li', 'Heng Ji'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-1_-resources-and-evaluation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1530',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['benchmarking', 'nlp datasets', 'metrics'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.452.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77487/poster_document/230bfa3a997136d079ef4916c99c4bb0.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77487/slideshow/8c2f48c2823d0424b87e05ab7005c40d.pptx',\n",
       "   'title': 'OpenPI-C: A Better Benchmark and Stronger Baseline for Open-Vocabulary State Tracking',\n",
       "   'tldr': 'Open-vocabulary state tracking is a more practical version of state tracking that aims to track state changes of entities throughout a process without restricting the state space and entity space. OpenPI (Tandon et al., 2020) is to date the only dataset annotated for open-vocabulary state tracking. ...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 77487,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77487-openpi-c-a-better-benchmark-and-stronger-baseline-for-open-vocabulary-state-tracking',\n",
       "   'video_url': None},\n",
       "  'P1538': {'abstract': 'Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks.\\nAlthough various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalization ability.\\nThis work presents instruction optimization, which optimizes training instructions with respect to generalization ability.\\nRather than manually tuning instructions, we introduce learnable instructions and optimize them with gradient descent by leveraging bilevel optimization.\\nExperimental results show that the learned instruction enhances the diversity of instructions and improves the generalization ability compared to using only manually created instructions.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.667',\n",
       "   'authors': ['Masaru Isonuma', 'Junichiro Mori', 'Ichiro Sakata'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-7_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1538',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot generation', 'text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.667.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77489/poster_document/13c608f7fcfad8ae1ed4d6d13305d602.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77489/slideshow/fb066108892654c9c2f5893954faa53b.pdf',\n",
       "   'title': 'Differentiable Instruction Optimization for Cross-Task Generalization',\n",
       "   'tldr': 'Instruction tuning has been attracting much attention to achieve generalization ability across a wide variety of tasks.\\nAlthough various types of instructions have been manually created for instruction tuning, it is still unclear what kind of instruction is optimal to obtain cross-task generalizatio...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77489,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77489-differentiable-instruction-optimization-for-cross-task-generalization',\n",
       "   'video_url': None},\n",
       "  'P1541': {'abstract': 'We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmarks IE datasets with the supervised setting. The state-of-the-art performance in low-resource scenarios also verifies the transferability and effectiveness of UniEX.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.907',\n",
       "   'authors': ['yang ping',\n",
       "    'JunYu Lu',\n",
       "    'ruyi gan',\n",
       "    'Junjie Wang',\n",
       "    'Yuxiang Zhang',\n",
       "    'Pingjian Zhang',\n",
       "    'Jiaxing Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-1_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P1541',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['open information extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.907.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76601/poster_document/606eb197edcf5d30f338fccf1821622d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76601/poster/5b353a99d834ce9f238212b38a0aaf66.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76601/slideshow/141833101907abbf927a216989c62fba.pptx',\n",
       "   'title': 'UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective',\n",
       "   'tldr': 'We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the tok...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76601,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76601-transforming-visual-scene-graphs-to-image-captions',\n",
       "   'video_url': None},\n",
       "  'P1542': {'abstract': 'In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. \\nIn this work, we aim to improve self-training for cross-lingual NER by combining representation learning and pseudo label refinement in one coherent framework.\\nOur proposed method, namely ContProto mainly comprises two components: (1) contrastive self-training and (2) prototype-based pseudo-labeling. \\nOur contrastive self-training facilitates span classification by separating clusters of different classes, and enhances cross-lingual transferability by producing closely-aligned representations between the source and target language. \\nMeanwhile, prototype-based pseudo-labeling effectively improves the accuracy of pseudo labels during training. \\nWe evaluate ContProto on multiple transfer pairs, and experimental results show our method brings substantial improvements over current state-of-the-art methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.222',\n",
       "   'authors': ['Ran Zhou',\n",
       "    'Xin Li',\n",
       "    'Lidong Bing',\n",
       "    'Erik Cambria',\n",
       "    'Chunyan Miao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P1542',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.222.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76602/poster_document/bbf6ac81caae5fad51bfbb1c1756883a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76602/poster/837313eb58c76690dd7980fa40310dad.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning',\n",
       "   'tldr': 'In cross-lingual named entity recognition (NER), self-training is commonly used to bridge the linguistic gap by training on pseudo-labeled target-language data. However, due to sub-optimal performance on target languages, the pseudo labels are often noisy and limit the overall performance. \\nIn this ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76602,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76602-improving-self-training-for-cross-lingual-named-entity-recognition-with-contrastive-and-prototype-learning',\n",
       "   'video_url': None},\n",
       "  'P1545': {'abstract': 'We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying  pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using Real Toxicity Prompts. Our experiments show that CFL  achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.720',\n",
       "   'authors': ['Rahul Madhavan',\n",
       "    'Rishabh Garg',\n",
       "    'Kahini Wadhawan',\n",
       "    'Sameep Mehta'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1545',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['causality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.720.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77490/poster_document/786b111a42b231256062294baf2220f4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77490/poster/7a66ab7105aaa84932cd799ceb35bf99.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77490/slideshow/64ec7705301aa8c5fa316eadf31faaab.pdf',\n",
       "   'title': 'CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation',\n",
       "   'tldr': 'We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architectu...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77490,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77490-seti-systematicity-evaluation-of-textual-inference',\n",
       "   'video_url': None},\n",
       "  'P1550': {'abstract': 'Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios.\\nTypically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.\\nHowever, when applied to zero-shot entity and relation extraction, vanilla prompt-based methods may struggle with the limited coverage of verbalizers to labels and the slow inference speed.\\nIn this work, we propose a novel Discriminate Soft Prompts (DSP) approach to take advantage of the prompt-based methods to strengthen the transmission of general knowledge.\\nSpecifically, we develop a discriminative prompt method, which reformulates zero-shot tasks into token discrimination tasks without having to construct verbalizers.\\nFurthermore, to improve the inference speed of the prompt-based methods, we design a soft prompt co-reference strategy, which leverages soft prompts to approximately refer to the vector representation of text tokens.\\nThe experimental results show that, our model outperforms baselines on two zero-shot entity recognition datasets with higher inference speed, and obtains a 7.5\\\\% average relation F1-score improvement over previous state-of-the-art models on Wiki-ZSL and FewRel.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.339',\n",
       "   'authors': ['Bo Lv',\n",
       "    'Xin Liu',\n",
       "    'Shaojie Dai',\n",
       "    'Nayu Liu',\n",
       "    'Fan Yang',\n",
       "    'Ping Luo',\n",
       "    'Yue Yu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-4_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P1550',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['zero/few-shot extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.339.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DSP: Discriminative Soft Prompts for Zero-Shot Entity and Relation Extraction',\n",
       "   'tldr': 'Prompt-based methods have shown their efficacy in transferring general knowledge within pre-trained language models (PLMs) for low-resource scenarios.\\nTypically, prompt-based methods convert downstream tasks to cloze-style problems and map all labels to verbalizers.\\nHowever, when applied to zero-sho...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 77493,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77493-dsp-discriminative-soft-prompts-for-zero-shot-entity-and-relation-extraction',\n",
       "   'video_url': None},\n",
       "  'P1555': {'abstract': 'Social media is one of the most highly sought resources for analyzing characteristics of the language by its users. In particular, many researchers utilized various linguistic features of mental health problems from social media. However, existing approaches to detecting mental disorders face critical challenges, such as the scarcity of high-quality data or the trade-off between addressing the complexity of models and presenting interpretable results grounded in expert domain knowledge. To address these challenges, we design a simple but flexible model that preserves domain-based interpretability. We propose a novel approach that captures the semantic meanings directly from the text and compares them to symptom-related descriptions. Experimental results demonstrate that our model outperforms relevant baselines on various mental disorder detection tasks. Our detailed analysis shows that the proposed model is effective at leveraging domain knowledge, transferable to other mental disorders, and providing interpretable detection results.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.681',\n",
       "   'authors': ['Hoyun Song', 'Jisu Shin', 'Huije Lee', 'Jong Park'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['poster-session-6_-computational-social-science-and-cultural-analytics-(poster)'],\n",
       "   'id': 'P1555',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['psycho-demographic trait prediction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.681.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76603/poster_document/b74f9753038f7df293393e9c6e0f6c56.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76603/poster/51b8e4ae523cd2d5a3694174b54ee3ea.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Simple and Flexible Modeling for Mental Disorder Detection by Learning from Clinical Questionnaires',\n",
       "   'tldr': 'Social media is one of the most highly sought resources for analyzing characteristics of the language by its users. In particular, many researchers utilized various linguistic features of mental health problems from social media. However, existing approaches to detecting mental disorders face critic...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76603,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76603-the-cringe-loss-learning-what-language-not-to-model',\n",
       "   'video_url': None},\n",
       "  'P1569': {'abstract': 'The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly due to limited datasets. Existing datasets suffer from a lack of fine-grained annotations, such as the toxic type and expressions with indirect toxicity. These fine-grained annotations are crucial factors for accurately detecting the toxicity of posts involved with lexical knowledge, which has been a challenge for researchers. To tackle this problem, we facilitate the fine-grained detection of Chinese toxic language by building a new dataset with benchmark results. First, we devised Monitor Toxic Frame, a hierarchical taxonomy to analyze the toxic type and expressions. Then, we built a fine-grained dataset ToxiCN, including both direct and indirect toxic samples. ToxiCN is based on an insulting vocabulary containing implicit profanity. We further propose a benchmark model, Toxic Knowledge Enhancement (TKE), by incorporating lexical features to detect toxic language. We demonstrate the usability of ToxiCN and the effectiveness of TKE based on a systematic quantitative and qualitative analysis.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.898',\n",
       "   'authors': ['Junyu Lu',\n",
       "    'Bo Xu',\n",
       "    'Xiaokun Zhang',\n",
       "    'Changrong Min',\n",
       "    'Liang Yang',\n",
       "    'Hongfei LIN'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-4_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1569',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['language resources'],\n",
       "   'languages': ['simplified chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.898.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76604/poster_document/1a4a6a033d532ac3bddfe6dd6713a35f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76604/poster/a1f6afc2e662a3a0567a8a9488edfae1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks',\n",
       "   'tldr': 'The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly due to limited datasets. Existing datasets suffer from a lack of fine-grained annotations, such as the toxic type and expressions ...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76604,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76604-organ-observation-guided-radiology-report-generation-via-tree-reasoning',\n",
       "   'video_url': None},\n",
       "  'P157': {'abstract': \"Learning multiscale Transformer models has been evidenced as a viable approach to  augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into multiscale Transformer has not yet been explored. In this work, we present a \\\\textbf{S}low-\\\\textbf{F}ast two-stream learning model, referred to as Tran\\\\textbf{SF}ormer, which utilizes a ``slow'' branch to deal with subword sequences and a ``fast'' branch to deal with longer character sequences. This model is efficient since the fast branch is very lightweight by reducing the model width, and yet provides useful fine-grained features for the slow branch. Our TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point) on several machine translation benchmarks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.430',\n",
       "   'authors': ['Bei Li',\n",
       "    'Yi Jing',\n",
       "    'Xu Tan',\n",
       "    'Zhen Xing',\n",
       "    'Tong Xiao',\n",
       "    'Jingbo Zhu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-4_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P157',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['modelling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.430.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77270/poster_document/278c03b28eb18e217292f89797ea6884.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TranSFormer: Slow-Fast Transformer for Machine Translation',\n",
       "   'tldr': 'Learning multiscale Transformer models has been evidenced as a viable approach to  augmenting machine translation systems. Prior research has primarily focused on treating subwords as basic units in developing such systems. However, the incorporation of fine-grained character-level features into mul...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77270,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77270-transformer-slow-fast-transformer-for-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1572': {'abstract': 'Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a generalized data augmentation method. Our training code and text style transfer model are open-sourced.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.441',\n",
       "   'authors': ['Daimeng Wei',\n",
       "    'Zhanglin Wu',\n",
       "    'Hengchao Shang',\n",
       "    'Zongyao Li',\n",
       "    'Minghan Wang',\n",
       "    'Jiaxin GUO',\n",
       "    'Xiaoyu Chen',\n",
       "    'Zhengzhe YU',\n",
       "    'Hao Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-7_-machine-translation-(poster)'],\n",
       "   'id': 'P1572',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['domain adaptation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.441.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76605/poster_document/0549a4fa733f961aec2fecdc3b831d84.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76605/poster/1312ea77e50109e60d9682dc0a28f1dd.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76605/slideshow/3b3ce235959569b4f2afe7a5f2962db2.pdf',\n",
       "   'title': 'Text Style Transfer Back-Translation',\n",
       "   'tldr': 'Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-liked inputs), since the source side of BT d...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76605,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76605-text-style-transfer-back-translation',\n",
       "   'video_url': None},\n",
       "  'P1575': {'abstract': 'We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. \\nAfter embedding, different graph embeddings contain diverse specific knowledge for generating the words with different part-of-speech, e.g., object/attribute embedding is good for generating nouns/adjectives. Motivated by this, we design a Mixture-of-Expert (MOE)-based decoder, where each expert is built on MHA, for discriminating the graph embeddings to generate different kinds of words. \\nSince both the encoder and decoder are built based on the MHA, as a result, we construct a simple and homogeneous encoder-decoder unlike the previous heterogeneous ones which usually apply Fully-Connected-based GNN and LSTM-based decoder. The homogeneous architecture enables us to unify the training configuration of the whole model instead of specifying different training strategies for diverse sub-networks as in the heterogeneous pipeline, which releases the training difficulty. Extensive experiments on the MS-COCO captioning benchmark validate the effectiveness of our TFSGC. The code is in: https://anonymous.4open.science/r/ACL23\\\\_TFSGC.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.694',\n",
       "   'authors': ['Xu Yang',\n",
       "    'Jiawei Peng',\n",
       "    'Zihua Wang',\n",
       "    'Haiyang Xu',\n",
       "    'Qinghao Ye',\n",
       "    'Chenliang Li',\n",
       "    'Songfang Huang',\n",
       "    'Fei Huang',\n",
       "    'Zhangzikang Li',\n",
       "    'Yu Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)'],\n",
       "   'id': 'P1575',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal content generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.694.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76606/poster_document/33e77979a4eac314b2ef4bb8c685a28d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76606/poster/9ebd2f95061fd77310e1d956f41aa918.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76606/slideshow/b4f4c08d3fc0f8c0ce7b0679d236a480.pdf',\n",
       "   'title': 'Transforming Visual Scene Graphs to Image Captions',\n",
       "   'tldr': 'We propose to TransForm Scene Graphs into more descriptive Captions (TFSGC). In TFSGC, we apply multi-head attention (MHA) to design the Graph Neural Network (GNN) for embedding scene graphs. \\nAfter embedding, different graph embeddings contain diverse specific knowledge for generating the words wit...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76606,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76606-weakly-supervised-vision-and-language-pre-training-with-relative-representations',\n",
       "   'video_url': None},\n",
       "  'P1580': {'abstract': 'Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which enables the complete capture of the relevance between queries and documents and simplifies the classic index-retrieval-rerank pipeline. Despite its attractive qualities, there remain several major challenges in model-based retrieval, including the discrepancy between pre-training and fine-tuning, and the discrepancy between training and inference. To deal with the above challenges, we propose a novel two-stage model-based retrieval approach called TOME, which makes two major technical contributions, including the utilization of tokenized URLs as identifiers and the design of a two-stage generation architecture. We also propose a number of training strategies to deal with the training difficulty as the corpus size increases. Extensive experiments and analysis on MS MARCO and Natural Questions demonstrate the effectiveness of our proposed approach, and we investigate the scaling laws of TOME by examining various influencing factors.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.336',\n",
       "   'authors': ['Ruiyang Ren',\n",
       "    'Wayne Xin Zhao',\n",
       "    'Jing Liu',\n",
       "    'Hua Wu',\n",
       "    'Ji-Rong Wen',\n",
       "    'Haifeng Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-7_-information-retrieval-and-text-mining-(virtual-poster)'],\n",
       "   'id': 'P1580',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['passage retrieval'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.336.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76607/poster_document/1173590de5a4fd645b71b7985b2f507b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76607/poster/38ca874c37fad8647433df38e0251318.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TOME: A Two-stage Approach for Model-based Retrieval',\n",
       "   'tldr': 'Recently, model-based retrieval has emerged as a new paradigm in text retrieval that discards the index in the traditional retrieval model and instead memorizes the candidate corpora using model parameters. This design employs a sequence-to-sequence paradigm to generate document identifiers, which e...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 76607,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76607-tome-a-two-stage-approach-for-model-based-retrieval',\n",
       "   'video_url': None},\n",
       "  'P1583': {'abstract': \"Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the context limit, which has already been heavily investigated, we explore an orthogonal direction: making LMs divide a problem into multiple contexts. We propose a new inference framework, called Recursion of Thought (RoT), which introduces several special tokens that the models can output to trigger context-related operations. Extensive experiments with multiple architectures including GPT-3 show that RoT dramatically improves LMs' inference capability to solve problems, whose solution consists of hundreds of thousands of tokens.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.40',\n",
       "   'authors': ['Soochan Lee', 'Gunhee Kim'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1583',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['fine-tuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.40.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77499/poster_document/03150f22f29e4c53b86ea9b9401fb969.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77499/slideshow/d10988e36abfe28293b26b459d5da051.pdf',\n",
       "   'title': 'Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context Reasoning with Language Models',\n",
       "   'tldr': \"Generating intermediate steps, or Chain of Thought (CoT), is an effective way to significantly improve language models' (LM) multi-step reasoning capability. However, the CoT lengths can grow rapidly with the problem complexity, easily exceeding the maximum context size. Instead of increasing the co...\",\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77499,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77499-recursion-of-thought-a-divide-and-conquer-approach-to-multi-context-reasoning-with-language-models',\n",
       "   'video_url': None},\n",
       "  'P1588': {'abstract': 'We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, dif- fusion models offer a promising training strat- egy that helps improve the generation quality. On the other hand, pre-trained denoising lan- guage models (e.g., BERT) can be used as a good initialization that accelerates convergence. We explore training BERT to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Sec- ond, we investigate several designs of incorpo- rating the time step into BERT. Experiments on unconditional text generation demonstrate that DiffusionBERT achieves significant improve- ment over existing diffusion models for text (e.g., D3PM and Diffusion-LM) and previous generative masked language models in terms of perplexity and BLEU score. Promising re- sults in conditional generation tasks show that DiffusionBERT can generate texts of compa- rable quality and more diverse than a series of established baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.248',\n",
       "   'authors': ['Zhengfu He',\n",
       "    'Tianxiang Sun',\n",
       "    'Qiong Tang',\n",
       "    'Kuanning Wang',\n",
       "    'Xuanjing Huang',\n",
       "    'Xipeng Qiu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-4_-generation-(virtual-poster)'],\n",
       "   'id': 'P1588',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.248.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76608/poster_document/aca68af02405e63aa84a7870a3f4b3d4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76608/poster/915988f2236343aa78c8f23253b17592.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76608/slideshow/c8879f95d9ae6417695bcff689bbac4f.pptx',\n",
       "   'title': 'DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models',\n",
       "   'tldr': 'We present DiffusionBERT, a new generative masked language model based on discrete dif- fusion models. Diffusion models and many pre- trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76608,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76608-speechmatrix-a-large-scale-mined-corpus-of-multilingual-speech-to-speech-translations',\n",
       "   'video_url': None},\n",
       "  'P1593': {'abstract': 'This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synthetic text recognition data, OFA-OCR outperforms the baselines and achieves state-of-the-art performance in the Chinese text recognition benchmark. Additionally, we construct an OCR pipeline with OFA-OCR, and we demonstrate that it can achieve competitive performance with the product-level API.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.37',\n",
       "   'authors': ['Junyang Lin',\n",
       "    'Xuancheng Ren',\n",
       "    'Yichang Zhang',\n",
       "    'Gao Liu',\n",
       "    'Peng Wang',\n",
       "    'An Yang',\n",
       "    'Chang Zhou'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)'],\n",
       "   'id': 'P1593',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal content generation', 'cross-modal application'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.37.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77502/poster_document/31f252e4abe05e0e7d963d8204e610e4.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Transferring General Multimodal Pretrained Models to Text Recognition',\n",
       "   'tldr': 'This paper proposes a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition. Specifically, we recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task. Without pretraining on large-scale annotated or synt...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 77502,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77502-transferring-general-multimodal-pretrained-models-to-text-recognition',\n",
       "   'video_url': None},\n",
       "  'P1595': {'abstract': 'Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how to assign ground truth labels to the parallelly generated tuple proposals remains under-exploited. The commonly utilized Hungarian algorithm for this procedure is restricted to handling one-to-one assignment among the desired tuples and tuple proposals, which ignores the correlation between proposals and affects the recall of the models. To solve this problem, we propose a dynamic many-to-one label assignment strategy named IOT. Concretely, the label assignment process in OIE is formulated as an Optimal Transport (OT) problem. We leverage the intersection-over-union (IoU) as the assignment quality measurement, and convert the problem of finding the best assignment solution to the one of solving the optimal transport plan by maximizing the IoU values. To further utilize the knowledge from the assignment, we design an Assignment-guided Multi-granularity loss (AM) by simultaneously considering word-level and tuple-level information. Experiment results show the proposed method outperforms the state-of-the-art models on three benchmarks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.272',\n",
       "   'authors': ['Kaiwen Wei',\n",
       "    'Yiran Yang',\n",
       "    'li jin',\n",
       "    'Xian Sun',\n",
       "    'Zequn Zhang',\n",
       "    'Jingyuan Zhang',\n",
       "    'Xiao yu Li',\n",
       "    'Linhao Zhang',\n",
       "    'Jintao Liu',\n",
       "    'Guo Zhi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-7_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P1595',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['open information extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.272.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76609/poster_document/a9a19b93137e71fc1529383f6ab935a2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76609/poster/0b0331a43fd017d912c774d418253ef0.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76609/slideshow/717eefc73ff14bfe4a8b15b3de650818.pdf',\n",
       "   'title': 'Guide the Many-to-One Assignment: Open Information Extraction via IoU-aware Optimal Transport',\n",
       "   'tldr': 'Open Information Extraction (OIE) seeks to extract structured information from raw text without the limitations of close ontology. Recently, the detection-based OIE methods have received great attention from the community due to their parallelism. However, as the essential step of those models, how ...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76609,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76609-guide-the-many-to-one-assignment-open-information-extraction-via-iou-aware-optimal-transport',\n",
       "   'video_url': None},\n",
       "  'P1605': {'abstract': 'Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce research on multi-modal dialogue pre-training. Yet another intriguing challenge emerges from the encompassing nature of multi-modal dialogue, which involves various modalities and tasks. Moreover, new forms of tasks may arise at unpredictable points in the future. Hence, it is essential for designed multi-modal dialogue models to possess sufficient flexibility to adapt to such scenarios. This paper proposes PaCE, a unified, structured, compositional multi-modal dialogue pre-training framework. It utilizes a combination of several fundamental experts to accommodate multiple dialogue-related tasks and can be pre-trained using limited dialogue and extensive non-dialogue multi-modal data. Furthermore, we propose a progressive training method where old experts from the past can assist new experts, facilitating the expansion of their capabilities. Experimental results demonstrate that PaCE achieves state-of-the-art results on eight multi-modal dialog benchmarks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.749',\n",
       "   'authors': ['Yunshui Li',\n",
       "    'Binyuan Hui',\n",
       "    'ZhiChao Yin',\n",
       "    'Min Yang',\n",
       "    'Fei Huang',\n",
       "    'Yongbin Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-1_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1605',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-modal dialogue systems'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.749.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76610/poster_document/b8a39f0c04b0215157269db3fbca3001.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76610/poster/f8563576a01701051394f11b1d09456a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76610/slideshow/cc3170c22cb3d3825d1b5f9cd3795460.pptx',\n",
       "   'title': 'PaCE: Unified Multi-modal Dialogue Pre-training with Progressive and Compositional Experts',\n",
       "   'tldr': 'Perceiving multi-modal information and fulfilling dialogues with humans is a long-term goal of artificial intelligence. Pre-training is commonly regarded as an effective approach for multi-modal dialogue. However, due to the limited availability of multi-modal dialogue data, there is still scarce re...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76610,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76610-pace-unified-multi-modal-dialogue-pre-training-with-progressive-and-compositional-experts',\n",
       "   'video_url': None},\n",
       "  'P1621': {'abstract': 'Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose  dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphological compositional generalisation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.175',\n",
       "   'authors': ['Francois Meyer', 'Jan Buys'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'event_ids': ['session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1621',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['morphological segmentation',\n",
       "    'subword representations',\n",
       "    'morphological analysis'],\n",
       "   'languages': ['xhosa', 'zulu', 'finnish', 'swati', 'tswana', 'afrikaans'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.175.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77506/poster_document/f9ff196bad288ed541143f1a6900cee0.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77506/poster/2ae63cbbe1ebe70e0c3dfab104e17188.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77506/slideshow/9b1742e5aceaa8cca7d33e95a51a3f06.pdf',\n",
       "   'title': 'Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation',\n",
       "   'tldr': 'Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm...',\n",
       "   'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "   'underline_id': 77506,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77506-language-model-acceptability-judgements-are-not-always-robust-to-context',\n",
       "   'video_url': None},\n",
       "  'P1623': {'abstract': 'Multilingual Knowledge Graph Completion (mKGC) aim at solving queries  in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multilingual pretrained language models contain extensive knowledge of different languages, its pretraining tasks cannot be directly aligned with the mKGC tasks. Moreover, the majority of KGs and PLMs currently available exhibit a pronounced English-centric bias. This makes it difficult for mKGC to achieve good results, particularly in the context of low-resource languages. To overcome previous problems, this paper introduces global and local knowledge constraints for mKGC. The former is used to constrain the reasoning of answer entities , while the latter is used to enhance the representation of query contexts. The proposed method makes the pretrained model better adapt to the mKGC task. Experimental results on public datasets  demonstrate that our method outperforms the previous SOTA on Hits@1 and Hits@10 by an average of 12.32\\\\% and 16.03\\\\%, which indicates that our proposed method has significant enhancement on mKGC.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.488',\n",
       "   'authors': ['Ran Song',\n",
       "    'Shizhu He',\n",
       "    'Shengxiang Gao',\n",
       "    'Li Cai',\n",
       "    'Kang Liu',\n",
       "    'Zhengtao YU',\n",
       "    'Jun Zhao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1623',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge graphs'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.488.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77507/poster_document/5a6b8d374c891e256bc27cd7f4f2a655.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multilingual Knowledge Graph Completion from Pretrained Language Models with Knowledge Constraints',\n",
       "   'tldr': 'Multilingual Knowledge Graph Completion (mKGC) aim at solving queries  in different languages by reasoning a tail entity thus improving multilingual knowledge graphs. Previous studies leverage multilingual pretrained language models (PLMs) and the generative paradigm to achieve mKGC. Although multil...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77507,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77507-multilingual-knowledge-graph-completion-from-pretrained-language-models-with-knowledge-constraints',\n",
       "   'video_url': None},\n",
       "  'P1624': {'abstract': 'Dungeons \\\\& Dragons (D\\\\&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information.\\nRecent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone.\\nHowever, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\\\\&D gameplay on Discord with true game state info.  We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D\\\\&D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality.\\nAdditionally, we show that LLMs can generate executable Avrae commands, particularly after finetuning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.229',\n",
       "   'authors': ['Andrew Zhu',\n",
       "    'Karmanya Aggarwal',\n",
       "    'Alexander H Feng',\n",
       "    'Lara J. Martin',\n",
       "    'Chris Callison-Burch'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-7_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1624',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.229.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76611/poster_document/d3c0f8781e4b5b212cbf787285ae2071.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76611/poster/3e4366e5a06c0bcd213797d0ff3f0886.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76611/slideshow/5366d5529a68fa9096a98f4452b8f709.pdf',\n",
       "   'title': 'FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information',\n",
       "   'tldr': 'Dungeons \\\\& Dragons (D\\\\&D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information.\\nRecent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use d...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76611,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76611-fireball-a-dataset-of-dungeons-and-dragons-actual-play-with-structured-game-state-information',\n",
       "   'video_url': None},\n",
       "  'P1626': {'abstract': 'One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work, we construct a new dataset called SafeConv for the research of conversational safety: (1) Besides the utterance-level safety labels, SafeConv also provides unsafe spans in an utterance, information able to indicate which words contribute to the detected unsafe behavior; (2) SafeConv provides safe alternative responses to continue the conversation when unsafe behavior detected, guiding the conversation to a gentle trajectory. \\n\\nBy virtue of the comprehensive annotation of SafeConv, we benchmark three powerful models for the mitigation of conversational unsafe behavior, including a checker to detect unsafe utterances, a tagger to extract unsafe spans, and a rewriter to convert an unsafe response to a safe version. Moreover, we explore the huge benefits brought by combining the models for explaining the emergence of unsafe behavior and detoxifying chatbots. Experiments show that the detected unsafe behavior could be well explained with unsafe spans and popular chatbots could be detoxified by a huge extent. The dataset is available at https://github.com/mianzhang/SafeConv.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.2',\n",
       "   'authors': ['Mian Zhang',\n",
       "    'Lifeng Jin',\n",
       "    'Linfeng Song',\n",
       "    'Haitao Mi',\n",
       "    'Wenliang Chen',\n",
       "    'Dong Yu'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-4_-resources-and-evaluation-(oral)'],\n",
       "   'id': 'P1626',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['corpus creation', 'benchmarking', 'nlp datasets'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.2.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76227/poster_document/0299acb5c4aef0941dfe58944b78c8d4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76227/poster/d7e40e3009397a8bb9a91901c7be9c15.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76227/slideshow/e6eb4b7100fb906457e8b958e217ffe7.pdf',\n",
       "   'title': 'SafeConv: Explaining and Correcting Conversational Unsafe Behavior',\n",
       "   'tldr': 'One of the main challenges open-domain end-to-end dialogue systems, or chatbots, face is the prevalence of unsafe behavior, such as toxic languages and harmful suggestions. However, existing dialogue datasets do not provide enough annotation to explain and correct such unsafe behavior. In this work,...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76227,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15233/lecture/76227-safeconv-explaining-and-correcting-conversational-unsafe-behavior',\n",
       "   'video_url': None},\n",
       "  'P1627': {'abstract': 'Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example organization (i.e., selection and permutation) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40\\\\% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code will be released to facilitate future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.79',\n",
       "   'authors': ['Zhiyong Wu', 'Yaoxiang Wang', 'Jiacheng Ye', 'Lingpeng Kong'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-3_-large-language-models-(oral)'],\n",
       "   'id': 'P1627',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.79.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76228/poster_document/4c47f52f0b9960c5bb7fffe2215b384d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76228/poster/c9ba17a31e26df7525c831d8c68f4b50.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering',\n",
       "   'tldr': 'Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76228,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15223/lecture/76228-self-adaptive-in-context-learning-an-information-compression-perspective-for-in-context-example-selection-and-ordering',\n",
       "   'video_url': None},\n",
       "  'P1634': {'abstract': 'Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more realistic and less explored setting, Zero-shot Conversational Question Generation  (ZeroCQG),  which requires no human-labeled conversations for training. To solve ZeroCQG, we propose a multi-stage knowledge transfer framework, Synthesize, Prompt, and trAnsfer with pRe-Trained lAnguage model (SPARTA) to effectively leverage knowledge from single-turn question generation instances. To validate the zero-shot performance of SPARTA, we conduct extensive experiments on three conversational datasets: CoQA, QuAC, and DoQA by transferring knowledge from three single-turn datasets: MS MARCO, NewsQA, and SQuAD. The experimental results demonstrate the superior performance of our method. Specifically, SPARTA has achieved 14.81 BLEU-4 (88.2\\\\% absolute improvement compared to T5) in CoQA with knowledge transferred from SQuAD.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.500',\n",
       "   'authors': ['Hongwei Zeng', 'Bifan Wei', 'Jun Liu', 'Weiping Fu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1634',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['question generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.500.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76612/poster_document/f761cef1b543cd049c4b23389d088765.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76612/poster/c2ff2edf3e49eff8d51f0451668d9638.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Synthesize, Prompt and Transfer: Zero-shot Conversational Question Generation with Pre-trained Language Model',\n",
       "   'tldr': 'Conversational question generation aims to generate questions that depend on both context and conversation history. Conventional works utilizing deep learning have shown promising results, but heavily rely on the availability of large-scale annotated conversations. In this paper, we introduce a more...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76612,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76612-synthesize-prompt-and-transfer-zero-shot-conversational-question-generation-with-pre-trained-language-model',\n",
       "   'video_url': None},\n",
       "  'P1636': {'abstract': \"Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superior generative capacity of large pretrained language models (PLM), recent work built on prompt engineering enables explanations generated without specific training. However, explanations generated through single-pass prompting often lack sufficiency and conciseness, due to the prompt complexity and hallucination issues. To discard the dross and take the essence of current PLM's results, we propose to produce sufficient and concise explanations via the information bottleneck (EIB) theory. EIB regenerates explanations by polishing the single-pass output of PLM but retaining the information that supports the contents being explained by balancing two information bottleneck objectives. Experiments on two different tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.765',\n",
       "   'authors': ['Qintong Li', 'Zhiyong Wu', 'Lingpeng Kong', 'Wei Bi'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1636',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['free-text/natural language explanations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.765.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77509/poster_document/2aa23fbbb71fb76d98ac1e6f91dc604d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77509/poster/16fa0f2831ee2560ae217d21496d1b9b.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77509/slideshow/8ea96781a9f576f5a034a5afaf846c83.pptx',\n",
       "   'title': 'Explanation Regeneration via Information Bottleneck',\n",
       "   'tldr': 'Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Thanks to the superio...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77509,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77509-explanation-regeneration-via-information-bottleneck',\n",
       "   'video_url': None},\n",
       "  'P1637': {'abstract': \"Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users' privacy. Therefore, it is important for researchers to determine the extent of profiling in a safe environment to facilitate proper use and make the public aware of the potential risks. Contrary to existing approaches on limited attributes, we explore open-domain Twitter user profile inference. We conduct a case study where we collect publicly available WikiData public figure profiles and use diverse WikiData predicates for profile inference. After removing sensitive attributes, our data contains over 150K public figure profiles from WikiData, over 50 different attribute predicates, and over 700K attribute values. We further propose a prompt-based generation method, which can infer values that are implicitly mentioned in the Twitter information. Experimental results show that the generation-based approach can infer more comprehensive user profiles than baseline extraction-based methods, but limitations still remain to be applied for real-world use. We also enclose a detailed ethical statement for our data, potential benefits and risks from this work, and our efforts to mitigate the risks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.198',\n",
       "   'authors': ['Haoyang Wen',\n",
       "    'Zhenxin Xiao',\n",
       "    'Eduard H Hovy',\n",
       "    'Alexander Hauptmann'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P1637',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['psycho-demographic trait prediction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.198.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77510/poster_document/1a8423978023a2cf7d1f122393cbd73c.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Open-Domain Twitter User Profile Inference',\n",
       "   'tldr': \"Twitter user profile inference utilizes information from Twitter to predict user attributes (e.g., occupation, location), which is controversial because of its usefulness for downstream applications and its potential to reveal users' privacy. Therefore, it is important for researchers to determine t...\",\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 77510,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77510-towards-open-domain-twitter-user-profile-inference',\n",
       "   'video_url': None},\n",
       "  'P1638': {'abstract': 'Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch between the dependency tree as a syntactic structure and the sentiment classification as a semantic task. To alleviate this gap, we replace the syntactic dependency tree with the semantic structure named Abstract Meaning Representation (AMR) and propose a model called AMR-based Path Aggregation Relational Network (APARN) to take full advantage of semantic structures. In particular, we design the path aggregator and the relation-enhanced self-attention mechanism that complement each other. The path aggregator extracts semantic features from AMRs under the guidance of sentence information, while the relation-enhanced self-attention mechanism in turn improves sentence features with refined semantic information. Experimental results on four public datasets demonstrate 1.13\\\\% average F1 improvement of APARN in ABSA when compared with state-of-the-art baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.19',\n",
       "   'authors': ['Fukun Ma',\n",
       "    'Xuming Hu',\n",
       "    'Aiwei Liu',\n",
       "    'Yawen Yang',\n",
       "    'Shuang Li',\n",
       "    'Philip S. Yu',\n",
       "    'Lijie Wen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1638',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.19.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76613/poster_document/9e796f0f6e5a5bd895907a53973b7314.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76613/poster/81314e83ff0d161c085d6acfba700fc6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'AMR-based Network for Aspect-based Sentiment Analysis',\n",
       "   'tldr': 'Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment classification task. Many recent works have used dependency trees to extract the relation between aspects and contexts and have achieved significant improvements. However, further improvement is limited due to the potential mismatch ...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76613,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76613-amr-based-network-for-aspect-based-sentiment-analysis',\n",
       "   'video_url': None},\n",
       "  'P1645': {'abstract': 'In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling.  To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2H), predicate-to-tail (P2T), and head-to-tail (H2T), where head/tail means the first/last word of the semantic argument span. As such, we use a CRF-based higher-order dependency parser and leverage Mean-Field Variational Inference (MFVI) for higher-order inference. Moreover, since semantic arguments of predicates are often constituents within a constituency parse tree, we can leverage such nice structural property by defining a TreeCRF distribution over all H2T edges, using the idea of partial marginalization to define structural training loss. We further leverage structured MFVI to enhance inference. We experiment on span-based SRL benchmarks, showing the effectiveness of both higher-order and structured  modeling and the combination thereof.  In addition, we show superior performance of structured MFVI against vanilla MFVI.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.58',\n",
       "   'authors': ['Wei Liu', 'Songlin Yang', 'Kewei Tu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1645',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['structured prediction', 'graphical models'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.58.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77512/poster_document/8e8831a274f0ae36b1bf18e4386f0eed.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77512/poster/1889bef041ad9cf527719f9beecc306c.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77512/slideshow/4bb793460b58786c5dfb0b075df8e495.pdf',\n",
       "   'title': 'Structured Mean-Field Variational Inference for Higher-Order Span-Based Semantic Role',\n",
       "   'tldr': 'In this work, we enhance higher-order graph-based approaches for span-based semantic role labeling (SRL) by means of structured modeling.  To decrease the complexity of higher-order modeling, we decompose the edge from predicate word to argument span into three different edges, predicate-to-head (P2...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77512,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77512-structured-mean-field-variational-inference-for-higher-order-span-based-semantic-role',\n",
       "   'video_url': None},\n",
       "  'P1653': {'abstract': 'Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.127',\n",
       "   'authors': ['Haoyang Wen', 'Alexander Hauptmann'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P1653',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.127.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76614/poster_document/de86d5ea51b7d41b499b6213f5b71446.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76614/poster/d35d95c0e97d0f30c8582677c094282e.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation',\n",
       "   'tldr': 'Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this pap...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76614,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76614-towards-reasoning-in-large-language-models-a-survey',\n",
       "   'video_url': None},\n",
       "  'P1658': {'abstract': 'Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.65',\n",
       "   'authors': ['Wenzheng Zhao', 'Yuanning Cui', 'Wei Hu'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-7_-information-extraction-(oral)'],\n",
       "   'id': 'P1658',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.65.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76229/poster_document/1cacc956ede7bac185ebd66558979f3e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76229/poster/0ae5e9f85b574ae080898efb0a312d34.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76229/slideshow/f8a9ff983e3a2b66de7a00bc3c32e2cf.pdf',\n",
       "   'title': 'Improving Continual Relation Extraction by Distinguishing Analogous Semantics',\n",
       "   'tldr': 'Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitti...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76229,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15275/lecture/76229-improving-continual-relation-extraction-by-distinguishing-analogous-semantics',\n",
       "   'video_url': None},\n",
       "  'P1662': {'abstract': 'We investigate the representation of pretrained language models and humans, using the idea of word definition modelinghow well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-written definition and its usage in example sentences. We then present a simple method DefBERT that integrates pretrained models with word semantics in dictionaries. We show its benefits on newly-proposed tasks of definition ranking and definition sense disambiguation. Furthermore, we present the results on standard word similarity tasks and short text classification tasks where models are required to encode semantics with only a few words. The results demonstrate the effectiveness of integrating word definitions and pretrained language models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.2',\n",
       "   'authors': ['Hwiyeol Jo'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-7_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1662',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['lexical relationships',\n",
       "    'compositionality',\n",
       "    'interpretability'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.2.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77514/poster_document/e50465d1b7050663a82d10e1f539b33d.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Self-Supervised Integration Method of Pretrained Language Models and Word Definitions',\n",
       "   'tldr': 'We investigate the representation of pretrained language models and humans, using the idea of word definition modelinghow well a word is represented by its definition, and vice versa. Our analysis shows that a word representation in pretrained language models does not successfully map its human-wri...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77514,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77514-towards-generative-event-factuality-prediction',\n",
       "   'video_url': None},\n",
       "  'P1667': {'abstract': 'Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing speech-text pre-training methods fail to explore the contextual information within a dialogue to enrich utterance representations. In this paper, we propose Speech-text Pre-training for spoken dialog understanding with ExpliCiT cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog pre-training model. Concretely, to consider the temporality of speech modality, we design a novel temporal position prediction task to capture the speech-text alignment. This pre-training task aims to predict the start and end time of each textual word in the corresponding speech waveform. In addition, to learn the characteristics of spoken dialogs, we generalize a response selection task from textual dialog pre-training to speech-text dialog pre-training scenarios. Experimental results on four different downstream speech-text tasks demonstrate the superiority of SPECTRA in learning speech-text alignment and multi-turn dialog context.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.438',\n",
       "   'authors': ['Tianshu Yu',\n",
       "    'haoyu gao',\n",
       "    'Ting-En Lin',\n",
       "    'Min Yang',\n",
       "    'Yuchuan Wu',\n",
       "    'Wentao Ma',\n",
       "    'chao wang',\n",
       "    'Fei Huang',\n",
       "    'Yongbin Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-1_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1667',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['spoken dialogue systems',\n",
       "    'task-oriented',\n",
       "    'multi-modal dialogue systems'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.438.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76615/poster_document/fb6c30e19e02398524ee29eac9c9e124.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76615/poster/cb8fd515562af32bdbb26258f44f2dba.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Speech-Text Pre-training for Spoken Dialog Understanding with Explicit Cross-Modal Alignment',\n",
       "   'tldr': 'Recently, speech-text pre-training methods have shown remarkable success in many speech and natural language processing tasks. However, most previous pre-trained models are usually tailored for one or two specific tasks, but fail to conquer a wide range of speech-text tasks. In addition, existing sp...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76615,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76615-speech-text-pre-training-for-spoken-dialog-understanding-with-explicit-cross-modal-alignment',\n",
       "   'video_url': None},\n",
       "  'P1670': {'abstract': 'As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings and thoughts of others via other-awareness. However, they ignore to include self-awareness to consider the own views of the self in their responses, which is a crucial process to achieve the empathy. To this end, we propose to generate Empathetic response with explicit Self-Other Awareness (EmpSOA). Specifically, three stages, self-other differentiation, self-other modulation and self-other generation, are devised to clearly maintain, regulate and inject the self-other aware information into the process of empathetic response generation. Both automatic and human evaluations on the benchmark dataset demonstrate the superiority of EmpSOA to generate more empathetic responses. {Our source code will be publicly available.}',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.843',\n",
       "   'authors': ['Weixiang Zhao', 'Yanyan Zhao', 'Xin Lu', 'Bing Qin'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1670',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.843.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77515/poster_document/e31615f4e9240cfe8df946d773d254bb.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77515/slideshow/b8aa58dcb16460b3b7701f8f734ab387.pdf',\n",
       "   'title': \"Don't Lose Yourself! Empathetic Response Generation via Explicit Self-Other Awareness\",\n",
       "   'tldr': 'As a critical step to achieve human-like chatbots, empathetic response generation has attained increasing interests. Previous attempts are incomplete and not sufficient enough to elicit empathy because they only stay on the initial stage of empathy to automatically sense and simulate the feelings an...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77515,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77515-don-t-lose-yourself-empathetic-response-generation-via-explicit-self-other-awareness',\n",
       "   'video_url': None},\n",
       "  'P1674': {'abstract': \"As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs' parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.797',\n",
       "   'authors': ['Yingji Li', 'Mengnan Du', 'Xin Wang', 'Ying Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-4_-ethics-and-nlp-(virtual-poster)'],\n",
       "   'id': 'P1674',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/unfairness mitigation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.797.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76616/poster_document/2ef434b80d9635ada89a7f6b555c1ebd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76616/poster/b66b63a91e9d566b1a4bdcebfdf33ce7.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76616/slideshow/9074e6671c895cd15f9c5232147570a1.pptx',\n",
       "   'title': 'Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases',\n",
       "   'tldr': 'As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 76616,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76616-prompt-tuning-pushes-farther-contrastive-learning-pulls-closer-a-two-stage-approach-to-mitigate-social-biases',\n",
       "   'video_url': None},\n",
       "  'P1676': {'abstract': 'Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. \\nExisting monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models.\\nIn this paper, we revisit the classic multi-way structures and develop a detachable model by assigning each language (or group of languages) to an individual branch that supports plug-and-play training and inference. To address the needs of learning representations for all languages in a unified space, we propose a novel efficient training recipe, upon which we build an effective detachable model, Lego-MT.\\nFor a fair comparison, we collect data from OPUS and build a translation benchmark covering 433 languages and 1.3B parallel data. \\nExperiments show that Lego-MT with 1.2B parameters brings an average gain of 3.2 spBLEU. It even outperforms M2M-100 with 12B parameters. \\nThe proposed training recipe brings a 28.2$\\\\times$ speedup over the conventional multi-way training method.{code and data repo: {https://github.com/CONE-MT/Lego-MT.git}.}',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.731',\n",
       "   'authors': ['Fei Yuan',\n",
       "    'Yinquan Lu',\n",
       "    'Wenhao Zhu',\n",
       "    'Lingpeng Kong',\n",
       "    'Lei Li',\n",
       "    'Yu Qiao',\n",
       "    'Jingjing Xu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-7_-machine-translation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1676',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.731.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77517/poster_document/28314a5c8c83d44d8c725a5ca4cc0179.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77517/poster/3a47b5fe9fa34085b231ee03b0addd1e.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77517/slideshow/ba266585c205787d6a496bd6de6eceb6.pptx',\n",
       "   'title': 'Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation',\n",
       "   'tldr': 'Multilingual neural machine translation (MNMT) aims to build a unified model for many language directions. \\nExisting monolithic models for MNMT encounter two challenges: parameter interference among languages and inefficient inference for large models.\\nIn this paper, we revisit the classic multi-way...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77517,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77517-lego-mt-learning-detachable-models-for-massively-multilingual-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1678': {'abstract': 'Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.110',\n",
       "   'authors': ['Chaoqun Liu',\n",
       "    'Wenxuan Zhang',\n",
       "    'Guizhen Chen',\n",
       "    'Xiaobao Wu',\n",
       "    'Anh Tuan Luu',\n",
       "    'Chip Hong Chang',\n",
       "    'Lidong Bing'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1678',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.110.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77518/poster_document/32a2af3910272fb2de69a644abebeeb1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77518/poster/02b481ef7ca44df0e446b4734e86af91.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77518/slideshow/fd9cb61110e72321a648b6136461b7d0.pptx',\n",
       "   'title': 'Zero-Shot Text Classification via Self-Supervised Tuning',\n",
       "   'tldr': 'Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised lea...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77518,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77518-zero-shot-text-classification-via-self-supervised-tuning',\n",
       "   'video_url': None},\n",
       "  'P1679': {'abstract': 'We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5\\\\% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.832',\n",
       "   'authors': ['Yau-Shian Wang',\n",
       "    'Ta-Chung Chi',\n",
       "    'Ruohong Zhang',\n",
       "    'YIMING YANG'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-3_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P1679',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.832.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76617/poster_document/30cbc2ca10bf711fcf72e6ed1b023df7.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76617/poster/8394cf9cb5e638cd7b903c74df7c30b3.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification',\n",
       "   'tldr': 'We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to t...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76617,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76617-uniex-an-effective-and-efficient-framework-for-unified-information-extraction-via-a-span-extractive-perspective',\n",
       "   'video_url': None},\n",
       "  'P1680': {'abstract': 'Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating through English).\\nOn the downside, adding more languages reduces model capacity per language, which is usually countered by increasing the overall model size, making training harder and inference slower.\\nIn this work, we introduce Language-Specific Transformer Layers (LSLs), which allow us to increase model capacity, while keeping the amount of computation and the number of parameters used in the forward pass constant. The key idea is to have some layers of the encoder be source or target language-specific, while keeping the remaining layers shared. We study the best way to place these layers using a neural architecture search inspired approach, and achieve an improvement of 1.3 chrF (1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and 1.9 chrF (2.2 spBLEU) on a shared decoder one.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.825',\n",
       "   'authors': ['Telmo Pires',\n",
       "    'Robin M. Schmidt',\n",
       "    'Yi-Hsiu Liao',\n",
       "    'Stephan Peitz'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-3_-machine-translation-(poster)'],\n",
       "   'id': 'P1680',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.825.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76618/poster_document/7bcab546ebb68116c0e1abdb051213ea.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76618/poster/244e5e20c924ee6bf55864479570494b.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76618/slideshow/270101394a4feb19f4ec11373417f015.pdf',\n",
       "   'title': 'Learning Language-Specific Layers for Multilingual Machine Translation',\n",
       "   'tldr': 'Multilingual Machine Translation promises to improve translation quality between non-English languages. This is advantageous for several reasons, namely lower latency (no need to translate twice), and reduced error cascades (e.g., avoiding losing gender and formality information when translating thr...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76618,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76618-learning-language-specific-layers-for-multilingual-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1684': {'abstract': \"A  challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data  zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied to the zero-shot scenarios, as it is not clear how to apply it unsupervisedly. \\n\\nOur method, Prompter, uses descriptions of target domain slots to generate dynamic prefixes that are concatenated to the key and values at each layer's self-attention mechanism. This allows for the use of prefix-tuning in zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD benchmarks. In generating prefixes, our analyses find that Prompter not only utilizes the semantics of slot descriptions but also how often the slots appear together in conversation. Moreover, Prompter's gains are due to its improved ability to distinguish ''none''-valued dialogue slots, compared against baselines.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.252',\n",
       "   'authors': ['Ibrahim Taha Aksu', 'Min-Yen Kan', 'Nancy Chen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-6_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P1684',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented', 'dialogue state tracking'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.252.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76619/poster_document/386455082943a3f729f8f5074efdf4a5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76619/poster/0ba013e727728ecc9e8b27f5468e0acd.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76619/slideshow/8c6b10cbf363ac032f30ecdadac7b9cd.pdf',\n",
       "   'title': 'Prompter: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain Adaptation',\n",
       "   'tldr': 'A  challenge in the Dialogue State Tracking (DST) field is adapting models to new domains without using any supervised data  zero-shot domain adaptation. Parameter-Efficient Transfer Learning (PETL) has the potential to address this problem due to its robustness. However, it has yet to be applied t...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76619,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76619-prompter-zero-shot-adaptive-prefixes-for-dialogue-state-tracking-domain-adaptation',\n",
       "   'video_url': None},\n",
       "  'P1691': {'abstract': 'Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, in this paper, we release two new datasets for ASQP, which contain the following characteristics: larger size, more words per sample, and higher density. With such datasets, we unveil the shortcomings of existing strong ASQP baselines and therefore propose a unified one-step solution for ASQP, namely One-ASQP, to detect the aspect categories and to identify the aspectopinion-sentiment (AOS) triplets simultaneously. Our One-ASQP holds several unique advantages: (1) by separating ASQP into two subtasks and solving them independently and simultaneously, we can avoid error propagation in pipeline-based methods and overcome slow training and inference in generation-based methods; (2) by introducing sentiment-specific horns tagging schema in a token-pair-based two-dimensional matrix, we can exploit deeper interactions between sentiment elements and efficiently decode the AOS triplets; (3) we design \"[NULL] token can help us effectively identify the implicit aspects or opinions. Experiments on two benchmark datasets and our released two datasets demonstrate the advantages of our One-ASQP. The two new datasets are publicly released at https://www.github.com/Datastory-CN/ASQP-Datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.777',\n",
       "   'authors': ['Junxian Zhou',\n",
       "    'Haiqin Yang',\n",
       "    'Yuxuan He',\n",
       "    'Hao Mou',\n",
       "    'JunBo Yang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1691',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument mining'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.777.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77521/poster_document/0c52a7a44349b41ee34735db833ca53c.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Unified One-Step Solution for Aspect Sentiment Quad Prediction',\n",
       "   'tldr': 'Aspect sentiment quad prediction (ASQP) is a challenging yet significant subtask in aspectbased sentiment analysis as it provides a complete aspect-level sentiment structure. However, existing ASQP datasets are usually small and low-density, hindering technical advancement. To expand the capacity, i...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77521,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77521-a-unified-one-step-solution-for-aspect-sentiment-quad-prediction',\n",
       "   'video_url': None},\n",
       "  'P1694': {'abstract': 'We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MABSA is tough. To alleviate the above issue, we perform three MABSA-related tasks with quite a small number of labeled multimodal samples. We first build diverse and comprehensive multimodal few-shot datasets according to the data distribution. To capture the specific prompt for each aspect term in a few-shot scenario, we propose a novel Generative Multimodal Prompt (GMP) model for MABSA, which includes the Multimodal Encoder module and the N-Stream Decoders module. We further introduce a subtask to predict the number of aspect terms in each instance to construct the multimodal prompt.\\nExtensive experiments on two datasets demonstrate that our approach outperforms strong baselines on two MABSA-related tasks in the few-shot setting.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.735',\n",
       "   'authors': ['Xiaocui Yang',\n",
       "    'Shi Feng',\n",
       "    'Daling Wang',\n",
       "    'Qi Sun',\n",
       "    'Wenfang Wu',\n",
       "    'Yifei Zhang',\n",
       "    'Pengfei Hong',\n",
       "    'Soujanya Poria'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1694',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.735.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77522/poster_document/5b29e454a0fde66f7da6ed2840288ce1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77522/poster/278049e7085738eb869b02e86d1104f6.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Few-shot Joint Multimodal Aspect-Sentiment Analysis Based on Generative Multimodal Prompt',\n",
       "   'tldr': 'We have witnessed the rapid proliferation of multimodal data on numerous social media platforms. Conventional studies typically require massive labeled data to train models for Multimodal Aspect-Based Sentiment Analysis (MABSA). However, collecting and annotating fine-grained multimodal data for MAB...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77522,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77522-contrastive-training-improves-zero-shot-classification-of-semi-structured-documents',\n",
       "   'video_url': None},\n",
       "  'P1695': {'abstract': 'Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.347',\n",
       "   'authors': ['Soumya Sanyal',\n",
       "    'Yichong Xu',\n",
       "    'Shuohang Wang',\n",
       "    'Ziyi Yang',\n",
       "    'Reid Pryzant',\n",
       "    'Wenhao Yu',\n",
       "    'Chenguang Zhu',\n",
       "    'Xiang Ren'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P1695',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reasoning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.347.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76620/poster_document/555ff140af1b88f7437cff9d2a2630fe.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76620/poster/73bbd4146c6cab36f05ffa8e58666270.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning',\n",
       "   'tldr': 'Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g.,...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76620,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76620-apollo-a-simple-approach-for-adaptive-pretraining-of-language-models-for-logical-reasoning',\n",
       "   'video_url': None},\n",
       "  'P1696': {'abstract': \"Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model's mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a corresponding benchmark, tense prediction accuracy. With the tense test set and the benchmark, researchers are able to measure the tense consistency performance of machine translation systems for the first time.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.164',\n",
       "   'authors': ['Yiming Ai', 'Zhiwei He', 'Kai Yu', 'Rui Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-3_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1696',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual corpora'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.164.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76621/poster_document/bfdee9db2628644737d3dad7910e104b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76621/poster/4662328191c3640a1ed5a6a6ecf0ab82.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'TeCS: A Dataset and Benchmark for Tense Consistency of Machine Translation',\n",
       "   'tldr': \"Tense inconsistency frequently occurs in machine translation. However, there are few criteria to assess the model's mastery of tense prediction from a linguistic perspective. In this paper, we present a parallel tense test set, containing French-English 552 utterances. We also introduce a correspond...\",\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76621,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76621-on-text-based-personality-computing-challenges-and-future-directions',\n",
       "   'video_url': None},\n",
       "  'P1697': {'abstract': 'In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generalization to out-of-distribution error patterns. Given that BERT is the backbone of most CSC models, this phenomenon has a significant negative impact. To address this issue, we are releasing a multi-domain benchmark LEMON, with higher quality and diversity than existing benchmarks, to allow a comprehensive assessment of the open domain generalization of CSC models. Then, we demonstrate that a very simple strategy  randomly masking 20\\\\% non-error tokens from the input sequence during fine-tuning  is sufficient for learning a much better language model without sacrificing the error model. This technique can be applied to any model architecture and achieves new state-of-the-art results on SIGHAN, ECSpell, and LEMON.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.600',\n",
       "   'authors': ['Hongqiu Wu', 'Shaohua Zhang', 'Yuchen Zhang', 'Hai Zhao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1697',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.600.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76622/poster_document/24b917cbc75d8fadaf3607ecc078542c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76622/poster/f3439630fe6b2cc79308cdd12abf2eed.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Rethinking Masked Language Modeling for Chinese Spelling Correction',\n",
       "   'tldr': 'In this paper, we study Chinese Spelling Correction (CSC) as a joint decision made by two separate models: a language model and an error model. Through empirical analysis, we find that fine-tuning BERT tends to over-fit the error model while under-fit the language model, resulting in poor generaliza...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76622,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76622-rethinking-masked-language-modeling-for-chinese-spelling-correction',\n",
       "   'video_url': None},\n",
       "  'P1706': {'abstract': 'The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs just for inference.\\nIn this work, we propose a pruning method that enables the removal of up to 80\\\\% of experts without further finetuning and with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics can identify language-specific experts.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.198',\n",
       "   'authors': ['Yeskendir Koishekenov',\n",
       "    'Alexandre Berard',\n",
       "    'Vassilina Nikoulina'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-6_-machine-translation-(poster)'],\n",
       "   'id': 'P1706',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['efficient inference for mt', 'multilingual mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.198.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76623/poster_document/bcc5dad2893c3d19fc64c75908906052.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76623/poster/ea8bd9c9814b0079acf9d45295aacef3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76623/slideshow/13e5ae89cabd630b14873de31ba852f4.pdf',\n",
       "   'title': 'Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model',\n",
       "   'tldr': 'The recently released NLLB-200 is a set of multilingual Neural Machine Translation models that cover 202 languages. The largest model is based on a Mixture of Experts architecture and achieves SoTA results across many language pairs. It contains 54.5B parameters and requires at least four 32GB GPUs ...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76623,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76623-u-creat-unsupervised-case-retrieval-using-events-extraction',\n",
       "   'video_url': None},\n",
       "  'P1714': {'abstract': 'Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of how masking ratio and masked content influence the MLM pre-training.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.400',\n",
       "   'authors': ['Dongjie Yang', 'Zhuosheng Zhang', 'Hai Zhao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1714',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.400.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76624/poster_document/e090831ca79415bdf28ff194e4a8500c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76624/poster/d5b2b3ca2446c833a25c6f530b69bdf2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76624/slideshow/bdd7c051a37d3ccbdb5f0e6dc1d2a2d5.pptx',\n",
       "   'title': 'Learning Better Masking for Better Language Model Pre-training',\n",
       "   'tldr': 'Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76624,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76624-learning-better-masking-for-better-language-model-pre-training',\n",
       "   'video_url': None},\n",
       "  'P1715': {'abstract': 'This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate whether each neuron is mainly specialized in a certain function, and find that the answer is yes. (2) function-based neuron grouping: we explore to find a structure that groups neurons into modules by function, and each module works for its corresponding function. Given the enormous amount of possible structures, we focus on Mixture-of-Experts as a promising candidate, which partitions neurons into experts and usually activates different experts for different inputs. Experimental results show that there are functional experts, where clustered are the neurons specialized in a certain function. Moreover, perturbing the activations of functional experts significantly affects the corresponding function. Finally, we study how modularity emerges during pre-training, and find that the modular structure is stabilized at the early stage, which is faster than neuron stabilization. It suggests that Transformer first constructs the modular structure and then learns fine-grained neuron functions. Our code and data are available at https://github.com/THUNLP/modularity-analysis.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.250',\n",
       "   'authors': ['Zhengyan Zhang',\n",
       "    'Zhiyuan Zeng',\n",
       "    'Yankai Lin',\n",
       "    'Chaojun Xiao',\n",
       "    'Xiaozhi Wang',\n",
       "    'Xu Han',\n",
       "    'Zhiyuan Liu',\n",
       "    'Ruobing Xie',\n",
       "    'Maosong Sun',\n",
       "    'Jie Zhou'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1715',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['probing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.250.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77524/poster_document/a83edcba52389e209f0566364d7c5b60.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Emergent Modularity in Pre-trained Transformers',\n",
       "   'tldr': 'This work examines the presence of modularity in pre-trained Transformers, a feature commonly found in human brains and thought to be vital for general intelligence. In analogy to human brains, we consider two main characteristics of modularity: (1) functional specialization of neurons: we evaluate ...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77524,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77524-emergent-modularity-in-pre-trained-transformers',\n",
       "   'video_url': None},\n",
       "  'P1716': {'abstract': 'Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning.\\nGiven the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.\\nBy searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs.\\nMeanwhile, it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features.\\nLastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method.\\nExperiments are conducted on various types of NLP classification tasks and demonstrate that the proposed method can achieve comparable results with other gradient-free and optimization-free baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.860',\n",
       "   'authors': ['Yulin Chen',\n",
       "    'Ning Ding',\n",
       "    'Xiaobin Wang',\n",
       "    'Shengding Hu',\n",
       "    'Haitao Zheng',\n",
       "    'Zhiyuan Liu',\n",
       "    'Pengjun Xie'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)'],\n",
       "   'id': 'P1716',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge tracing/discovering/inducing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.860.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76625/poster_document/f1d7b866e8de0146e3181634007db368.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76625/poster/e65f9cd477de281db739663e80b849cd.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Lottery Prompts for Pre-trained Language Models',\n",
       "   'tldr': 'Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning.\\nGiven the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, ...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76625,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76625-exploring-lottery-prompts-for-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P1719': {'abstract': 'Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source language and that of the non-source languages. This paper analyzes the fine-tuning process, discovers when the performance gap changes and identifies which network weights affect the overall performance most. Additionally, the paper seeks to answer to what extent the gap can be reduced by reducing forgetting. Based on the analysis results, a method named Fine-tuning slow and fast with four training policies is proposed to address these issues. Experimental results show the proposed method outperforms baselines by a clear margin.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.221',\n",
       "   'authors': ['Yiduo Guo',\n",
       "    'Yaobo Liang',\n",
       "    'Dongyan Zhao',\n",
       "    'Bing Liu',\n",
       "    'Nan Duan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-3_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P1719',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.221.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76626/poster_document/8d2cf7c60d4b0cf1dc3b8bd4900cc2e2.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast',\n",
       "   'tldr': 'Existing research has shown that a multilingual pre-trained language model fine-tuned with one (source) language also performs well on downstream tasks for non-source languages, even though no fine-tuning is done on these languages. However, there is a clear gap between the performance of the source...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76626,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76626-analyzing-and-reducing-the-performance-gap-in-cross-lingual-transfer-with-fine-tuning-slow-and-fast',\n",
       "   'video_url': None},\n",
       "  'P1721': {'abstract': 'Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the pseudo-groundtruth, and then train a neural AES model by learning from the aggregation of these quality signals. To aggregate these inconsistent quality signals into a unified supervision, we view the AES task as a ranking problem, and design a special Deep Pairwise Rank Aggregation (DPRA) loss for training. In the DPRA loss, we set a learnable confidence weight for each signal to address the conflicts among signals, and train the neural AES model in a pairwise way to disentangle the cascade effect among partial-order pairs. Experiments on eight prompts of ASPA dataset show that ULRA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both transductive and inductive settings. Further, our approach achieves comparable performance with many existing domain-adapted supervised models, showing the effectiveness of ULRA. The code is available at https://github.com/tenvence/ulra.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.782',\n",
       "   'authors': ['Cong Wang',\n",
       "    'Zhiwei Jiang',\n",
       "    'Yafeng Yin',\n",
       "    'Zifeng Cheng',\n",
       "    'Shiping Ge',\n",
       "    'Qing Gu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-4_-information-retrieval-and-text-mining-(virtual-poster)'],\n",
       "   'id': 'P1721',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['document representation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.782.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76627/poster_document/9a5358b06ebb8778b1f9b895e55974db.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76627/poster/814c2239ef690c8a20879f24048e6631.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76627/slideshow/a6be61a93ec210e8ead533b1cbf487fa.pdf',\n",
       "   'title': 'Aggregating Multiple Heuristic Signals as Supervision for Unsupervised Automated Essay Scoring',\n",
       "   'tldr': 'Automated Essay Scoring (AES) aims to evaluate the quality score for input essays. In this work, we propose a novel unsupervised AES approach ULRA, which does not require groundtruth scores of essays for training. The core idea of our ULRA is to use multiple heuristic quality signals as the pseudo-g...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 76627,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76627-aggregating-multiple-heuristic-signals-as-supervision-for-unsupervised-automated-essay-scoring',\n",
       "   'video_url': None},\n",
       "  'P1728': {'abstract': 'We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent encodings, and extractive summaries by selecting the sentences assigned to the same frequent encodings. Our method is attributable, because the model identifies sentences used to generate the summary as part of the summarization process. It scales easily to many hundreds of input reviews, because aggregation is performed in the latent space rather than over long sequences of tokens. We also demonstrate that our appraoch enables a degree of control, generating aspect-specific summaries by restricting the model to parts of the encoding space that correspond to desired aspects (e.g., location or food). Automatic and human evaluation on two datasets from different domains demonstrates that our method generates summaries that are more informative than prior work and better grounded in the input reviews.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.473',\n",
       "   'authors': ['Tom Hosking', 'Hao Tang', 'Mirella Lapata'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['session-4_-summarization-(oral)'],\n",
       "   'id': 'P1728',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['extractive summarisation',\n",
       "    'abstractive summarisation',\n",
       "    'multi-document summarization',\n",
       "    'architectures',\n",
       "    'factuality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.473.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76230/poster_document/a79960c3a30f1f5e95060eabff638074.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76230/poster/1f09cd8c12d6888c854aabd296835b6d.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Attributable and Scalable Opinion Summarization',\n",
       "   'tldr': 'We propose a method for unsupervised opinion summarization that encodes sentences from customer reviews into a hierarchical discrete latent space, then identifies common opinions based on the frequency of their encodings. We are able to generate both abstractive summaries by decoding these frequent ...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76230,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15235/lecture/76230-attributable-and-scalable-opinion-summarization',\n",
       "   'video_url': None},\n",
       "  'P1737': {'abstract': \"Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for 'belly' and 'womb'. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose Conceptualizer, a method that creates a bipartite directed alignment graph between source language concepts and sets of target language strings. In a detailed linguistic analysis across all languages for one concept ('bird') and an evaluation on gold standard data for 32 Swadesh concepts, we show that Conceptualizer has good alignment accuracy. We demonstrate the potential of research on conceptualization in NLP with two experiments. (1) We define crosslingual stability of a concept as the degree to which it has 1-1 correspondences across languages, and show that concreteness predicts stability. (2) We represent each language by its conceptualization pattern for 83 concepts, and define a similarity measure on these representations. The resulting measure for the conceptual similarity between two languages is complementary to standard genealogical, typological, and surface similarity measures. For four out of six language families, we can assign languages to their correct family based on conceptual similarity with accuracies between 54\\\\% and 87\\\\%\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.726',\n",
       "   'authors': ['Yihong Liu',\n",
       "    'Haotian Ye',\n",
       "    'Leonie Weissweiler',\n",
       "    'Philipp Wicke',\n",
       "    'Renhao Pei',\n",
       "    'Robert Zangenfeind',\n",
       "    'Hinrich Schtze'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-6_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P1737',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual evaluation', 'dialects and language varieties'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.726.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76628/poster_document/4fcb0930ac735b77daa3f6682dacca74.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76628/poster/84fa37d6f92ad242b6a3305496268aaf.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76628/slideshow/8bfd2576cea7fcf8e04bd0527cab4c28.pdf',\n",
       "   'title': 'A Crosslingual Investigation of Conceptualization in 1335 Languages',\n",
       "   'tldr': \"Languages differ in how they divide up the world into concepts and words; e.g., in contrast to English, Swahili has a single concept for 'belly' and 'womb'. We investigate these differences in conceptualization across 1,335 languages by aligning concepts in a parallel corpus. To this end, we propose...\",\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76628,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76628-a-crosslingual-investigation-of-conceptualization-in-1335-languages',\n",
       "   'video_url': None},\n",
       "  'P1744': {'abstract': 'Addressee recognition aims to identify addressees in multi-party conversations. While state-of-the-art addressee recognition models have achieved promising performance, they still suffer from the issue of robustness when applied in real-world scenes. When exposed to a noisy environment, these models regard the noise as input and identify the addressee in a pre-given addressee closed set, while the addressees of the noise do not belong to this closed set,  thus leading to the wrong identification of addressee. To this end, we propose a Robust Addressee Recognition (RAR) method, which discrete the addressees into a character codebook, making it able to represent open set addressees and robust in a noisy environment. Experimental results show that the introduction of the addressee character codebook helps to represent the open set addressees and highly improves the robustness of addressee recognition even if the input is noise.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.50',\n",
       "   'authors': ['Pengcheng Zhu',\n",
       "    'Wei Zhou',\n",
       "    'Kuncai Zhang',\n",
       "    'Yuankai Ma',\n",
       "    'Haiqing Chen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-1_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P1744',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.50.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76629/poster_document/4cb1d53d7e9bcd32d85282f2f6b23a4a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76629/poster/d7797eb20616e6a2edf522e3b8d458cc.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Robust Learning for Multi-party Addressee Recognition with Discrete Addressee Codebook',\n",
       "   'tldr': 'Addressee recognition aims to identify addressees in multi-party conversations. While state-of-the-art addressee recognition models have achieved promising performance, they still suffer from the issue of robustness when applied in real-world scenes. When exposed to a noisy environment, these models...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76629,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76629-robust-learning-for-multi-party-addressee-recognition-with-discrete-addressee-codebook',\n",
       "   'video_url': None},\n",
       "  'P1752': {'abstract': 'Text revision is a necessary process to improve text quality. \\nDuring this process, writers constantly edit texts out of different edit intentions.\\nIdentifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts according to one specific edit intention.\\nIn this work, we aim to build a multi-intent text revision system that could revise texts without explicit intent annotation.\\nOur system is based on prefix-tuning, which first gets prefixes for every edit intent, and then trains a prefix transfer module, enabling the system to selectively leverage the knowledge from various prefixes according to the input text.\\nWe conduct experiments on the IteraTeR dataset, and the results show that our system outperforms baselines. \\nThe system can significantly improve the SARI score with more than 3\\\\% improvements, which thrives on the learned editing intention prefixes.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.105',\n",
       "   'authors': ['Ruining Chong',\n",
       "    'Cunliang Kong',\n",
       "    'Liu Wu',\n",
       "    'Zhenghao Liu',\n",
       "    'Ziye Jin',\n",
       "    'Liner Yang',\n",
       "    'Yange Fan',\n",
       "    'Hanghang Fan',\n",
       "    'Erhong Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P1752',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.105.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76630/poster_document/fb2a69758cd43708d35cf6011eb6f5a2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76630/poster/c4e79e01d92d5e37abe4cde368a1d768.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Leveraging Prefix Transfer for Multi-Intent Text Revision',\n",
       "   'tldr': 'Text revision is a necessary process to improve text quality. \\nDuring this process, writers constantly edit texts out of different edit intentions.\\nIdentifying edit intention for a raw text is always an ambiguous work, and most previous work on revision systems mainly focuses on editing texts accord...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76630,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76630-learning-action-conditions-from-instructional-manuals-for-instruction-understanding',\n",
       "   'video_url': None},\n",
       "  'P176': {'abstract': 'Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-training is dominated by the high-performance class in selecting candidates, and deteriorates the low-performance class with the bias of generated pseudo label. To address the class-level imbalance performance, we propose a class-rebalancing self-training framework for improving the distantly-supervised named entity recognition. In candidate selection, a class-wise flexible threshold is designed to fully explore other classes besides the high-performance class. In label generation, injecting the distant label, a hybrid pseudo label is adopted to provide straight semantic information for the low-performance class. Experiments on five flat and two nested datasets show that our model achieves state-of-the-art results. We also conduct extensive research to analyze the effectiveness of the flexible threshold and the hybrid pseudo label.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.703',\n",
       "   'authors': ['Qi Li',\n",
       "    'Tingyu Xie',\n",
       "    'Peng Peng',\n",
       "    'Hongwei Wang',\n",
       "    'Gaoang Wang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-1_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P176',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.703.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77277/poster_document/7fbcd5298d01f55dde4dfe3bf6f62fe0.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Class-Rebalancing Self-Training Framework for Distantly-Supervised Named Entity Recognition',\n",
       "   'tldr': 'Distant supervision reduces the reliance on human annotation in the named entity recognition tasks. The class-level imbalanced distant annotation is a realistic and unexplored problem, and the popular method of self-training can not handle class-level imbalanced learning. More importantly, self-trai...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 77277,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77277-a-class-rebalancing-self-training-framework-for-distantly-supervised-named-entity-recognition',\n",
       "   'video_url': None},\n",
       "  'P1772': {'abstract': 'With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27\\\\% and ChatGPT by up to 6.86\\\\% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51\\\\% (up to 23.73\\\\% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.529',\n",
       "   'authors': ['Mathieu Ravaut', 'Shafiq Joty', 'Nancy Chen'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['session-1_-summarization-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1772',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.529.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77528/poster_document/654fb999a205d7ad4f3f5cbb5037e528.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Unsupervised Summarization Re-ranking',\n",
       "   'tldr': 'With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 77528,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77528-unsupervised-summarization-re-ranking',\n",
       "   'video_url': None},\n",
       "  'P1778': {'abstract': 'Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectively using the denoising data. Extensive experiments in language generation benchmarks show that GanLM with the powerful language understanding capability outperforms various strong pre-trained language models (PLMs) and achieves state-of-the-art performance.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.522',\n",
       "   'authors': ['Jian Yang',\n",
       "    'Shuming Ma',\n",
       "    'Li Dong',\n",
       "    'Shaohan Huang',\n",
       "    'Haoyang Huang',\n",
       "    'Yuwei Yin',\n",
       "    'Dongdong Zhang',\n",
       "    'Liqun Yang',\n",
       "    'Furu Wei',\n",
       "    'Zhoujun Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-7_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P1778',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual pre-training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.522.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76631/poster_document/eb2226d83b77bde3a577bad240e6348b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76631/poster/b768c63f90bc555c1a7debe8d8526dda.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76631/slideshow/91e05238a833faaf24212369c37aee8a.pdf',\n",
       "   'title': 'GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator',\n",
       "   'tldr': 'Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encode...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76631,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76631-ganlm-encoder-decoder-pre-training-with-an-auxiliary-discriminator',\n",
       "   'video_url': None},\n",
       "  'P178': {'abstract': \"Despite the popularity of Shapley Values in explaining neural text classification models, computing them is prohibitive for large pretrained models due to a large number of model evaluations. In practice, Shapley Values are often estimated with a small number of stochastic model evaluations. However, we show that the estimated Shapley Values are sensitive to random seed choices -- the top-ranked features often have little overlap across different seeds, especially on examples with longer input texts. This can only be mitigated by aggregating thousands of model evaluations, which on the other hand, induces substantial computational overheads. To mitigate the trade-off between stability and efficiency, we develop an amortized model that directly predicts each input feature's Shapley Value without additional model evaluations. It is trained on a set of examples whose Shapley Values are estimated from a large number of model evaluations to ensure stability. Experimental results on two text classification datasets demonstrate that our amortized model estimates Shapley Values accurately with up to 60 times speedup compared to traditional methods. Further, our model does not suffer from stability issues as inference is deterministic. We release our code at https://github.com/yangalan123/Amortized-Interpretability.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.483',\n",
       "   'authors': ['Chenghao Yang',\n",
       "    'Fan Yin',\n",
       "    'He He',\n",
       "    'Kai-Wei Chang',\n",
       "    'Xiaofei Ma',\n",
       "    'Bing Xiang'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)'],\n",
       "   'id': 'P178',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['feature attribution'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.483.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76179/poster_document/8bec9533b15cf83ed90dc7322794c890.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76179/poster/257aeaac87239f8c0943e4d49ddb2966.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76179/slideshow/be93ef0740d2dbbf5d6c43d0fa8bd931.pptx',\n",
       "   'title': 'Efficient Shapley Values Estimation by Amortization for Text Classification',\n",
       "   'tldr': 'Despite the popularity of Shapley Values in explaining neural text classification models, computing them is prohibitive for large pretrained models due to a large number of model evaluations. In practice, Shapley Values are often estimated with a small number of stochastic model evaluations. However...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76179,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15249/lecture/76179-efficient-shapley-values-estimation-by-amortization-for-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1780': {'abstract': 'Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and the event pair interaction information. In particular, 1) we design an event-causal-label-aware module to model the event causal label information, in which we design the event causal label prediction task as an auxiliary task of ECI, aiming to predict which events are involved in the causal relationship (we call them causality-related events) by mining the dependencies between events. 2) We further design an event pair interaction graph module to model the interaction information between event pairs, in which we construct the interaction graph with event pairs as nodes and leverage graph attention mechanism to model the degree of dependency between event pairs. The experimental results show that our approach outperforms previous state-of-the-art methods on two benchmark datasets EventStoryLine and Causal-TimeBank.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.655',\n",
       "   'authors': ['Ruili Pu',\n",
       "    'Yang Li',\n",
       "    'Suge Wang',\n",
       "    'Deyu Li',\n",
       "    'Jianxing Zheng',\n",
       "    'Jian Liao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-1_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P1780',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['event extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.655.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77529/poster_document/4712a3f48b0352848490a45f6a172acd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77529/poster/681d1d270eec804665dd32988ab40bf3.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Enhancing Event Causality Identification with Event Causal Label and Event Pair Interaction Graph',\n",
       "   'tldr': 'Most existing event causality identification (ECI) methods rarely consider the event causal label information and the interaction information between event pairs. In this paper, we propose a framework to enrich the representation of event pairs by introducing the event causal label information and t...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 77529,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77529-enhancing-event-causality-identification-with-event-causal-label-and-event-pair-interaction-graph',\n",
       "   'video_url': None},\n",
       "  'P1785': {'abstract': 'Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De-En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.131',\n",
       "   'authors': ['Shushu Wang',\n",
       "    'Jing Wu',\n",
       "    'Kai Fan',\n",
       "    'Wei Luo',\n",
       "    'Jun Xiao',\n",
       "    'Zhongqiang Huang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-1_-machine-translation-(poster)'],\n",
       "   'id': 'P1785',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['online adaptation for mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.131.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76632/poster_document/db9006b2484e402e541d74f0e7326e04.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76632/poster/95eda497a06fde1416a2d3b54c0398c2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Better Simultaneous Translation with Monotonic Knowledge Distillation',\n",
       "   'tldr': 'Simultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix train...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76632,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76632-exploring-speaker-related-information-in-spoken-language-understanding-for-better-speaker-diarization',\n",
       "   'video_url': None},\n",
       "  'P1786': {'abstract': \"Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with robust patterns carring innate robustness. In this paper, we show that robust and non-robust instances in the training dataset, though are both important for test performance, have contrary impacts on robustness, which makes it possible to build a highly robust model by leveraging the training dataset in a more effective way. We propose a new method that can distinguish between robust instances from non-robust ones according to the model's sensitivity to perturbations on individual instances during training. Surprisingly, we find that the model under standard training easily overfits the robust instances by relying on their simple patterns before the model completely learns their robust features. Finally, we propose a new mitigation algorithm to further release the potential of robust instances. Experimental results show that proper use of robust instances in the original dataset is a new line to achieve highly robust models.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.146',\n",
       "   'authors': ['Rui Zheng',\n",
       "    'Zhiheng Xi',\n",
       "    'Qin Liu',\n",
       "    'Wenbin Lai',\n",
       "    'Tao Gui',\n",
       "    'Qi Zhang',\n",
       "    'Xuanjing Huang',\n",
       "    'Jin Ma',\n",
       "    'Ying Shan',\n",
       "    'Weifeng Ge'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1786',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['adversarial attacks/examples/training', 'robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.146.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77530/poster_document/8eec3cd1560ba50c17b5c0779cfc85dc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77530/poster/365e1f510ffa29ad7391d2dd604cdbdc.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Characterizing the Impacts of Instances on Robustness',\n",
       "   'tldr': 'Building robust deep neural networks (DNNs) against adversarial attacks is an important but challenging task. Previous defense approaches mainly focus on developing new model structures or training algorithms, but they do little to tap the potential of training instances, especially instances with r...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77530,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77530-characterizing-the-impacts-of-instances-on-robustness',\n",
       "   'video_url': None},\n",
       "  'P1793': {'abstract': \"Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies contrast-aware adversarial training to generate worst-case samples and uses joint class-spread contrastive learning to extract structured representations. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training (CAT) strategy to learn more diverse features from context and enhance the model's context robustness. Under the framework with CAT, we develop a sequence-based SACL-LSTM to learn label-consistent and context-robust features for ERC. Experiments on three datasets show that SACL-LSTM achieves state-of-the-art performance on ERC. Extended experiments prove the effectiveness of SACL and CAT.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.606',\n",
       "   'authors': ['Dou Hu', 'Yinan Bao', 'Lingwei Wei', 'Wei Zhou', 'Songlin Hu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P1793',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['emotion detection and analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.606.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76633/poster_document/5a007042840c7b5671df0012caff515d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76633/poster/43e9c5802a1e09374cabda33cb512b7a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76633/slideshow/7357791f941880c5fec2a0e609fa029a.pdf',\n",
       "   'title': 'Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations',\n",
       "   'tldr': 'Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations in a supervised manner. SACL applies c...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76633,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76633-supervised-adversarial-contrastive-learning-for-emotion-recognition-in-conversations',\n",
       "   'video_url': None},\n",
       "  'P1798': {'abstract': 'Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep and narrow local minima of the loss landscape (high sharpness) while normal samples, which differs distinctly from adversarial ones, reside in the loss surface that is more flatter (low sharpness).om the perspective of sharpness via visualizing the input loss landscape of models.  Based on this, we propose a simple and effective sharpness-based detector to distinct adversarial samples by maximizing the loss increment within the region where the inference sample is located. Considering that the notion of sharpness of a loss landscape is relative, we further propose an adaptive optimization strategy in an attempt to fairly compare the relative sharpness among different samples. Experimental results show that our approach can outperform previous detection methods by large margins (average +6.6 F1 score) for four advanced attack strategies considered in this paper across three text classification tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.717',\n",
       "   'authors': ['Rui Zheng',\n",
       "    'Shihan Dou',\n",
       "    'Yuhao Zhou',\n",
       "    'Qin Liu',\n",
       "    'Tao Gui',\n",
       "    'Qi Zhang',\n",
       "    'Zhongyu Wei',\n",
       "    'Xuanjing Huang',\n",
       "    'Menghan Zhang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1798',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['security/privacy'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.717.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77533/poster_document/daae424463975e4a7bbb2fb15fed35cb.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Detecting Adversarial Samples through Sharpness of Loss Landscape',\n",
       "   'tldr': 'Deep neural networks (DNNs) have been proven to be sensitive towards perturbations on input samples, and previous works highlight that adversarial samples are even more vulnerable than normal ones. In this work, this phenomenon is illustrated frWe first show that adversarial samples locate in steep ...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77533,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77533-detecting-adversarial-samples-through-sharpness-of-loss-landscape',\n",
       "   'video_url': None},\n",
       "  'P1801': {'abstract': 'Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable method for their task. The effort associated with those techniques, such as in writing examples, explanations, instructions, etc. further limits their adoption among non-expert users. In this paper, we propose a simple prompting strategy Help Me Think where we encourage large\\nlanguage models (such as GPT3 and ChatGPT) to help non-expert users by asking a set of relevant questions and leveraging user answers to execute the task. We demonstrate the efficacy of our technique Help Me Think on a variety of tasks. Specifically, we focus on tasks that are hard for average humans and require significant thinking to perform. We hope our work will encourage the development of unconventional ways to harness the power of large language models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.751',\n",
       "   'authors': ['Swaroop Mishra', 'Elnaz Nouri'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1801',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.751.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77534/poster_document/8eb7d388f8fda1958ad3bd3161f79410.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models',\n",
       "   'tldr': 'Controlling the text generated by language models and customizing the content has been a long-standing challenge. Existing prompting techniques proposed in pursuit of providing control are task-specific and lack generality; this provides overwhelming choices for non-expert users to find a suitable m...',\n",
       "   'track': 'Spotlight - Metropolitan Centre',\n",
       "   'underline_id': 77534,\n",
       "   'underline_url': None,\n",
       "   'video_url': None},\n",
       "  'P1806': {'abstract': 'Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensive experiments and in-depth analyses strongly demonstrate the effectiveness of our proposed model and training framework.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.192',\n",
       "   'authors': ['Zhibin Lan',\n",
       "    'Jiawei Yu',\n",
       "    'Xiang Li',\n",
       "    'Wen Zhang',\n",
       "    'Jian Luan',\n",
       "    'Bin Wang',\n",
       "    'Degen Huang',\n",
       "    'Jinsong Su'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['poster-session-7_-machine-translation-(poster)'],\n",
       "   'id': 'P1806',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.192.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76634/poster_document/e96b85da5fb70bed315357c1a3e0e4f9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76634/poster/f179b1b3935ada3905379f53e26e59f9.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Better Text Image Translation with Multimodal Codebook',\n",
       "   'tldr': 'Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly availabl...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76634,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76634-world-to-words-grounded-open-vocabulary-acquisition-through-fast-mapping-in-vision-language-models',\n",
       "   'video_url': None},\n",
       "  'P1808': {'abstract': 'While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and are not accessible to everyone. In addition, numerical reasoning is measured using a single score on existing datasets. As a result, we do not have a clear understanding of the strengths and shortcomings of existing models on different numerical reasoning aspects and therefore, potential ways to improve them apart from scaling them up. Inspired by CheckList (Ribeiro et al., 2020), we introduce a multi-view evaluation set for numerical reasoning in English, called FERMAT. Instead of reporting a single score on a whole dataset, FERMAT evaluates models on various key numerical reasoning aspects such as number understanding, mathematical operations, and training dependency. Apart from providing a comprehensive evaluation of models on different numerical reasoning aspects, FERMAT enables a systematic and automated generation of an arbitrarily large training or evaluation set for each aspect.The datasets and codes are publicly available to generate further multi-view data for ulterior tasks and languages.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.838',\n",
       "   'authors': ['Jasivan Alex Sivakumar', 'Nafise Sadat Moosavi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-3_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1808',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['benchmarking', 'nlp datasets', 'evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.838.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76635/poster_document/234d7d86099dd411ea889f54bc307978.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76635/poster/7f62494ffb05bac579bc481f5c1dca0c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76635/slideshow/967a1ab8ef3b3862dde55e2b27f81464.pdf',\n",
       "   'title': 'FERMAT: An Alternative to Accuracy for Numerical Reasoning',\n",
       "   'tldr': 'While pre-trained language models achieve impressive performance on various NLP benchmarks, they still struggle with tasks that require numerical reasoning. Recent advances in improving numerical reasoning are mostly achieved using very large language models that contain billions of parameters and a...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76635,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76635-distantly-supervised-course-concept-extraction-in-moocs-with-academic-discipline',\n",
       "   'video_url': None},\n",
       "  'P181': {'abstract': 'Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining.\\nUntil now, massively multitask learning required simultaneous access to all datasets in the mixture and heavy compute resources that are only available to well-resourced teams. \\n\\nIn this paper, we propose ColD Fusion, a method that provides the benefits of multitask learning but leverages distributed computation and requires limited communication and no sharing of data. Consequentially, ColD Fusion can create a synergistic loop, where finetuned models can be recycled to continually improve the pretrained model they are based on.\\nWe show that ColD Fusion yields comparable benefits to multitask training by producing a model that (a) attains strong performance on all of the datasets it was multitask trained on and (b) is a better starting point for finetuning on unseen datasets. \\nWe find ColD Fusion outperforms RoBERTa and even previous multitask models. Specifically, when training and testing on 35 diverse datasets, ColD Fusion-based model outperforms RoBERTa by 2.19 points on average without any changes to the architecture.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.46',\n",
       "   'authors': ['Shachar Don-Yehiya',\n",
       "    'Elad Venezian',\n",
       "    'Colin Raffel',\n",
       "    'Noam Slonim',\n",
       "    'Leshem Choshen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-2_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P181',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-task learning',\n",
       "    'transfer learning / domain adaptation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.46.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76377/poster_document/fe59b99cd93487c932f213080324193f.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning',\n",
       "   'tldr': 'Pretraining has been shown to scale well with compute, data size and data diversity. Multitask learning trains on a mixture of supervised datasets and produces improved performance compared to self-supervised pretraining.\\nUntil now, massively multitask learning required simultaneous access to all da...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76377,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76377-cold-fusion-collaborative-descent-for-distributed-multitask-finetuning',\n",
       "   'video_url': None},\n",
       "  'P1810': {'abstract': 'Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news article and multiple target headlines. Toward this, we propose a novel model called DIVHSK. It has two components:\\nKEYSELECT for selecting the important keywords, and SEQGEN, for finally generating the multiple diverse headlines. In KEYSELECT, we cluster the self-attention heads of the last layer of the pre-trained encoder and select the most-attentive theme and general keywords from the source article. Then, cluster-specific keyword sets guide the SEQGEN, a pre-trained encoder-decoder model, to generate diverse yet semantically similar headlines. The proposed model consistently outperformed existing literature and our strong baselines and emerged as a state-of-the-art model. We have also created a high-quality multi-reference headline dataset from news articles.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.118',\n",
       "   'authors': ['Venkatesh E',\n",
       "    'Kaushal Kumar Maurya',\n",
       "    'Deepak Kumar',\n",
       "    'Maunendra Sankar Desarkar'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-4_-generation-(virtual-poster)'],\n",
       "   'id': 'P1810',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation', 'model architectures'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.118.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77535/poster_document/d669d5d3e8d46610a7b8362681e6a3e8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77535/poster/d00a2759b25c7424c398d4bd1590e279.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77535/slideshow/0e83c24602059b41d789ac62c1826896.pdf',\n",
       "   'title': 'DivHSK: Diverse Headline Generation using Self-Attention based Keyword Selection',\n",
       "   'tldr': 'Diverse headline generation is an NLP task where given a news article, the goal is to generate multiple headlines that are true to the content of the article but are different among themselves. This task aims to exhibit and exploit semantically similar one-to-many relationships between a source news...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77535,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77535-divhsk-diverse-headline-generation-using-self-attention-based-keyword-selection',\n",
       "   'video_url': None},\n",
       "  'P1816': {'abstract': 'User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those that currently do require unnecessary modifications to model architecture\\nor do not make full use of user/product associations. The contribution of this work is twofold: i) a method to explicitly employ historical reviews belonging to the same user/product in initializing representations, and ii) efficient incorporation of textual associations between users and products via a user-product cross-context module. Experiments on the IMDb, Yelp-2013 and Yelp-2014 English benchmarks with BERT, SpanBERT and Longformer pretrained language models show that our approach substantially outperforms previous state-of-the-art.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.92',\n",
       "   'authors': ['Chenyang Lyu',\n",
       "    'Linyi Yang',\n",
       "    'Yue Zhang',\n",
       "    'Yvette Graham',\n",
       "    'Jennifer Foster'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P1816',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.92.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77536/poster_document/20aa84fbb4270e9ac191f245dfaae7b1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77536/poster/ccc2abd9a0d52d4763b4b179e3b19c12.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77536/slideshow/0f2dc38acbe7d644ba08941c5c6acaec.pdf',\n",
       "   'title': 'Exploiting Rich Textual User-Product Context for Improving Personalized Sentiment Analysis',\n",
       "   'tldr': 'User and product information associated with a review is useful for sentiment polarity prediction. Typical approaches incorporating such information focus on modeling users and products as implicitly learned representation vectors. Most do not exploit the potential of historical reviews, or those th...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 77536,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77536-stop-pre-training-adapt-visual-language-models-to-unseen-languages',\n",
       "   'video_url': None},\n",
       "  'P1817': {'abstract': 'Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a significant margin when evaluated on the GLUE benchmark. Additionally, we introduce a novel adapter technique that directly applies the adapter to pre-trained parameters instead of the hidden representation, thereby achieving identical inference speed to that of full fine-tuning. Through extensive experiments, our proposed method attains a new state-of-the-art outcome in terms of both performance and storage efficiency, storing only 0.03\\\\% parameters of full fine-tuning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.233',\n",
       "   'authors': ['Baohao Liao', 'Yan Meng', 'Christof Monz'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-3_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P1817',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['transfer learning / domain adaptation',\n",
       "    'parameter-efficient finetuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.233.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76636/poster_document/6a47869db1d6626f2e56dc58e7315690.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76636/poster/89484a029b5e9097a77c2cd2511d11aa.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76636/slideshow/b9182085b6eba6522c212867976047d2.pdf',\n",
       "   'title': 'Parameter-Efficient Fine-Tuning without Introducing New Latency',\n",
       "   'tldr': 'Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constr...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76636,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76636-parameter-efficient-fine-tuning-without-introducing-new-latency',\n",
       "   'video_url': None},\n",
       "  'P1820': {'abstract': 'In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each document cluster as a proxy for its summary. Our objective thus does not require human written summaries and can be utilized for pretraining on a dataset consisting solely of document sets. Through zero-shot, few-shot, and fully supervised experiments on multiple MDS datasets, we show that our model \\\\textit{Centrum} is better or comparable to a state-of-the-art model. We make the pretrained and fine-tuned models freely available to the research community{{https://github.com/ratishsp/centrum}}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.13',\n",
       "   'authors': ['Ratish Surendran Puduppully',\n",
       "    'Parag Jain',\n",
       "    'Nancy Chen',\n",
       "    'Mark Steedman'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-2_-summarization-(poster)'],\n",
       "   'id': 'P1820',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-document summarization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.13.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76637/poster_document/90ed41a2401f7d20243e8a55ee6b6d5b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76637/poster/7ca8544875402c3230fa2c2688d0b424.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Multi-Document Summarization with Centroid-Based Pretraining',\n",
       "   'tldr': 'In Multi-Document Summarization (MDS), the input can be modeled as a set of documents, and the output is its summary. In this paper, we focus on pretraining objectives for MDS. Specifically, we introduce a novel pretraining objective, which involves selecting the ROUGE-based centroid of each documen...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76637,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76637-multi-document-summarization-with-centroid-based-pretraining',\n",
       "   'video_url': None},\n",
       "  'P1821': {'abstract': 'When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (\"windows), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved documents. Our results highlight Parallel Context Windows as a promising method for applying off-the-shelf LLMs in a range of settings that require long text sequences. We make our code publicly available at https://github.com/ ai21labs/parallel-context-windows.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.352',\n",
       "   'authors': ['Nir Ratner',\n",
       "    'Yoav Levine',\n",
       "    'Yonatan Belinkov',\n",
       "    'Ori Ram',\n",
       "    'Inbal Magar',\n",
       "    'Omri Abend',\n",
       "    'Ehud Dov Karpas',\n",
       "    'Amnon Shashua',\n",
       "    'Kevin Leyton-Brown',\n",
       "    'Yoav Shoham'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-4_-large-language-models-(poster)'],\n",
       "   'id': 'P1821',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.352.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76638/poster_document/d183f94eaacb36056dd64145405a7ae0.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76638/poster/a6b158a24921a0b1d9b84be077be6ec3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Parallel Context Windows for Large Language Models',\n",
       "   'tldr': 'When applied to processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off- the-shelf LLMs. We present Parallel Context Windows (PCW), a method tha...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76638,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76638-measuring-progress-in-fine-grained-vision-and-language-understanding',\n",
       "   'video_url': None},\n",
       "  'P1823': {'abstract': \"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model's memorization of the adversarial training data. \\nWe attempt to mitigate robust overfitting by combining regularization methods with adversarial training. \\nFollowing the philosophy that prevents the model from memorizing the adversarial data, we find that flooding, a regularization method with loss scaling, can mitigate robust overfitting for pre-trained language models. Eventually, we investigate the effect of flooding levels and evaluate the models' adversarial robustness under textual attacks. Extensive experiments demonstrate that our methods can mitigate robust overfitting upon three top adversarial training methods and further promote adversarial robustness.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.340',\n",
       "   'authors': ['Bin Zhu', 'Yanghui Rao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1823',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['adversarial training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.340.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77537/poster_document/e30287312bcfb66d4284a9a5cd03a5d3.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploring Robust Overfitting for Pre-trained Language Models',\n",
       "   'tldr': \"We identify the robust overfitting issue for pre-trained language models by showing that the robust test loss increases as the epoch grows. Through comprehensive exploration of the robust loss on the training set, we attribute robust overfitting to the model's memorization of the adversarial trainin...\",\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77537,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77537-exploring-robust-overfitting-for-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P1829': {'abstract': 'The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored in terms of effectively training NMT models with high CR abilities and devising accurate automatic evaluation metrics. This paper presents a comprehensive study aimed at expanding the understanding of CR in NMT.\\nFor the training, we confirm the effectiveness of incorporating pretrained knowledge into NMT models and subsequently utilizing these models as robust testbeds for investigating CR in NMT. For the evaluation, we propose a novel entity-aware evaluation method that takes into account both the NMT candidate and important entities in the candidate, which is more aligned with human judgement. Based on the strong testbed and evaluation methods, we identify challenges in training NMT models with high CR abilities and suggest directions for further unlabeled data utilization and model design. We hope that our methods and findings will contribute to advancing the research of CR in NMT. Source data, code and scripts are freely available at https://github.com/YutongWang1216/CR-NMT.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.866',\n",
       "   'authors': ['Xuebo Liu',\n",
       "    'Yutong Wang',\n",
       "    'Derek F. Wong',\n",
       "    'Runzhe Zhan',\n",
       "    'Liangxuan Yu',\n",
       "    'Min Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-4_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P1829',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.866.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76639/poster_document/566068b171fdfd3715c6066b4d20a2cc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76639/poster/75ab3ee0888b6d20a752200016f15057.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Revisiting Commonsense Reasoning in Machine Translation: Training, Evaluation and Challenge',\n",
       "   'tldr': 'The ability of commonsense reasoning (CR) decides whether a neural machine translation (NMT) model can move beyond pattern recognition. Despite the rapid advancement of NMT and the use of pretraining to enhance NMT models, research on CR in NMT is still in its infancy, leaving much to be explored in...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76639,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76639-is-continuous-prompt-a-combination-of-discrete-promptsquestion-towards-a-novel-view-for-interpreting-continuous-prompts',\n",
       "   'video_url': None},\n",
       "  'P1831': {'abstract': 'Monolingual word alignment is crucial to model semantic interactions between sentences.\\nIn particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. \\nIdentification of null alignment is useful on its own to reason about the semantic similarity of sentences by indicating there exists information inequality. \\nTo achieve unbalanced word alignment that values both alignment and null alignment, this study shows that the family of optimal transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and powerful approaches even without tailor-made techniques.\\nOur extensive experiments covering unsupervised and supervised settings indicate that our generic OT-based alignment methods are competitive against the state-of-the-arts specially designed for word alignment, remarkably on challenging datasets with high null alignment frequencies.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.219',\n",
       "   'authors': ['Yuki Arase', 'Han Bao', 'Sho Yokoi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P1831',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['word/phrase alignment'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.219.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76640/poster_document/cf0fc4d89c954d53a59c8eb6d32bcf05.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76640/poster/4612527d21c7697530d91412a14eb34f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76640/slideshow/078649d847f5db7505fdd4003a771fab.pdf',\n",
       "   'title': 'Unbalanced Optimal Transport for Unbalanced Word Alignment',\n",
       "   'tldr': 'Monolingual word alignment is crucial to model semantic interactions between sentences.\\nIn particular, null alignment, a phenomenon in which words have no corresponding counterparts, is pervasive and critical in handling semantically divergent sentences. \\nIdentification of null alignment is useful o...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76640,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76640-unbalanced-optimal-transport-for-unbalanced-word-alignment',\n",
       "   'video_url': None},\n",
       "  'P1833': {'abstract': 'Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention pruning), which performs task-specific pruning with knowledge distillation and yields highly effective models. Different from common approaches that prune each attention head as a whole, GRAIN inspects and prunes intra-attention structures, which greatly expands the structure search space and enables more flexible models. We also propose a gradient separation strategy that reduces the interference of distillation on pruning for a better combination of the two approaches. Experiments on GLUE, SQuAD, and CoNLL 2003 show that GRAIN notably outperforms other methods, especially in the high sparsity regime, and achieves 6~7x speedups while maintaining 93\\\\%~99\\\\% performance. Under extreme compression where only 3\\\\% transformer weights remain, the pruned model is still competitive compared to larger models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.156',\n",
       "   'authors': ['Ziqing Yang', 'Yiming Cui', 'Xin Yao', 'Shijin Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1833',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model compression methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.156.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76641/poster_document/922567cbad7874a56aa11e68404ca546.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76641/poster/6ebd21e9de92fcd519e7a27d42d96894.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76641/slideshow/ac2452cf44253a5f7b6a5039febcf600.pdf',\n",
       "   'title': 'Gradient-based Intra-attention Pruning on Pre-trained Language Models',\n",
       "   'tldr': 'Pre-trained language models achieve superior performance but are computationally expensive. Techniques such as pruning and knowledge distillation have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method GRAIN (gradient-based intra-attention prunin...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76641,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76641-gradient-based-intra-attention-pruning-on-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P1834': {'abstract': 'Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the  largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS achieves state-of-the-art performances on a diverse set of Arabic classification and generative tasks. Moreover, AraMUS shows impressive few-shot learning abilities compared with the best existing Arabic PLMs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.181',\n",
       "   'authors': ['Asaad Alghamdi',\n",
       "    'Xinyu Duan',\n",
       "    'Wei Jiang',\n",
       "    'Zhenhai Wang',\n",
       "    'Yimeng Wu',\n",
       "    'Qingrong Xia',\n",
       "    'Zhefeng Wang',\n",
       "    'Yi ZHENG',\n",
       "    'Mehdi Rezagholizadeh',\n",
       "    'baoxing Huai',\n",
       "    'Peilun Cheng',\n",
       "    'Abbas Ghaddar'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1834',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training'],\n",
       "   'languages': ['arabic'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.181.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77539/poster/02a2a48e449772ca21c4bf9b64290d09.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'AraMUS: Pushing the Limits of Data and Model Scale for Arabic Natural Language Processing',\n",
       "   'tldr': 'Developing monolingual large Pre-trained Language Models (PLMs) is shown to be very successful in handling different tasks in Natural Language Processing (NLP). In this work, we present AraMUS, the  largest Arabic PLM with 11B parameters trained on 529GB of high-quality Arabic textual data. AraMUS a...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77539,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77539-aramus-pushing-the-limits-of-data-and-model-scale-for-arabic-natural-language-processing',\n",
       "   'video_url': None},\n",
       "  'P1843': {'abstract': \"Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given static layers into fully dynamic ones where all parameters are dynamic (at least within a single layer) and vary with the input. However, such a fully dynamic setting may cause redundant parameters and high deployment costs, limiting the applicability of dynamic networks to a broader range of tasks and models. The main contributions of our work are challenging the basic commonsense in dynamic networks and proposing a partially dynamic network, namely PAD-Net, to transform the redundant dynamic parameters into static ones. Also, we further design Iterative Mode Partition to partition dynamic and static parameters efficiently. Our method is comprehensively supported by large-scale experiments with two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on both image classification and GLUE benchmarks. Encouragingly, we surpass the fully dynamic networks by $+0.7\\\\%$ top-1 acc with only $30\\\\%$ dynamic parameters for ResNet-50 and $+1.9\\\\%$ average score in language understanding with only $50\\\\%$ dynamic parameters for BERT. Code will be released at: {https://github.com/Shwai-He/PAD-Net}.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.803',\n",
       "   'authors': ['Shwai He',\n",
       "    'Liang Ding',\n",
       "    'Daize Dong',\n",
       "    'Boan Liu',\n",
       "    'Fuqiang Yu',\n",
       "    'Dacheng Tao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1843',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model compression methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.803.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76642/poster_document/f3c4170e563d36ee3b62543b022e3c55.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76642/poster/15b437171cca5e32aa57a3a197b6652d.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76642/slideshow/87a019e843b9e7c7980d2696b71e1419.pptx',\n",
       "   'title': 'PAD-Net: An Efficient Framework for Dynamic Networks',\n",
       "   'tldr': \"Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture of Experts (MoE), have been extensively explored as they can considerably improve the model's representation power with acceptable computational cost. The common practice in implementing dynamic networks is to convert the given st...\",\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76642,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76642-pad-net-an-efficient-framework-for-dynamic-networks',\n",
       "   'video_url': None},\n",
       "  'P1847': {'abstract': 'Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the errors propagated from the retriever, but also demands additional effort to develop both the retriever and the reader, which further makes it slower since they are not runnable in parallel. In this work, we propose a method to directly predict answers with a phrase retrieval scheme for a sequence of words, reducing the conventional two distinct subtasks into a single one. Also, for the first time, we study its capability for ODConvQA tasks. However, simply adopting it is largely problematic, due to the dependencies between previous and current turns in a conversation. To address this problem, we further introduce a novel contrastive learning strategy, making sure to reflect previous turns when retrieving the phrase for the current context, by maximizing representational similarities of consecutive turns in a conversation while minimizing irrelevant conversational contexts. We validate our model on two ODConvQA datasets, whose experimental results show that it substantially outperforms the relevant baselines with the retriever-reader. Code is available at: https://github.com/starsuzi/PRO-ConvQA.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.374',\n",
       "   'authors': ['Soyeong Jeong', 'Jinheon Baek', 'Sung Ju Hwang', 'Jong Park'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1847',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.374.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77541/poster_document/d88574516190a0831db5676b6e2ddcf8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77541/poster/3bab5300735b1ee2068e755d487f153f.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77541/slideshow/5cbfae63300854745f44a2b14ff2f8d0.pdf',\n",
       "   'title': 'Phrase Retrieval for Open Domain Conversational Question Answering with Conversational Dependency Modeling via Contrastive Learning',\n",
       "   'tldr': 'Open-Domain Conversational Question Answering (ODConvQA) aims at answering questions through a multi-turn conversation based on a retriever-reader pipeline, which retrieves passages and then predicts answers with them. However, such a pipeline approach not only makes the reader vulnerable to the err...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77541,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77541-phrase-retrieval-for-open-domain-conversational-question-answering-with-conversational-dependency-modeling-via-contrastive-learning',\n",
       "   'video_url': None},\n",
       "  'P1854': {'abstract': \"We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection.  Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and leverage a pointer network to autoregressively select spans by that order. To deal with \\n discontinuous spans, we consecutively select their subspans from left to right, label all but last subspans with special discontinuous labels and the last subspan as the whole discontinuous spans' labels. We use simple heuristic to output valid trees so that our approach is able to predict all possible continuous and discontinuous constituency trees without sacrificing data coverage and without \\n the need to use expensive chart-based parsing algorithms. Experiments on multiple continuous and discontinuous benchmarks show that our model achieves state-of-the-art or competitive performance.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.469',\n",
       "   'authors': ['Songlin Yang', 'Kewei Tu'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'event_ids': ['session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)'],\n",
       "   'id': 'P1854',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['constituency parsing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.469.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76231/poster_document/643405c6c48cdb001e3e0d0cfa8b9016.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76231/poster/9055505dcb86bfc526081a67aaba9261.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76231/slideshow/97b0b044b60df99007907353fbf26cce.pdf',\n",
       "   'title': \"Don't Parse, Choose Spans! Continuous and Discontinuous Constituency Parsing via Autoregressive Span Selection\",\n",
       "   'tldr': 'We present a simple and unified approach for both continuous and discontinuous constituency parsing via autoregressive span selection.  Constituency parsing aims to produce a set of non-crossing spans so that they can form a constituency parse tree. We sort gold spans using a predefined order and le...',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'underline_id': 76231,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15211/lecture/76231-don-t-parse-choose-spans-continuous-and-discontinuous-constituency-parsing-via-autoregressive-span-selection',\n",
       "   'video_url': None},\n",
       "  'P1855': {'abstract': \"Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge distillation on target-language unlabeled data. However, these methods may suffer from label noise due to the automatic labeling process. In this paper, we propose CoLaDa, a Collaborative Label Denoising Framework, to address this problem. Specifically, we first explore a model-collaboration-based denoising scheme that enables models trained on different data sources to collaboratively denoise pseudo labels used by each other. We then present an instance-collaboration-based strategy that considers the label consistency of each token's neighborhood in the representation space for denoising. Experiments on different benchmark datasets show that the proposed CoLaDa achieves superior results compared to previous methods, especially when generalizing to distant languages.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.330',\n",
       "   'authors': ['Tingting Ma',\n",
       "    'Qianhui Wu',\n",
       "    'Huiqiang Jiang',\n",
       "    'Brje F. Karlsson',\n",
       "    'Tiejun Zhao',\n",
       "    'Chin-Yew Lin'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P1855',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.330.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76643/poster_document/986887408cc500615e199b4b2f07f01a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76643/poster/c4ed8aa43d9274d2e7b9e6e8dfd827c5.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CoLaDa: A Collaborative Label Denoising Framework for Cross-lingual Named Entity Recognition',\n",
       "   'tldr': 'Cross-lingual named entity recognition (NER) aims to train an NER system that generalizes well to a target language by leveraging labeled data in a given source language. Previous work alleviates the data scarcity problem by translating source-language labeled data or performing knowledge distillati...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76643,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76643-colada-a-collaborative-label-denoising-framework-for-cross-lingual-named-entity-recognition',\n",
       "   'video_url': None},\n",
       "  'P1856': {'abstract': 'Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.243',\n",
       "   'authors': ['Yang Luo',\n",
       "    'Xiaozhe REN',\n",
       "    'Zangwei Zheng',\n",
       "    'ZHUO JIANG',\n",
       "    'Xin Jiang',\n",
       "    'Yang You'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1856',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['optimization methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.243.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76644/poster_document/800269e93ed2e4d4359f751606459969.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76644/poster/a38d32c50aab9ffdcae26a9cabba83c2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'CAME: Confidence-guided Adaptive Memory Efficient Optimization',\n",
       "   'tldr': 'Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76644,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76644-came-confidence-guided-adaptive-memory-efficient-optimization',\n",
       "   'video_url': None},\n",
       "  'P1857': {'abstract': 'Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.61',\n",
       "   'authors': ['Sukmin Cho', 'Soyeong Jeong', 'Jeong yeon Seo', 'Jong Park'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-1_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1857',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['re-ranking'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.61.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77543/poster_document/7f2fd02ce2b8c188a8e97199e065ec17.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77543/poster/186dcdb613494bd1cb4f2676f5c5d67f.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77543/slideshow/b49d2d0b3aed3aba4d04e787edca9a13.pdf',\n",
       "   'title': 'Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker',\n",
       "   'tldr': 'Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker w...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 77543,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77543-discrete-prompt-optimization-via-constrained-generation-for-zero-shot-re-ranker',\n",
       "   'video_url': None},\n",
       "  'P186': {'abstract': 'Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on faithful reasoning steps. The reasoning steps are organized as a structured entailment tree, which shows how premises are used to produce intermediate conclusions that can prove the correctness of the answer. We formulate the task as a discrete decision-making problem and solve it through the interaction of a reasoning environment and a controller. The environment is modular and contains several basic task-oriented modules, while the controller proposes actions to assemble the modules. Since the search space could be large, we introduce a Monte-Carlo planning algorithm to do a look-ahead search and select actions that will eventually lead to high-quality steps. FAME achieves advanced performance on the standard benchmark. It can produce valid and faithful reasoning steps compared with large language models with a much smaller model size.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.218',\n",
       "   'authors': ['Ruixin Hong',\n",
       "    'Hongming Zhang',\n",
       "    'Hong Zhao',\n",
       "    'Dong Yu',\n",
       "    'Changshui Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-1_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P186',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['interpretability', 'reasoning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.218.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76378/poster_document/476ce90de06db2d04dc3de0507f422ff.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Faithful Question Answering with Monte-Carlo Planning',\n",
       "   'tldr': 'Although large language models demonstrate remarkable question-answering performances, revealing the intermediate reasoning steps that the models faithfully follow remains challenging. In this paper, we propose FAME (FAithful question answering with MontE-carlo planning) to answer questions based on...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76378,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76378-faithful-question-answering-with-monte-carlo-planning',\n",
       "   'video_url': None},\n",
       "  'P1867': {'abstract': 'Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribute prediction is independent, and fall short of predicting the appearance of new nodes that were not observed in the past. In this paper, we address two interrelated questions; (1) can we exploit task interdependence to improve prediction accuracy? and (2) can we predict new nodes with their attributes? We propose a unified framework that predicts node attributes and topology changes such as the appearance and disappearance of links and the emergence and loss of nodes. This frame-work comprises components for independent and interactive prediction and for predicting new nodes. Our experimental study using real-world data confirms that our interdependent prediction framework achieves higher accuracy than methods based on independent prediction.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.765',\n",
       "   'authors': ['Shohei Yamasaki',\n",
       "    'Yuya Sasaki',\n",
       "    'Panagiotis Karras',\n",
       "    'Makoto Onizuka'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-6_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P1867',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['graph-based methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.765.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76645/poster_document/4ea92e9e1839adc96d619e5bf82cd85b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76645/poster/50b18815fa8755f767241255aea23f09.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76645/slideshow/bddb41c75f77f82bbae64e6644f24a0c.pdf',\n",
       "   'title': 'Holistic Prediction on a Time-Evolving Attributed Graph',\n",
       "   'tldr': 'Graph-based prediction is essential in NLP tasks such as temporal knowledge graph completion. A cardinal question in this field is, how to predict the future links, nodes, and attributes of a time-evolving attributed graph? Unfortunately, existing techniques assume that each link, node, and attribut...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76645,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76645-holistic-prediction-on-a-time-evolving-attributed-graph',\n",
       "   'video_url': None},\n",
       "  'P187': {'abstract': 'In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods.\\n\\nOne of the more prominent methods involves using the text-generation capabilities of language models. In this paper, we propose Text AUgmentation by Dataset Reconstruction (TAU-DR), a novel method of data augmentation for text classification. We conduct experiments on several multi-class datasets, showing that our approach improves the current state-of-the-art techniques for data augmentation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.466',\n",
       "   'authors': ['Adir Rahamim',\n",
       "    'Guy Uziel',\n",
       "    'Esther Goldbraich',\n",
       "    'Ateret Anaby Tavor'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P187',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data augmentation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.466.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77279/poster_document/22893ee657052df8afe9acd8a09378a0.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Text Augmentation Using Dataset Reconstruction for Low-Resource Classification',\n",
       "   'tldr': 'In the deployment of real-world text classification models, label scarcity is a common problem and as the number of classes increases, this problem becomes even more complex. An approach to addressing this problem is by applying text augmentation methods.\\n\\nOne of the more prominent methods involves ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77279,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77279-text-augmentation-using-dataset-reconstruction-for-low-resource-classification',\n",
       "   'video_url': None},\n",
       "  'P1876': {'abstract': 'We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.290',\n",
       "   'authors': ['Takashi Wada',\n",
       "    'Yuji Matsumoto',\n",
       "    'Timothy Baldwin',\n",
       "    'Jey Han Lau'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-1_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1876',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-word expressions', 'paraphrasing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.290.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77547/poster/97fd2956b74493245204221efa00fd63.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77547/slideshow/6fc6620b9da744a0143e39e7e987c824.pdf',\n",
       "   'title': 'Unsupervised Paraphrasing of Multiword Expressions',\n",
       "   'tldr': 'We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEv...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77547,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77547-few-shot-reranking-for-multi-hop-qa-via-language-model-prompting',\n",
       "   'video_url': None},\n",
       "  'P1879': {'abstract': 'The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from them, and to date, there are no such systematic studies focused solely on English to Indian language MT. This paper fills this gap through a Multidimensional Quality Metric (MQM) dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems. We evaluate 16 metrics and show that, pre-trained metrics like COMET have the highest correlations with annotator scores as opposed to n-gram metrics like BLEU. We further leverage our MQM annotations to develop an Indic-COMET metric and show that it outperforms COMET counterparts in both human scores correlations and robustness scores in Indian languages. Additionally, we show that the Indic-COMET can outperform COMET on some unseen Indian languages. We hope that our dataset and analysis will facilitate further research in Indic MT evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.795',\n",
       "   'authors': ['Ananya Sai B',\n",
       "    'Tanay Dixit',\n",
       "    'Vignesh Nagarajan',\n",
       "    'Anoop Kunchukuttan',\n",
       "    'Pratyush Kumar',\n",
       "    'Mitesh M. Khapra',\n",
       "    'Raj Dabre'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-7_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P1879',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['corpus creation',\n",
       "    'evaluation',\n",
       "    'datasets for low resource languages',\n",
       "    'metrics'],\n",
       "   'languages': ['hindi', 'gujarati', 'marathi', 'tamil', 'malayalam'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.795.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76647/poster_document/b683ca52dd1e19662e9ead81f80f293b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76647/poster/03c3bfecdf8adad4b1edae4250c8e482.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76647/slideshow/ab1a9fa60dd0fc914e07f47cd55614ec.pdf',\n",
       "   'title': 'IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages',\n",
       "   'tldr': 'The rapid growth of machine translation (MT) systems necessitates meta-evaluations of evaluation metrics to enable selection of those that best reflect MT quality. Unfortunately, most meta-evaluation studies focus on European languages, the observations for which may not always apply to other langua...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76647,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76647-infoverse-a-universal-framework-for-dataset-characterization-with-multidimensional-meta-information',\n",
       "   'video_url': None},\n",
       "  'P1880': {'abstract': 'Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for pre-trained models are designed for understanding tasks. In this paper, we propose task-agnostic backdoor attacks for code pre-trained models. Our backdoored model is pre-trained with two learning strategies (i.e., Poisoned Seq2Seq learning and token representation learning) to support the multi-target attack of downstream code understanding and generation tasks. During the deployment phase, the implanted backdoors in the victim models can be activated by the designed triggers to achieve the targeted attack. We evaluate our approach on two code understanding tasks and three code generation tasks over seven datasets. Extensive experimental results demonstrate that our approach effectively and stealthily attacks code-related downstream tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.399',\n",
       "   'authors': ['Yanzhou Li',\n",
       "    'Shangqing Liu',\n",
       "    'Kangjie Chen',\n",
       "    'Xiaofei Xie',\n",
       "    'Tianwei Zhang',\n",
       "    'Yang Liu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P1880',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training', 'security and privacy'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.399.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76648/poster_document/351ae5495181081c53e5ef35b5b9422a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76648/poster/b8b02897d8ada7ffb257445edd0bb56a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76648/slideshow/e6f9fd6c95f8d0dc975079fd3705fb78.pdf',\n",
       "   'title': 'Multi-target Backdoor Attacks for Code Pre-trained Models',\n",
       "   'tldr': 'Backdoor attacks for neural code models have gained considerable attention due to the advancement of code intelligence. However, most existing works insert triggers into task-specific data for code-related downstream tasks, thereby limiting the scope of attacks. Moreover, the majority of attacks for...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76648,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76648-multi-target-backdoor-attacks-for-code-pre-trained-models',\n",
       "   'video_url': None},\n",
       "  'P1886': {'abstract': 'Argument Quality Detection is an emerging field in NLP which has seen significant recent development. However, existing datasets in this field suffer from a lack of quality, quantity and diversity of topics and arguments, specifically the presence of vague arguments that are not persuasive in nature. In this paper, we leverage a combined experience of 10+ years of Parliamentary Debating to create a dataset that covers significantly more topics and has a wide range of sources to capture more diversity of opinion. With 34,890 high-quality argument-analysis pairs (a term we introduce in this paper), this is also the largest dataset of its kind to our knowledge. In addition to this contribution, we introduce an innovative argument scoring system based on instance-level annotator reliability and propose a quantitative model of scoring the relevance of arguments to a range of topics.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.778',\n",
       "   'authors': ['Omkar Jayant Joshi', 'Priya N Pitre', 'Yashodhara Haribhakta'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['poster-session-5_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)'],\n",
       "   'id': 'P1886',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument quality assessment'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.778.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76649/poster_document/6cf407e1d561f59f52028f4b36d77a04.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76649/poster/1aebfa617cd281eec79893ec9a6b3861.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76649/slideshow/5bafa281ebdfc585bf81ec54928382c6.pptx',\n",
       "   'title': 'ArgAnalysis35K : A large-scale dataset for Argument Quality Analysis',\n",
       "   'tldr': 'Argument Quality Detection is an emerging field in NLP which has seen significant recent development. However, existing datasets in this field suffer from a lack of quality, quantity and diversity of topics and arguments, specifically the presence of vague arguments that are not persuasive in nature...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76649,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/76649-arganalysis35k-a-large-scale-dataset-for-argument-quality-analysis',\n",
       "   'video_url': None},\n",
       "  'P1890': {'abstract': 'When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. \\nOn the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from additional information about context, writing prompt and argument type. We then explore the different combinations of three tasks: automated span detection, type and quality prediction. Results show that a multi-task learning approach combining the three tasks outperforms sequential approaches that first learn to segment and then predict the quality/type of a segment.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.825',\n",
       "   'authors': ['Yuning Ding', 'Marie Bexte', 'Andrea Horbach'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-1_-nlp-applications-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P1890',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.825.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77552/poster_document/6531d288fafcc52ed482151b8b05539a.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77552/poster/ca323a0b8cdf5f6c26ae508a6f43f319.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77552/slideshow/f11daad3970c451d5669c4f9833630c6.pdf',\n",
       "   'title': 'Score It All Together: A Multi-Task Learning Study on Automatic Scoring of Argumentative Essays',\n",
       "   'tldr': 'When scoring argumentative essays in an educational context, not only the presence or absence of certain argumentative elements but also their quality is important. \\nOn the recently published student essay dataset PERSUADE, we first show that the automatic scoring of argument quality benefits from a...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77552,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77552-transfer-and-active-learning-for-dissonance-detection-addressing-the-rare-class-challenge',\n",
       "   'video_url': None},\n",
       "  'P1897': {'abstract': 'Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training settings and can not be generalized to all politicians and other tasks. In this paper, we propose a Unified Pre-training Architecture for Political Actor Modeling based on language (UPPAM). In UPPAM, we aggregate statements to represent political actors and learn the mapping from languages to representation, instead of learning the representation of particular persons. We further design structure-aware contrastive learning and behavior-driven contrastive learning tasks, to inject multidimensional information in the political context into the mapping. In this framework, we can profile political actors from different aspects and solve various downstream tasks. Experimental results demonstrate the effectiveness and capability of generalization of our method.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.670',\n",
       "   'authors': ['Xinyi Mou', 'Zhongyu Wei', 'Qi Zhang', 'Xuanjing Huang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P1897',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp tools for social analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.670.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76650/poster_document/a6b89c8ef8e05e091dcbba643b8d1375.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76650/poster/3c6e8fc2e17b3ed7c5080777dc9d9b4a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76650/slideshow/963ba343f033e82686960fcf37467a0a.pdf',\n",
       "   'title': 'UPPAM: A Unified Pre-training Architecture for Political Actor Modeling based on Language',\n",
       "   'tldr': 'Modeling political actors is at the core of quantitative political science. Existing works have incorporated contextual information to better learn the representation of political actors for specific tasks through graph models. However, they are limited to the structure and objective of training set...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76650,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76650-uppam-a-unified-pre-training-architecture-for-political-actor-modeling-based-on-language',\n",
       "   'video_url': None},\n",
       "  'P1915': {'abstract': 'Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages? Can models compositionally generalize cross-lingually? As a first step to answering these questions, recent work used neural machine translation to translate datasets for evaluating compositional generalization in semantic parsing. However, we show that this entails critical semantic distortion. To address this limitation, we craft a faithful rule-based translation of the MCWQ dataset from English to Chinese and Japanese. Even with the resulting robust benchmark, which we call MCWQ-R, we show that the distribution of compositions still suffers due to linguistic divergences, and that multilingual models still struggle with cross-lingual compositional generalization. Our dataset and methodology will serve as useful resources for the study of cross-lingual compositional generalization in other tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.93',\n",
       "   'authors': ['Zi Wang', 'Daniel Hershcovich'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-1_-multilingualism-and-cross-lingual-nlp-(oral)'],\n",
       "   'id': 'P1915',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer',\n",
       "    'multilingual benchmarks',\n",
       "    'multilingual evaluation'],\n",
       "   'languages': ['chinese', 'japanese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.93.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76232/poster_document/5d4a79237d7fa369a00118a96ce0fa6d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76232/poster/7affa4d5a2ae030610b9443277651482.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76232/slideshow/7c655dfe5f99eb53d31a73ccacff3681.pdf',\n",
       "   'title': 'On Evaluating Multilingual Compositional Generalization with Translated Datasets',\n",
       "   'tldr': 'Compositional generalization allows efficient learning and human-like inductive biases. Since most research investigating compositional generalization in NLP is done on English, important questions remain underexplored. Do the necessary compositional generalization abilities differ across languages?...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76232,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15199/lecture/76232-on-evaluating-multilingual-compositional-generalization-with-translated-datasets',\n",
       "   'video_url': None},\n",
       "  'P1916': {'abstract': 'Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text which further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments validate the superiority of the proposed method which outperforms the state-of-the-art by 6.17\\\\% accuracy. We also conduct comprehensive ablations of each component, and systematically study the roles of varying types of knowledge. Codes and knowledge data are to be released.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.614',\n",
       "   'authors': ['Qingyi Si',\n",
       "    'Yuchen Mo',\n",
       "    'Zheng Lin',\n",
       "    'HUISHAN JI',\n",
       "    'Weiping Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-1_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1916',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodal qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.614.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76651/poster_document/8843b239c779f966b768c76595d2c9c8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76651/poster/10e8dc477b11e93fa05e081fd834348b.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Combo of Thinking and Observing for Outside-Knowledge VQA',\n",
       "   'tldr': 'Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language spa...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76651,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76651-combo-of-thinking-and-observing-for-outside-knowledge-vqa',\n",
       "   'video_url': None},\n",
       "  'P1917': {'abstract': 'The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance and the long size of legal documents, BM25 remains a strong baseline for ranking the cited prior documents. In this work, we explore the role of events in legal case retrieval and propose an unsupervised retrieval method-based pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find that the proposed unsupervised retrieval method significantly increases performance compared to BM25 and makes retrieval faster by a considerable margin, making it applicable to real-time case retrieval systems. Our proposed system is generic, we show that it generalizes across two different legal systems (Indian and Canadian), and it shows state-of-the-art performance on the benchmarks for both the legal systems (IL-PCR and COLIEE corpora).',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.777',\n",
       "   'authors': ['Abhinav Joshi',\n",
       "    'Akshat Sharma',\n",
       "    'Sai Kiran Tanikella',\n",
       "    'Ashutosh Modi'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-7_-nlp-applications-(poster)'],\n",
       "   'id': 'P1917',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['legal nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.777.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76652/poster_document/c3eec7dca46d52dccc68ae3d9307e972.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76652/poster/08c19d7157dabbf803ef14e9193a8f2c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76652/slideshow/ee1c42867b3857eeb63d7325115a5fac.pdf',\n",
       "   'title': 'U-CREAT: Unsupervised Case Retrieval using Events extrAcTion',\n",
       "   'tldr': 'The task of Prior Case Retrieval (PCR) in the legal domain is about automatically citing relevant (based on facts and precedence) prior legal cases in a given query case. To further promote research in PCR, in this paper, we propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76652,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76652-direct-fact-retrieval-from-knowledge-graphs-without-entity-linking',\n",
       "   'video_url': None},\n",
       "  'P1922': {'abstract': 'Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on specific multilingual PLMs, which leads to a much larger model or give up potentially helpful knowledge from target PLMs.  In this paper, we propose a new monolingual PLM-sponsored NMT model to let both encoder and decoder enjoy PLM enhancement to alleviate such obvious inconvenience. Especially, incorporating a newly proposed frequency-weighted embedding transformation algorithm, PLM embeddings can be effectively exploited in terms of the representations of the NMT decoder. We evaluate our model on IWSLT14 En-De, De-En, WMT14 En-De, and En-Fr tasks, and the results show that our proposed PLM enhancement gives significant improvement and even helps achieve new state-of-the-art.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.222',\n",
       "   'authors': ['Sufeng Duan', 'Hai Zhao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-4_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P1922',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training for mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.222.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77556/poster_document/5942552ab298ddf178224cb6d3674a1d.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Encoder and Decoder, Not One Less for Pre-trained Language Model Sponsored NMT',\n",
       "   'tldr': 'Well pre-trained contextualized representations from pre-trained language model (PLM) have been shown helpful for enhancing various natural language processing tasks, surely including neural machine translation (NMT). However, existing methods either consider encoder-only enhancement or rely on spec...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77556,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77556-continual-knowledge-distillation-for-neural-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P1924': {'abstract': 'In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models\\' in-context learning ability by pre-training the model on a large collection of \"intrinsic tasks\" in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the Super-NaturalInstrctions benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github.com/thu-coai/PICL.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.267',\n",
       "   'authors': ['Yuxian Gu', 'Li Dong', 'Furu Wei', 'Minlie Huang'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-3_-large-language-models-(oral)'],\n",
       "   'id': 'P1924',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training', 'prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.267.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76233/poster_document/2e67f4916cd1871265a335f31edeb6e4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76233/poster/49fbdbe6f8550469aa36d6d1774f22d0.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76233/slideshow/cd226ae41c7156e5eb06ad74fa4cb697.pptx',\n",
       "   'title': 'Pre-Training to Learn in Context',\n",
       "   'tldr': 'In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly traine...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76233,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15223/lecture/76233-pre-training-to-learn-in-context',\n",
       "   'video_url': None},\n",
       "  'P1925': {'abstract': 'Recent advances in pre-trained language models (PLMs) have facilitated the development of\\ncommonsense reasoning tasks. However, existing methods rely on multi-hop knowledge\\nretrieval and thus suffer low accuracy due to\\nembedded noise in the acquired knowledge.\\nIn addition, these methods often attain high\\ncomputational costs and nontrivial knowledge\\nloss because they encode the knowledge independently of the PLM, making it less relevant to the task and thus resulting in a poor\\nlocal optimum. In this work, we propose MultiView Knowledge Retrieval with Prompt Tuning (MVP-Tuning). MVP-Tuning leverages\\nsimilar question-answer pairs in the training set\\nto improve knowledge retrieval and employs\\na single prompt-tuned PLM to model knowledge and input text jointly. We conduct our experiments on five commonsense reasoning QA\\nbenchmarks to show that MVP-Tuning outperforms all other baselines in 4 out of 5 datasets\\nwith less than 2\\\\% trainable parameters. MVPTuning even gets a new state-of-the-art result\\non OpenBookQA and is number one on the\\nleaderboard.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.750',\n",
       "   'authors': ['Yongfeng Huang',\n",
       "    'Yanyang Li',\n",
       "    'Yichong Xu',\n",
       "    'Lin Zhang',\n",
       "    'ruyi gan',\n",
       "    'Jiaxing Zhang',\n",
       "    'Liwei Wang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1925',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['commonsense qa', 'knowledge base qa', 'open-domain qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.750.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76653/poster_document/43138e5ad4271742804be5a86adefa9c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76653/poster/305d31e8e019f8970444cf61e57399d2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MVP-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning',\n",
       "   'tldr': 'Recent advances in pre-trained language models (PLMs) have facilitated the development of\\ncommonsense reasoning tasks. However, existing methods rely on multi-hop knowledge\\nretrieval and thus suffer low accuracy due to\\nembedded noise in the acquired knowledge.\\nIn addition, these methods often attain...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76653,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76653-mvp-tuning-multi-view-knowledge-retrieval-with-prompt-tuning-for-commonsense-reasoning',\n",
       "   'video_url': None},\n",
       "  'P1934': {'abstract': 'kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability issue of this approach: what knowledge does the NMT model need? We propose the notion of local correctness (LAC) as a new angle, which describes the potential translation correctness for a single entry and for a given neighborhood. Empirical study shows that our investigation successfully finds the conditions where the NMT model could easily fail and need related knowledge. Experiments on six diverse target domains and two language-pairs show that pruning according to local correctness brings a light and more explainable memory for kNN-MT domain adaptation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.177',\n",
       "   'authors': ['Wenhao Zhu',\n",
       "    'Shujian Huang',\n",
       "    'Yunzhe Lv',\n",
       "    'Xin Zheng',\n",
       "    'Jiajun CHEN'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-4_-machine-translation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1934',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['domain adaptation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.177.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77557/poster_document/3e2b97f9ad7469b04c14fe69e361db31.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77557/poster/616b73c5fc75a2304ea41f9640378d06.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77557/slideshow/c3e667cf0c1f4ad76fc6f319382ee356.pdf',\n",
       "   'title': 'What Knowledge Is Needed? Towards Explainable Memory for kNN-MT Domain Adaptation',\n",
       "   'tldr': 'kNN-MT presents a new paradigm for domain adaptation by building an external datastore, which usually saves all target language token occurrences in the parallel corpus. As a result, the constructed datastore is usually large and possibly redundant. In this paper, we investigate the interpretability...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77557,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77557-what-knowledge-is-neededquestion-towards-explainable-memory-for-knn-mt-domain-adaptation',\n",
       "   'video_url': None},\n",
       "  'P1936': {'abstract': 'Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when end-task data are unavailable. In this work, we explore whether the cross-lingual transferability can be activated without end-task data. We propose a cross-lingual transfer method, named PlugIn-X. PlugIn-X disassembles monolingual and multilingual Transformers into sub-modules, and reassembles them to be the multilingual end-task model. After representation adaptation, PlugIn-X finally performs cross-lingual transfer in a plug-and-play style. Experimental results show that PlugIn-X successfully activates the cross-lingual transferability of multilingual Transformers without accessing end-task data. Moreover, we analyze how the cross-model representation alignment affects the cross-lingual transferability.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.796',\n",
       "   'authors': ['Zewen Chi', 'Heyan Huang', 'Xian-Ling Mao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1936',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.796.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77558/poster_document/cc4e9b7630f63dde63fa4b6c252fb48a.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Can Cross-Lingual Transferability of Multilingual Transformers Be Activated Without End-Task Data?',\n",
       "   'tldr': 'Pretrained multilingual Transformers have achieved great success in cross-lingual transfer learning. Current methods typically activate the cross-lingual transferability of multilingual Transformers by fine-tuning them on end-task data. However, the methods cannot perform cross-lingual transfer when...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77558,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77558-can-cross-lingual-transferability-of-multilingual-transformers-be-activated-without-end-task-dataquestion',\n",
       "   'video_url': None},\n",
       "  'P1937': {'abstract': 'Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.759',\n",
       "   'authors': ['Zhiheng Xi',\n",
       "    'Rui Zheng',\n",
       "    'Yuansen Zhang',\n",
       "    'Xuanjing Huang',\n",
       "    'Zhongyu Wei',\n",
       "    'Minlong Peng',\n",
       "    'Mingming Sun',\n",
       "    'Qi Zhang',\n",
       "    'Tao Gui'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1937',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['transfer learning / domain adaptation',\n",
       "    'model compression methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.759.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77559/poster_document/fe891ec87291d4bd9b71820965eec64e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77559/poster/c678e8f0754e78562517590772fcd123.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Connectivity Patterns are Task Embeddings',\n",
       "   'tldr': 'Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77559,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77559-reinforced-active-learning-for-low-resource-domain-specific-multi-label-text-classification',\n",
       "   'video_url': None},\n",
       "  'P1939': {'abstract': 'Compositional generalization is a basic mechanism in human language\\n  learning, which current neural networks struggle with.  A recently\\n  proposed Disentangled sequence-to-sequence model\\n  (Dangle) shows promising generalization capability by learning\\n  specialized encodings for each decoding step. We introduce two key\\n  modifications to this model which encourage more disentangled\\n  representations and improve its compute and memory efficiency,\\n  allowing us to tackle compositional generalization in a more\\n  realistic setting.  Specifically, instead of adaptively re-encoding\\n  source keys and values at each time step, we disentangle their\\n  representations and only re-encode keys periodically, at some\\n  interval.  Our new architecture leads to better generalization\\n  performance across existing tasks and datasets, and a new machine\\n  translation benchmark which we create by\\n  detecting naturally occurring compositional patterns in\\n  relation to a training set. We show this methodology  better emulates real-world\\n  requirements than artificial challenges.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.108',\n",
       "   'authors': ['Hao Zheng', 'Mirella Lapata'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P1939',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generalization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.108.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Real-World Compositional Generalization with Disentangled Sequence-to-Sequence Learning',\n",
       "   'tldr': 'Compositional generalization is a basic mechanism in human language\\n  learning, which current neural networks struggle with.  A recently\\n  proposed Disentangled sequence-to-sequence model\\n  (Dangle) shows promising generalization capability by learning\\n  specialized encodings for each decoding step....',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77560,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77560-real-world-compositional-generalization-with-disentangled-sequence-to-sequence-learning',\n",
       "   'video_url': None},\n",
       "  'P194': {'abstract': \"Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the value of our model for a practical setting when a parallel corpus is only available for a few languages, but a lack of parallel corpora resources persists for many other low-resource languages. Our model can work well even with a small number of parallel sentences, and be used as an add-on module to any backbones and other tasks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.581',\n",
       "   'authors': ['Xiyang Hu',\n",
       "    'Xinchi Chen',\n",
       "    'Peng Qi',\n",
       "    'Deguang Kong',\n",
       "    'Kunlun Liu',\n",
       "    'William Yang Wang',\n",
       "    'zhiheng huang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P194',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.581.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77280/poster_document/96c978214151cc094553cd0ce063642c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77280/poster/9c4dbdc1f24f6aa112c8c881be8d2891.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77280/slideshow/105818c8dbc324bbe77ac61d17aa839a.pdf',\n",
       "   'title': 'Language Agnostic Multilingual Information Retrieval with Contrastive Learning',\n",
       "   'tldr': 'Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77280,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77280-language-agnostic-multilingual-information-retrieval-with-contrastive-learning',\n",
       "   'video_url': None},\n",
       "  'P1940': {'abstract': \"ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths. In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths. The events in ATOMIC are normalized to a consistent pattern at first. We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC. We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC. Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines. We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC's advantages in Knowledge Coverage and Multi-hop Paths. Both the source code of Rel-CSKGC and Dense-ATOMIC are publicly available on https://github.com/NUSTM/Dense-ATOMIC.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.742',\n",
       "   'authors': ['Xiangqing Shen', 'Siwei Wu', 'Rui Xia'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)'],\n",
       "   'id': 'P1940',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reasoning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.742.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76234/poster_document/b508293939f1f71032d804506e75f9a4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76234/poster/b8b8fca43034d09c583368213e315347.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76234/slideshow/f8f878d7818dad4ce2a9f888af55c946.pdf',\n",
       "   'title': 'Dense-ATOMIC: Towards Densely-connected ATOMIC with High Knowledge Coverage and Massive Multi-hop Paths',\n",
       "   'tldr': 'ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday if-then knowledge triplets, i.e., {head event, relation, tail event}. The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76234,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15263/lecture/76234-dense-atomic-towards-densely-connected-atomic-with-high-knowledge-coverage-and-massive-multi-hop-paths',\n",
       "   'video_url': None},\n",
       "  'P1952': {'abstract': \"The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how \\\\& knowing-that task that requires the model to answer factoid-style, procedure-style, and inconsistent questions about user manuals. We resolve this task by jointly representing the sTeps and fActs in a gRAh (TARA), which supports a unified inference of various questions. Towards a systematical benchmarking study, we design a heuristic method to automatically parse user manuals into TARAs and build an annotated dataset to test the model's ability in answering real-world questions. Empirical results demonstrate that representing user manuals as TARAs is a desired solution for the MRC of user manuals. An in-depth investigation of TARA further sheds light on the issues and broader impacts of future representations of user manuals. We hope our work can move the MRC of user manuals to a more complex and realistic stage.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.671',\n",
       "   'authors': ['Hongru Liang',\n",
       "    'Jia Liu',\n",
       "    'Weihong Du',\n",
       "    'Dingnan Jin',\n",
       "    'Wenqiang Lei',\n",
       "    'Zujie Wen',\n",
       "    'Jiancheng Lv'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1952',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reading comprehension'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.671.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77561/poster_document/b7acf84f49a2c5d2bbd106e07804ca7c.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Knowing-how & Knowing-that: A New Task for Machine Comprehension of User Manuals',\n",
       "   'tldr': 'The machine reading comprehension (MRC) of user manuals has huge potential in customer service. However, current methods have trouble answering complex questions. Therefore, we introduce the knowing-how \\\\& knowing-that task that requires the model to answer factoid-style, procedure-style, and incons...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77561,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77561-knowing-how-and-knowing-that-a-new-task-for-machine-comprehension-of-user-manuals',\n",
       "   'video_url': None},\n",
       "  'P1959': {'abstract': 'Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are better causal reasoners. We further intervene on the prompts from different aspects, and discover that the key point is the programming structure. Code and data are available at https://github.com/xxxiaol/magic-if.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.574',\n",
       "   'authors': ['Xiao Liu',\n",
       "    'Da Yin',\n",
       "    'Chen Zhang',\n",
       "    'Yansong Feng',\n",
       "    'Dongyan Zhao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P1959',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'interpretability/analysis', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.574.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77564/poster_document/878fba9be392e28437a99ba1913bbf76.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77564/poster/2934dcaf67d052db241bb4d1f902974f.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77564/slideshow/8c1f1be92382b8baaf75a557049b9303.pdf',\n",
       "   'title': 'The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code',\n",
       "   'tldr': 'Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given th...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77564,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77564-the-magic-of-if-investigating-causal-reasoning-abilities-in-large-language-models-of-code',\n",
       "   'video_url': None},\n",
       "  'P1969': {'abstract': 'There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while leaving other languages underexplored.\\nTo alleviate these limitations, in this paper, we present CATS, a pragmatic Chinese answer-to-sequence dataset with large scale and high quality. The dataset aims to generate textual descriptions for the answer in the practical TableQA system.\\nFurther, to bridge the structural gap between the input SQL and table and establish better semantic alignments, we propose a Unified Graph Transformation approach to establish a joint encoding space for the two hybrid knowledge resources and convert this task to a graph-to-text problem. The experiment results demonstrate the effectiveness of our proposed method. Further analysis on CATS attests to both the high quality and challenges of the dataset',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.168',\n",
       "   'authors': ['Liang Li',\n",
       "    'Ruiying Geng',\n",
       "    'Chengyang Fang',\n",
       "    'Bing Li',\n",
       "    'Can Ma',\n",
       "    'Rongyu Cao',\n",
       "    'Binhua Li',\n",
       "    'Fei Huang',\n",
       "    'Yongbin Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['poster-session-2_-generation-(poster)'],\n",
       "   'id': 'P1969',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data-to-text generation'],\n",
       "   'languages': ['chinese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.168.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76654/poster_document/421a74df70596652e0dc4e6dd0819728.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76654/poster/482460645d520d81b6e015bc0e2b060c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76654/slideshow/fdc853a98a6501a43e34048eac50d6ad.pdf',\n",
       "   'title': 'CATS: A Pragmatic Chinese Answer-to-Sequence Dataset with Large Scale and High Quality',\n",
       "   'tldr': 'There are three problems existing in the popular data-to-text datasets. First, the large-scale datasets either contain noise or lack real application scenarios. Second, the datasets close to real applications are relatively small in size. Last, current datasets bias in the English language while lea...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76654,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76654-uinauil-a-unified-benchmark-for-italian-natural-language-understanding',\n",
       "   'video_url': None},\n",
       "  'P197': {'abstract': 'Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increasingly identifying annotator disagreement as pervasive and meaningful. They also question the performance of a system when annotators disagree. Particularly when minority views are disregarded, especially among groups that may already be underrepresented in the annotator population. In this paper, we introduce CrowdOpinion, an unsupervised learning based approach that uses language features and label distributions to pool similar items into larger samples of label distributions. We experiment with four generative and one density-based clustering method, applied to five linear combinations of label distributions and features. We use five publicly available benchmark datasets (with varying levels of annotator disagreements) from social media (Twitter, Gab, and Reddit). We also experiment in the wild using a dataset from Facebook, where annotations come from the platform itself by users reacting to posts. We evaluate CrowdOpinion as a label distribution prediction task using KL-divergence and a single-label problem using accuracy measures.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.54',\n",
       "   'authors': ['Tharindu Cyril Weerasooriya',\n",
       "    'Sarah Luger',\n",
       "    'Saloni Poddar',\n",
       "    'Ashiqur KhudaBukhsh',\n",
       "    'Christopher Homan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['poster-session-7_-ethics-and-nlp-(poster)'],\n",
       "   'id': 'P197',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['human factors in nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.54.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76379/poster_document/196298a29884d3dc7b8352bb57fd8a64.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Subjective Crowd Disagreements for Subjective Data: Uncovering Meaningful CrowdOpinion with Population-level Learning',\n",
       "   'tldr': 'Human-annotated data plays a critical role in the fairness of AI systems, including those that deal with life-altering decisions or moderating human-created web/social media content. Conventionally, annotator disagreements are resolved before any learning takes place. However, researchers are increa...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 76379,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76379-subjective-crowd-disagreements-for-subjective-data-uncovering-meaningful-crowdopinion-with-population-level-learning',\n",
       "   'video_url': None},\n",
       "  'P1970': {'abstract': 'Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterogeneous information over text and table, and deficient ability for different reasoning operations. In this paper, we propose a three-stage TextTableQA framework S3HQA, which comprises of retriever, selector, and reasoner. We use a retriever with refinement training to solve the noisy labeling problem. Then, a hybrid selector considers the linked relationships between heterogeneous data to select the most relevant factual knowledge. For the final stage, instead of adapting a reading comprehension module like in previous methods, we employ a generation-based reasoner to obtain answers. This includes two approaches: a row-wise generator and an LLM prompting generator~(first time used in this task). The experimental results demonstrate that our method achieves competitive results in the few-shot setting. When trained on the full dataset, our approach outperforms all baseline methods, ranking first on the HybridQA leaderboard.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.147',\n",
       "   'authors': ['Fangyu Lei',\n",
       "    'Xiang Li',\n",
       "    'Yifan Wei',\n",
       "    'Shizhu He',\n",
       "    'Yiming Huang',\n",
       "    'Jun Zhao',\n",
       "    'Kang Liu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1970',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reading comprehension', 'multihop qa', 'table qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.147.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76655/poster_document/c9b789f2f5c62daff9ae40b232fcd792.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76655/poster/c150c0e32481a68c47ace34bd1b56fe9.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'S3HQA: A Three-Stage Approach for Multi-hop Text-Table Hybrid Question Answering',\n",
       "   'tldr': 'Answering multi-hop questions over hybrid factual knowledge from the given text and table (TextTableQA) is a challenging task. Existing models mainly adopt a retriever-reader framework, which have several deficiencies, such as noisy labeling in training retriever, insufficient utilization of heterog...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76655,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76655-s3hqa-a-three-stage-approach-for-multi-hop-text-table-hybrid-question-answering',\n",
       "   'video_url': None},\n",
       "  'P1977': {'abstract': 'Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant information in unstructured knowledge sentences. To address these limitations, we propose a segment-level and category-oriented network (SLCO). Our approach includes a segment-level and prompt-based knowledge retrieval method to mitigate the similarity bias problem and a category-based grounding method to alleviate interference from irrelevant information in knowledge sentences. Experimental results show that our SLCO can eliminate interference and improve the overall performance of the KB-REC task.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.557',\n",
       "   'authors': ['Yuqi Bu',\n",
       "    'Xin Wu',\n",
       "    'Liuwu Li',\n",
       "    'Yi Cai',\n",
       "    'Qiong Liu',\n",
       "    'Qingbao Huang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)'],\n",
       "   'id': 'P1977',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal application'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.557.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77567/poster_document/434b7b3fd192b4b108a6012384ca6ab4.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Segment-Level and Category-Oriented Network for Knowledge-Based Referring Expression Comprehension',\n",
       "   'tldr': 'Knowledge-based referring expression comprehension (KB-REC) aims to identify visual objects referred to by expressions that incorporate knowledge. Existing methods employ sentence-level retrieval and fusion methods, which may lead to issues of similarity bias and interference from irrelevant informa...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 77567,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77567-segment-level-and-category-oriented-network-for-knowledge-based-referring-expression-comprehension',\n",
       "   'video_url': None},\n",
       "  'P1980': {'abstract': 'Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a unified training-free framework, we propose KB-BINDER, which for the first time enables few-shot in-context learning over KBQA tasks. Firstly, KB-BINDER leverages large language models like Codex to generate logical forms as the draft for a specific question by imitating a few demonstrations. Secondly,  KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score matching. The experimental results on four public heterogeneous KBQA datasets show that KB-BINDER can achieve a strong performance with only a few in-context demonstrations. Especially on GraphQA and 3-hop MetaQA, KB-BINDER can even outperform the state-of-the-art trained models. On GrailQA and WebQSP, our model is also on par with other fully-trained models. We believe KB-BINDER can serve as an important baseline for future research. We plan to release all the code and data. Our code is available at https://github.com/ltl3A87/KB-BINDER.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.385',\n",
       "   'authors': ['Tianle Li',\n",
       "    'Xueguang Ma',\n",
       "    'Alex Zhuang',\n",
       "    'Yu Gu',\n",
       "    'Yu Su',\n",
       "    'Wenhu Chen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-2_-large-language-models-(poster)'],\n",
       "   'id': 'P1980',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.385.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76656/poster_document/8c982a4f91f55c0d2776fada48b279ae.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76656/poster/e143441ef9ee0ecb3bef16dc855ae772.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76656/slideshow/d58d3e13e7fb0ad380c709aac4e705ba.pptx',\n",
       "   'title': 'Few-shot In-context Learning on Knowledge Base Question Answering',\n",
       "   'tldr': 'Question answering over knowledge bases is considered a difficult problem due to the challenge of generalizing to a wide variety of possible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized tra...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76656,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76656-few-shot-in-context-learning-on-knowledge-base-question-answering',\n",
       "   'video_url': None},\n",
       "  'P1985': {'abstract': 'We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. \\nExisting question generation systems widely differentiate questions by literal information such as question words and answer types to generate semantically relevant questions for a given context.\\nHowever, they rarely consider the comprehension nature of questions, i.e., the different comprehension capabilities embodied by different questions.\\nIn comparison, our SkillQG is able to tailor a fine-grained assessment and improvement to the capabilities of questions answering models built on it.\\nSpecifically, we first frame the comprehension type of questions based on a hierarchical skill-based schema.\\nWe then formulate SkillQG as a skill-conditioned question generator.\\nFurthermore, to improve the controllability of generation, we augment the input text with skill-specific question focus and knowledge, which are constructed by iteratively prompting the pre-trained language models.\\nEmpirical results demonstrate that SkillQG outperforms baselines in terms of quality, relevance, and skill-controllability while showing a promising performance boost in downstream question answering task.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.870',\n",
       "   'authors': ['Xiaoqiang Wang', 'Bang Liu', 'Siliang Tang', 'Lingfei Wu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P1985',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reading comprehension', 'question generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.870.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77568/poster_document/6091d349526515c14ae119bb84bb350c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77568/poster/c9697e66ffbba459739320dc7c4fdccd.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77568/slideshow/d0c2ffd8b6f883f3d1abeb2bab7df791.pdf',\n",
       "   'title': 'SkillQG: Learning to Generate Question for Reading Comprehension Assessment',\n",
       "   'tldr': 'We present SkillQG: a question generation framework with controllable comprehension types for assessing and improving machine reading comprehension models. \\nExisting question generation systems widely differentiate questions by literal information such as question words and answer types to generate ...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77568,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77568-using-neural-machine-translation-for-generating-diverse-challenging-exercises-for-language-learner',\n",
       "   'video_url': None},\n",
       "  'P1996': {'abstract': 'In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary significantly and the lack of a universal ground truth for the counterfactual edits imposes an insuperable barrier on their evaluation. We propose a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers. We show that by iteratively feeding the counterfactual to the explainer we can obtain valuable insights into the behaviour of both the predictor and the explainer models, and infer patterns that would be otherwise obscured. Using this methodology, we conduct a thorough analysis and propose a novel metric to evaluate the consistency of counterfactual generation approaches with different characteristics across available performance indicators.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.606',\n",
       "   'authors': ['George Filandrianos',\n",
       "    'Edmund G Dervakos',\n",
       "    'Orfeas Menis Mastromichalakis',\n",
       "    'Chrysoula Zerva',\n",
       "    'Giorgos Stamou'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P1996',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['counterfactual/contrastive explanations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.606.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77569/poster_document/1d198c30bec098b82eff058ff587f72e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77569/poster/a16eb5a206c0d69ff567b4613514b24e.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77569/slideshow/5767681be6ae2f9f1f28290021282b0b.key',\n",
       "   'title': 'Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors',\n",
       "   'tldr': 'In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus spe...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77569,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77569-counterfactuals-of-counterfactuals-a-back-translation-inspired-approach-to-analyse-counterfactual-editors',\n",
       "   'video_url': None},\n",
       "  'P2006': {'abstract': 'Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define \\\\textit{attention resolution} as an indicator of extrapolation. Then we propose two designs to improve the above metric of Transformers. Specifically, we introduce a relative position embedding to explicitly maximize attention resolution. Moreover, we use blockwise causal attention during inference for better resolution. We evaluate different Transformer variants with language modeling. Experimental results show that our model achieves strong performance in both interpolation and extrapolation settings. The code will be available at {https://aka.ms/LeX-Transformer}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.816',\n",
       "   'authors': ['Yutao Sun',\n",
       "    'Li Dong',\n",
       "    'Barun Patra',\n",
       "    'Shuming Ma',\n",
       "    'Shaohan Huang',\n",
       "    'Alon Benhaim',\n",
       "    'Vishrav Chaudhary',\n",
       "    'Xia Song',\n",
       "    'Furu Wei'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P2006',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['pre-training'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.816.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76657/poster_document/61ba23c97f49f0e69e13a1456fe29dde.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76657/poster/641bcd758374622d1a47c4356da07abb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Length-Extrapolatable Transformer',\n",
       "   'tldr': 'Position modeling plays a critical role in Transformers. In this paper, we focus on length extrapolation, i.e., training on short texts while evaluating longer sequences. We define \\\\textit{attention resolution} as an indicator of extrapolation. Then we propose two designs to improve the above metric...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76657,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76657-a-length-extrapolatable-transformer',\n",
       "   'video_url': None},\n",
       "  'P2012': {'abstract': 'Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.361',\n",
       "   'authors': ['Mehran Kazemi',\n",
       "    'Najoung Kim',\n",
       "    'Deepti Bhatia',\n",
       "    'Xin Xu',\n",
       "    'Deepak Ramachandran'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-5_-large-language-models-(poster)'],\n",
       "   'id': 'P2012',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.361.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76658/poster_document/1d532f4c2fc851c73c2deabd6f8dffd2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76658/poster/864cbe5da4b5ca41aa33e0f80c8205fc.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LAMBADA: Backward Chaining for Automated Reasoning in Natural Language',\n",
       "   'tldr': 'Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a co...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76658,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15254/poster/76658-lambada-backward-chaining-for-automated-reasoning-in-natural-language',\n",
       "   'video_url': None},\n",
       "  'P2018': {'abstract': 'Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.14',\n",
       "   'authors': ['Andrea Gregor de Varda', 'Marco Marelli'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'event_ids': ['session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)'],\n",
       "   'id': 'P2018',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['computational psycholinguistics'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.14.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76659/poster_document/d5b7a76bd5d0c377bbcbf159c91e15cc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76659/poster/fe2e406b68499ef1f086f166cc20c77a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Scaling in Cognitive Modelling: a Multilingual Approach to Human Reading Times',\n",
       "   'tldr': 'Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodologi...',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'underline_id': 76659,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76659-scaling-in-cognitive-modelling-a-multilingual-approach-to-human-reading-times',\n",
       "   'video_url': None},\n",
       "  'P202': {'abstract': 'We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sentences into fragments, clustering the fragments to abstract away syntactic variations of the same meaning, and predicting predicate-argument relations between the fragments. To better model the statistical dependencies between predicates and their arguments, we further conduct a hierarchical Pitman-Yor process. An improved Metropolis-Hastings merge-split sampler is proposed to speed up the mixing and convergence of Markov chains by leveraging pre-trained distributed representations. The experimental results show that the models achieve better accuracy on both question-answering and relation extraction tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.726',\n",
       "   'authors': ['Zixuan Ling',\n",
       "    'Xiaoqing Zheng',\n",
       "    'Jianhan Xu',\n",
       "    'Jinshu Lin',\n",
       "    'Kai-Wei Chang',\n",
       "    'Cho-Jui Hsieh',\n",
       "    'Xuanjing Huang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'event_ids': ['session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)'],\n",
       "   'id': 'P202',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['semantic parsing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.726.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77281/poster_document/1aee14ab7bc597aaf8ad7ee15e9a5caf.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77281/poster/3936c88b7e6b9a8e6d130250779338c5.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77281/slideshow/0632b33e8eb0eb069784d0bd2aeec157.pdf',\n",
       "   'title': 'Enhancing Unsupervised Semantic Parsing with Distributed Contextual Representations',\n",
       "   'tldr': 'We extend a non-parametric Bayesian model of (Titov and Klementiev, 2011) to deal with homonymy and polysemy by leveraging distributed contextual word and phrase representations pre-trained on a large collection of unlabelled texts. Then, unsupervised semantic parsing is performed by decomposing sen...',\n",
       "   'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "   'underline_id': 77281,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77281-hucurl-human-induced-curriculum-discovery',\n",
       "   'video_url': None},\n",
       "  'P2021': {'abstract': 'Languages are dynamic entities, where the meanings associated with words constantly change with time.\\nDetecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions.\\nExisting work on semantic variation prediction have predominantly focused on comparing some form of an averaged contextualised representation of a target word computed from a given corpus.\\nHowever, some of the previously associated meanings of a target word can become obsolete over time (e.g. meaning of gay as happy), while novel usages of existing words are observed (e.g. meaning of cell as a mobile phone).\\nWe argue that mean representations alone cannot accurately capture such semantic variations and propose a method that uses the entire cohort of the contextualised embeddings of the target word, which we refer to as the sibling distribution.\\nExperimental results on SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show that our method outperforms prior work that consider only the mean embeddings, and is comparable to the current state-of-the-art. \\nMoreover, a qualitative analysis shows that our method detects important semantic changes in words that are not captured by the existing methods.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.429',\n",
       "   'authors': ['Taichi Aida', 'Danushka Bollegala'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'event_ids': ['session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2021',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['linguistic theories'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.429.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77575/poster_document/fdd032a9bae73564f616937f8473f6ff.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77575/slideshow/36f9db5e41bf0fee5c88ca218e4c5c67.pdf',\n",
       "   'title': 'Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings',\n",
       "   'tldr': 'Languages are dynamic entities, where the meanings associated with words constantly change with time.\\nDetecting the semantic variation of words is an important task for various NLP applications that must make time-sensitive predictions.\\nExisting work on semantic variation prediction have predominant...',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'underline_id': 77575,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77575-unsupervised-semantic-variation-prediction-using-the-distribution-of-sibling-embeddings',\n",
       "   'video_url': None},\n",
       "  'P2023': {'abstract': 'Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless, the improvement offered by existing handcrafted augmentation strategies is limited when successful systematic generalization of neural sequence models requires multi-grained compositional bias (i.e., not limited to either lexical or structural biases only) or differentiation of training sequences in an imbalanced difficulty distribution. To address the two challenges, we first propose a novel compositional augmentation strategy dubbed Span Substitution (SpanSub) that enables multi-grained composition of substantial substructures in the whole training set. Over and above that, we introduce the Learning to Substitute Span (L2S2) framework which empowers the learning of span substitution probabilities in SpanSub in an end-to-end manner by maximizing the loss of neural sequence models, so as to outweigh those challenging compositions with elusive concepts and novel surroundings. Our empirical results on three standard compositional generalization benchmarks, including SCAN, COGS and GeoQuery (with an improvement of at most 66.5\\\\%, 10.3\\\\%, 1.2\\\\%, respectively), demonstrate the superiority of SpanSub, L2S2 and their combination.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.157',\n",
       "   'authors': ['Zhaoyi Li', 'Ying Wei', 'Defu Lian'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-5_-semantics_-lexical-(oral)'],\n",
       "   'id': 'P2023',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['compositionality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.157.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76235/poster_document/1482d282c676f21b533bdd92858913c5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76235/poster/4d9fd8a1c5d6eef310115e022d4f0cf1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning to Substitute Spans towards Improving Compositional Generalization',\n",
       "   'tldr': 'Despite the rising prevalence of neural sequence models, recent empirical evidences suggest their deficiency in compositional generalization. One of the current de-facto solutions to this problem is compositional data augmentation, aiming to incur additional compositional inductive bias. Nonetheless...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 76235,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15253/lecture/76235-learning-to-substitute-spans-towards-improving-compositional-generalization',\n",
       "   'video_url': None},\n",
       "  'P2024': {'abstract': 'Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard \"ground truth\\'\\' as the distribution of all labels that a population of annotators could produce, if asked (and of which we only have a small sample). We next introduce DisCo (Distribution from Context), a simple neural model that learns to predict this distribution. The model takes annotator-item pairs, rather than items alone, as input, and performs inference by aggregating over all annotators. Despite its simplicity, our experiments show that, on six benchmark datasets, our model is competitive with, and frequently outperforms, other, more complex models that either do not model specific annotators or were not designed for label distribution learning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.287',\n",
       "   'authors': ['Tharindu Cyril Weerasooriya',\n",
       "    'Alexander Ororbia',\n",
       "    'Raj B Bhensadadia',\n",
       "    'Ashiqur KhudaBukhsh',\n",
       "    'Christopher Homan'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-4_-ethics-and-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2024',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.287.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Disagreement Matters: Preserving Label Diversity by Jointly Modeling Item and Annotator Label Distributions with DisCo',\n",
       "   'tldr': 'Annotator disagreement is common whenever human judgment is needed for supervised learning. It is conventional to assume that one label per item represents ground truth. However, this obscures minority opinions, if present. We regard \"ground truth\\'\\' as the distribution of all labels that a populatio...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77577,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77577-disagreement-matters-preserving-label-diversity-by-jointly-modeling-item-and-annotator-label-distributions-with-disco',\n",
       "   'video_url': None},\n",
       "  'P203': {'abstract': 'Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. The question of what is considered appropriate in a controversial discussion has not yet been systematically addressed. In this paper, we operationalize appropriate language in argumentation for the first time. In particular, we model appropriateness through the absence of flaws, grounded in research on argument quality assessment, especially in aspects from rhetoric. From these, we derive a new taxonomy of 14 dimensions that determine inappropriate language in online discussions. Building on three  argument quality corpora, we then create a corpus of 2191 arguments annotated for the 14 dimensions. Empirical analyses support that the taxonomy covers the concept of appropriateness comprehensively, showing several plausible correlations with argument quality dimensions. Moreover, results of baseline approaches to assessing appropriateness suggest that all dimensions can be modeled computationally on the corpus.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.238',\n",
       "   'authors': ['Timon Ziegenbein',\n",
       "    'Shahbaz Syed',\n",
       "    'Felix Lange',\n",
       "    'Martin Potthast',\n",
       "    'Henning Wachsmuth'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['poster-session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)'],\n",
       "   'id': 'P203',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['argument quality assessment'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.238.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76380/poster_document/ccaa6e0e0fc44fe276625f6a2133e097.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76380/slideshow/96d48f5a2cf8b066843af720224f2664.pdf',\n",
       "   'title': 'Modeling Appropriate Language in Argumentation',\n",
       "   'tldr': 'Online discussion moderators must make ad-hoc decisions about whether the contributions of discussion participants are appropriate or should be removed to maintain civility. Existing research on offensive language and the resulting tools cover only one aspect among many involved in such decisions. T...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76380,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76380-modeling-appropriate-language-in-argumentation',\n",
       "   'video_url': None},\n",
       "  'P2033': {'abstract': 'Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation. We find that limitations of glosses in general and limitations of specific datasets are not discussed in a transparent manner and that there is no common standard for evaluation.\\n\\nTo address these issues, we put forward concrete recommendations for future research on gloss translation. Our suggestions advocate awareness of the inherent limitations of gloss-based approaches, realistic datasets, stronger baselines and convincing evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.60',\n",
       "   'authors': ['Mathias Mller',\n",
       "    'Zifan Jiang',\n",
       "    'Amit Moryossef',\n",
       "    'Annette Rios',\n",
       "    'Sarah Ebling'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['poster-session-4_-theme_-reality-check-(poster)'],\n",
       "   'id': 'P2033',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['methodology'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.60.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76660/poster_document/b734e71177a7a2c36128754b3b0d2b9c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76660/poster/c978eff6c609ef8a2d3a1dd6b8a12790.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Considerations for meaningful sign language machine translation based on glosses',\n",
       "   'tldr': 'Automatic sign language processing is gaining popularity in Natural Language Processing (NLP) research (Yin et al., 2021). In machine translation (MT) in particular, sign language translation based on glosses is a prominent approach. In this paper, we review recent works on neural gloss translation....',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 76660,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76660-considerations-for-meaningful-sign-language-machine-translation-based-on-glosses',\n",
       "   'video_url': None},\n",
       "  'P2036': {'abstract': \"Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise generation for online language learning. To this end, we combine a knowledge tracing model that estimates each student's evolving knowledge states from their learning history and a controlled text generation model that generates exercise sentences based on the student's current estimated knowledge state and instructor requirements of desired properties (e.g., domain knowledge and difficulty). We train and evaluate our model on real-world learner interaction data from Duolingo and demonstrate that LMs guided by student states can generate superior exercises. Then, we discuss the potential use of our model in educational applications using various simulations. These simulations show that our model can adapt to students' individual abilities and can facilitate their learning efficiency by personalizing learning sequences.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.567',\n",
       "   'authors': ['Peng Cui', 'Mrinmaya Sachan'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-6_-nlp-applications-(oral)'],\n",
       "   'id': 'P2036',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.567.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76236/poster_document/dc9e263123c18c8300c09d3840be3214.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76236/poster/5c1ac03d706d9bd6781918f93e449e2f.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76236/slideshow/019858396904527b108cb2cba773b2a7.pdf',\n",
       "   'title': 'Adaptive and Personalized Exercise Generation for Online Language Learning',\n",
       "   'tldr': 'Adaptive learning aims to provide customized educational activities (e.g., exercises) to address individual learning needs. However, manual construction and delivery of such activities is a laborious process. Thus, in this paper, we study a novel task of adaptive and personalized exercise generation...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76236,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15259/lecture/76236-adaptive-and-personalized-exercise-generation-for-online-language-learning',\n",
       "   'video_url': None},\n",
       "  'P2037': {'abstract': \"Due to the rapid upgrade of social platforms, most of today's fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.37',\n",
       "   'authors': ['Ziwei Chen',\n",
       "    'Linmei Hu',\n",
       "    'Weixin Li',\n",
       "    'Yingxia Shao',\n",
       "    'Liqiang Nie'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P2037',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['fact checking, rumour/misinformation detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.37.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76661/poster_document/b16a94104e83a33f06499fdb2b7eff43.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76661/poster/bcaa903b9a112cb2ff3c7bdc0ab8ede2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection',\n",
       "   'tldr': \"Due to the rapid upgrade of social platforms, most of today's fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thu...\",\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76661,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76661-causal-intervention-and-counterfactual-reasoning-for-multi-modal-fake-news-detection',\n",
       "   'video_url': None},\n",
       "  'P2039': {'abstract': 'Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and racial/migrant bias in four languages. We demonstrate its usefulness by answering a simple but important question that an engineer might need to answer when deploying a system: What biases do systems import from pre-trained models when compared to a baseline with no pre-training? Our evaluation corpus, by virtue of being counterfactual, not only reveals which models have less bias, but also pinpoints changes in model bias behaviour, which enables more targeted mitigation strategies. We release our code and evaluation corpora to facilitate future research.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.272',\n",
       "   'authors': ['Seraphina Goldfarb-Tarrant',\n",
       "    'Adam Lopez',\n",
       "    'Roi Blanco',\n",
       "    'Diego Marcheggiani'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-1_-ethics-and-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2039',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': ['spanish', 'german', 'japanese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.272.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77580/poster_document/968647466c6e382390f89b6ae328de35.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages',\n",
       "   'tldr': 'Sentiment analysis (SA) systems are used in many products and hundreds of languages. Gender and racial biases are well-studied in English SA systems, but understudied in other languages, with few resources for such studies. To remedy this, we build a counterfactual evaluation corpus for gender and r...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77580,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77580-bias-beyond-english-counterfactual-tests-for-bias-in-sentiment-analysis-in-four-languages',\n",
       "   'video_url': None},\n",
       "  'P2046': {'abstract': 'Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.6\\\\% increase over best baselines.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.245',\n",
       "   'authors': ['Xinyu Zhu',\n",
       "    'Junjie Wang',\n",
       "    'Lin Zhang',\n",
       "    'Yuxiang Zhang',\n",
       "    'Yongfeng Huang',\n",
       "    'ruyi gan',\n",
       "    'Jiaxing Zhang',\n",
       "    'Yujiu Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-7_-question-answering-(virtual-poster)'],\n",
       "   'id': 'P2046',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reasoning', 'math qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.245.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76662/poster_document/3b67170ec7a5d181035100b0f2d505cc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76662/poster/e2e24f4906b2cc8a45a58ca7dd8b125c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Solving Math Word Problems via Cooperative Reasoning induced Language Models',\n",
       "   'tldr': 'Large-scale pre-trained language models (PLMs) bring new opportunities to challenging problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision a...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76662,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76662-solving-math-word-problems-via-cooperative-reasoning-induced-language-models',\n",
       "   'video_url': None},\n",
       "  'P2048': {'abstract': 'Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we propose a new approach for Multi-task learning which is based on stacking the weights of Neural Networks as a tensor. We show that low-rank updates in the canonical polyadic tensor decomposition of this tensor of weights lead to a simple, yet efficient algorithm, which without loss of performance allows to reduce considerably the model parameters. We investigate the interactions between tasks inside the model as well as the inclusion of sparsity to find the best tensor rank and to increase the compression rate. Our strategy is consistent with recent efforts that attempt to use constraints to fine-tune some model components. More precisely, we achieve equivalent performance as the state-of-the-art on the General Language Understanding Evaluation benchmark by training only 0.3 of the parameters per task while not modifying the baseline weights.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.476',\n",
       "   'authors': ['Alexandre Daniel AUDIBERT',\n",
       "    'Massih R Amini',\n",
       "    'Konstantin Usevich',\n",
       "    'Marianne Clausel'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2048',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-task learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.476.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77582/slideshow/b5b51c4a09d40ade4063521b70e1c256.pdf',\n",
       "   'title': 'Low-Rank Updates of pre-trained Weights for Multi-Task Learning',\n",
       "   'tldr': 'Multi-Task Learning used with pre-trained models has been quite popular in the field of Natural Language Processing in recent years. This framework remains still challenging due to the complexity of the tasks and the challenges associated with fine-tuning large pre-trained models. In this paper, we ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77582,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77582-low-rank-updates-of-pre-trained-weights-for-multi-task-learning',\n",
       "   'video_url': None},\n",
       "  'P2055': {'abstract': 'Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metrics, and whether the metrics are effective for comprehensive, efficient, and reliable SG evaluation. To address the issues, we establish HAUSER, a holistic and automatic evaluation system for the SG task, which consists of five criteria from three perspectives and automatic metrics for each criterion. Through extensive experiments, we verify that our metrics are significantly more correlated with human ratings from each perspective compared with prior automatic metrics. Resources of HAUSER are publicly available at https://github.com/Abbey4799/HAUSER.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.702',\n",
       "   'authors': ['Qianyu He',\n",
       "    'Yikai Zhang',\n",
       "    'Jiaqing Liang',\n",
       "    'Yuncheng Huang',\n",
       "    'Yanghua Xiao',\n",
       "    'Yunwen Chen'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-5_-generation-(oral)'],\n",
       "   'id': 'P2055',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.702.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76237/poster_document/3a94456376377b714bdb220c7bc4d8fc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76237/poster/7036bef276e100f174aa0044125c91c3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76237/slideshow/ea91806e7aff11a1d61f14f95f82c6c2.pdf',\n",
       "   'title': 'HAUSER: Towards Holistic and Automatic Evaluation of Simile Generation',\n",
       "   'tldr': 'Similes play an imperative role in creative writing such as story and dialogue generation. Proper evaluation metrics are like a beacon guiding the research of simile generation (SG). However, it remains under-explored as to what criteria should be considered, how to quantify each criterion into metr...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76237,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15251/lecture/76237-hauser-towards-holistic-and-automatic-evaluation-of-simile-generation',\n",
       "   'video_url': None},\n",
       "  'P2056': {'abstract': \"PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common.\\nIt presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively.\\nMethods developed in the literature, however, cannot be deployed to real gameplay\\nsince they only tackle some subtasks of the game,\\nand they require additional reference chains inputs, whose extraction process is imperfect.\\nTherefore, we propose a reference chain-free listener model\\nthat directly addresses the game's predictive task, i.e., deciding whether an image is shared with partner.\\nOur DeBERTa-based listener model reads the full dialogue, and utilizes\\nCLIPScore features to assess utterance-image relevance.\\nWe achieve >77\\\\% accuracy on unseen sets of images/game themes, outperforming baseline by >17 points.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.121',\n",
       "   'authors': ['Shih-Lun Wu', 'Yi-Hui Chou', 'Liangze Li'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-2_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P2056',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-modal dialogue systems'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.121.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76663/poster_document/96067d5e855a1f9d392666b238baa787.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76663/poster/f362100e1dd3e4e2dc6f2a9047442520.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76663/slideshow/ff67bb909904d541306ca99ae633468e.pdf',\n",
       "   'title': 'Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain',\n",
       "   'tldr': 'PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common.\\nIt presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively.\\nM...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76663,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76663-listener-model-for-the-photobook-referential-game-with-clipscores-as-implicit-reference-chain',\n",
       "   'video_url': None},\n",
       "  'P2057': {'abstract': 'Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.25',\n",
       "   'authors': ['Jian Wang', 'Dongding Lin', 'Wenjie Li'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2057',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['applications', 'grounded dialog', 'conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.25.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77584/poster_document/ff857de9f7f0b515a5e4e20bca0a2d96.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77584/slideshow/2b16f3b10d728aa235f340a8606b0165.pdf',\n",
       "   'title': 'Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue',\n",
       "   'tldr': 'Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored tas...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77584,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77584-dialogue-planning-via-brownian-bridge-stochastic-process-for-goal-directed-proactive-dialogue',\n",
       "   'video_url': None},\n",
       "  'P2058': {'abstract': \"Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early-Exit and Multi-Model, when training data is limited. First, we observe that for models with the same architecture and size, individual Multi-Model classifiers outperform their Early-Exit counterparts by an average of 2.3\\\\%. We show that this gap is caused by Early-Exit classifiers sharing model parameters during training, resulting in conflicting gradient updates of model weights. We find that despite this gap, Early-Exit still provides a better speed-accuracy trade-off due to the overhead of the Multi-Model approach. To address these issues, we propose SWEET (Separating Weights for Early-Exit Transformers) an Early-Exit fine-tuning method that assigns each classifier its own set of unique model weights, not updated by other classifiers. We compare SWEET's speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find that it outperforms both methods at fast speeds while maintaining comparable scores to Early- Exit at slow speeds. Moreover, SWEET individual classifiers outperform Early-Exit ones by 1.1\\\\% on average. SWEET enjoys the benefits of both methods, paving the way for further reduction of inference costs in NLP.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.829',\n",
       "   'authors': ['Daniel Rotem',\n",
       "    'Michael Hassid',\n",
       "    'Jonathan Mamou',\n",
       "    'Roy Schwartz'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['poster-session-7_-interpretability-and-analysis-of-models-for-nlp-(poster)'],\n",
       "   'id': 'P2058',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['hardness of samples'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.829.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76664/poster_document/8dfef44d45a14d3b783573c42d7fd30e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76664/poster/956557cf1e77e0ffa7d7f49d7c78f63d.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference in Low Resource Settings',\n",
       "   'tldr': 'Adaptive inference is a simple method for reducing inference costs. The method works by maintaining multiple classifiers of different capacities, and allocating resources to each test instance according to its difficulty. In this work, we compare the two main approaches for adaptive inference, Early...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76664,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76664-finding-the-sweet-spot-analysis-and-improvement-of-adaptive-inference-in-low-resource-settings',\n",
       "   'video_url': None},\n",
       "  'P2059': {'abstract': 'The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes available since numerous unwritten languages exist worldwide. In this paper, we aim to utilize large amounts of target-side monolingual data to enhance ST without transcripts. Motivated by the remarkable success of back translation in MT, we develop a back translation algorithm for ST (BT4ST) to synthesize pseudo ST data from monolingual target data. To ease the challenges posed by short-to-long generation and one-to-many mapping, we introduce self-supervised discrete units and achieve back translation by cascading a target-to-unit model and a unit-to-speech model. With our synthetic ST data, we achieve an average boost of 2.3 BLEU on MuST-C En-De, En-Fr, and En-Es datasets. More experiments show that our method is especially effective in low-resource scenarios.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.251',\n",
       "   'authors': ['Qingkai Fang', 'Yang Feng'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-1_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P2059',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['speech translation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.251.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76665/poster_document/d5b7899eec2b4cb479e899097af53533.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76665/poster/ab7c818e955b8b61718e189f25570fd8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Back Translation for Speech-to-text Translation Without Transcripts',\n",
       "   'tldr': 'The success of end-to-end speech-to-text translation (ST) is often achieved by utilizing source transcripts, e.g., by pre-training with automatic speech recognition (ASR) and machine translation (MT) tasks, or by introducing additional ASR and MT data. Unfortunately, transcripts are only sometimes a...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76665,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76665-pal-to-lend-a-helping-hand-towards-building-an-emotion-adaptive-polite-and-empathetic-counseling-conversational-agent',\n",
       "   'video_url': None},\n",
       "  'P206': {'abstract': 'Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jointly learns to generate discourse connectives between arguments and predict discourse relations based on the arguments and the generated connectives. To prevent our relation classifier from being misled by poor connectives generated at the early stage of training while alleviating the discrepancy between training and inference, we adopt Scheduled Sampling to the joint learning. We evaluate our method on three benchmarks, PDTB 2.0, PDTB 3.0, and PCC. Results show that our joint model significantly outperforms various baselines on three datasets, demonstrating its superiority for the task.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.874',\n",
       "   'authors': ['Wei Liu', 'Michael Strube'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Discourse and Pragmatics',\n",
       "   'event_ids': ['poster-session-7_-discourse-and-pragmatics-(poster)'],\n",
       "   'id': 'P206',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['discourse relations', 'discourse parsing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.874.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76381/poster_document/d9f4305ca6c6db91b3acf1100f77497c.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76381/slideshow/fc8eb16b768833492781dc4b793fad25.pdf',\n",
       "   'title': 'Annotation-Inspired Implicit Discourse Relation Classification with Auxiliary Discourse Connective Generation',\n",
       "   'tldr': 'Implicit discourse relation classification is a challenging task due to the absence of discourse connectives. To overcome this issue, we design an end-to-end neural model to explicitly generate discourse connectives for the task, inspired by the annotation process of PDTB. Specifically, our model jo...',\n",
       "   'track': 'Discourse and Pragmatics',\n",
       "   'underline_id': 76381,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76381-annotation-inspired-implicit-discourse-relation-classification-with-auxiliary-discourse-connective-generation',\n",
       "   'video_url': None},\n",
       "  'P2061': {'abstract': 'In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task.\\nWe observe LLMs are more prone to failure in these cases.\\nTo address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.152',\n",
       "   'authors': ['Rohan Bhambhoria', 'Lei Chen', 'Xiaodan Zhu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-3_-nlp-applications-(poster)'],\n",
       "   'id': 'P2061',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['financial/business nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.152.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76666/poster_document/e7d38072117d8a9d1c40d88d0e3729ec.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76666/poster/91dbaa02fd7fd080fc181c3aa0c571b2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76666/slideshow/8411c2309f07982e81cc6ff285c9cbb0.pdf',\n",
       "   'title': 'A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification',\n",
       "   'tldr': 'In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to addre...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76666,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76666-a-simple-and-effective-framework-for-strict-zero-shot-hierarchical-classification',\n",
       "   'video_url': None},\n",
       "  'P2067': {'abstract': 'For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly annotated training data that contains incorrect labels. However, the performance of models trained solely with such data can be limited by the errors in the automatic annotation. Recently, there are a few approaches that no longer follow this conventional way. But without using sufficient direct entity typing supervision may also cause them to yield inferior performance. In this paper, we propose a new approach that can avoid the need of creating distantly labeled data whenever there is a new type schema. We first train an entity typing model that have an extremely board type coverage by using the ultra-fine entity typing data. Then, when there is a need to produce a model for a newly designed fine-grained entity type schema. We can simply fine-tune the previously trained model with a small number of examples annotated under this schema. Experimental results show that our approach achieves outstanding performance for FET under the few-shot setting. It can also outperform state-of-the-art weak supervision based methods after fine-tuning the model with only a small size manually annotated training set.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.126',\n",
       "   'authors': ['Hongliang Dai', 'Ziqian Zeng'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-7_-information-extraction-(virtual-poster)'],\n",
       "   'id': 'P2067',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.126.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76667/poster_document/f9f2109f38356730dad3e1bda37bb3d5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76667/poster/de24c164ef1d06538685e7dac95b5e8c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'From Ultra-Fine to Fine: Fine-tuning Ultra-Fine Entity Typing Models to Fine-grained',\n",
       "   'tldr': 'For the task of fine-grained entity typing (FET), due to the use of a large number of entity types, it is usually considered too costly to manually annotating a training dataset that contains an ample number of examples for each type. A common way to address this problem is to use distantly annotate...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76667,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76667-from-ultra-fine-to-fine-fine-tuning-ultra-fine-entity-typing-models-to-fine-grained',\n",
       "   'video_url': None},\n",
       "  'P2068': {'abstract': \"Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, and often low quality, content. In this paper, we design a novel method to generate candidates for re-ranking that addresses these issues.  We ground each candidate abstract on its own unique content plan and generate distinct plan-guided abstracts using a model's top beam. More concretely, a standard language model (a BART LM) auto-regressively generates elemental discourse unit (EDU) content plans with an extractive copy mechanism. The top K beams from the content plan generator are then used to guide a separate LM, which produces a single abstractive candidate for each distinct plan. We apply an existing re-ranker (BRIO) to abstractive candidates generated from our method, as well as baseline decoding methods. We show large relevance improvements over previously published methods on widely used single document news article corpora, with ROUGE-2 F1 gains of 0.88, 2.01, and 0.38 on CNN / Dailymail, NYT, and Xsum, respectively. A human evaluation on CNN / DM validates these results. Similarly, on 1k samples from CNN / DM, we show that prompting GPT-3 to follow EDU plans outperforms sampling-based methods by by 1.05 ROUGE-2 F1 points. Code to generate and realize plans is available at https://github.com/griff4692/edu-sum.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.151',\n",
       "   'authors': ['Griffin Adams',\n",
       "    'Alex Fabbri',\n",
       "    'Faisal Ladhak',\n",
       "    'Nomie Elhadad',\n",
       "    'Kathleen McKeown'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-1_-summarization-(poster)'],\n",
       "   'id': 'P2068',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation', 'architectures'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.151.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76668/poster_document/283402d040b6da1dba607c29a4efe420.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76668/slideshow/10b8428ac7b38cb8a9fcb06dbcf80ea6.pdf',\n",
       "   'title': 'Generating EDU Extracts for Plan-Guided Summary Re-Ranking',\n",
       "   'tldr': 'Two-step approaches, in which summary candidates are generated-then-reranked to return a single summary, can improve ROUGE scores over the standard single-step approach. Yet, standard decoding methods (i.e., beam search, nucleus sampling, and diverse beam search) produce candidates with redundant, a...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76668,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76668-a-comparative-analysis-of-the-effectiveness-of-rare-tokens-on-creative-expression-using-rambert',\n",
       "   'video_url': None},\n",
       "  'P2071': {'abstract': 'Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.130',\n",
       "   'authors': ['Shoutao Guo', 'Shaolei Zhang', 'Yang Feng'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-7_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P2071',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['domain adaptation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.130.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76669/poster_document/66638cd2489d3e238199bdf3ef0d84dd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76669/poster/9e642db440ead00520164a942604923c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning Optimal Policy for Simultaneous Machine Translation via Binary Search',\n",
       "   'tldr': 'Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76669,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76669-opensr-open-modality-speech-recognition-via-maintaining-multi-modality-alignment',\n",
       "   'video_url': None},\n",
       "  'P2079': {'abstract': 'Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However,  there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The question we have addressed in this paper is to what extent the critical amount of data varies for languages of different morphological typology, in particular those that have a rich inflectional morphology, and whether the tokenization method to preprocess the data can make a difference. These details can be important for low-resourced languages that need to plan the production of datasets. We evaluated intrinsically and extrinsically the differences of five different languages with different pretraining dataset sizes and three different tokenization methods for each. The results confirm that the size of the vocabulary due to morphological characteristics is directly correlated with both the LM perplexity and the performance of two typical downstream tasks such as NER identification and POS labeling. The experiments also provide new evidence that a canonical tokenizer can reduce perplexity by more than a half for a polysynthetic language like Quechua as well as raising F1 from 0.8 to more than 0.9 in both downstream tasks with a LM trained with only 6M tokens.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.699',\n",
       "   'authors': ['Rodolfo Joel Zevallos', 'Nuria Bel'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-4_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P2079',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['evaluation'],\n",
       "   'languages': ['german', 'french', 'turkish', 'quechua'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.699.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76670/poster_document/59efc49194028a9442ac6b729983f750.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76670/poster/4491547fa34de513cfe232138d43c2d2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Hints on the data for language modeling of synthetic languages with transformers',\n",
       "   'tldr': 'Language Models (LM) are becoming more and more useful for providing representations upon which to train Natural Language Processing applications. However,  there is now clear evidence that attention-based transformers require a critical amount of language data to produce good enough LMs. The questi...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76670,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76670-hints-on-the-data-for-language-modeling-of-synthetic-languages-with-transformers',\n",
       "   'video_url': None},\n",
       "  'P2080': {'abstract': 'Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive learning, the encoding of long documents (Ks of words) is still challenging with respect to both efficiency and quality considerations. Therefore, we train Longfomer-based document encoders using a state-of-the-art unsupervised contrastive learning method (SimCSE). Further on, we complement the baseline method -siamese neural network- with additional convex neural networks based on functional Bregman divergence aiming to enhance the quality of the output document representations. We show that overall the combination of a self-contrastive siamese network and our proposed neural Bregman network outperforms the baselines in two linear classification settings on three long document topic classification tasks from the legal and biomedical domains.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.771',\n",
       "   'authors': ['Daniel Saggau',\n",
       "    'Mina Rezaei',\n",
       "    'Bernd Bischl',\n",
       "    'Ilias Chalkidis'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P2080',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['contrastive learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.771.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77585/poster_document/6a680dc93de324d29326055efda06714.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77585/slideshow/04bc550e7d3a54369ba8893dceb97987.pdf',\n",
       "   'title': 'Efficient Document Embeddings via Self-Contrastive  Bregman Divergence Learning',\n",
       "   'tldr': 'Learning quality document embeddings is a fundamental problem in natural language processing (NLP), information retrieval (IR), recommendation systems, and search engines. Despite recent advances in the development of transformer-based models that produce sentence embeddings with self-contrastive le...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77585,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77585-efficient-document-embeddings-via-self-contrastive-bregman-divergence-learning',\n",
       "   'video_url': None},\n",
       "  'P2086': {'abstract': \"Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with ``\\\\textit{Let's think step by step}'' as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors,  missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.147',\n",
       "   'authors': ['Lei Wang',\n",
       "    'Wanyu Xu',\n",
       "    'Yihuai Lan',\n",
       "    'Zhiqiang Hu',\n",
       "    'Yunshi Lan',\n",
       "    'Roy Ka-Wei Lee',\n",
       "    'Ee-Peng Lim'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(virtual-poster)'],\n",
       "   'id': 'P2086',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.147.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76671/poster_document/9826717ce4e85148d776e3cf7906dc7f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76671/poster/bc4058ddb3dea7f813ac3132a96f4a29.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76671/slideshow/50c4f80d8e873ae03b9da2b3a534c62b.pptx',\n",
       "   'title': 'Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models',\n",
       "   'tldr': 'Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76671,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76671-plan-and-solve-prompting-improving-zero-shot-chain-of-thought-reasoning-by-large-language-models',\n",
       "   'video_url': None},\n",
       "  'P2091': {'abstract': \"The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.163',\n",
       "   'authors': ['Anthony B Sicilia', 'Malihe Alikhani'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-4_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P2091',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['bias/toxicity'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.163.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76672/poster_document/09ce4480e15e29f08ed844ca38bf7a04.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76672/poster/7133ce1591d44f8df0654ed484ad7185.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76672/slideshow/674c319968ca43a10640c8bb4f50d3b7.pdf',\n",
       "   'title': 'Learning to Generate Equitable Text in Dialogue from Biased Training Data',\n",
       "   'tldr': \"The ingrained principles of fairness in a dialogue system's decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts t...\",\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76672,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76672-graph-based-relation-mining-for-context-free-out-of-vocabulary-word-embedding-learning',\n",
       "   'video_url': None},\n",
       "  'P2092': {'abstract': 'Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks.\\nIn this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets---two in the legal domain and two in the biomedical domain, each with two levels of label granularity--- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. \\nUsing encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.360',\n",
       "   'authors': ['Yova Kementchedjhieva', 'Ilias Chalkidis'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2092',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['healthcare applications, clincial nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.360.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77586/slideshow/9fe13e46e570cbeb8baf4136e59aaf0c.pdf',\n",
       "   'title': 'An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text',\n",
       "   'tldr': 'Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks.\\nIn this study, we compare four methods for multi-label classification, two based on an encoder only,...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77586,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77586-an-exploration-of-encoder-decoder-approaches-to-multi-label-classification-for-legal-and-biomedical-text',\n",
       "   'video_url': None},\n",
       "  'P2093': {'abstract': 'Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the teacher asking about this gap in an effective manner, thus creating a rich and interactive educational experience. We focus on the problem of generating such gap-focused questions (GFQs) automatically. We define the task, highlight key desired aspects of a good GFQ, and propose a model that satisfies these. Finally, we provide an  evaluation by human annotators of our generated questions compared against human generated ones, demonstrating competitive performance.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.20',\n",
       "   'authors': ['Roni Rabin',\n",
       "    'Alexandre Djerbetian',\n",
       "    'Roee Engelberg',\n",
       "    'Lidan Hackmon',\n",
       "    'Gal Elidan',\n",
       "    'Reut Tsarfaty',\n",
       "    'Amir Globerson'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-3_-question-answering-(poster)'],\n",
       "   'id': 'P2093',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['question generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.20.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76673/poster_document/779d0ef1ffe56a401bc74361e7e69e5e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76673/poster/10437be16058659dd76f6e8e01668a25.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Covering Uncommon Ground: Gap-Focused Question Generation for Answer Assessment',\n",
       "   'tldr': 'Human communication often involves information gaps between the interlocutors. For example, in an educational dialogue a student often provides an answer that is incomplete, and there is a gap between this answer and the perfect one expected by the teacher. Successful dialogue then hinges on the tea...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76673,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76673-covering-uncommon-ground-gap-focused-question-generation-for-answer-assessment',\n",
       "   'video_url': None},\n",
       "  'P2095': {'abstract': 'Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses.\\nFor this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning methods.\\nHowever, not all existing sense embeddings cover all senses of ambiguous words equally well due to the discrepancies in their training resources.\\nTo address this problem, we propose the first-ever meta-sense embedding method -- Neighbour Preserving Meta-Sense Embeddings, which learns meta-sense embeddings by combining multiple independently trained source sense embeddings such that the sense neighbourhoods computed from the source embeddings are preserved in the meta-embedding space.\\nOur proposed method can combine source sense embeddings that cover different sets of word senses.\\nExperimental results on Word Sense Disambiguation (WSD) and Word-in-Context (WiC) tasks show that the proposed meta-sense embedding method consistently outperforms several competitive baselines.{An anonymised version of the source code implementation for our proposed method is submitted to reviewing system. \\nBoth source code and the learnt meta-sense embeddings will be publicly released upon paper acceptance.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.165',\n",
       "   'authors': ['Haochen Luo', 'Yi Zhou', 'Danushka Bollegala'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-7_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2095',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['word embeddings'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.165.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77587/poster_document/a30fe25a8f7d0a5cbb655b01668c18a5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77587/poster/e5cfc92b2f2b895e58e0499eaf5fb61c.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77587/slideshow/32579d541d22de1a211d37cdbb761d8b.pdf',\n",
       "   'title': 'Together We Make Sense--Learning Meta-Sense Embeddings',\n",
       "   'tldr': 'Sense embedding learning methods learn multiple vectors for a given ambiguous word, corresponding to its different word senses.\\nFor this purpose, different methods have been proposed in prior work on sense embedding learning that use different sense inventories, sense-tagged corpora and learning met...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77587,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77587-together-we-make-sense-learning-meta-sense-embeddings',\n",
       "   'video_url': None},\n",
       "  'P2098': {'abstract': \"A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as always' or rarely') and mimicking humans in compositionally learning complex tasks. Here, we present LaSQuE, a method that can learn zero-shot classifiers from language explanations by using three new strategies - (1) modeling the semantics of linguistic quantifiers in explanations (including exploiting ordinal strength relationships, such as always' > likely'), (2) aggregating information from multiple explanations using an attention-based mechanism, and (3) model training via curriculum learning. With these strategies, LaSQuE outperforms prior work, showing an absolute gain of up to 7\\\\% in generalizing to unseen real-world classification tasks.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.467',\n",
       "   'authors': ['Sayan Ghosh', 'Rakesh R. Menon', 'Shashank Srivastava'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P2098',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.467.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77589/poster_document/48af9e0bda4b81cee9bd96db6307c7fe.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LaSQuE: Improved Zero-Shot Classification from Explanations Through Quantifier Modeling and Curriculum Learning',\n",
       "   'tldr': \"A hallmark of human intelligence is the ability to learn new concepts purely from language. Several recent approaches have explored training machine learning models via natural language supervision. However, these approaches fall short in leveraging linguistic quantifiers (such as always' or rarel...\",\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77589,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77589-lasque-improved-zero-shot-classification-from-explanations-through-quantifier-modeling-and-curriculum-learning',\n",
       "   'video_url': None},\n",
       "  'P21': {'abstract': 'Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that understands the discourse structure of multi-party dialogues, namely, to whom each utterance is replying. However, due to the lack of explicitly annotated discourse labels in multi-party dialogue corpora, previous works fail to scale up the pre-training process by putting aside the unlabeled multi-party conversational data for nothing. To fully utilize the unlabeled data, we propose to treat the discourse structures as latent variables, then jointly infer them and pre-train the discourse-aware model by unsupervised latent variable inference methods. Experiments on multiple downstream tasks show that our pre-trained model outperforms strong baselines by large margins and achieves state-of-the-art (SOTA) results, justifying the effectiveness of our method. The official implementation of this paper is available at https://github.com/EricLee8/MPD\\\\_EMVI.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.533',\n",
       "   'authors': ['Yiyang Li', 'Xinting Huang', 'Wei Bi', 'Hai Zhao'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-4_-dialogue-and-interactive-systems-(virtual-poster)'],\n",
       "   'id': 'P21',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.533.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76359/poster_document/1ca5c87f81dafbecb11d5fe7ef643b07.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76359/poster/f229ba5aeff5baad9bece4fe499aa30f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pre-training Multi-party Dialogue Models with Latent Discourse Inference',\n",
       "   'tldr': 'Multi-party dialogues are more difficult for models to understand than one-to-one two-party dialogues, since they involve multiple interlocutors, resulting in interweaving reply-to relations and information flows. To step over these obstacles, an effective way is to pre-train a model that understand...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76359,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76359-pre-training-multi-party-dialogue-models-with-latent-discourse-inference',\n",
       "   'video_url': None},\n",
       "  'P2105': {'abstract': \"Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user's needs. Further, the topic evolutions produced by DTMs tend to contain generic terms that are not indicative of their designated time steps. To address these issues, we propose the task of discriminative dynamic topic discovery. This task aims to discover topic evolutions from temporal corpora that distinctly align with a set of user-provided category names and uniquely capture topics at each time step. We solve this task by developing DynaMiTE, a framework that ensembles semantic similarity, category indicative, and time indicative scores to produce informative topic evolutions. Through experiments on three diverse datasets, including the use of a newly-designed human evaluation experiment, we demonstrate that DynaMiTE is a practical and efficient framework for helping users discover high-quality topic evolutions suited to their interests.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.14',\n",
       "   'authors': ['Nishant Balepur',\n",
       "    'Shivam Agarwal',\n",
       "    'Karthik Venkat Ramanan',\n",
       "    'Susik Yoon',\n",
       "    'Diyi Yang',\n",
       "    'Jiawei Han'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-7_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2105',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['document representation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.14.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77590/poster_document/d319274e5062ba85c24657f40105667f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77590/poster/d66fdc8011ac4bbded488e7569a15f73.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77590/slideshow/90113e587afc420d5192257bc7a2c216.pdf',\n",
       "   'title': 'DynaMiTE: Discovering Explosive Topic Evolutions with User Guidance',\n",
       "   'tldr': \"Dynamic topic models (DTMs) analyze text streams to capture the evolution of topics. Despite their popularity, existing DTMs are either fully supervised, requiring expensive human annotations, or fully unsupervised, producing topic evolutions that often do not cater to a user's needs. Further, the t...\",\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 77590,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77590-dynamite-discovering-explosive-topic-evolutions-with-user-guidance',\n",
       "   'video_url': None},\n",
       "  'P2106': {'abstract': \"Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping the model's raw behavior. However, existing work only studies the monolingual scenario, which lacks the cross-lingual transferability to perform editing simultaneously across languages. In this work, we focus on cross-lingual model editing. Firstly, we define the cross-lingual model editing task and corresponding metrics, where an edit in one language propagates to the others. Next, we propose a framework to naturally adapt monolingual model editing approaches to the cross-lingual scenario using parallel corpus. Further, we propose language anisotropic editing to improve cross-lingual editing by amplifying different subsets of parameters for each language. On the newly defined cross-lingual model editing task, we empirically demonstrate the failure of monolingual baselines in propagating the edit to multiple languages and the effectiveness of the proposed language anisotropic model editing. Our code is publicly available at https://github.com/franklear/LiME.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.343',\n",
       "   'authors': ['Yang Xu', 'Yutai Hou', 'Wanxiang Che', 'Min Zhang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P2106',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.343.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77591/poster_document/19006c89a0ca5ec3216dfb8813f705ba.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Language Anisotropic Cross-Lingual Model Editing',\n",
       "   'tldr': 'Multilingual pre-trained language models can learn task-specific abilities or memorize facts across multiple languages but inevitably make undesired predictions with specific inputs. Under similar observation, model editing aims to post-hoc calibrate a model targeted to specific inputs with keeping ...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77591,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77591-language-anisotropic-cross-lingual-model-editing',\n",
       "   'video_url': None},\n",
       "  'P211': {'abstract': 'For most natural language processing tasks, the dominant practice is to finetune \\nlarge pretrained transformer models (e.g., BERT) using smaller downstream datasets.\\nDespite the success of this approach, it remains unclear to what extent these gains\\nare attributable to the massive background corpora employed for pretraining versus \\nto the pretraining objectives themselves. This paper introduces a large-scale study of self-pretraining, where the same (downstream) training data is used for both pretraining and finetuning.\\nIn experiments addressing both ELECTRA and RoBERTa models and 10 distinct downstream classification datasets, we observe that self-pretraining \\nrivals standard pretraining on the BookWiki corpus  (despite using around 10x--500x less data), outperforming the latter on 7 and 5 datasets, respectively.\\nSurprisingly, these task-specific pretrained models often perform well on other tasks,\\nincluding the GLUE benchmark. Besides classification tasks, self-pretraining \\nalso provides benefits on structured output prediction tasks such as span based question answering and commonsense inference, often providing more than 50\\\\% of the performance boosts \\nprovided by pretraining on the BookWiki corpus. Our results hint that in many scenarios,  performance gains attributable to pretraining are driven primarily by the pretraining objective itself \\nand are not always attributable to the use of external pretraining data in massive amounts.\\nThese findings are especially relevant in light of concerns about intellectual property \\nand offensive content in web-scale pretraining data.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.682',\n",
       "   'authors': ['Kundan Krishna',\n",
       "    'Saurabh Garg',\n",
       "    'Jeffrey P. Bigham',\n",
       "    'Zachary Lipton'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-1_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P211',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['self-supervised learning',\n",
       "    'transfer learning / domain adaptation',\n",
       "    'representation learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.682.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76382/poster_document/db292d68771b6d60607c36347aaf03b8.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Downstream Datasets Make Surprisingly Good Pretraining Corpora',\n",
       "   'tldr': 'For most natural language processing tasks, the dominant practice is to finetune \\nlarge pretrained transformer models (e.g., BERT) using smaller downstream datasets.\\nDespite the success of this approach, it remains unclear to what extent these gains\\nare attributable to the massive background corpora...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76382,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76382-downstream-datasets-make-surprisingly-good-pretraining-corpora',\n",
       "   'video_url': None},\n",
       "  'P2112': {'abstract': 'Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words~CITATION.\\nThis similarity underestimation problem is particularly severe for high frequent words.\\nAlthough this problem has been noted in prior work, no solution has been proposed thus far.\\nWe observe that the $\\\\ell_2$ norm of contextualised embeddings of a word correlates with its log-frequency in the pretraining corpus.\\nConsequently, the larger $\\\\ell_2$ norms associated with the high frequent words reduce the cosine similarity values measured between them, thus underestimating the similarity scores.\\nTo solve this issue, we propose a method to \\\\emph{discount} the $\\\\ell_2$ norm of a contextualised word embedding by the frequency of that word in a corpus when measuring the cosine similarities between words.\\nWe show that the so called \\\\emph{stop} words behave differently from the rest of the words, which require special consideration during their discounting process.\\nExperimental results on a contextualised word similarity dataset show that our proposed discounting method accurately solves the similarity underestimation problem.{An anonymized version of the source code of our proposed method is submitted to the reviewing system.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.550',\n",
       "   'authors': ['Saeth Wannasuphoprasit', 'Yi Zhou', 'Danushka Bollegala'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-1_-semantics_-lexical-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2112',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['word embeddings'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.550.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77592/poster/1fbb2c55cbb347b4628df8506cd77e1c.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77592/slideshow/55c9d431b82268e060ec9957c2657453.pdf',\n",
       "   'title': 'Solving Cosine Similarity Underestimation between High Frequency Words by $\\\\ell_2$ Norm Discounting',\n",
       "   'tldr': 'Cosine similarity between two words, computed using their contextualised token embeddings obtained from masked language models (MLMs) such as BERT has shown to underestimate the actual similarity between those words~CITATION.\\nThis similarity underestimation problem is particularly severe for high fr...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 77592,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77592-solving-cosine-similarity-underestimation-between-high-frequency-words-by-dashell_2-norm-discounting',\n",
       "   'video_url': None},\n",
       "  'P2114': {'abstract': 'In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.260',\n",
       "   'authors': ['Chenzhengyi Liu',\n",
       "    'Jie Huang',\n",
       "    'Kerui Zhu',\n",
       "    'Kevin Chen-Chuan Chang'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-5_-semantics_-lexical-(oral)'],\n",
       "   'id': 'P2114',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['lexical relationships'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.260.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76238/poster_document/1e2f533f260ad1e5eb6e99800ea67063.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76238/poster/12a835eb2752423a17a291db96dfe76b.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships',\n",
       "   'tldr': 'In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE t...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 76238,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15253/lecture/76238-dimongen-diversified-generative-commonsense-reasoning-for-explaining-concept-relationships',\n",
       "   'video_url': None},\n",
       "  'P2116': {'abstract': \"Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. \\nPrior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained language models, their capability to detect this transfer should be investigated. \\nTo this end, this paper proposes to probe the ability of GPT-3 to detect metaphoric language and predict the metaphor's source domain without any pre-set domains. We experiment with different training sample configurations for fine-tuning and few-shot prompting on two distinct datasets. When provided 12 few-shot samples in the prompt, GPT-3 generates the correct source domain for a new sample with an accuracy of 65.15\\\\% in English and 34.65\\\\% in Spanish. GPT's most common error is a hallucinated source domain for which no indicator is present in the sentence. Other common errors include identifying a sequence as literal even though a metaphor is present and predicting the wrong source domain based on specific words in the sequence that are not metaphorically related to the target domain.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.58',\n",
       "   'authors': ['Lennart Wachowiak', 'Dagmar Gromann'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-5_-semantics_-lexical-(oral)'],\n",
       "   'id': 'P2116',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['metaphor'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.58.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76239/poster_document/14c927d3c89885afeae8712ddbae0e9c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76239/poster/6c39c4982d3ce32f5b56fe3f4c5c4e90.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76239/slideshow/7dee114373ab2d416900c7c412d9084b.pdf',\n",
       "   'title': 'Does GPT-3 Grasp Metaphors? Identifying Metaphor Mappings with Generative Language Models',\n",
       "   'tldr': 'Conceptual metaphors present a powerful cognitive vehicle to transfer knowledge structures from a source to a target domain. \\nPrior neural approaches focus on detecting whether natural language sequences are metaphoric or literal. We believe that to truly probe metaphoric knowledge in pre-trained la...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 76239,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15253/lecture/76239-does-gpt-3-grasp-metaphorsquestion-identifying-metaphor-mappings-with-generative-language-models',\n",
       "   'video_url': None},\n",
       "  'P2119': {'abstract': 'When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create GrailQAbility, a new benchmark KBQA dataset with unanswerability, by first identifying various forms of KB incompleteness that make questions unanswerable, and then systematically adapting GrailQA (a popular KBQA dataset with only answerable questions). Experimenting with three state-of-the-art KBQA models, we find that all three models suffer a drop in performance even after suitable adaptation for unanswerable questions. In addition, these often detect unanswerability for wrong reasons and find specific forms of unanswerability particularly difficult to handle. This underscores the need for further research in making KBQA systems robust to unanswerability.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.576',\n",
       "   'authors': ['Mayur Patidar',\n",
       "    'Prayushi Faldu',\n",
       "    'Avinash Kumar Singh',\n",
       "    'Lovekesh Vig',\n",
       "    'Indrajit Bhattacharya',\n",
       "    'Mausam -'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-3_-question-answering-(poster)'],\n",
       "   'id': 'P2119',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['knowledge base qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.576.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76674/poster_document/ec5db52f122b2de6c20452a94ad97a6d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76674/poster/76f5d8caa6d50f6a40b54f1d994acda9.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Do I have the Knowledge to Answer? Investigating Answerability of Knowledge Base Questions',\n",
       "   'tldr': 'When answering natural language questions over knowledge bases, missing facts, incomplete schema and limited scope naturally lead to many questions being unanswerable. While answerability has been explored in other QA settings, it has not been studied for QA over knowledge bases (KBQA). We create Gr...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76674,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76674-fermat-an-alternative-to-accuracy-for-numerical-reasoning',\n",
       "   'video_url': None},\n",
       "  'P2122': {'abstract': 'Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model capabilities, more effectively extracting knowledge during inference from a given set of model parameters.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.580',\n",
       "   'authors': ['Ariel Gera',\n",
       "    'Roni Friedman',\n",
       "    'Ofir Arviv',\n",
       "    'Chulaka Gunasekara',\n",
       "    'Benjamin Sznajder',\n",
       "    'Noam Slonim',\n",
       "    'Eyal Shnarch'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-6_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P2122',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generative models'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.580.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76675/poster_document/d0bae2e3a0f9f5069ae61b87161bca7b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76675/poster/fe859018e7242e787005c3a8fe7a820f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers',\n",
       "   'tldr': 'Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76675,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76675-the-benefits-of-bad-advice-autocontrastive-decoding-across-model-layers',\n",
       "   'video_url': None},\n",
       "  'P2123': {'abstract': 'Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them.\\nFocusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. \\nOur analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. \\nWe then develop an English question-generation model which we demonstrate via automatic and human evaluation produces less ambiguous questions. \\nWe further show that the question generation objective we use allows the model to integrate answer group information without any direct supervision.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.569',\n",
       "   'authors': ['Elias Stengel-Eskin',\n",
       "    'Jimena Guallar-Blasco',\n",
       "    'Yi Zhou',\n",
       "    'Benjamin Van Durme'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-2_-language-grounding-to-vision,-robotics,-and-beyond-(oral)'],\n",
       "   'id': 'P2123',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['vision question answering'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.569.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76240/poster_document/107a3e7c65ac323a810601f9ae54a4c2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76240/poster/984b1ce96b2d4f7ff47bce7b167c845f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76240/slideshow/7f3d0b979d7a84e4aa01e0666c644c23.pdf',\n",
       "   'title': 'Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA',\n",
       "   'tldr': 'Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them.\\nFocusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76240,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15210/lecture/76240-why-did-the-chicken-cross-the-roadquestion-rephrasing-and-analyzing-ambiguous-questions-in-vqa',\n",
       "   'video_url': None},\n",
       "  'P2125': {'abstract': 'Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.100',\n",
       "   'authors': ['Zhiming Mao', 'Huimin Wang', 'Yiming Du', 'Kam-Fai Wong'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-1_-information-retrieval-and-text-mining-(virtual-poster)'],\n",
       "   'id': 'P2125',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['contrastive learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.100.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76676/poster_document/5a9e7af080446286e85cb031e13938e4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76676/poster/0b7db52ad3dfa7f11b9ae778c628dd9a.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76676/slideshow/d9e781fd788ee11096290f94ef92614f.pdf',\n",
       "   'title': 'UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation',\n",
       "   'tldr': 'Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, ...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 76676,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76676-unitrec-a-unified-text-to-text-transformer-and-joint-contrastive-learning-framework-for-text-based-recommendation',\n",
       "   'video_url': None},\n",
       "  'P2126': {'abstract': \"Biographical event detection is a relevant task that allows for the exploration and comparison of the ways in which people's lives are told and represented. This may support several real-life applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was aligned with 5 existing corpora in order to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859.  Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.691',\n",
       "   'authors': ['Marco Antonio Stranisci',\n",
       "    'Rossana Damiano',\n",
       "    'Enrico Mensa',\n",
       "    'Viviana Patti',\n",
       "    'Daniele P. Radicioni',\n",
       "    'Tommaso Caselli'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-1_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P2126',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['corpus creation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.691.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76677/poster_document/40c962092b99d82f96120720f163b8ad.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76677/poster/7212ce8973330483a1fe62489d8f01f3.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events',\n",
       "   'tldr': \"Biographical event detection is a relevant task that allows for the exploration and comparison of the ways in which people's lives are told and represented. This may support several real-life applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite th...\",\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76677,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76677-wikibio-a-semantic-resource-for-the-intersectional-analysis-of-biographical-events',\n",
       "   'video_url': None},\n",
       "  'P2127': {'abstract': 'Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. \\nIn this work, we assess whether a top-$p$ set is indeed aligned with its probabilistic meaning in various linguistic contexts.\\nWe employ conformal prediction, a calibration procedure that focuses on the construction of minimal prediction sets according to a desired confidence level, to calibrate the parameter $p$ as a function of the entropy of the next word distribution. We find that OPT models are overconfident, and that calibration shows a moderate inverse scaling with model size.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.3',\n",
       "   'authors': ['Shauli Ravfogel', 'Yoav Goldberg', 'Jacob Goldberger'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2127',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['calibration/uncertainty'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.3.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77593/poster_document/afa05748761d30a7ccf2023dc655fe67.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Conformal Nucleus Sampling',\n",
       "   'tldr': 'Language models generate text based on successively sampling the next word. A decoding procedure based on nucleus (top-$p$) sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability $p$. \\nIn this work, we assess whether a top-$p$ set is indeed alig...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77593,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77593-conformal-nucleus-sampling',\n",
       "   'video_url': None},\n",
       "  'P2129': {'abstract': 'Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful.\\nHowever, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood.\\nIn this work, we formally define the notion of linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications.\\nWe show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept.\\nHowever, we constructively demonstrate that a multiclass log-linear model \\\\emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of linear guardedness as a downstream bias mitigation technique.\\nThese findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between intrinsic and extrinsic bias in neural models.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.523',\n",
       "   'authors': ['Shauli Ravfogel', 'Yoav Goldberg', 'Ryan Cotterell'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-6_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P2129',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['representation learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.523.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76678/poster_document/30b66913d11b28929d6ff85f506e1539.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76678/poster/77f4034b8730604f09d03026d764e3d8.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76678/slideshow/2e752bcf5e9c76a37629b6396fe0c816.pdf',\n",
       "   'title': 'Linear Guardedness and its Implications',\n",
       "   'tldr': 'Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful.\\nHowever, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood.\\nIn this work, ...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76678,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76678-linear-guardedness-and-its-implications',\n",
       "   'video_url': None},\n",
       "  'P2130': {'abstract': 'Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words.\\nWe propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates.\\nGiven two snapshots $C_1$ and $C_2$ of a corpus taken respectively at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised method to select (a) \\\\emph{pivot} terms related to both $C_1$ and $C_2$, and (b) \\\\emph{anchor} terms that are associated with a specific pivot term in each individual snapshot.\\nWe then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms.\\nMoreover, we propose an automatic method to learn time-sensitive templates from $C_1$ and $C_2$, without requiring any human supervision.\\nNext, we use the generated prompts to adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts.\\nMultiple experiments show that our proposed method significantly reduces the perplexity of test sentences in $C_2$, outperforming the current state-of-the-art.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.520',\n",
       "   'authors': ['Xiaohang Tang', 'Yi Zhou', 'Danushka Bollegala'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-5_-information-extraction-(oral)'],\n",
       "   'id': 'P2130',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['lexical semantic change'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.520.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76241/poster_document/c310f87bbeb6ccea69ba7c02efaa110e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76241/poster/06f9c55f28ea2d0fed3d9d8a5e282459.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation',\n",
       "   'tldr': 'Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words.\\nWe propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates.\\nGiven two snapshots $C_1$ and $C_2$ of a corpus taken respectively at t...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 76241,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15250/lecture/76241-learning-dynamic-contextualised-word-embeddings-via-template-based-temporal-adaptation',\n",
       "   'video_url': None},\n",
       "  'P2131': {'abstract': 'In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.697',\n",
       "   'authors': ['Simone Tedeschi',\n",
       "    'Johan Bos',\n",
       "    'Thierry Declerck',\n",
       "    'Jan Haji',\n",
       "    'Daniel Hershcovich',\n",
       "    'Eduard H Hovy',\n",
       "    'Alexander Koller',\n",
       "    'Simon Krek',\n",
       "    'Steven Schockaert',\n",
       "    'Rico Sennrich',\n",
       "    'Ekaterina Shutova',\n",
       "    'Roberto Navigli'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Theme: Reality Check',\n",
       "   'event_ids': ['session-2_-theme_-reality-check-(oral)'],\n",
       "   'id': 'P2131',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['right for the wrong reasons', 'ai hype & expectations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.697.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76242/poster_document/f798fff59f4a345c495f6960f278830f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76242/poster/a4649c6041db21b36558f53d54a949d4.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76242/slideshow/3364dc39de42c21d7315dc6678a9e968.pdf',\n",
       "   'title': \"What's the Meaning of Superhuman Performance in Today's NLU?\",\n",
       "   'tldr': 'In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. Th...',\n",
       "   'track': 'Theme: Reality Check',\n",
       "   'underline_id': 76242,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15204/lecture/76242-what-s-the-meaning-of-superhuman-performance-in-today-s-nluquestion',\n",
       "   'video_url': None},\n",
       "  'P2132': {'abstract': 'Multilingual neural machine translation has witnessed remarkable progress in recent years. \\nHowever, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others.\\nExisting balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier{In Pareto optimization, Pareto optimal solutions refer to solutions in which none of the objectives can be improved without sacrificing at least one of the other objectives. The set of all Pareto optimal solutions forms a Pareto frontier.}.\\nIn this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs.\\nSpecifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation.\\nFurthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. \\nExperimental results on the widely-used WMT and TED datasets show that our method significantly pushes the Pareto frontier and outperforms baselines by up to +2.46 BLEU{Our code will be released upon acceptance.}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.211',\n",
       "   'authors': ['yichong huang',\n",
       "    'Xiaocheng Feng',\n",
       "    'Xinwei Geng',\n",
       "    'Baohang Li',\n",
       "    'Bing Qin'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-6_-machine-translation-(oral)'],\n",
       "   'id': 'P2132',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.211.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76243/poster_document/412452546723df2b0fdb0c8d48ee855e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76243/poster/9c04448a10ac0851ec2fc7edb92150bb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Higher Pareto Frontier in Multilingual Machine Translation',\n",
       "   'tldr': 'Multilingual neural machine translation has witnessed remarkable progress in recent years. \\nHowever, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others.\\nExistin...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76243,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15261/lecture/76243-towards-higher-pareto-frontier-in-multilingual-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P2133': {'abstract': 'Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than thousands of data points, DePlot+LLM with just one-shot prompting achieves a 29.4\\\\% improvement over finetuned SOTA on human-written queries from the task of chart QA.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.660',\n",
       "   'authors': ['Fangyu Liu',\n",
       "    'Julian Martin Eisenschlos',\n",
       "    'Francesco Piccinno',\n",
       "    'Syrine Krichene',\n",
       "    'Chenxi Pang',\n",
       "    'Kenton Lee',\n",
       "    'Mandar Joshi',\n",
       "    'Wenhu Chen',\n",
       "    'Nigel Collier',\n",
       "    'Yasemin Altun'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['session-4_-question-answering-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2133',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodal qa'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.660.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77594/poster_document/06d0a8b0a189ef3ee1f0dfee9b67d216.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77594/poster/392f9e7375a9168653351d26137b7d68.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77594/slideshow/7c14e230b0da6fb8119b1ae16c2a8623.pdf',\n",
       "   'title': 'DePlot: One-shot visual language reasoning by plot-to-table translation',\n",
       "   'tldr': 'Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 77594,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77594-structsp-efficient-fine-tuning-of-task-oriented-dialog-system-by-using-structure-aware-boosting-and-grammar-constraints',\n",
       "   'video_url': None},\n",
       "  'P2134': {'abstract': \"Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based Encoder-Decoder (TAED) for speech-to-text tasks. The new method leverages AED's strength in non-monotonic sequence to sequence learning while retaining Transducer's streaming property. In the proposed framework, Transducer and AED share the same speech encoder. The predictor in  Transducer is replaced by the decoder in the AED model, and the outputs of the decoder are conditioned on the speech inputs instead of outputs from an unconditioned language model. The proposed solution ensures that the model is optimized by covering all possible read/write scenarios and creates a matched environment for streaming applications. We evaluate the proposed approach on the \\\\textsc{MuST-C} dataset and the findings demonstrate that TAED  performs significantly better than Transducer for offline automatic speech recognition (ASR) and speech-to-text translation (ST) tasks. In the streaming case, TAED outperforms Transducer in the ASR task and one ST direction while comparable results are achieved in another translation direction.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.695',\n",
       "   'authors': ['Yun Tang',\n",
       "    'Anna Y Sun',\n",
       "    'Hirofumi Inaguma',\n",
       "    'Xinyue Chen',\n",
       "    'Ning Dong',\n",
       "    'Xutai Ma',\n",
       "    'Paden D Tomasello',\n",
       "    'Juan Pino'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['session-4_-speech-and-multimodality-(virtual-poster)'],\n",
       "   'id': 'P2134',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['spoken language translation'],\n",
       "   'languages': ['spanish; castilian', 'german'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.695.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76679/poster_document/bdab67af1e9b0da313fb54809ea17a66.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76679/poster/089e75f00725efc202c529e03ce07171.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76679/slideshow/4c241954076f86d410c3cc51afab8c35.pdf',\n",
       "   'title': 'Hybrid Transducer and Attention based Encoder-Decoder Modeling for Speech-to-Text Tasks',\n",
       "   'tldr': 'Transducer and Attention based Encoder-Decoder (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-to-text tasks. In order to leverage strengths of both modeling methods, we propose a solution ...',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 76679,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76679-can-nli-provide-proper-indirect-supervision-for-low-resource-biomedical-relation-extractionquestion',\n",
       "   'video_url': None},\n",
       "  'P2136': {'abstract': \"For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations.  Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.306',\n",
       "   'authors': ['Dongqi Pu', 'Yifan Wang', 'Vera Demberg'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['session-4_-summarization-(virtual-poster)'],\n",
       "   'id': 'P2136',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.306.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76680/poster_document/8100b5874d2495b604ca49483e617346.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76680/poster/f5182e2433e6e95f858d2cedfc1e51f6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization',\n",
       "   'tldr': 'For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the vari...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76680,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76680-incorporating-distributions-of-discourse-structure-for-long-document-abstractive-summarization',\n",
       "   'video_url': None},\n",
       "  'P2140': {'abstract': 'The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the cross-modal information present in multimodal data. Such information from the visual modality, such as color, size, and shape, provides additional attributes that users can include in their inputs. Thus, we propose cross-modal attribute insertions as a realistic perturbation strategy for vision-and-language data that inserts visual attributes of the objects in the image into the corresponding text (e.g., \"girl on a chair\\'\\' to \"little girl on a wooden chair\\'\\'). Our proposed approach for cross-modal attribute insertions is modular, controllable, and task-agnostic. We find that augmenting input text using cross-modal insertions causes state-of-the-art approaches for text-to-image retrieval and cross-modal entailment to perform poorly, resulting in relative drops of ~15\\\\% in MRR and ~20\\\\% in F1 score, respectively. Crowd-sourced annotations demonstrate that cross-modal insertions lead to higher quality augmentations for multimodal data than augmentations using text-only data, and are equivalent in quality to original examples. We release the code to encourage robustness evaluations of deep vision-and-language models: https://github.com/claws-lab/multimodal-robustness-xmai',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.890',\n",
       "   'authors': ['Shivaen Ramshetty', 'Gaurav Verma', 'Srijan Kumar'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-6_-nlp-applications-(poster)'],\n",
       "   'id': 'P2140',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multimodal applications'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.890.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76681/poster_document/1e87aa78b0da870fb88a2617de1a2aa9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76681/poster/ef049638552f5c0c709ede4be8199177.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76681/slideshow/4889333368bd2f610ee9de676a93740e.pdf',\n",
       "   'title': 'Cross-Modal Attribute Insertions for Assessing the Robustness of Vision-and-Language Learning',\n",
       "   'tldr': 'The robustness of multimodal deep learning models to realistic changes in the input text is critical for applicability on important tasks such as text-to-image retrieval and cross-modal entailment. To measure robustness, several existing approaches edit the text data, but without leveraging the cros...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76681,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76681-mustie-multimodal-structural-transformer-for-web-information-extraction',\n",
       "   'video_url': None},\n",
       "  'P2145': {'abstract': 'In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities.\\nThis paper presents the first multi-dimensional analysis of bias in English VL models, focusing on gender, ethnicity, and age as dimensions.\\nWhen subjects are input as images, pre-trained VL models complete a neutral template with a hurtful word 5\\\\% of the time, with higher percentages for female and young subjects.\\nBias presence in downstream models has been tested on Visual Question Answering. We developed a novel bias metric called the Vision-Language Association Test based on questions designed to elicit biased associations between stereotypical concepts and targets. Our findings demonstrate that pre-trained VL models contain biases that are perpetuated in downstream tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.403',\n",
       "   'authors': ['Gabriele Ruggeri', 'Debora Nozza'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-1_-ethics-and-nlp-(virtual-poster)'],\n",
       "   'id': 'P2145',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.403.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77595/poster/b2fdb93966578db54134809786d53143.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77595/slideshow/bc69c743bcf8355dcff17a71fbab88ff.pdf',\n",
       "   'title': 'A Multi-dimensional study on Bias in Vision-Language models',\n",
       "   'tldr': 'In recent years, joint Vision-Language (VL) models have increased in popularity and capability. Very few studies have attempted to investigate bias in VL models, even though it is a well-known issue in both individual modalities.\\nThis paper presents the first multi-dimensional analysis of bias in En...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77595,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77595-a-multi-dimensional-study-on-bias-in-vision-language-models',\n",
       "   'video_url': None},\n",
       "  'P2152': {'abstract': 'Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a {\\\\em conjunct resolution} task that operates directly on the text and makes use of a {\\\\em split-and-rephrase} paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.762',\n",
       "   'authors': ['Royi Rassin', 'Yoav Goldberg', 'Reut Tsarfaty'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)'],\n",
       "   'id': 'P2152',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['reasoning', 'paraphrase generation', 'text simplification'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.762.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76682/poster_document/57e985c4511471eb83ffce7e371d0cda.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76682/poster/558a782d4740fec0eab6f1823b460366.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Conjunct Resolution in the Face of Verbal Omissions',\n",
       "   'tldr': 'Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of th...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 76682,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76682-cobra-frames-contextual-reasoning-about-effects-and-harms-of-offensive-statements',\n",
       "   'video_url': None},\n",
       "  'P2153': {'abstract': 'Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data through linguistic rules. However, this approach is not practical for languages with more complex morphology than English. We hypothesise that creating training data in the reverse direction, i.e. starting from gender-fair text, is easier for morphologically complex languages and show that it matches the performance of state-of-the-art rewriting models for English. To eliminate the rule-based nature of data creation, we instead propose using machine translation models to create gender-biased text from real gender-fair text via round-trip translation. Our approach allows us to train a rewriting model for German without the need for elaborate handcrafted rules. The outputs of this model increased gender-fairness as shown in a human evaluation study.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.246',\n",
       "   'authors': ['Chantal Amrhein',\n",
       "    'Florian Schottmann',\n",
       "    'Rico Sennrich',\n",
       "    'Samuel Lubli'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['poster-session-7_-ethics-and-nlp-(poster)'],\n",
       "   'id': 'P2153',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/unfairness mitigation'],\n",
       "   'languages': ['german'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.246.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76683/poster_document/040b42fb830a650d232182021507b51c.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76683/poster/80ea1dc7d23703dc51e0acbbc389bc4f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model',\n",
       "   'tldr': 'Natural language generation models reproduce and often amplify the biases present in their training data. Previous research explored using sequence-to-sequence rewriting models to transform biased model outputs (or original texts) into more gender-fair language by creating pseudo training data throu...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 76683,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76683-exploiting-biased-models-to-de-bias-text-a-gender-fair-rewriting-model',\n",
       "   'video_url': None},\n",
       "  'P2154': {'abstract': 'Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\\\\% to 21.99\\\\% and 18.42\\\\% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.151',\n",
       "   'authors': ['Lucie Charlotte Magister',\n",
       "    'Jonathan Mallinson',\n",
       "    'Jakub Dominik Adamek',\n",
       "    'Eric Malmi',\n",
       "    'Aliaksei Severyn'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-7_-machine-learning-for-nlp-(virtual-poster)'],\n",
       "   'id': 'P2154',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.151.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76684/poster_document/de53a132efbf9b626792fcba8f922c80.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76684/poster/1f08737e5f57f7bbb4ce3c393d50ca2f.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Teaching Small Language Models to Reason',\n",
       "   'tldr': 'Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explo...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76684,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76684-teaching-small-language-models-to-reason',\n",
       "   'video_url': None},\n",
       "  'P2158': {'abstract': \"In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker's personal characteristics and experiences in episodic memory (Rubin et al., 2003; Conway, 2009). In this work, we extend persona-based dialogue to the multimodal domain and make two main contributions. First, we present the first multimodal persona-based dialogue dataset named MPCHAT, which extends persona with both text and images to contain episodic memories. Second, we empirically show that incorporating multimodal persona, as measured by three proposed multimodal persona-grounded dialogue tasks (i.e., next response prediction, grounding persona prediction, and speaker identification), leads to statistically significant performance improvements across all tasks. Thus, our work highlights that multimodal persona is crucial for improving multimodal dialogue comprehension, and our MPCHAT serves as a high-quality resource for this research.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.189',\n",
       "   'authors': ['Jaewoo Ahn', 'Yeda Song', 'Sangdoo Yun', 'Gunhee Kim'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-3_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P2158',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multi-modal dialogue systems'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.189.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76685/poster_document/193da72ddc7e3c71e610b312c4f620f1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76685/poster/8d8ae97ca5b19ffed7b7c791e0441986.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76685/slideshow/1e7159b63b9d6db347c9aed18180cd18.pdf',\n",
       "   'title': 'MPCHAT: Towards Multimodal Persona-Grounded Conversation',\n",
       "   'tldr': \"In order to build self-consistent personalized dialogue agents, previous research has mostly focused on textual persona that delivers personal facts or personalities. However, to fully describe the multi-faceted nature of persona, image modality can help better reveal the speaker's personal characte...\",\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76685,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76685-mpchat-towards-multimodal-persona-grounded-conversation',\n",
       "   'video_url': None},\n",
       "  'P2160': {'abstract': 'Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and predicts discrete acoustic units subsequently. We enhance the model performance by subword prediction in the first-pass decoder, advanced two-pass decoder architecture design and search strategy, and better training regularization. To leverage large amounts of unlabeled text data, we pre-train the first-pass text decoder based on the self-supervised denoising auto-encoding task. Experimental evaluations on benchmark datasets at various data scales demonstrate that UnitY outperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU with 2.83x decoding speed-up. We show that the proposed methods boost the performance even when predicting spectrogram in the second pass. However, predicting discrete units achieves 2.51x decoding speed-up compared to that case.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.872',\n",
       "   'authors': ['Hirofumi Inaguma',\n",
       "    'Sravya Popuri',\n",
       "    'Ilia Kulikov',\n",
       "    'Peng-Jen Chen',\n",
       "    'Changhan Wang',\n",
       "    'Yu-An Chung',\n",
       "    'Yun Tang',\n",
       "    'Ann Lee',\n",
       "    'Shinji Watanabe',\n",
       "    'Juan Pino'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Speech and Multimodality',\n",
       "   'event_ids': ['poster-session-1_-speech-and-multimodality-(poster)'],\n",
       "   'id': 'P2160',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['spoken language translation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.872.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76686/poster/2638bd793fcc2bca2dcaf5f377a5f326.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units',\n",
       "   'tldr': 'Direct speech-to-speech translation (S2ST), in which all components can be optimized jointly, is advantageous over cascaded approaches to achieve fast inference with a simplified pipeline. We present a novel two-pass direct S2ST architecture, UnitY, which first generates textual representations and ...',\n",
       "   'track': 'Speech and Multimodality',\n",
       "   'underline_id': 76686,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76686-unity-two-pass-direct-speech-to-speech-translation-with-discrete-units',\n",
       "   'video_url': None},\n",
       "  'P2164': {'abstract': \"A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input.\\nIn this work, we first explore the summarization models' robustness against perturbations including word-level synonym substitution and noise.\\nTo create semantic-consistent substitutes, we propose a SummAttacker, which is an efficient approach to generating adversarial samples based on pre-trained language models.\\nExperimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets.\\nNext, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation.\\nSpecifically, the first vulnerability factor we found is the low diversity of the training inputs.\\nCorrespondingly, we expose the encoder to more diverse cases created by SummAttacker in the input space.\\nThe second factor is the vulnerability of the decoder, and we propose an augmentation in the latent space of the decoder to improve its robustness.\\nConcretely, we create virtual cases by manifold softmixing two decoder hidden states of similar semantic meanings.\\nExperimental results on Gigaword and CNN/DM datasets demonstrate that our approach achieves significant improvements over strong baselines and exhibits higher robustness on noisy, attacked, and clean datasets\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.378',\n",
       "   'authors': ['Xiuying Chen',\n",
       "    'Guodong Long',\n",
       "    'Chongyang Tao',\n",
       "    'Mingzhe Li',\n",
       "    'Xin Gao',\n",
       "    'Chengqi Zhang',\n",
       "    'Xiangliang Zhang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-1_-summarization-(poster)'],\n",
       "   'id': 'P2164',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.378.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76687/poster_document/d96f6bb184d6fd4c95dd97654170424d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76687/poster/0b68f04ce19d3ecd834ae2a7a8fef6a9.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving the Robustness of Summarization Systems with Dual Augmentation',\n",
       "   'tldr': \"A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input.\\nIn this work, we first explore the summarization models' robustness against perturbations including word-level synonym substitution and noise.\\nTo create se...\",\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76687,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76687-improving-the-robustness-of-summarization-systems-with-dual-augmentation',\n",
       "   'video_url': None},\n",
       "  'P2165': {'abstract': 'State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge and could learn the nuances of poetry from data alone, reducing the degree of human supervision required. In this work, we investigate end-to-end poetry generation conditioned on styles such as rhyme, meter, and alliteration. We identify and address lack of training data and mismatching tokenization algorithms as possible limitations of past attempts. In particular, we successfully pre-train ByGPT5, a new token-free decoder-only language model, and fine-tune it on a large custom corpus of English and German quatrains annotated with our styles. We show that ByGPT5 outperforms other models such as mT5, ByT5, GPT-2 and ChatGPT, while also being more parameter efficient and performing favorably compared to humans. In addition, we analyze its runtime performance and demonstrate that it is not prone to memorization. We make our code, models, and datasets publicly available.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.406',\n",
       "   'authors': ['Jonas Belouadi', 'Steffen Eger'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-5_-generation-(oral)'],\n",
       "   'id': 'P2165',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['human evaluation',\n",
       "    'automatic evaluation',\n",
       "    'analysis',\n",
       "    'model architectures'],\n",
       "   'languages': ['german'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.406.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76244/poster_document/ccc540d972f596f8c5e9b9533bfcba37.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76244/poster/e98e6986faa3015c81f44b65f3b4a9aa.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76244/slideshow/fbdaf005b6aebc6ddfbff2675c51c4dc.pdf',\n",
       "   'title': 'ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models',\n",
       "   'tldr': 'State-of-the-art poetry generation systems are often complex. They either consist of task-specific model pipelines, incorporate prior knowledge in the form of manually created constraints, or both. In contrast, end-to-end models would not suffer from the overhead of having to model prior knowledge a...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76244,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15251/lecture/76244-bygpt5-end-to-end-style-conditioned-poetry-generation-with-token-free-language-models',\n",
       "   'video_url': None},\n",
       "  'P2166': {'abstract': \"While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good performance in type-level lexical tasks. While existing work primarily focused on the lexical specialization of monolingual PLMs with immense quantities of monolingual constraints, in this work we expose massively multilingual transformers (MMTs, e.g., mBERT or XLM-R) to multilingual lexical knowledge at scale, leveraging BabelNet as the readily available rich source of multilingual and cross-lingual type-level lexical knowledge. Concretely, we use BabelNet's multilingual synsets to create synonym pairs (or synonym-gloss pairs) across 50 languages and then subject the MMTs (mBERT and XLM-R) to a lexical specialization procedure guided by a contrastive objective. We show that such massively multilingual lexical specialization brings substantial gains in two standard cross-lingual lexical tasks, bilingual lexicon induction and cross-lingual word similarity, as well as in cross-lingual sentence retrieval. Crucially, we observe gains for languages unseen in specialization, indicating that multilingual lexical specialization enables generalization to languages with no lexical constraints. In a series of subsequent controlled experiments, we show that the number of specialization constraints plays a much greater role than the set of languages from which they originate.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.426',\n",
       "   'authors': ['Tommaso Green', 'Simone Paolo Ponzetto', 'Goran Glava'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-4_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P2166',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mutlilingual representations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.426.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76688/poster_document/ed37b6befdd9c1a9a39ac72af711db91.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76688/poster/87adf4ad66a797a5b3fc304fe5106c8d.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76688/slideshow/012062236594a4daecd48a7be9627027.pdf',\n",
       "   'title': 'Massively Multilingual Lexical Specialization of Multilingual Transformers',\n",
       "   'tldr': 'While pretrained language models (PLMs) primarily serve as general-purpose text encoders that can be fine-tuned for a wide variety of downstream tasks, recent work has shown that they can also be rewired to produce high-quality word representations (i.e., static word embeddings) and yield good perfo...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76688,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76688-tackling-modality-heterogeneity-with-multi-view-calibration-network-for-multimodal-sentiment-detection',\n",
       "   'video_url': None},\n",
       "  'P2178': {'abstract': 'Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work, we attempt to address these requirements applied to the important problem of legal citation prediction (LCP). We design the task with parallels to the thought-process of lawyers, i.e., with reference to both precedents and legislative provisions. After initial experimental results, we refine the target citation predictions with the feedback of legal experts. Additionally, we introduce a prototype architecture to add interpretability, achieving strong performance while adhering to decision parameters used by lawyers. Our study builds on and leverages the state-of-the-art language processing models for law, while addressing vital considerations for high-stakes tasks with practical societal impact.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.301',\n",
       "   'authors': ['Chu Fei Luo',\n",
       "    'Rohan Bhambhoria',\n",
       "    'Samuel Dahan',\n",
       "    'Xiaodan Zhu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-4_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P2178',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['legal nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.301.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77599/poster_document/2386c93abb7dff93fae079493f6778a9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77599/poster/f03592de1d0618bb283cc8c2142e3819.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77599/slideshow/21624bcf3a4b88d5f85ab1b1189a3ab0.pdf',\n",
       "   'title': 'Prototype-Based Interpretability for Legal Citation Prediction',\n",
       "   'tldr': 'Deep learning has made significant progress in the past decade, and demonstrates potential to solve problems with extensive social impact. In high-stakes decision making areas such as law, experts often require interpretability for automatic systems to be utilized in practical settings. In this work...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77599,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77599-prototype-based-interpretability-for-legal-citation-prediction',\n",
       "   'video_url': None},\n",
       "  'P2181': {'abstract': 'Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.910',\n",
       "   'authors': ['Luyu Gao',\n",
       "    'Zhuyun Dai',\n",
       "    'Panupong Pasupat',\n",
       "    'Anthony Chen',\n",
       "    'Arun Tejasvi Chaganty',\n",
       "    'Yicheng Fan',\n",
       "    'Vincent Y Zhao',\n",
       "    'Ni Lao',\n",
       "    'Hongrae Lee',\n",
       "    'Da-Cheng Juan',\n",
       "    'Kelvin Guu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)'],\n",
       "   'id': 'P2181',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['interactive and collaborative generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.910.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76689/poster_document/b0ffaa514da9c9a3278a12e869615c34.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76689/poster/d85d42b0486b0e6af3d02353bc39c790.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'RARR: Researching and Revising What Language Models Say, Using Language Models',\n",
       "   'tldr': 'Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for at...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76689,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76689-rarr-researching-and-revising-what-language-models-say-using-language-models',\n",
       "   'video_url': None},\n",
       "  'P2186': {'abstract': \"We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to automatically derive semantic characterizations of lexical items in two grammatical constructions: nouns in subject or object position within the same sentence, and the AANN construction (e.g., `a beautiful three days'). We show that a word in subject position is interpreted as more agentive than the very same word in object position, and that the nouns in the AANN construction are interpreted as more measurement-like than when in the canonical alternation. Our method can probe the distributional meaning of syntactic constructions at a templatic level, abstracted away from specific lexemes.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.14',\n",
       "   'authors': ['Gabriella Chronis', 'Kyle Mahowald', 'Katrin Erk'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'event_ids': ['session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(oral)'],\n",
       "   'id': 'P2186',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['linguistic theories'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.14.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76245/poster_document/f7a3455ecc0939c75dab8441e579ddc5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76245/poster/525808ae9f41105b11d32f824a59f154.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Method for Studying Semantic Construal in Grammatical Constructions with Interpretable Contextual Embedding Spaces',\n",
       "   'tldr': 'We study semantic construal in grammatical constructions using large language models. First, we project contextual word embeddings into three interpretable semantic spaces, each defined by a different set of psycholinguistic feature norms. We validate these interpretable spaces and then use them to ...',\n",
       "   'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "   'underline_id': 76245,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15252/lecture/76245-a-method-for-studying-semantic-construal-in-grammatical-constructions-with-interpretable-contextual-embedding-spaces',\n",
       "   'video_url': None},\n",
       "  'P2187': {'abstract': \"Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining challenge of AES. In this paper, we propose a robust model: prompt- and trait relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay representation by essay-prompt attention and utilizing the topic-coherence feature extracted by the topic-modeling mechanism without access to labeled data; therefore, our model considers the prompt adherence of an essay, even in a cross-prompt setting. To facilitate multi-trait scoring, we design trait-similarity loss that encapsulates the correlations of traits. Experiments prove the efficacy of our model, showing state-of-the-art results for all prompts and traits. Significant improvements in low-resource-prompt and inferior traits further indicate our model's strength.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.98',\n",
       "   'authors': ['Heejin Do', 'Yunsu Kim', 'Gary Geunbae Lee'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2187',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.98.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77600/poster_document/60d3e38071282df86462a89a3e313821.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77600/poster/9ac5408bb56de28bf86d53232c304ce1.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77600/slideshow/0daff31ee7596d50bc57f39a7a7b1f9a.pdf',\n",
       "   'title': 'Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring',\n",
       "   'tldr': 'Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pr...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 77600,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77600-prompt-and-trait-relation-aware-cross-prompt-essay-trait-scoring',\n",
       "   'video_url': None},\n",
       "  'P2192': {'abstract': 'Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeople-based evaluation is less expensive and easier to scale, but its quality is unknown for reflections. Therefore, we explore whether laypeople can be an alternative to experts in evaluating a fundamental quality aspect: coherence and context-consistency. We do so by asking a group of laypeople and a group of experts to annotate both synthetic reflections and human reflections from actual therapists. We find that both laypeople and experts are reliable annotators and that they have moderate-to-strong inter-group correlation, which shows that laypeople can be trusted for such evaluations. We also discover that GPT-3 mostly produces coherent and consistent reflections, and we explore changes in evaluation results when the source of synthetic reflections changes to GPT-3 from the less powerful GPT-2.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.382',\n",
       "   'authors': ['Zixiu Wu',\n",
       "    'Simone Balloccu',\n",
       "    'Ehud Reiter',\n",
       "    'Rim Helaoui',\n",
       "    'Diego Reforgiato Recupero',\n",
       "    'Daniele Riboni'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-5_-generation-(oral)'],\n",
       "   'id': 'P2192',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['human evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.382.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76246/poster_document/8b56c92593781077908692e113ce28b3.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76246/poster/2639772277d209b5748e3c4c6bccd73e.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76246/slideshow/f8ac7f2e5ad44d53da0019ff526f97f6.pptx',\n",
       "   'title': 'Are Experts Needed? On Human Evaluation of Counselling Reflection Generation',\n",
       "   'tldr': 'Reflection is a crucial counselling skill where the therapist conveys to the client their interpretation of what the client said. Language models have recently been used to generate reflections automatically, but human evaluation is challenging, particularly due to the cost of hiring experts. Laypeo...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76246,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15251/lecture/76246-are-experts-neededquestion-on-human-evaluation-of-counselling-reflection-generation',\n",
       "   'video_url': None},\n",
       "  'P2197': {'abstract': 'Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to\\nassign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar \\ndata can substantially improve the performance of MIAs.\\nHowever, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.719',\n",
       "   'authors': ['Justus Mattern',\n",
       "    'Fatemehsadat Mireshghallah',\n",
       "    'Zhijing Jin',\n",
       "    'Bernhard Schoelkopf',\n",
       "    'Mrinmaya Sachan',\n",
       "    'Taylor Berg-Kirkpatrick'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2197',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['security and privacy'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.719.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77602/poster_document/a765b80b7f8f301ded768c1dcb532c88.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77602/slideshow/0aa9bed955b2bebcabfe403c310f3c1e.pdf',\n",
       "   'title': 'Membership Inference Attacks against Language Models via Neighbourhood Comparison',\n",
       "   'tldr': 'Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to\\nassign higher probab...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77602,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77602-membership-inference-attacks-against-language-models-via-neighbourhood-comparison',\n",
       "   'video_url': None},\n",
       "  'P2200': {'abstract': 'We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.\\nBuilding on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.\\nBy grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.\\nOur analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.32',\n",
       "   'authors': ['Alessandro Stolfo',\n",
       "    'Zhijing Jin',\n",
       "    'Kumar Shridhar',\n",
       "    'Bernhard Schoelkopf',\n",
       "    'Mrinmaya Sachan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-3_-nlp-applications-(poster)'],\n",
       "   'id': 'P2200',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mathematical nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.32.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76690/poster_document/5b16328e9ffa115762ad9cc48227170e.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76690/poster/fca01a19eb8f0dca5ad9adf8ba0dfbb1.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models',\n",
       "   'tldr': 'We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description whe...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76690,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76690-pvgru-generating-diverse-and-relevant-dialogue-responses-via-pseudo-variational-mechanism',\n",
       "   'video_url': None},\n",
       "  'P2201': {'abstract': 'The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. \\nIn this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective.\\nTo that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while \"peeking\" into other topically-related documents.\\nIn a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information.\\nThis novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. \\nFurther, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization).\\nFollowing this scheme, we pre-train our model - termed QAmden - and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7\\\\%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.110',\n",
       "   'authors': ['Avi Caciularu',\n",
       "    'Matthew Peters',\n",
       "    'Jacob Goldberger',\n",
       "    'Ido Dagan',\n",
       "    'Arman Cohan'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-1_-summarization-(poster)'],\n",
       "   'id': 'P2201',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['query-focused summarization', 'multi-document summarization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.110.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76691/poster_document/d4ff47e94ea865694d174a9a5ef416f1.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76691/poster/bfbeaea9ef6a8ec6e1aeaf87c8ad3585.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76691/slideshow/ae0692b92d889e2b3cc3287a600b799f.pdf',\n",
       "   'title': 'Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering',\n",
       "   'tldr': 'The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. \\nIn this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-t...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76691,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76691-towards-identifying-fine-grained-depression-symptoms-from-memes',\n",
       "   'video_url': None},\n",
       "  'P2204': {'abstract': 'Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two dif- ferent factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By per- turbing explanations on three controlled tasks, we show that both factors contribute to the ef- fectiveness of explanations. We further study how to form maximally effective sets of expla- nations for solving a given test query. We find that LLMs can benefit from the complemen- tarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as comple- mentary, which successfully improves the in- context learning performance across three real- world tasks on multiple LLMs.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.273',\n",
       "   'authors': ['Xi Ye',\n",
       "    'Srinivasan Iyer',\n",
       "    'Asli Celikyilmaz',\n",
       "    'Veselin Stoyanov',\n",
       "    'Greg Durrett',\n",
       "    'Ramakanth Pasunuru'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2204',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting', 'interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.273.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77603/poster_document/936406bdb3c2860c0441ab069d94d0a8.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77603/poster/ddc8b03af936064c22eb54aaa9ba5997.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Complementary Explanations for Effective In-Context Learning',\n",
       "   'tldr': 'Large language models (LLMs) have exhibited remarkable capabilities in learning from expla- nations in prompts, but there has been limited understanding of exactly how these explana- tions function or why they are effective. This work aims to better understand the mechanisms by which explanations ar...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 77603,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77603-complementary-explanations-for-effective-in-context-learning',\n",
       "   'video_url': None},\n",
       "  'P2205': {'abstract': 'We investigate response generation for multi-turn dialogue in generative chatbots. Existing generative models\\nbased on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the history, which makes\\nmodels unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences\\nbetween dialogues that are similar in composition. In this paper, we propose Pseudo-Variational Gated Recurrent Unit (PVGRU). The key novelty of PVGRU is a recurrent summarizing variable that\\naggregates the accumulated distribution variations of subsequences. We train PVGRU without relying on posterior knowledge, thus avoiding the training-inference inconsistency problem. PVGRU can perceive subtle semantic variability through summarizing variables that are optimized by two objectives we employ for training: distribution consistency and reconstruction. In addition, we build a Pseudo-Variational Hierarchical Dialogue\\n(PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the diversity and\\nrelevance of responses on two benchmark datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.185',\n",
       "   'authors': ['Yongkang Liu',\n",
       "    'Shi Feng',\n",
       "    'Daling Wang',\n",
       "    'Yifei Zhang',\n",
       "    'Hinrich Schtze'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['poster-session-2_-dialogue-and-interactive-systems-(poster)'],\n",
       "   'id': 'P2205',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['conversational modeling'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.185.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76692/poster_document/cdfa9a9fb2216fdabd1361d5b36166a6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76692/poster/861ee34d0ab2f51da35c19ab527de60e.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76692/slideshow/3071950008f7d9ba5cd43dbdc90b7a8a.pdf',\n",
       "   'title': 'PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism',\n",
       "   'tldr': 'We investigate response generation for multi-turn dialogue in generative chatbots. Existing generative models\\nbased on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the history, which makes\\nmodels unable to capture the subtle variability observed in different dia...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 76692,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76692-towards-open-world-product-attribute-mining-a-lightly-supervised-approach',\n",
       "   'video_url': None},\n",
       "  'P2209': {'abstract': \"Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Our model is designed with explicit summarization and simplification models and guides the generation using the main keywords of a source text.\\nIn order to evaluate our new model, we use two existing benchmark datasets for simplification, namely D-Wikipedia and Wiki-Doc. We compare our model's performance with state of the art and show that SIMSUM achieves top results on the D-Wikipedia dataset SARI (+1.20), D-SARI (+1.64), and FKGL (-0.35) scores, improving over the best baseline models. In order to evaluate the quality of the generated text, we analyze the outputs from different models qualitatively and demonstrate the merit of our new model. Our code and datasets are available.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.552',\n",
       "   'authors': ['Sofia Blinova',\n",
       "    'Xinyu Zhou',\n",
       "    'Martin Jaggi',\n",
       "    'Carsten Eickhoff',\n",
       "    'Seyed Ali Bahrainian'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['poster-session-6_-generation-(poster)'],\n",
       "   'id': 'P2209',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.552.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76693/poster_document/1d7123aa988a9749d87b3741f74382d2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76693/poster/9bbf9b9bd185f6b8fb1e568fdce0faa6.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'SIMSUM: Document-level Text Simplification via Simultaneous Summarization',\n",
       "   'tldr': 'Document-level text simplification is a specific type of simplification which involves simplifying documents consisting of several sentences by rewriting them into fewer or more sentences. In this paper, we propose a new two-stage framework SIMSUM for automated document-level text simplification. Ou...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76693,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15264/poster/76693-simsum-document-level-text-simplification-via-simultaneous-summarization',\n",
       "   'video_url': None},\n",
       "  'P2221': {'abstract': 'A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as \"like\" or \"as\". The simile phenomena are unique and complex in a real-life dialogue scene where the tenor and the vehicle can be verbal phrases or sentences, mentioned by different speakers, exist in different sentences, or occur in reversed order. However, the current simile research usually focuses on similes in a triplet tuple (tenor, property, vehicle) or a single sentence where the tenor and vehicle are usually entities or noun phrases, which could not reflect complex simile phenomena in real scenarios. In this paper, we propose a novel and high-quality multilingual simile dialogue (MSD) dataset to facilitate the study of complex simile phenomena. The MSD is the largest manually annotated simile data ($\\\\sim$21K) and it contains both English and Chinese data. Meanwhile, the MSD data can also be used on dialogue tasks to test the ability of dialogue systems when using similes. We design 3 simile tasks (recognition, interpretation, and generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each task, we provide experimental results from strong pre-trained or state-of-the-art models. The experiments demonstrate the challenge of MSD and we will release the data/code on GitHub.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.453',\n",
       "   'authors': ['Longxuan Ma',\n",
       "    'Wei-Nan Zhang',\n",
       "    'Shuhan Zhou',\n",
       "    'churui sun',\n",
       "    'Changxin Ke',\n",
       "    'Ting Liu'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-1_-resources-and-evaluation-(virtual-poster)'],\n",
       "   'id': 'P2221',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.453.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77607/poster_document/0d8302a4c734fc437430e30659e5d40d.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77607/slideshow/c55d43c08be3a19f4ddb7aea186d8f01.pdf',\n",
       "   'title': 'I run as fast as a rabbit, can you? A Multilingual Simile Dialogues Datasets',\n",
       "   'tldr': 'A simile is a figure of speech that compares two different things (called the tenor and the vehicle) via shared properties. The tenor and the vehicle are usually connected with comparator words such as \"like\" or \"as\". The simile phenomena are unique and complex in a real-life dialogue scene where th...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 77607,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77607-i-run-as-fast-as-a-rabbit-can-youquestion-a-multilingual-simile-dialogues-datasets',\n",
       "   'video_url': None},\n",
       "  'P2222': {'abstract': \"Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM's MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM's MT output which reveals some interesting properties and prospects for future work.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.859',\n",
       "   'authors': ['David Vilar',\n",
       "    'Markus Freitag',\n",
       "    'Colin Cherry',\n",
       "    'Jiaming Luo',\n",
       "    'Viresh Ratnakar',\n",
       "    'George Foster'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-1_-large-language-models-(oral)'],\n",
       "   'id': 'P2222',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['automatic evaluation',\n",
       "    'few-shot/zero-shot mt',\n",
       "    'human evaluation',\n",
       "    'pre-training for mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.859.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76247/poster_document/29c147800dea8d1fbbcb80bffb933051.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76247/poster/441184d3fccbf94f4e1ce9cf2fb78bb2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Prompting PaLM for Translation: Assessing Strategies and Performance',\n",
       "   'tldr': 'Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) perfo...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76247,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15195/lecture/76247-prompting-palm-for-translation-assessing-strategies-and-performance',\n",
       "   'video_url': None},\n",
       "  'P2223': {'abstract': 'A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.389',\n",
       "   'authors': ['Amr Keleg', 'Walid Magdy'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P2223',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual benchmarks', 'multilingual evaluation'],\n",
       "   'languages': ['arabic', 'korean'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.389.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77608/poster_document/c8a5419db1b4e17c9ae2676568a31cd5.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77608/poster/d24dc0e06cc749a0aabd356ea23c493c.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77608/slideshow/d88f4b5f33882e11adb26ed84bb79665.pdf',\n",
       "   'title': 'DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models',\n",
       "   'tldr': 'A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilin...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77608,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77608-dlama-a-framework-for-curating-culturally-diverse-facts-for-probing-the-knowledge-of-pretrained-language-models',\n",
       "   'video_url': None},\n",
       "  'P2226': {'abstract': 'We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar domains. We demonstrate that this approach improves generalization performance to new domains. Incorporating anti-racist texts as counterexamples to white supremacist language mitigates bias.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.17',\n",
       "   'authors': ['Michael Miller Yoder',\n",
       "    'Ahmad Diab',\n",
       "    'David West Brown',\n",
       "    'Kathleen M Carley'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['poster-session-2_-computational-social-science-and-cultural-analytics-(poster)'],\n",
       "   'id': 'P2226',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['hate-speech detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.17.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76694/poster_document/5d0705d94d2ecc15c3bfc58c1918ce2b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76694/poster/45c17eb084849487d0bf762dc24fa778.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A Weakly Supervised Classifier and Dataset of White Supremacist Language',\n",
       "   'tldr': 'We present a dataset and classifier for detecting the language of white supremacist extremism, a growing issue in online hate speech. Our weakly supervised classifier is trained on large datasets of text from explicitly white supremacist domains paired with neutral and anti-racist data from similar ...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76694,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76694-big-c-a-multimodal-multi-purpose-dataset-for-bemba',\n",
       "   'video_url': None},\n",
       "  'P2229': {'abstract': 'Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60\\\\%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50\\\\% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.307',\n",
       "   'authors': ['Ehsan Kamalloo',\n",
       "    'Nouha Dziri',\n",
       "    'Charles Clarke',\n",
       "    'Davood Rafiei'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-4_-resources-and-evaluation-(oral)'],\n",
       "   'id': 'P2229',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets', 'evaluation methodologies', 'evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.307.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76248/poster_document/3063e7a2e48836b731617f3b09b74783.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76248/poster/aa9e6553cd31069e1df2fdbaa25ab77c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76248/slideshow/d4e97b7ca2adaa561a7ee0b8934ecbc2.pdf',\n",
       "   'title': 'Evaluating Open-Domain Question Answering in the Era of Large Language Models',\n",
       "   'tldr': 'Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative mo...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76248,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15233/lecture/76248-evaluating-open-domain-question-answering-in-the-era-of-large-language-models',\n",
       "   'video_url': None},\n",
       "  'P2232': {'abstract': \"Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose \\\\{pasted macro `MODEL'\\\\}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.78',\n",
       "   'authors': ['Tanay Dixit', 'Fei Wang', 'Muhao Chen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['poster-session-7_-summarization-(poster)'],\n",
       "   'id': 'P2232',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['abstractive summarisation', 'factuality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.78.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76695/poster_document/232ea53f88342d3d4f85e3710a5988d4.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76695/poster/8982d12384384cc10054a3f53e456f68.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality',\n",
       "   'tldr': \"Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose \\\\{pasted macro `MODEL'\\\\}name (i.e. Effective Factual Summarization),...\",\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 76695,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76695-improving-factuality-of-abstractive-summarization-without-sacrificing-summary-quality',\n",
       "   'video_url': None},\n",
       "  'P2238': {'abstract': 'Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to  one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.280',\n",
       "   'authors': ['Robert A. Morabito', 'Jad Kabbara', 'Ali Emami'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-7_-ethics-and-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2238',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation',\n",
       "    'model bias/unfairness mitigation',\n",
       "    'ethical considerations in nlp applications',\n",
       "    'transparency'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.280.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77610/poster_document/2f8eb7599d894d547cb44270173acc23.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77610/poster/3adfb62239b711b2e767b01aab41f6fb.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77610/slideshow/46fc11bf881de517ef1e0d9e4ad5b1e0.pptx',\n",
       "   'title': 'Debiasing should be Good and Bad: Measuring the Consistency of Debiasing  Techniques in Language Models',\n",
       "   'tldr': 'Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77610,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77610-debiasing-should-be-good-and-bad-measuring-the-consistency-of-debiasing-techniques-in-language-models',\n",
       "   'video_url': None},\n",
       "  'P2240': {'abstract': 'Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs.\\nConversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives.\\nHowever, non-verbal predicates appear in the benchmarks we commonly use to measure progress in SRL less frequently than in some real-world settings -- newspaper headlines, dialogues, and tweets, among others.\\nIn this paper, we put forward a new PropBank dataset which boasts wide coverage of multiple predicate types. Thanks to it, we demonstrate empirically that standard benchmarks do not provide an accurate picture of the current situation in SRL and that state-of-the-art systems are still incapable of transferring knowledge across different predicate types.\\nHaving observed these issues, we also present a novel, manually-annotated challenge set designed to give equal importance to verbal, nominal, and adjectival predicate-argument structures. We use such dataset to investigate whether we can leverage different linguistic resources to promote knowledge transfer.\\nIn conclusion, we claim that SRL is far from \"solved\", and its integration with other semantic tasks might enable significant improvements in the future, especially for the long tail of non-verbal predicates, thereby facilitating further research on SRL for non-verbal predicates.\\nWe release our software and datasets at https://github.com/sapienzanlp/exploring-srl.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.783',\n",
       "   'authors': ['Riccardo Orlando', 'Simone Conia', 'Roberto Navigli'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'event_ids': ['session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2240',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['semantic textual similarity'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.783.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77612/poster_document/aebfd39fe3be6fb92b51544a4673b20c.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77612/slideshow/92d6d9eb26daad55fd4c7fb837b64828.pdf',\n",
       "   'title': 'Exploring Non-Verbal Predicates in Semantic Role Labeling: Challenges and Opportunities',\n",
       "   'tldr': 'Although we have witnessed impressive progress in Semantic Role Labeling (SRL), most of the research in the area is carried out assuming that the majority of predicates are verbs.\\nConversely, predicates can also be expressed using other parts of speech, e.g., nouns and adjectives.\\nHowever, non-verba...',\n",
       "   'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "   'underline_id': 77612,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77612-exploring-non-verbal-predicates-in-semantic-role-labeling-challenges-and-opportunities',\n",
       "   'video_url': None},\n",
       "  'P2243': {'abstract': \"Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT.\\nHeterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.580',\n",
       "   'authors': ['Ganesh Jawahar',\n",
       "    'Subhabrata Mukherjee',\n",
       "    'Xiaodong Liu',\n",
       "    'Young Jin Kim',\n",
       "    'Muhammad Abdul-Mageed',\n",
       "    'Laks Lakshmanan, V.S.',\n",
       "    'Ahmed Hassan Awadallah',\n",
       "    'Sebastien Bubeck',\n",
       "    'Jianfeng Gao'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2243',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model compression methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.580.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77613/poster_document/36bcb1fe58744aee1e088b0eb7c029dd.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77613/poster/8dd91a831c8d0eb876fc32899d76efd4.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77613/slideshow/abd0968353880944ae846d805bdb10b1.pdf',\n",
       "   'title': 'AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation',\n",
       "   'tldr': 'Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE wor...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77613,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77613-automoe-heterogeneous-mixture-of-experts-with-adaptive-computation-for-efficient-neural-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P2244': {'abstract': 'News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult task in Computational Linguistics since narratives are often  intertwined and only implicitly conveyed in text. In this paper, we consider a more feasible proxy task: Identify the distinct sets of aligned story actors responsible for sustaining the issue-specific narratives. Discovering aligned actors, and the groups these alignments create, brings us closer to estimating the narrative that each group represents. With the help of Large Language Models (LLM), we address this task by: (i) Introducing a corpus of text segments rich in narrative content associated with six different current  issues; (ii) Introducing a novel two-step graph-based framework that (a) identifies alignments between actors (INCANT) and (b) extracts aligned actor groups using the network structure (TAMPA). Amazon Mechanical Turk evaluations demonstrate the effectiveness of our framework. Across domains, alignment relationships from INCANT are accurate (macro F1 >= 0.75) and actor groups from TAMPA are preferred over 2 non-trivial baseline models (ACC >= 0.75).',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.497',\n",
       "   'authors': ['Pavan Holur',\n",
       "    'David Chong',\n",
       "    'Timothy R Tangherlini',\n",
       "    'Vwani Roychowdhury'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-3_-computational-social-science-and-cultural-analytics-(oral)'],\n",
       "   'id': 'P2244',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection', 'nlp tools for social analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.497.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76249/poster_document/897c83c8f25a43a332c46219e7128d45.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76249/poster/20d603b949969dd3395d12d895cbad08.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76249/slideshow/f671adf0d6cbf5097be68c01ca922e7f.pdf',\n",
       "   'title': 'My side, your side and the evidence: Discovering aligned actor groups and the narratives they weave',\n",
       "   'tldr': 'News reports about emerging issues often include several conflicting story lines. Individual stories can be conceptualized as samples from an underlying mixture of competing narratives. The automated identification of these distinct narratives from unstructured text is a fundamental yet difficult ta...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76249,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15226/lecture/76249-my-side-your-side-and-the-evidence-discovering-aligned-actor-groups-and-the-narratives-they-weave',\n",
       "   'video_url': None},\n",
       "  'P2247': {'abstract': 'Stance detection aims to detect the stance toward a corresponding target. Existing works use the assumption that the target is known in advance, which is often not the case in the wild. Given a text from social media platforms, the target information is often unknown due to implicit mentions in the source text and it is infeasible to have manual target annotations at a large scale. Therefore, in this paper, we propose a new task Target-Stance Extraction (TSE) that aims to extract the (target, stance) pair from the text. We benchmark the task by proposing a two-stage framework that first identifies the relevant target in the text and then detects the stance given the predicted target and text. Specifically, we first propose two different settings: Target Classification and Target Generation, to identify the potential target from a given text. Then we propose a multi-task approach that takes target prediction as the auxiliary task to detect the stance toward the predicted target. We evaluate the proposed framework on both in-target stance detection in which the test target is always seen in the training stage and zero-shot stance detection that needs to detect the stance for the targets that are unseen during the training phase. The new TSE task can facilitate future research in the field of stance detection.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.560',\n",
       "   'authors': ['Yingjie Li', 'Krishna K Garg', 'Cornelia Caragea'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)'],\n",
       "   'id': 'P2247',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.560.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76250/poster_document/54d3f09c3ce3ec03aed16299dc0afe20.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76250/poster/728896258ed1f96f36c623883438163c.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'A New Direction in Stance Detection: Target-Stance Extraction in the Wild',\n",
       "   'tldr': 'Stance detection aims to detect the stance toward a corresponding target. Existing works use the assumption that the target is known in advance, which is often not the case in the wild. Given a text from social media platforms, the target information is often unknown due to implicit mentions in the ...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76250,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15209/lecture/76250-a-new-direction-in-stance-detection-target-stance-extraction-in-the-wild',\n",
       "   'video_url': None},\n",
       "  'P2251': {'abstract': 'In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select and present to the user. Existing TOD systems handle this ambiguity by randomly selecting one or few results and presenting their names to the user. However, in a real scenario, users do not always accept a randomly recommended entity, and users should have access to more comprehensive information about the search results. To address this limitation, we propose a novel task called Comparison-Based database search Ambiguity handling (CBA), which handles ambiguity in database search results by comparing the properties of multiple entities to enable users to choose according to their preferences. Accordingly, we introduce a new framework for automatically collecting high-quality dialogue data along with the Disambiguating Schema-guided Dialogue (DSD) dataset, an augmented version of the SGD dataset. Experimental studies on the DSD dataset demonstrate that training baseline models with the dataset effectively address the CBA task. Our dataset and code will be publicized.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.249',\n",
       "   'authors': ['Yongil Kim',\n",
       "    'Yerin Hwang',\n",
       "    'Joongbo Shin',\n",
       "    'Hyunkyung Bae',\n",
       "    'Kyomin Jung'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Dialogue and Interactive Systems',\n",
       "   'event_ids': ['session-7_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2251',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['task-oriented'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.249.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77614/poster_document/fbb53de0a2ab62d453c297bf07d89bd6.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77614/slideshow/94f0eac2d067c546f24ab21b8f2fa5f6.pdf',\n",
       "   'title': 'Injecting Comparison Skills in Task-Oriented Dialogue Systems for Database Search Results Disambiguation',\n",
       "   'tldr': 'In task-oriented dialogue (TOD) systems designed to aid users accomplish specific goals in one or more domains, the agent retrieves entities that satisfy user constraints from the database. However, when multiple database search results exist, an ambiguity occurs regarding which results to select an...',\n",
       "   'track': 'Dialogue and Interactive Systems',\n",
       "   'underline_id': 77614,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77614-injecting-comparison-skills-in-task-oriented-dialogue-systems-for-database-search-results-disambiguation',\n",
       "   'video_url': None},\n",
       "  'P2258': {'abstract': 'We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xsim, we show that xsim++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xsim++ also reports performance for different error types, offering more fine-grained feedbacks for model development.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.10',\n",
       "   'authors': ['Mingda Chen',\n",
       "    'Kevin Heffernan',\n",
       "    'Onur elebi',\n",
       "    'Alexandre Mourachko',\n",
       "    'Holger Schwenk'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-2_-machine-translation-(oral)'],\n",
       "   'id': 'P2258',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['multilingual mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.10.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76251/poster_document/1d019d376d90ea674de26f62fcb8fe00.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76251/poster/820dcf4834243df369ea0b765a316acb.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages',\n",
       "   'tldr': 'We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xsim++. In comparison to xsim, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which m...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 76251,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15206/lecture/76251-xsim-an-improved-proxy-to-bitext-mining-performance-for-low-resource-languages',\n",
       "   'video_url': None},\n",
       "  'P2261': {'abstract': 'The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs ``well-formed\" verbalized text to interact with it, either to fine-tune it or to exploit it. However, there are indications that commonly used PTLMs already encode enough linguistic knowledge to allow the use of minimal (or none) textual context for some linguistically motivated tasks, thus notably reducing human effort, the need for data pre-processing, and favoring techniques that are language neutral since do not rely on syntactic structures. \\n\\nIn this work, we explore this idea for the tasks of lexical relation classification (LRC) and graded Lexical Entailment (LE). After fine-tuning PTLMs for LRC with different verbalizations, our evaluation results show that very simple prompts are competitive for LRC and significantly outperform graded LE SoTA. \\nIn order to gain a better insight into this phenomenon, we perform a number of quantitative statistical analyses on the results, as well as a qualitative visual exploration based on embedding projections.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.308',\n",
       "   'authors': ['Lucia Pitarch',\n",
       "    'Jordi Bernad',\n",
       "    'Lacramioara Dranca',\n",
       "    'Carlos Bobed Lisbona',\n",
       "    'Jorge Gracia'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['poster-session-4_-semantics_-lexical-(poster)'],\n",
       "   'id': 'P2261',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['lexical relationships'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.308.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76696/poster_document/c6b79acef8ac80daaa9c4bb80cbf34d6.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76696/poster/fd0e0f2eb1cf82eaccfb62b4d5eba149.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'No clues good clues: out of context Lexical Relation Classification',\n",
       "   'tldr': 'The accurate prediction of lexical relations between words is a challenging task in Natural Language Processing (NLP). The most recent advances in this direction come with the use of pre-trained language models (PTLMs). A PTLM typically needs ``well-formed\" verbalized text to interact with it, eithe...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 76696,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76696-a-systematic-study-and-comprehensive-evaluation-of-chatgpt-on-benchmark-datasets',\n",
       "   'video_url': None},\n",
       "  'P2262': {'abstract': 'How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8\\\\% in accuracy on text classification and 15.6\\\\% on the GLUE benchmark. Our source code is available at https://anonymous.4open. science/r/NPPrompt.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.869',\n",
       "   'authors': ['Xuandong Zhao',\n",
       "    'Siqi Ouyang',\n",
       "    'Zhiguo Yu',\n",
       "    'Ming Wu',\n",
       "    'Lei Li'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-4_-large-language-models-(oral)'],\n",
       "   'id': 'P2262',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['prompting'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.869.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76252/poster_document/577da5f2d124802745c1665194b42701.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76252/poster/fa0cbdeb31af1a404234c30d7db78a9d.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Pre-trained Language Models Can be Fully Zero-Shot Learners',\n",
       "   'tldr': 'How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or ma...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76252,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15234/lecture/76252-pre-trained-language-models-can-be-fully-zero-shot-learners',\n",
       "   'video_url': None},\n",
       "  'P2269': {'abstract': 'Document retrieval is a key stage of standard Web search engines. \\nExisting dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. \\nTo overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\\nHowever, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases.\\nIn this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.\\n\\nSurprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\\\\% of the corresponding documents contain correct answers to open-domain questions.\\nIn this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map questions to document identifiers.\\nExperiments demonstrate that our method can consistently achieve better retrieval performance than existing retrieval approaches by a significant margin on three open-domain question answering benchmarks, under both zero and few-shot settings.\\nThe code for this work can be found at {https://github.com/Ziems/llm-url}.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.167',\n",
       "   'authors': ['Noah Ziems', 'Wenhao Yu', 'Zhihan Zhang', 'Meng Jiang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Retrieval and Text Mining',\n",
       "   'event_ids': ['session-7_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)'],\n",
       "   'id': 'P2269',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['passage retrieval',\n",
       "    'dense retrieval',\n",
       "    'document representation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.167.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77616/poster_document/67d83395143c19a8076ae18b9c7d5ad9.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77616/poster/abade21cd4e7f9d6d86857826b68df79.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Large Language Models are Built-in Autoregressive Search Engines',\n",
       "   'tldr': 'Document retrieval is a key stage of standard Web search engines. \\nExisting dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. \\nTo overcome this limitation, recent autoregressive search engines replace ...',\n",
       "   'track': 'Information Retrieval and Text Mining',\n",
       "   'underline_id': 77616,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77616-large-language-models-are-built-in-autoregressive-search-engines',\n",
       "   'video_url': None},\n",
       "  'P2270': {'abstract': \"Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbearing annotation costs. While active learning (AL) has shown promise in training models with a small amount of annotated data, AL's reliance on the model's behavior for selective sampling can lead to an accumulation of unwanted bias rather than bias mitigation. However, infusing clustering with AL can overcome the bias issue of both AL and traditional annotation methods while exploiting AL's annotation efficiency. In this paper, we propose a novel adaptive clustering-based active learning algorithm, D-CALM, that dynamically adjusts clustering and annotation efforts in response to an estimated classifier error-rate. Experiments on eight datasets for a diverse set of text classification tasks, including emotion, hatespeech, dialog act, and book type detection, demonstrate that our proposed algorithm significantly outperforms baseline AL approaches with both pretrained transformers and traditional Support Vector Machines. D-CALM showcases robustness against different measures of information gain and, as evident from our analysis of label and error distribution, can significantly reduce unwanted model bias.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.342',\n",
       "   'authors': ['Sabit Hassan', 'Malihe Alikhani'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-1_-ethics-and-nlp-(virtual-poster)'],\n",
       "   'id': 'P2270',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.342.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77617/poster_document/8db66c1598b3a46b7ef0d5a31f7863af.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77617/poster/66154aa4befc4daae5a36bc2e8b64d42.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77617/slideshow/0570b14e61739604014034e7f6812524.pdf',\n",
       "   'title': 'D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias',\n",
       "   'tldr': 'Despite recent advancements, NLP models continue to be vulnerable to bias. This bias often originates from the uneven distribution of real-world data and can propagate through the annotation process. Escalated integration of these models in our lives calls for methods to mitigate bias without overbe...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77617,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77617-d-calm-a-dynamic-clustering-based-active-learning-approach-for-mitigating-bias',\n",
       "   'video_url': None},\n",
       "  'P2272': {'abstract': \"Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of QE framework to extracting quality parallel corpus from the pseudo-parallel corpus.. By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model. The baseline model is the one that is trained on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training instances, shows an improvement of up to 0.6 BLEU points for Hindi-Bengali language pair, compared to the baseline model. This demonstrates the promise of transfer learning in the setting under discussion. QE systems typically require in the order of (7K-25K) of training data. Our Hindi-Bengali QE is trained on only 500 instances of training that is 1/40th of the normal requirement and achieves comparable performance. All the scripts and datasets utilized in this study will be publicly available.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.892',\n",
       "   'authors': ['Akshay Batheja', 'Pushpak Bhattacharyya'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Translation',\n",
       "   'event_ids': ['session-1_-machine-translation-(virtual-poster)'],\n",
       "   'id': 'P2272',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['few-shot/zero-shot mt'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.892.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77618/poster_document/8277da91b1c2dddf0d124f11dd25942d.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77618/poster/e3f2e2960855e088459ef6a4cb156be6.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77618/slideshow/08e2810f482dae8baeba3c70a6cff886.pptx',\n",
       "   'title': 'A Little is Enough: Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation',\n",
       "   'tldr': 'Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Est...',\n",
       "   'track': 'Machine Translation',\n",
       "   'underline_id': 77618,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77618-a-little-is-enough-few-shot-quality-estimation-based-corpus-filtering-improves-machine-translation',\n",
       "   'video_url': None},\n",
       "  'P2273': {'abstract': 'Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns highquality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.46',\n",
       "   'authors': ['Anastasiia Razdaibiedina', 'Aleksandr V. Brechalov'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-3_-nlp-applications-(poster)'],\n",
       "   'id': 'P2273',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['educational applications, gec, essay scoring'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.46.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76697/poster_document/b13e651d3239db0b9c1da2314d0bf70f.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76697/poster/a37b06b0d11dbc24955e9e821f2ff2d0.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76697/slideshow/3acc8c716bb289d6d4473472bdda9d52.pdf',\n",
       "   'title': 'MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents',\n",
       "   'tldr': 'Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pretrained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level re...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76697,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76697-miread-simple-method-for-learning-high-quality-representations-from-scientific-documents',\n",
       "   'video_url': None},\n",
       "  'P2274': {'abstract': \"Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information.\\nHowever, these tasks entangle the consolidation phase with the often subjective and ill-defined content selection requirement, impeding proper assessment of models' consolidation capabilities. \\nIn this paper, we suggest revisiting the sentence union generation task as an effective well-defined testbed for assessing text consolidation capabilities, decoupling the consolidation challenge from subjective content selection.\\nTo support research on this task, we present refined annotation methodology and tools for crowdsourcing sentence union, create the largest union dataset to date and provide an analysis of its rich coverage of various consolidation aspects.\\nWe then propose a comprehensive evaluation protocol for union generation, including both human and automatic evaluation. \\nFinally, as baselines, we evaluate state-of-the-art language models on the task, along with a detailed analysis of their capacity to address multi-text consolidation challenges and their limitations.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.440',\n",
       "   'authors': ['Eran Hirsch',\n",
       "    'Valentina Pyatkin',\n",
       "    'Ruben Wolhandler',\n",
       "    'Avi Caciularu',\n",
       "    'Asi Shefer',\n",
       "    'Ido Dagan'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2274',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['text-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.440.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77619/poster_document/eb6f971567d7faa9df25d112900d0aeb.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77619/poster/d8f450626784807a4c0ddeab430bd484.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77619/slideshow/eab1b23d996f14775070324748a3ac99.pdf',\n",
       "   'title': 'Revisiting Sentence Union Generation as a Testbed for Text Consolidation',\n",
       "   'tldr': 'Tasks involving text generation based on multiple input texts, such as multi-document summarization, long-form question answering and contemporary dialogue applications, challenge models for their ability to properly consolidate partly-overlapping multi-text information.\\nHowever, these tasks entangl...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77619,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77619-towards-reference-free-text-simplification-evaluation-with-a-bert-siamese-network-architecture',\n",
       "   'video_url': None},\n",
       "  'P228': {'abstract': 'Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MVP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MVP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MVP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MVP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MVP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.240',\n",
       "   'authors': ['Zhibin Gou', 'qingyan guo', 'Yujiu Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'event_ids': ['session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)'],\n",
       "   'id': 'P228',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['stance detection'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.240.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76383/poster_document/00f163f5d6e9276f6da6f6e12d7a47a1.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76383/slideshow/6cc258d7f1279ff844fec3216e5b869b.pdf',\n",
       "   'title': 'MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction',\n",
       "   'tldr': 'Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tupl...',\n",
       "   'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "   'underline_id': 76383,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76383-mvp-multi-view-prompting-improves-aspect-sentiment-tuple-prediction',\n",
       "   'video_url': None},\n",
       "  'P2283': {'abstract': 'Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as language technologies gain hold across the globe. To address this gap, we present SeeGULL, a broad-coverage stereotype dataset, built by utilizing generative capabilities of large language models such as PaLM, and GPT-3, and leveraging a globally diverse rater pool to validate the prevalence of those stereotypes in society. SeeGULL is in English, and contains stereotypes about identity groups spanning 178 countries across 8 different geo-political regions across 6 continents, as well as state-level identities within the US and India. We also include fine-grained offensiveness scores for different stereotypes and demonstrate their global disparities. Furthermore, we include comparative annotations about the same groups by annotators living in the region vs. those that are based in North America, and demonstrate that within-region stereotypes about groups differ from those prevalent in North America.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.548',\n",
       "   'authors': ['Akshita Jha',\n",
       "    'Aida Mostafazadeh Davani',\n",
       "    'Chandan K Reddy',\n",
       "    'Shachi Dave',\n",
       "    'Vinodkumar Prabhakaran',\n",
       "    'Sunipa Dev'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['poster-session-2_-ethics-and-nlp-(poster)'],\n",
       "   'id': 'P2283',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.548.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76698/poster_document/699abd717239b5a2d54b79d5fcc56b19.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76698/poster/5483ace7dd621a368f9fc6e7f78f9123.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76698/slideshow/c1f007b2955678c7ae9a88c73ec55e03.pdf',\n",
       "   'title': 'SeeGULL: A Stereotype Benchmark with Broad Geo-Cultural Coverage Leveraging Generative Models',\n",
       "   'tldr': 'Stereotype benchmark datasets are crucial to detect and mitigate social stereotypes about groups of people in NLP models. However, existing datasets are limited in size and coverage, and are largely restricted to stereotypes prevalent in the Western society. This is especially problematic as languag...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 76698,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76698-mad-tsc-a-multilingual-aligned-news-dataset-for-target-dependent-sentiment-classification',\n",
       "   'video_url': None},\n",
       "  'P2284': {'abstract': 'Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.269',\n",
       "   'authors': ['Michael Joseph Ryan', 'Tarek Naous', 'Wei Xu'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['poster-session-2_-resources-and-evaluation-(poster)'],\n",
       "   'id': 'P2284',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['benchmarking',\n",
       "    'language resources',\n",
       "    'multilingual corpora',\n",
       "    'nlp datasets',\n",
       "    'automatic evaluation of datasets'],\n",
       "   'languages': ['russian',\n",
       "    'italian',\n",
       "    'urdu',\n",
       "    'japanese',\n",
       "    'spanish',\n",
       "    'danish',\n",
       "    'brazilian portuguese',\n",
       "    'french',\n",
       "    'german',\n",
       "    'slovene',\n",
       "    'basque'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.269.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76699/poster_document/a4b5ad93de34aa6f3f86430acf2c33aa.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76699/poster/63dfc09b2674bc7d5557cfc1273bae77.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76699/slideshow/e0cab53450941d08857a7f1ba123cffa.pdf',\n",
       "   'title': 'Revisiting non-English Text Simplification: A Unified Multilingual Benchmark',\n",
       "   'tldr': 'Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sente...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76699,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76699-revisiting-non-english-text-simplification-a-unified-multilingual-benchmark',\n",
       "   'video_url': None},\n",
       "  'P2288': {'abstract': 'Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. \\nAlthough the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language generation tasks falls short in quality and sometimes outputs an incorrect language. In our study, we show that the fine-tuning process learns language invariant representations, which is beneficial for classification tasks but harmful for generation tasks. Motivated by this, we propose a simple method to regularize the model from learning language invariant representations and a method to select model checkpoints without a development set in the target language, both resulting in better generation quality. Experiments on three semantically diverse generation tasks show that our method reduces the accidental translation problem by 68\\\\% and improves the ROUGE-L score by 1.5 on average.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.789',\n",
       "   'authors': ['Tianjian Li', 'Kenton Murray'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2288',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-lingual transfer'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.789.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77620/poster/2222606dc11ba1fc1ec4f8264c1cf852.png',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Why Does Zero-Shot Cross-Lingual Generation Fail? An Explanation and a Solution',\n",
       "   'tldr': 'Zero-shot cross-lingual transfer is when a multilingual model is trained to perform a task in one language and then is applied to another language. \\nAlthough the zero-shot cross-lingual transfer approach has achieved success in various classification tasks, its performance on natural language genera...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 77620,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77620-gvdoc-graph-based-visual-document-classification',\n",
       "   'video_url': None},\n",
       "  'P2289': {'abstract': 'Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.539',\n",
       "   'authors': ['Neal G Lawton',\n",
       "    'Anoop Kumar',\n",
       "    'Govind Thattai',\n",
       "    'Aram Galstyan',\n",
       "    'Greg Ver Steeg'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2289',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['parameter-efficient finetuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.539.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77621/poster_document/b7cc9f0a411b23a98dacf6ced28344da.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77621/poster/f51bb0de1d62cf5be3e04d7680394f89.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77621/slideshow/77fc143e41fdbc114085d7deba0527d6.pdf',\n",
       "   'title': 'Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models',\n",
       "   'tldr': 'Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET archit...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77621,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77621-neural-architecture-search-for-parameter-efficient-fine-tuning-of-large-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P2291': {'abstract': 'Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilingual semantic parsing is critical as codeswitching between languages is prevalent for bilingual users. In this work we dramatically improve the zero-shot performance of a multilingual and codeswitched semantic parsing system using two stages of multilingual alignment. First, we show that contrastive alignment pretraining improves \\\\textit{both} English performance and transfer efficiency. We then introduce a constrained optimization approach for hyperparameter-free adversarial alignment during finetuning. Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT transfer performance by 3x, 6x, and 81x on the Spanglish, Hinglish and Multilingual Task Oriented Parsing benchmarks respectively and outperforms XLM-R and mT5-Large using 3.2x fewer parameters.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.199',\n",
       "   'authors': ['William Held',\n",
       "    'Christopher Hidey',\n",
       "    'Fei Liu',\n",
       "    'Eric Y Zhu',\n",
       "    'Rahul Goel',\n",
       "    'Diyi Yang',\n",
       "    'Rushin Shah'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P2291',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code-switching',\n",
       "    'multilingualism',\n",
       "    'cross-lingual transfer',\n",
       "    'mutlilingual representations',\n",
       "    'multilingual evaluation'],\n",
       "   'languages': ['spanish-english codemixing',\n",
       "    'hindi-english codemixing',\n",
       "    'spanish',\n",
       "    'french',\n",
       "    'german',\n",
       "    'hindi',\n",
       "    'thai'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.199.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76700/poster_document/5d85311edcba4f32c1e174b1eff4bfaa.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76700/poster/d0c42bf2bc2239ccc00700d569ce69db.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76700/slideshow/92bb3662b7238c8e351bdaae43ac070c.pdf',\n",
       "   'title': 'DAMP: Doubly Aligned Multilingual Parser for Task-Oriented Dialogue',\n",
       "   'tldr': 'Modern virtual assistants use internal semantic parsing engines to convert user utterances to actionable commands. However, prior work has demonstrated multilingual models are less robust for semantic parsing compared to other tasks. In global markets such as India and Latin America, robust multilin...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76700,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76700-damp-doubly-aligned-multilingual-parser-for-task-oriented-dialogue',\n",
       "   'video_url': None},\n",
       "  'P2296': {'abstract': 'Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content may use a toxicity classifier to filter generated text. In practice, the generated text to classify, which is determined by user prompts, may come from a wide range of distributions.\\n\\nIn this paper, we show that the performance of controlled generation may be poor if the distributions of text in response to user prompts differ from the distribution the predictor was trained on. To address this problem, we cast controlled generation under distribution shift as an invariant learning problem: the most effective predictor should be invariant across multiple text environments. We then discuss a natural solution that arises from this characterization and propose heuristics for selecting natural environments.\\n\\nWe study this characterization and the proposed method empirically using both synthetic and real data. Experiments demonstrate both the challenge of distribution shift in controlled generation and the potential of invariance methods in this setting.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.179',\n",
       "   'authors': ['Carolina Zheng',\n",
       "    'Claudia Shi',\n",
       "    'Keyon Vafa',\n",
       "    'Amir Feder',\n",
       "    'David Blei'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['poster-session-7_-machine-learning-for-nlp-(poster)'],\n",
       "   'id': 'P2296',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['generalization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.179.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76701/poster_document/9db5f2095dea2c0b0b3fb749e7440e25.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76701/poster/b2acc0b7e6de02c62485bd295f20bb69.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76701/slideshow/fcacbb3877774a772bd08ae5ff0f694b.pdf',\n",
       "   'title': 'An Invariant Learning Characterization of Controlled Text Generation',\n",
       "   'tldr': 'Controlled generation refers to the problem of creating text that contains stylistic or semantic attributes of interest. Many approaches reduce this problem to training a predictor of the desired attribute. For example, researchers hoping to deploy a large language model to produce non-toxic content...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76701,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76701-an-invariant-learning-characterization-of-controlled-text-generation',\n",
       "   'video_url': None},\n",
       "  'P2297': {'abstract': 'We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Specifically, PREADD contrasts the output logits generated using a raw prompt against those generated using a prefix-prepended prompt, enabling both positive and negative control with respect to any attribute encapsulated by the prefix. We evaluate PREADD on three taskstoxic output mitigation, gender bias reduction, and sentiment controland find that PREADD outperforms not only prompting baselines, but also an auxiliary-expert control method, by 12\\\\% or more in relative gain on our main metrics for each task.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.636',\n",
       "   'authors': ['Jonathan Pei', 'Kevin Yang', 'Dan Klein'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-7_-generation-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2297',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['inference methods'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.636.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77622/poster_document/b1925d1ded1b2279ed10d6be159aa105.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77622/poster/9a4f2bbce44d1691ffb30413b3dc1e11.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77622/slideshow/de4ae26b5a7c1ba34a58e630c78305af.pdf',\n",
       "   'title': 'PREADD: Prefix-Adaptive Decoding for Controlled Text Generation',\n",
       "   'tldr': 'We propose Prefix-Adaptive Decoding (PREADD), a flexible method for controlled text generation. Unlike existing methods that use auxiliary expert models to control for attributes, PREADD does not require an external model, instead relying on linearly combining output logits from multiple prompts. Sp...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 77622,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77622-task-aware-retrieval-with-instructions',\n",
       "   'video_url': None},\n",
       "  'P2298': {'abstract': 'It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probing cultural biases and assumptions present in commonsense reasoning systems, with a specific focus on food-related customs. We test several CommonsenseQA systems on FORK, and while we see high performance on questions about the US culture, the poor performance of these systems on questions about non-US cultures highlights systematic cultural assumptions aligned with US over non-US cultures.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.631',\n",
       "   'authors': ['Shramay Palta', 'Rachel Rudinger'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['session-7_-ethics-and-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2298',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/fairness evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.631.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77623/poster_document/5ca66cf669e99f335ce98858355d912b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77623/poster/6548c44a8774cdf7e8f8b33266f45657.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77623/slideshow/a31b8f2ebf74f2eb71e60e08af92768e.pdf',\n",
       "   'title': 'FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models',\n",
       "   'tldr': 'It is common sense that one should prefer to eat a salad with a fork rather than with a chainsaw. However, for eating a bowl of rice, the choice between a fork and a pair of chopsticks is culturally relative. We introduce FORK, a small, manually-curated set of CommonsenseQA-style questions for probi...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 77623,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77623-fork-a-bite-sized-test-set-for-probing-culinary-cultural-biases-in-commonsense-reasoning-models',\n",
       "   'video_url': None},\n",
       "  'P23': {'abstract': 'Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful visual information and thus limiting relation extraction performance. In this paper, we propose a novel MMRE framework to better capture the deeper correlations of text, entity pair, and image/objects, so as to mine more helpful information for the task, termed as DGF-PT. We first propose a prompt-based autoregressive encoder, which builds the associations of intra-modal and inter-modal features related to the task, respectively by entity-oriented and object-oriented prefixes. To better integrate helpful visual information, we design a dual-gated fusion module to distinguish the importance of image/objects and further enrich text representations. In addition, a generative decoder is introduced with entity type restriction on relations, better filtering out candidates. Extensive experiments conducted on the benchmark dataset show that our approach achieves excellent performance compared to strong competitors, even in the few-shot situation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.572',\n",
       "   'authors': ['Qian Li',\n",
       "    'Shu Guo',\n",
       "    'Cheng Ji',\n",
       "    'Xutan Peng',\n",
       "    'Shiyao Cui',\n",
       "    'Jianxin Li',\n",
       "    'Lihong Wang'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Information Extraction',\n",
       "   'event_ids': ['session-4_-information-extraction-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P23',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['named entity recognition and relation extraction'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.572.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77254/poster_document/e2d895cc6b660eedc613bfd1f9a7f506.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77254/slideshow/6f2fdf19cb21a0a6794c89678216c2bb.pdf',\n",
       "   'title': 'Dual-Gated Fusion with Prefix-Tuning for Multi-Modal Relation Extraction',\n",
       "   'tldr': 'Multi-Modal Relation Extraction (MMRE) aims at identifying the relation between two entities in texts that contain visual clues. Rich visual content is valuable for the MMRE task, but existing works cannot well model finer associations among different modalities, failing to capture the truly helpful...',\n",
       "   'track': 'Information Extraction',\n",
       "   'underline_id': 77254,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77254-dual-gated-fusion-with-prefix-tuning-for-multi-modal-relation-extraction',\n",
       "   'video_url': None},\n",
       "  'P230': {'abstract': 'Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into multi-view compressed representations before feeding them into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level lowresource NLP tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.264',\n",
       "   'authors': ['Linlin Liu',\n",
       "    'Xingxuan Li',\n",
       "    'Megh Thakkar',\n",
       "    'Xin Li',\n",
       "    'Shafiq Joty',\n",
       "    'Luo Si',\n",
       "    'Lidong Bing'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)'],\n",
       "   'id': 'P230',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.264.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76384/poster_document/30477fdbb61f4cdb708d59a69c0dd98d.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76384/slideshow/de4f46d7021b7f4ee461d697f670a286.pdf',\n",
       "   'title': 'Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations',\n",
       "   'tldr': 'Due to the huge amount of parameters, finetuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts rand...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76384,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76384-towards-robust-low-resource-fine-tuning-with-multi-view-compressed-representations',\n",
       "   'video_url': None},\n",
       "  'P2305': {'abstract': \"Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BLIND, a method for bias removal with no prior knowledge of the demographics in the dataset. While training a model on a downstream task, BLIND detects biased samples using an auxiliary model that predicts the main model's success, and down-weights those samples during the training process. Experiments with racial and gender biases in sentiment classification and occupation classification tasks demonstrate that BLIND mitigates social biases without relying on a costly demographic annotation process. Our method is competitive with other methods that require demographic information and sometimes even surpasses them.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.490',\n",
       "   'authors': ['Hadas Orgad', 'Yonatan Belinkov'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Ethics and NLP',\n",
       "   'event_ids': ['poster-session-7_-ethics-and-nlp-(poster)'],\n",
       "   'id': 'P2305',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['model bias/unfairness mitigation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.490.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76702/poster_document/065344bfc8d24a5e485cf14f48a834e0.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76702/poster/3ac772b81e91c8e1aac6a38eecf5dfd7.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'BLIND: Bias Removal With No Demographics',\n",
       "   'tldr': 'Models trained on real-world data tend to imitate and amplify social biases. Common methods to mitigate biases require prior information on the types of biases that should be mitigated (e.g., gender or racial bias) and the social groups associated with each data sample. In this work, we introduce BL...',\n",
       "   'track': 'Ethics and NLP',\n",
       "   'underline_id': 76702,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76702-blind-bias-removal-with-no-demographics',\n",
       "   'video_url': None},\n",
       "  'P2307': {'abstract': 'In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations. We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with finetuning.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.78',\n",
       "   'authors': ['Itay Levy', 'Ben Bogin', 'Jonathan Berant'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-4_-question-answering-(poster)'],\n",
       "   'id': 'P2307',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['semantic parsing', 'generalization'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.78.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76703/poster_document/8ec75ce3ab6587f02ac616a4face2afc.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76703/poster/b4b0a319f9c74976daa0504ecce1f7f2.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76703/slideshow/a8a9199e61ee3c1c4c118430a08ef1d5.pdf',\n",
       "   'title': 'Diverse Demonstrations Improve In-context Compositional Generalization',\n",
       "   'tldr': 'In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional genera...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76703,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15237/poster/76703-diverse-demonstrations-improve-in-context-compositional-generalization',\n",
       "   'video_url': None},\n",
       "  'P2311': {'abstract': 'Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However, capturing disagreement can increase annotation time and expense. Fortunately, for many tasks, not all examples are equally controversial; we develop an active learning approach, Disagreement Aware Active Learning (DAAL) that concentrates annotations on examples where model entropy and annotator entropy are the most different. Because we cannot know the true entropy of annotations on unlabeled examples, we estimate a model that predicts annotator entropy trained using very few multiply-labeled examples. We find that traditional uncertainty-based active learning underperforms simple passive learning on tasks with high levels of disagreement, but that our active learning approach is able to successfully improve on passive and active baselines, reducing the number of annotations required by at least 24\\\\% on average across several datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.658',\n",
       "   'authors': ['Connor T Baumler', 'Anna Sotnikova', 'Hal Daum III'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-4_-machine-learning-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2311',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['human-in-the-loop / active learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.658.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77624/poster_document/4a6c5a492823c1a61b7b6500f0681322.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77624/poster/e3b5f4b46daca267e42fa5c61e1bd9af.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77624/slideshow/69f17787be64589787eddf8d42516bed.pdf',\n",
       "   'title': 'Which Examples Should be Multiply Annotated? Active Learning When Annotators May Disagree',\n",
       "   'tldr': 'Linguistic annotations, especially for controversial topics like hate speech detection, are frequently contested due to annotator backgrounds and positionalities. In such situations, preserving this disagreement through the machine learning pipeline can be important for downstream use cases. However...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 77624,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77624-which-examples-should-be-multiply-annotatedquestion-active-learning-when-annotators-may-disagree',\n",
       "   'video_url': None},\n",
       "  'P2317': {'abstract': 'With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To help researchers tackle these critical challenges, we introduce DiffusionDB, the first large-scale text-to-image prompt dataset totaling 6.5TB, containing 14 million images generated by Stable Diffusion, 1.8 million unique prompts, and hyperparameters specified by real users. We analyze the syntactic and semantic characteristics of prompts. We pinpoint specific hyperparameter values and prompt styles that can lead to model errors and present evidence of potentially harmful model usage, such as the generation of misinformation. The unprecedented scale and diversity of this human-actuated dataset provide exciting research opportunities in understanding the interplay between prompts and generative models, detecting deepfakes, and designing human-AI interaction tools to help users more easily use these models. DiffusionDB is publicly available at: https://poloclub.github.io/diffusiondb.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.51',\n",
       "   'authors': ['Zijie J. Wang',\n",
       "    'Evan Montoya',\n",
       "    'David Munechika',\n",
       "    'Haoyang Yang',\n",
       "    'Benjamin Hoover',\n",
       "    'Duen Horng Chau'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Resources and Evaluation',\n",
       "   'event_ids': ['session-4_-resources-and-evaluation-(oral)'],\n",
       "   'id': 'P2317',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp datasets'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.51.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76253/poster_document/1f60bfe91d68f16bf4ac63122df670e2.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76253/poster/adb68dd6d35a8aee83b6cd9ed7053c08.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76253/slideshow/ced1bf724fb4b5ec432cce27dbffff99.pdf',\n",
       "   'title': 'DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models',\n",
       "   'tldr': 'With recent advancements in diffusion models, users can generate high-quality images by writing text prompts in natural language. However, generating images with desired details requires proper prompts, and it is often unclear how a model reacts to different prompts or what the best prompts are. To ...',\n",
       "   'track': 'Resources and Evaluation',\n",
       "   'underline_id': 76253,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15233/lecture/76253-diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-models',\n",
       "   'video_url': None},\n",
       "  'P2320': {'abstract': 'NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filtering can obscure the true capabilities of models to overcome biases, which might never be removed in full from the dataset. We suggest that in order to drive the development of models robust to subtle biases, dataset biases should be amplified in the training set. We introduce an evaluation framework defined by a bias-amplified training set and an anti-biased test set, both automatically extracted from existing datasets. Experiments across three notions of bias, four datasets and two models show that our framework is substantially more challenging for models than the original data splits, and even more challenging than hand-crafted challenge sets. Our evaluation framework can use any existing dataset, even those considered obsolete, to test model robustness. We hope our work will guide the development of robust models that do not rely on superficial biases and correlations. To this end, we publicly release our code and data.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.833',\n",
       "   'authors': ['Yuval Reif', 'Roy Schwartz'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2320',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data shortcuts/artifacts',\n",
       "    'hardness of samples',\n",
       "    'robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.833.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77626/poster/3742dddf603f4c6eb8b0f99ef6b1395a.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77626/slideshow/3d89d0101a61d395bc7a7e320ad4fd8c.pdf',\n",
       "   'title': 'Fighting Bias With Bias: Promoting Model Robustness by Amplifying Dataset Biases',\n",
       "   'tldr': 'NLP models often rely on superficial cues known as dataset biases to achieve impressive performance, and can fail on examples where these biases do not hold. Recent work sought to develop robust, unbiased models by filtering biased examples from training sets. In this work, we argue that such filter...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 77626,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77626-fighting-bias-with-bias-promoting-model-robustness-by-amplifying-dataset-biases',\n",
       "   'video_url': None},\n",
       "  'P2321': {'abstract': 'Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained as a retrieval task using a dual-encoder retrieval setup, and can then be subsequently utilized for evaluating a generated summary given an input document, without gold reference summaries. RISE is especially well suited when working on new datasets where one may not have reference summaries available for evaluation. We conduct comprehensive experiments on the SummEval benchmark (Fabbri et al., 2021) and a long document summarization benchmark. The results show that RISE consistently achieves higher correlation with human evaluations compared to many past approaches to summarization evaluation. Furthermore, RISE also demonstrates data-efficiency and generalizability across languages.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.865',\n",
       "   'authors': ['David Uthus', 'Jianmo Ni'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Summarization',\n",
       "   'event_ids': ['session-7_-summarization-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-centre-(spotlight)'],\n",
       "   'id': 'P2321',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['evaluation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.865.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77627/poster_document/40cc6c29069bf6d9d1cbcadf63c65e05.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77627/poster/5e4433c377caf4071017a80ddcbf8bed.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77627/slideshow/6d326e7a78d7bd3cad4aa2f9abd6637b.pdf',\n",
       "   'title': 'RISE: Leveraging Retrieval Techniques for Summarization Evaluation',\n",
       "   'tldr': 'Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained ...',\n",
       "   'track': 'Summarization',\n",
       "   'underline_id': 77627,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/77627-rise-leveraging-retrieval-techniques-for-summarization-evaluation',\n",
       "   'video_url': None},\n",
       "  'P2325': {'abstract': \"We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, which includes the setting (e.g., restaurant), the agents' contingent roles (waiter, customer), their attributes (age, gender), and other physical, social, and cultural constraints (e.g., the temperature or the country of operation). In total, NormBank contains 63k unique constraints from a taxonomy that we introduce and iteratively refine here. Constraints then apply in different combinations to frame social norms. Under these manipulations, norms are non-monotonic  one can cancel an inference by updating its frame even slightly. Still, we find evidence that neural models can help reliably extend the scope and coverage of NormBank. We further demonstrate the utility of this resource with a series of transfer experiments. For data and code, see https://github.com/SALT-NLP/normbank\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.429',\n",
       "   'authors': ['Caleb Ziems',\n",
       "    'Jane Dwivedi-Yu',\n",
       "    'Yi-Chia Wang',\n",
       "    'Alon Halevy',\n",
       "    'Diyi Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P2325',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['nlp tools for social analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.429.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76704/poster_document/862dfb685534ca5501973adb89be83c0.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76704/slideshow/7bcc9aeead2f5a17e2b0732503eca677.pdf',\n",
       "   'title': 'NormBank: A Knowledge Bank of Situational Social Norms',\n",
       "   'tldr': 'We present NormBank, a knowledge bank of 155k situational norms. This resource is designed to ground flexible normative reasoning for interactive, assistive, and collaborative AI systems. Unlike prior commonsense resources, NormBank grounds each inference within a multivalent sociocultural frame, wh...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 76704,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76704-holographic-ccg-parsing',\n",
       "   'video_url': None},\n",
       "  'P233': {'abstract': 'As a special task of natural language generation, conditional lyrics generation needs to consider the structure of generated lyrics and the relationship between lyrics and music. \\nDue to various forms of conditions, a lyrics generation system is expected to generate lyrics conditioned on different signals, such as music scores, music audio, or partially-finished lyrics, etc. \\nHowever, most of the previous works have ignored the musical attributes hidden behind the lyrics and the structure of the lyrics. Additionally, most works only handle limited lyrics generation conditions, such as lyrics generation based on music score or partial lyrics, they can not be easily extended to other generation conditions with the same framework.\\nIn this paper, we propose a unified structure-aware lyrics generation framework named UniLG. Specifically, we design compound templates that incorporate textual and musical information to improve structure modeling and unify the different lyrics generation conditions.\\nExtensive experiments demonstrate the effectiveness of our framework. Both objective and subjective evaluations show significant improvements in generating structural lyrics.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.56',\n",
       "   'authors': ['Tao Qian',\n",
       "    'Fan Lou',\n",
       "    'Jiatong Shi',\n",
       "    'Yuning Wu',\n",
       "    'Shuai Guo',\n",
       "    'Xiang Yin',\n",
       "    'Qin Jin'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Generation',\n",
       "   'event_ids': ['session-1_-generation-(virtual-poster)'],\n",
       "   'id': 'P233',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['data-to-text generation'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.56.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76385/poster_document/e8db4391498f3c7fa28a5bcfb2739e3b.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76385/slideshow/4dc16bbbbe795c1d57355e99aa6f891e.pdf',\n",
       "   'title': 'UniLG: A Unified Structure-aware Framework for Lyrics Generation',\n",
       "   'tldr': 'As a special task of natural language generation, conditional lyrics generation needs to consider the structure of generated lyrics and the relationship between lyrics and music. \\nDue to various forms of conditions, a lyrics generation system is expected to generate lyrics conditioned on different s...',\n",
       "   'track': 'Generation',\n",
       "   'underline_id': 76385,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/76385-unilg-a-unified-structure-aware-framework-for-lyrics-generation',\n",
       "   'video_url': None},\n",
       "  'P2330': {'abstract': 'Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural language (NL) intents from users, we build ARCADE, a benchmark of 1078 code generation problems using the pandas data analysis framework in data science notebooks. ARCADE features multiple rounds of NL-to-code problems from the same notebook. It requires a model to understand rich multi-modal contexts, such as existing notebook cells and their execution states as well as previous turns of interaction. To establish a strong baseline on this challenging task, we develop PaChiNCo, a 62B code language model (LM) for Python computational notebooks, which significantly outperforms public code LMs. Finally, we explore few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions. Arcade is publicly available at https://github.com/google-research/arcade-nl2code/.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.9',\n",
       "   'authors': ['Pengcheng Yin',\n",
       "    'Wen-Ding Li',\n",
       "    'Kefan Xiao',\n",
       "    'Abhishek K Rao',\n",
       "    'Yeming Wen',\n",
       "    'Kensen Shi',\n",
       "    'Joshua Howland',\n",
       "    'Paige Bailey',\n",
       "    'Michele Catasta',\n",
       "    'Henryk Michalewski',\n",
       "    'Oleksandr Polozov',\n",
       "    'Charles Sutton'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-7_-nlp-applications-(poster)'],\n",
       "   'id': 'P2330',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['code generation and understanding'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.9.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76705/poster_document/1be9287d78a3959744c10f41593eb1da.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Natural Language to Code Generation in Interactive Data Science Notebooks',\n",
       "   'tldr': 'Computational notebooks, such as Jupyter notebooks, are interactive computing environments that are ubiquitous among data scientists to perform data wrangling and analytic tasks. To measure the performance of AI pair programmers that automatically synthesize programs for those tasks given natural la...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76705,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76705-natural-language-to-code-generation-in-interactive-data-science-notebooks',\n",
       "   'video_url': None},\n",
       "  'P2331': {'abstract': 'Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve the dialect robustness of existing systems. Finally, we partner with native speakers of Chicano and Indian English to release new gold-standard variants of the popular CoQA task. To execute the transformation code, run model checkpoints, and download both synthetic and gold-standard dialectal benchmark datasets, see http://value-nlp.org.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.44',\n",
       "   'authors': ['Caleb Ziems',\n",
       "    'William Held',\n",
       "    'Jingfeng Yang',\n",
       "    'Jwala Dhamala',\n",
       "    'Rahul Gupta',\n",
       "    'Diyi Yang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)'],\n",
       "   'id': 'P2331',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['linguistic variation', 'dialects and language varieties'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.44.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76706/poster_document/2353ea7e53a44e09500eb87e737bd307.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76706/slideshow/54e94cef1155674407cb3c636aeea46d.pdf',\n",
       "   'title': 'Multi-VALUE: A Framework for Cross-Dialectal English NLP',\n",
       "   'tldr': 'Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Curren...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76706,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76706-multi-value-a-framework-for-cross-dialectal-english-nlp',\n",
       "   'video_url': None},\n",
       "  'P2333': {'abstract': 'Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.  In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern India. AxomiyaBERTa is trained only on the masked language modeling (MLM) task, without the typical additional next sentence prediction (NSP) objective, and our results show that in resource-scarce settings for very low-resource languages like Assamese, MLM alone can be successfully leveraged for a range of tasks.  AxomiyaBERTa achieves SOTA on token-level tasks like Named Entity Recognition and also performs well on \"longer-context\" tasks like Cloze-style QA and Wiki Title Prediction, with the assistance of a novel embedding disperser and phonological signals respectively. Moreover, we show that AxomiyaBERTa can leverage phonological signals for even more challenging tasks, such as a novel cross-document coreference task on a translated version of the ECB+ corpus, where we present a new SOTA result for an LRL. Our source code and evaluation scripts may be found at https://github.com/csu-signal/axomiyaberta.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.739',\n",
       "   'authors': ['Abhijnan Nath', 'Sheikh Abdul Mannan', 'Nikhil Krishnaswamy'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Linguistic Diversity',\n",
       "   'event_ids': ['session-4_-linguistic-diversity-(virtual-poster)',\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)'],\n",
       "   'id': 'P2333',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['less-resourced languages',\n",
       "    'indigenous languages',\n",
       "    'resources for less-resourced languages'],\n",
       "   'languages': ['assamese'],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.739.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77629/poster_document/377e169fa0267bf1645be8ae2cfabe87.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77629/poster/44a63af809030bcf3686b928cd4fd625.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77629/slideshow/f874ca693ba6011183d4fbd39a6ff8f3.pdf',\n",
       "   'title': 'AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese',\n",
       "   'tldr': 'Despite their successes in NLP, Transformer-based language models still require extensive computing resources and suffer in low-resource or low-compute settings.  In this paper, we present AxomiyaBERTa, a novel BERT model for Assamese, a morphologically-rich low-resource language (LRL) of Eastern In...',\n",
       "   'track': 'Linguistic Diversity',\n",
       "   'underline_id': 77629,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/77629-axomiyaberta-a-phonologically-aware-transformer-model-for-assamese',\n",
       "   'video_url': None},\n",
       "  'P2336': {'abstract': \"Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models.\\nPrior KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. \\nIn this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher's soft labels and predictions can further improve student generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new framework and loss function that preserves the semantic similarities of teacher and student training examples. \\nWe show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for task-specific knowledge distillation on the GLUE benchmark.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.97',\n",
       "   'authors': ['Jianyi Zhang',\n",
       "    'Aashiq Muhamed',\n",
       "    'Aditya Anantharaman',\n",
       "    'Guoyin Wang',\n",
       "    'Changyou Chen',\n",
       "    'Kai Zhong',\n",
       "    'Qingjun Cui',\n",
       "    'Yi Xu',\n",
       "    'Belinda Zeng',\n",
       "    'Trishul Chilimbi',\n",
       "    'Yiran Chen'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['session-3_-large-language-models-(oral)'],\n",
       "   'id': 'P2336',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['fine-tuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.97.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76254/poster_document/cbfc9260c2e5c26849de9452590700a3.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76254/poster/49926398cefe3fdaf72e5d3b2c387c0b.png',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models',\n",
       "   'tldr': 'Knowledge Distillation (KD) is one of the most effective approaches to deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the large-scale models to smaller student models.\\nPrior KD approaches use the soft labels and intermediate a...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76254,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15223/lecture/76254-reaugkd-retrieval-augmented-knowledge-distillation-for-pre-trained-language-models',\n",
       "   'video_url': None},\n",
       "  'P2339': {'abstract': 'Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation.  \\nWe show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval - the last of which we introduce in this paper. Overall, our model outperforms both a strong contrastive and generative baseline on these tasks.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.673',\n",
       "   'authors': ['John Wieting',\n",
       "    'Jonathan Clark',\n",
       "    'William Cohen',\n",
       "    'Graham Neubig',\n",
       "    'Taylor Berg-Kirkpatrick'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'event_ids': ['poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)'],\n",
       "   'id': 'P2339',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['mutlilingual representations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.673.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76707/poster/93596f6ca2cd6ade27cbb0f039aafdca.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval',\n",
       "   'tldr': 'Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve o...',\n",
       "   'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "   'underline_id': 76707,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76707-beyond-contrastive-learning-a-variational-generative-model-for-multilingual-retrieval',\n",
       "   'video_url': None},\n",
       "  'P2343': {'abstract': 'Warning: This paper contains content that may be offensive or upsetting. \\nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. \\n\\nWe introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. \\n\\nTo study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement\\'s offensiveness (29\\\\% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.findings-acl.392',\n",
       "   'authors': ['Xuhui Zhou',\n",
       "    'Hao Zhu',\n",
       "    'Akhila Yerukola',\n",
       "    'Thomas Davidson',\n",
       "    'Jena D. Hwang',\n",
       "    'Swabha Swayamdipta',\n",
       "    'Maarten Sap'],\n",
       "   'category': 'Findings',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Computational Social Science and Cultural Analytics',\n",
       "   'event_ids': ['session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)'],\n",
       "   'id': 'P2343',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['hate-speech detection', 'sociolinguistics'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.findings-acl.392.pdf',\n",
       "   'paper_type': 'findings',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/77630/poster_document/08ef3a53017d5ea7268aa69369c28c03.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/77630/poster/eb2b263867e10a08cf60c20e5ba6626f.jpg',\n",
       "   'program': 'Findings',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/77630/slideshow/6a65ea3d925a5af474dff6aaf05b8d78.pdf',\n",
       "   'title': 'COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements',\n",
       "   'tldr': 'Warning: This paper contains content that may be offensive or upsetting. \\nUnderstanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance \"your English is very good may implicitly signal an ...',\n",
       "   'track': 'Computational Social Science and Cultural Analytics',\n",
       "   'underline_id': 77630,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15200/poster/77630-bootstrapping-neural-relation-and-explanation-classifiers',\n",
       "   'video_url': None},\n",
       "  'P2349': {'abstract': 'A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).\\nText documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval.\\nHowever, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks.\\nCurrent pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration.\\nTo this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton.\\nPatton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure.\\nWe conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.387',\n",
       "   'authors': ['Bowen Jin',\n",
       "    'Wentao Zhang',\n",
       "    'Yu Zhang',\n",
       "    'Yu Meng',\n",
       "    'Xinyang Zhang',\n",
       "    'Qi Zhu',\n",
       "    'Jiawei Han'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Machine Learning for NLP',\n",
       "   'event_ids': ['session-2_-machine-learning-for-nlp-(oral)'],\n",
       "   'id': 'P2349',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['graph-based methods', 'self-supervised learning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.387.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76255/poster_document/52651bc4219fc2b8864934325e28608b.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76255/poster/aa784739abe4abc22f4f1fd2337ed545.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Patton: Language Model Pretraining on Text-Rich Networks',\n",
       "   'tldr': 'A real-world text corpus sometimes comprises not only text documents, but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships).\\nText documents and semantic connections form a text-rich network, which empowers a wide range of d...',\n",
       "   'track': 'Machine Learning for NLP',\n",
       "   'underline_id': 76255,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15205/lecture/76255-patton-language-model-pretraining-on-text-rich-networks',\n",
       "   'video_url': None},\n",
       "  'P2350': {'abstract': 'Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P  that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may \"over-generalize, in the sense that they produce non-human-like text. Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P relative to Q, is a better reflection of how a human would evaluate text generated by a model. Hence, we propose learning with MixCE, an objective that mixes the forward and reverse cross-entropies. We evaluate models trained with this objective on synthetic data settings (where P is known) and real data, and show that the resulting models yield better generated text without complex decoding strategies.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.502',\n",
       "   'authors': ['Shiyue Zhang',\n",
       "    'Shijie Wu',\n",
       "    'Ozan Irsoy',\n",
       "    'Steven Lu',\n",
       "    'Mohit Bansal',\n",
       "    'Mark Dredze',\n",
       "    'David Rosenberg'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-2_-large-language-models-(poster)'],\n",
       "   'id': 'P2350',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['fine-tuning'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.502.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76708/poster_document/24f9864692caaaf1d4a6018c695e3bff.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76708/slideshow/cde53d8246d3239ab0d55dd50f38a6e0.pdf',\n",
       "   'title': 'MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies',\n",
       "   'tldr': 'Autoregressive language models are trained by minimizing the cross-entropy of the model distribution Q relative to the data distribution P  that is, minimizing the forward cross-entropy, which is equivalent to maximum likelihood estimation (MLE). We have observed that models trained in this way may...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76708,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76708-mixce-training-autoregressive-language-models-by-mixing-forward-and-reverse-cross-entropies',\n",
       "   'video_url': None},\n",
       "  'P2353': {'abstract': 'Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to the VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model while having ~20 times fewer parameters.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.43',\n",
       "   'authors': ['Masoud Monajatipoor',\n",
       "    'Liunian Harold Li',\n",
       "    'Mozhdeh Rouhsedaghat',\n",
       "    'Lin Yang',\n",
       "    'Kai-Wei Chang'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['poster-session-2_-language-grounding-to-vision,-robotics,-and-beyond-(poster)'],\n",
       "   'id': 'P2353',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['cross-modal pretraining'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.43.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76709/poster_document/bac230258df954ce9cf3bc7f2fdf87bc.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models',\n",
       "   'tldr': 'Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. ...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76709,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76709-metavl-transferring-in-context-learning-ability-from-language-models-to-vision-language-models',\n",
       "   'video_url': None},\n",
       "  'P2354': {'abstract': 'The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized intervention, it is crucial to identify fine-grained mental health symptoms. To this end, we conduct a focused study on depression disorder and introduce a new task of identifying fine-grained depressive symptoms from memes. Toward this, we create a high-quality dataset (RESTORE) annotated with 8 fine-grained depression symptoms based on the clinically adopted PHQ-9 questionnaire.\\nWe benchmark RESTORE on 20 strong monomodal and multimodal methods. Additionally, we show how imposing orthogonal constraints on textual and visual feature representations in a multimodal setting can enforce the model to learn non-redundant and de-correlated features leading to a better prediction of fine-grained depression symptoms. Further, we conduct an extensive human analysis and elaborate on the limitations of existing multimodal models that often overlook the implicit connection between visual and textual elements of a meme.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.495',\n",
       "   'authors': ['Shweta Yadav',\n",
       "    'Cornelia Caragea',\n",
       "    'Chenye Zhao',\n",
       "    'Naincy Kumari',\n",
       "    'Marvin A Solberg',\n",
       "    'Tanmay Sharma'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['session-7_-nlp-applications-(virtual-poster)'],\n",
       "   'id': 'P2354',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['healthcare applications, clincial nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.495.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76710/poster_document/e64621fc7f0ad2b58173acc9e1a80185.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76710/poster/727007e5d8130c4e7fcec9109074c3c7.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Towards Identifying Fine-Grained Depression Symptoms from Memes',\n",
       "   'tldr': 'The past decade has observed significant attention toward developing computational methods for classifying social media data based on the presence or absence of mental health conditions. In the context of mental health, for clinicians to make an accurate diagnosis or provide personalized interventio...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76710,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15279/poster/76710-memory-efficient-nllb-200-language-specific-expert-pruning-of-a-massively-multilingual-machine-translation-model',\n",
       "   'video_url': None},\n",
       "  'P2356': {'abstract': 'Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic featuresas opposed to incorrect linear featureswhen performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.629',\n",
       "   'authors': ['Aaron Mueller', 'Tal Linzen'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)'],\n",
       "   'id': 'P2356',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['probing', 'robustness'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.629.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76711/poster_document/d0a3d636d05abb3fefe5d3763d9f5b77.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76711/slideshow/e57e5f20e341b0d5bb7d3faa544b3d44.pdf',\n",
       "   'title': 'How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases',\n",
       "   'tldr': 'Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic featuresas opposed to incorrect linear featureswhen performing tasks after fine-tuning. We test what a...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76711,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15227/poster/76711-how-to-plant-trees-in-language-models-data-and-architectural-effects-on-the-emergence-of-syntactic-inductive-biases',\n",
       "   'video_url': None},\n",
       "  'P2358': {'abstract': 'Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive performance gains, what models learn from IT remains understudied. In this work, we analyze how models utilize instructions during IT by comparing model training with altered vs. original instructions. Specifically, we create simplified task definitions by removing all semantic components and only leaving the output space information, and delusive examples that contain incorrect input-output mapping. Our experiments show that models trained on simplified task definition or delusive examples can achieve comparable performance to the ones trained on the original instructions and examples. Furthermore, we introduce a random baseline to perform zeroshot classification tasks, and find it achieves similar performance (42.6\\\\% exact-match) as IT does (43\\\\% exact-match) in low resource setting, while both methods outperform naive T5 significantly (30\\\\% per exact-match). Our analysis provides evidence that the impressive performance gain of current IT models can come from picking up superficial patterns, such as learning the output format and guessing. Our study highlights the urgent need for more reliable IT methods and evaluation.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.113',\n",
       "   'authors': ['Po-Nien Kung', 'Nanyun Peng'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Large Language Models',\n",
       "   'event_ids': ['poster-session-2_-large-language-models-(poster)'],\n",
       "   'id': 'P2358',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['interpretability/analysis'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.113.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76712/poster_document/5dc467146ff8c34be29b3ba3ab00a5f3.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning',\n",
       "   'tldr': 'Recent works on instruction tuning (IT) have achieved great performance with zero-shot generalizability to unseen tasks. With additional context (e.g., task definition, examples) provided to models for fine-tuning, they achieved much higher performance than untuned models. Despite impressive perform...',\n",
       "   'track': 'Large Language Models',\n",
       "   'underline_id': 76712,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15337/poster/76712-do-models-really-learn-to-follow-instructionsquestion-an-empirical-study-of-instruction-tuning',\n",
       "   'video_url': None},\n",
       "  'P236': {'abstract': 'Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and languages. Therefore, sufficient labeled pairs are usually not available. To deal with the label shortage problem, we present a simple yet effective zero-shot approach MultiCapCLIP that can generate visual captions for different scenarios and languages without any labeled vision-caption pairs of downstream datasets. In the training stage, MultiCapCLIP only requires text data for input. Then it conducts two main steps: 1) retrieving concept prompts that preserve the corresponding domain knowledge of new scenarios; 2) auto-encoding the prompts to learn writing styles to output captions in a desired language. In the testing stage, MultiCapCLIP instead takes visual data as input directly to retrieve the concept prompts to generate the final visual descriptions. The extensive experiments on image and video captioning across four benchmarks and four languages (i.e., English, Chinese, German, and French) confirm the effectiveness of our approach. Compared with state-of-the-art zero-shot and weakly-supervised methods, our method achieves 4.8\\\\% and 21.5\\\\% absolute improvements in terms of BLEU@4 and CIDEr metrics. Our code is available at https://github.com/yangbang18/MultiCapCLIP.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.664',\n",
       "   'authors': ['Bang Yang',\n",
       "    'Fenglin Liu',\n",
       "    'Xian Wu',\n",
       "    'Yaowei Wang',\n",
       "    'Xu Sun',\n",
       "    'Yuexian Zou'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'event_ids': ['session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)'],\n",
       "   'id': 'P236',\n",
       "   'is_paper': True,\n",
       "   'keywords': [],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.664.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76386/poster_document/1c37018bd18c6d9e1db181c9a75e5405.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76386/poster/d7ce525270fba7797327fef801fa974c.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76386/slideshow/bc6a68f0ba338fbe98460cada94d49ee.pdf',\n",
       "   'title': 'MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning',\n",
       "   'tldr': 'Supervised visual captioning models typically require a large scale of images or videos paired with descriptions in a specific language (i.e., the vision-caption pairs) for training. However, collecting and labeling large-scale datasets is time-consuming and expensive for many scenarios and language...',\n",
       "   'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "   'underline_id': 76386,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15240/poster/76386-revisiting-sentence-union-generation-as-a-testbed-for-text-consolidation',\n",
       "   'video_url': None},\n",
       "  'P2364': {'abstract': \"Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their ability to compositionally generalize. In this work, we approach semantic parsing from, quite literally, the opposite direction; that is, we introduce a neural semantic parsing generation method that constructs logical forms from the bottom up, beginning from the logical form's leaves. The system we introduce is lazy in that it incrementally builds up a set of potential semantic parses, but only expands and processes the most promising candidate parses at each generation step. Such a parsimonious expansion scheme allows the system to maintain an arbitrarily large set of parse hypotheses that are never realized and thus incur minimal computational overhead. We evaluate our approach on compositional generalization; specifically, on the challenging CFQ dataset and two other Text-to-SQL datasets where we show that our novel, bottom-up semantic parsing technique outperforms general-purpose semantic parsers while also being competitive with semantic parsers that have been tailored to each task.\",\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.470',\n",
       "   'authors': ['Maxwell Crouse',\n",
       "    'Pavan Kapanipathi',\n",
       "    'Subhajit Chaudhury',\n",
       "    'Tahira Naseem',\n",
       "    'Ramon Fernandez Astudillo',\n",
       "    'Achille Fokoue',\n",
       "    'Tim Klinger'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Question Answering',\n",
       "   'event_ids': ['poster-session-7_-question-answering-(poster)'],\n",
       "   'id': 'P2364',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['semantic parsing'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.470.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76713/poster_document/ca89feb78b472e511e7a0e91a42f7233.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76713/slideshow/3d305b583865b409b1dca67752a7a023.pdf',\n",
       "   'title': 'Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing',\n",
       "   'tldr': 'Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their ...',\n",
       "   'track': 'Question Answering',\n",
       "   'underline_id': 76713,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15298/poster/76713-laziness-is-a-virtue-when-it-comes-to-compositionality-in-neural-semantic-parsing',\n",
       "   'video_url': None},\n",
       "  'P2367': {'abstract': 'In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In this paper, we present a domain-general and model-agnostic formulation of compositionality as a constraint on symmetries of data distributions rather than models. Informally, we prove that whenever a task can be solved by a compositional model, there is a corresponding data augmentation scheme --- a procedure for transforming examples into other well-formed examples --- that imparts compositional inductive bias on any model trained to solve the same task. We describe a procedure called LexSym that discovers these transformations automatically, then applies them to training data for ordinary neural sequence models. Unlike existing compositional data augmentation procedures, LexSym can be deployed agnostically across text, structured data, and even images. It matches or surpasses state-of-the-art, task-specific models on COGS semantic parsing, SCAN and Alchemy instruction following, and CLEVR-CoGenT visual question answering datasets.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.38',\n",
       "   'authors': ['Ekin Akyurek', 'Jacob Andreas'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Semantics: Lexical',\n",
       "   'event_ids': ['session-5_-semantics_-lexical-(oral)'],\n",
       "   'id': 'P2367',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['compositionality'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.38.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': None,\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76256/poster/14a41ec2c3869b551d690a347b27b119.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'LexSym: Compositionality as Lexical Symmetry',\n",
       "   'tldr': 'In tasks like semantic parsing, instruction following, and question answering, standard deep networks fail to generalize compositionally from small datasets. Many existing approaches overcome this limitation with model architectures that enforce a compositional process of sentence interpretation. In...',\n",
       "   'track': 'Semantics: Lexical',\n",
       "   'underline_id': 76256,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15253/lecture/76256-lexsym-compositionality-as-lexical-symmetry',\n",
       "   'video_url': None},\n",
       "  'P2373': {'abstract': 'While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge bases to significantly improve on an existing adversarial attack for detecting inconsistent NLEs. We apply our attack to high-performing NLE models and show that models with higher NLE quality do not necessarily generate fewer inconsistencies. Moreover, we propose an off-the-shelf mitigation method to alleviate inconsistencies by grounding the model into external background knowledge. Our method decreases the inconsistencies of previous high-performing NLE models as detected by our attack.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-short.47',\n",
       "   'authors': ['Myeongjun Jang',\n",
       "    'Bodhisattwa Prasad Majumder',\n",
       "    'Julian McAuley',\n",
       "    'Thomas Lukasiewicz',\n",
       "    'Oana-Maria Camburu'],\n",
       "   'category': 'Main-Oral',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'event_ids': ['session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)'],\n",
       "   'id': 'P2373',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['free-text/natural language explanations'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-short.47.pdf',\n",
       "   'paper_type': 'Main-Oral',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76257/poster_document/2fdd6e319488ed8ea981ddaf49c58a93.pdf',\n",
       "   'preview_image': 'https://assets.underline.io/lecture/76257/poster/35b643ff623f8e6e86ebe531f8721a31.jpg',\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': 'https://assets.underline.io/lecture/76257/slideshow/7864d696ea7a170158a9ea55620e6dd9.pptx',\n",
       "   'title': 'KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating Inconsistencies in Natural Language Explanations',\n",
       "   'tldr': 'While recent works have been considerably improving the quality of the natural language explanations (NLEs) generated by a model to justify its predictions, there is very limited research in detecting and alleviating inconsistencies among generated NLEs. In this work, we leverage external knowledge ...',\n",
       "   'track': 'Interpretability and Analysis of Models for NLP',\n",
       "   'underline_id': 76257,\n",
       "   'underline_url': 'https://underline.io/events/395/sessions/15249/lecture/76257-know-how-to-make-up-your-mind-adversarially-detecting-and-alleviating-inconsistencies-in-natural-language-explanations',\n",
       "   'video_url': None},\n",
       "  'P2379': {'abstract': 'Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or minimal additional model training required.',\n",
       "   'anthology_url': 'https://aclanthology.org/2023.acl-long.763',\n",
       "   'authors': ['Mounica Maddela',\n",
       "    'Megan Ung',\n",
       "    'Jing Xu',\n",
       "    'Andrea Madotto',\n",
       "    'Heather M Foran',\n",
       "    'Y-Lan Boureau'],\n",
       "   'category': 'Main-Poster',\n",
       "   'demo_url': None,\n",
       "   'display_track': 'NLP Applications',\n",
       "   'event_ids': ['poster-session-1_-nlp-applications-(poster)'],\n",
       "   'id': 'P2379',\n",
       "   'is_paper': True,\n",
       "   'keywords': ['healthcare applications, clincial nlp'],\n",
       "   'languages': [],\n",
       "   'material': None,\n",
       "   'paper_pdf': 'https://aclanthology.org/2023.acl-long.763.pdf',\n",
       "   'paper_type': 'Main-Poster',\n",
       "   'poster_pdf': 'https://assets.underline.io/lecture/76714/poster_document/2017a9e72d18e505620026c7f0a1fd3f.pdf',\n",
       "   'preview_image': None,\n",
       "   'program': 'Main',\n",
       "   'similar_paper_ids': [],\n",
       "   'slides_pdf': None,\n",
       "   'title': 'Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts',\n",
       "   'tldr': 'Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse ded...',\n",
       "   'track': 'NLP Applications',\n",
       "   'underline_id': 76714,\n",
       "   'underline_url': 'https://underline.io/events/395/posters/15197/poster/76714-training-models-to-generate-recognize-and-reframe-unhelpful-thoughts',\n",
       "   'video_url': None},\n",
       "  ...},\n",
       " 'plenaries': {'acl-rolling-review-update-and-discussion': {'abstract': 'Mausam, Professor, IIT Delhi (ARR EIC), Jonathan K. Kummerfeld,\\nAssistant Professor, University of Sydney (ARR CTO)\\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 14:1514:45\\n\\nThis session will contain a presentation on progress in ARR over the\\npast year and provide an opportunity for community questions and\\ndiscussion.\\n\\nWe will briefly present:\\n\\nPersonnel Updates New aspects: Tracks, Senior Action Editors\\nImprovements, e.g. changes to the review - paper matching process\\nStatistics on timeliness and paper outcomes Next steps\\n\\nWith that context we will open the floor to questions.\\n',\n",
       "   'bio': '',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'acl-rolling-review-update-and-discussion',\n",
       "   'image_url': None,\n",
       "   'institution': None,\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': None,\n",
       "   'room': 'Metropolitan',\n",
       "   'session': 'acl-rolling-review-update-and-discussion',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: ACL Rolling Review Update and Discussion',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'ethics-panel': {'abstract': 'Karn Fort, Min-Yen Kan and Yulia Tsvetkov (ACL Ethics Committee\\nco-chairs) Committee Members: Luciana Benotti, Mark Dredze, Pascale\\nFung, Dirk Hovy, Jin-Dong Kim, Malvina Nissim\\nTuesday, July 11, 2023 - Room: Pier 4&5 - Time: 16:1517:45\\n\\nWe present our ACL Ethics Committees progress over the last few years.\\nOf core interest, we will present the results of the ACL stakeholder\\nsurvey about the role of ethics and ethics training exposure. Results\\nfrom the survey respondents indicate that ethics is of primary interest\\nto the community and that there is a mandate for the further creation\\nand dissemination of ethics related training for authors, reviewers and\\nevent organisers. We will briefly review the survey results and feature\\na lengthed question and answer session in support of extended dialogue\\nwith our community. Our session will culminate through a dialogue with\\nour sessions participants in a moderated panel that includes\\nparticipation from the entire ethics committee.\\n',\n",
       "   'bio': '',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'ethics-panel',\n",
       "   'image_url': None,\n",
       "   'institution': None,\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': None,\n",
       "   'room': 'Pier 4\\\\&5',\n",
       "   'session': 'ethics-panel',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: Ethics Panel',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models': {'abstract': 'Alison Gopnik\\nUniversity of California, Berkeley\\n\\nWednesday, July 12 - Time: 14:0015:00 EDT\\n\\nAbstract: Its natural to ask whether large language models like LaMDA or\\nGPT-3 are intelligent agents. But I argue that this is the wrong\\nquestion. Intelligence and agency are the wrong categories for\\nunderstanding them. Instead, these Al systems are what we might call\\ncultural technologies, like writing, print, libraries, internet search\\nengines or even language itself. They are new techniques for passing on\\ninformation from one group of people to another. Cultural technologies\\narent like intelligent humans, but they are essential for human\\nintelligence. Many animals can transmit some information from one\\nindividual or one generation to another, but no animal does it as much\\nas we do or accumulates as much information over time, . New\\ntechnologies that make cultural transmission easier and more effective\\nhave been among the greatest engines of human progress, but they have\\nalso led to negative as well as positive social consequences. Moreover,\\nwhile cultural technologies allow transmission of existing information\\ncultural evolution, which is central to human success, also depends on\\ninnovation, exploration and causal learning. Comparing LLMs responses\\nin prompts based on developmental psychology experiments to the\\nresponses of children may provide insight into which capacities can be\\nlearned through language and cultural transmission, and which require\\ninnovation and exploration in the physical world. I will present results\\nfrom several studies making such comparisons.\\n',\n",
       "   'bio': ' Alison Gopnik is a professor of psychology and affiliate professor\\nof philosophy at the University of California at Berkeley, and a member\\nof the Berkeley AI Research Group. She received her BA from McGill\\nUniversity and her PhD. from Oxford University. She is a leader in the\\nstudy of cognitive science and of childrens learning and development\\nand was one of the founders of the field of theory of mind, an\\noriginator of the theory of cognitive development, and the first to\\napply Bayesian probabilistic models to childrens learning. She has\\nreceived both the APS Lifetime Achievement Cattell and William James\\nAwards, the Bradford Washburn Award for Science Communication, and the\\nSRCD Lifetime Achievement Award for Basic Science in Child Development.\\nShe is an elected member of the Society of Experimental Psychologists\\nand the American Academy of Arts and Sciences and a Cognitive Science\\nSociety, American Association for the Advancement of Science, and\\nGuggenheim Fellow. She was 2022-23 President of the Association for\\nPsychological Science.\\n\\nShe is the author or coauthor of over 140 journal articles and several\\nbooks including Words, thoughts and theories MIT Press, 1997, and the\\nbestselling and critically acclaimed popular books The Scientist in the\\nCrib William Morrow, 1999, The Philosophical Baby; What childrens\\nminds tell us about love, truth and the meaning of life 2009, and The\\nGardener and the Carpenter 2016, Farrar, Strauss and Giroux, the latter\\ntwo won the Cognitive Development Society Best Book Prize in 2009 and\\n2016. Since 2013 she has written the Mind and Matter column for the Wall\\nStreet Journal and she has also written widely about cognitive science\\nand psychology for The New York Times, The Economist, The Atlantic, The\\nNew Yorker, Scientific American, The Times Literary Supplement, The New\\nYork Review of Books, New Scientist and Slate, among others. Her TED\\ntalk on her work has been viewed more than 5.2 million times. She has\\nfrequently appeared on TV, radio and podcasts including The Charlie\\nRose Show, The Colbert Report, Radio Lab and The Ezra Klein Show.\\nShe lives in Berkeley with her husband Alvy Ray Smith and has three\\nchildren and five grandchildren.\\n',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models',\n",
       "   'image_url': 'invited2.jpg',\n",
       "   'institution': 'University of California at Berkeley',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': 'Alison Gopnik',\n",
       "   'room': 'Metropolitan',\n",
       "   'session': 'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'memorial': {'abstract': '[image]\\n\\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 13:0013:30\\n\\nDragomir Radev, the A. Bartlett Giamatti Professor of Computer Science\\nat Yale University, passed away this year on Wed, March 29th. Drago\\ncontributed in substantial ways to research in NLP, to the organization\\nof the ACL and to mentoring the next generation of computational\\nlinguists. Dragos role in our ACL community spans four decades. He was\\nrecognized for his work over this period through his selection as an ACL\\nFellow in 2018 for his significant contributions to text summarization\\nand question answering, and through his receipt of the Distinguished ACL\\nService Award in 2022. In this session, speakers from different time\\nperiods of his life will discuss his contributions to the field and the\\nimpact his life had on so many of us.\\n',\n",
       "   'bio': '',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'memorial',\n",
       "   'image_url': 'drago.jpg',\n",
       "   'institution': None,\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': None,\n",
       "   'room': 'Metropolitan',\n",
       "   'session': 'memorial',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: Memorial',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'navigating-nlp-in-the-era-of-llm': {'abstract': 'Join us for a panel featuring experts Sara Hooker (Cohere), Swaroop Mishra (Google DeepMind), and Danqi Chen (Princeton), who will provide invaluable insights into navigating the tempestuous seas of NLP in the era of large language models. This discussion will guide students and early career researchers through impactful directions, progress-making strategies, offering perspectives from academia and industry.\\n\\nTuesday, July 11 - Time: 13:4514:30 EDT\\n\\nRoom: Pier 2&3',\n",
       "   'bio': '',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'navigating-nlp-in-the-era-of-llm',\n",
       "   'image_url': None,\n",
       "   'institution': None,\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': 'Danqi Chen (Princeton), Swaroop Mishra (Google DeepMind) and Sara Hooker (Cohere)',\n",
       "   'room': 'Pier 2\\\\&3',\n",
       "   'session': 'navigating-nlp-in-the-era-of-llm',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: Navigating NLP in the Era of Large Language Models',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'the-future-of-computational-linguistics-in-the-llm-age': {'abstract': 'Chair: Iryna Gurevych\\nTechnische Universitt Darmstadt\\n\\nTuesday, July 11 - Time: 14:45-15:45\\n\\nThis is a panel discussion with:\\n\\n-   Dan Klein (UC Berkeley)\\n\\n-   Meg Mitchell (Hugging Face)\\n\\n-   Roy Schwartz (the Hebrew University of Jerusalem)\\n\\nThey will present short statements (5 to 7 min.) related to the main\\ntopic of the panel\\n\\n-   New opportunities (e.g., artificial general intelligence,\\n    responsible NLP);\\n\\n-   Technical challenges (e.g., multimodality, instruction-tuning, etc.)\\n\\n-   Real life problems & societal implications (e.g., hallucinations,\\n    biases, future job market);\\n\\n-   LLMs and the future of NLP; and\\n\\n-   Open-science vs. commercial LLMs\\n\\nFollowed by discussion with the panel and audience.\\n',\n",
       "   'bio': '',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'the-future-of-computational-linguistics-in-the-llm-age',\n",
       "   'image_url': None,\n",
       "   'institution': None,\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': 'Dan Klein (UC Berkeley), Meg Mitchell (Hugging Face), Roy Schwartz (Hebrew University of Jerusalem)',\n",
       "   'room': 'Metropolitan',\n",
       "   'session': 'the-future-of-computational-linguistics-in-the-llm-age',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: The Future of Computational Linguistics in the LLM Age',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None},\n",
       "  'two-paths-to-intelligence': {'abstract': 'Keynote: Geoffrey Hinton\\nCohere\\n\\nMonday, July 10 - Time: 09:3010:30 EDT\\n\\nAbstract: I will briefly describe the forty year history of neural net\\nlanguage models with particular attention to whether they understand\\nwhat they are saying. I will then discuss some of the main differences\\nbetween digital and biological intelligences and speculate on how the\\nbrain could implement something like transformers. I will conclude by\\naddressing the contentious issue of whether current multimodal LLMs have\\nsubjective experience.\\n',\n",
       "   'bio': ' Geoffrey Hinton received his PhD in Artificial Intelligence from\\nEdinburgh in 1978. After five years as a faculty member at\\nCarnegie-Mellon he became a fellow of the Canadian Institute for\\nAdvanced Research and moved to the University of Toronto where he is now\\nan emeritus professor. He is also the Chief Scientific Adviser at the\\nVector Institute.\\n\\nHe was one of the researchers who introduced the backpropagation\\nalgorithm and the first to use backpropagation for learning word\\nembeddings. His other contributions to neural network research include\\nBoltzmann machines, distributed representations, time-delay neural nets,\\nmixtures of experts, variational learning and deep learning. His\\nresearch group in Toronto made major breakthroughs in deep learning that\\nrevolutionized speech recognition and object classification.\\n\\nHe is a fellow of the UK Royal Society and a foreign member of the US\\nNational Academy of Engineering, the US National Academy of Sciences and\\nthe American Academy of Arts and Sciences. His awards include the David\\nE. Rumelhart prize, the IJCAI award for research excellence, the Killam\\nprize for Engineering, the Royal Society Royal Medal, the NSERC Herzberg\\nGold Medal, the IEEE James Clerk Maxwell Gold medal, the NEC C&C award,\\nthe BBVA award, the Honda Prize and the Turing Award.\\n',\n",
       "   'chairs': [],\n",
       "   'end_time': None,\n",
       "   'id': 'two-paths-to-intelligence',\n",
       "   'image_url': 'invited1.jpg',\n",
       "   'institution': 'University of Toronto (emeritus)',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'presenter': 'Geoffrey Hinton',\n",
       "   'room': 'Metropolitan',\n",
       "   'session': 'two-paths-to-intelligence',\n",
       "   'start_time': None,\n",
       "   'title': 'Plenary: Two Paths to Intelligence',\n",
       "   'track': 'Plenary',\n",
       "   'type': 'Plenary Sessions',\n",
       "   'video_url': None}},\n",
       " 'sessions': {'AmericasNLP': {'display_name': 'W20 - The 3rd Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'AmericasNLP',\n",
       "   'name': 'W20 - The 3rd Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'AmericasNLP': {'anthology_venue_id': 'ACL',\n",
       "     'booklet_id': 'workshop_20',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Manuel Mager',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Arturo Oncevay',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Enora Rice',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Abteen Ebrahimi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Shruti Rijhwani',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Alexis Palmer',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Katharina Kann',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'AmericasNLP aims to (a) encourage research on NLP, computational linguistics, corpus linguistics, and speech around the globe to work on native American languages; (b) )connect researchers and professionals from underrepresented communities and native speakers of endangered languages with the machine learning and natural language processing communities; and (c) )promote research on both neural and non-neural machine learning approaches suitable for low-resource languages.',\n",
       "     'end_time': None,\n",
       "     'id': 'AmericasNLP',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 3',\n",
       "     'session': 'AmericasNLP',\n",
       "     'short_name': 'AmericasNLP',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://turing.iimas.unam.mx/americasnlp/'}}},\n",
       "  'BEA': {'display_name': 'W7 - The 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'BEA',\n",
       "   'name': 'W7 - The 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'BEA': {'anthology_venue_id': 'BEA',\n",
       "     'booklet_id': 'workshop_7',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Ekaterina Kochmar',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Jill Burstein',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Andrea Horbach',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ronja Laarmann-Quante',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Nitin Madnani',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Anas Tack',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Victoria Yaneva',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Zheng Yuan',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Torsten Zesch',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The BEA Workshop is a leading venue for NLP innovation in the context of educational applications. It is one of the largest one-day workshops in the ACL community with over 100 registered attendees in the past several years. The growing interest in educational applications and a diverse community of researchers involved resulted in the creation of the Special Interest Group in Educational Applications (SIGEDU) in 2017, which currently has over 300 members.',\n",
       "     'end_time': None,\n",
       "     'id': 'BEA',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Harbour A',\n",
       "     'session': 'BEA',\n",
       "     'short_name': 'BEA',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sig-edu.org/bea/2023'}}},\n",
       "  'BioNLP-ST': {'display_name': 'W13 - The 22nd Workshop on Biomedical Natural Language Processing and Shared Tasks (BioNLP-ST)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'BioNLP-ST',\n",
       "   'name': 'W13 - The 22nd Workshop on Biomedical Natural Language Processing and Shared Tasks (BioNLP-ST)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'BioNLP-ST': {'anthology_venue_id': 'BioNLP',\n",
       "     'booklet_id': 'workshop_13',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Kevin Bretonnel Cohen',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Dina Demner-Fushman',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Sophia Ananiadou',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Jun-ichi Tsujii',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The BioNLP workshop associated with the ACL SIGBIOMED special interest group has established itself as the primary venue for presenting foundational research in language processing for the biological and medical domains. The workshop is running every year since 2002 and continues getting stronger. BioNLP welcomes and encourages work on languages other than English, and inclusion and diversity. BioNLP truly encompasses the breadth of the domain and brings together researchers in bio- and clinical NLP from all over the world. The workshop will continue presenting work on a broad and interesting range of topics in NLP. The interest to biomedical language has broadened significantly due to the COVID-19 pandemic and continues to grow: as access to information becomes easier and more people generate and access health-related text, it becomes clearer that only language technologies can enable and support adequate use of the biomedical text.',\n",
       "     'end_time': None,\n",
       "     'id': 'BioNLP-ST',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 2',\n",
       "     'session': 'BioNLP-ST',\n",
       "     'short_name': 'BioNLP-ST',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://aclweb.org/aclwiki/BioNLP_Workshop'}}},\n",
       "  'Birds of a Feather 1': {'display_name': 'Queer in AI Workshop',\n",
       "   'end_time': '2023-07-09T21:45:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-09T21:45:00+00:00',\n",
       "     'id': 'birds-of-a-feather-1',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-1',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-09T13:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 1',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-1',\n",
       "   'name': 'Queer in AI Workshop',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T13:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 10': {'display_name': 'Social: Reinforcement learning for language models',\n",
       "   'end_time': '2023-07-12T14:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T14:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-10',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-10',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-12T13:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 10',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-10',\n",
       "   'name': 'Social: Reinforcement learning for language models',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T13:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 11': {'display_name': 'Mentoring early-career students in Pier 9',\n",
       "   'end_time': '2023-07-12T16:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T16:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-11',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-11',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-12T15:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 11',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-11',\n",
       "   'name': 'Mentoring early-career students in Pier 9',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T15:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 12': {'display_name': 'Queer in AI Lunch Social',\n",
       "   'end_time': '2023-07-12T17:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T17:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-12',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-12',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 3',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-12T16:30:00+00:00',\n",
       "     'track': 'Birds of a Feather 12',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-12',\n",
       "   'name': 'Queer in AI Lunch Social',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 13': {'display_name': 'How to know your true market value as and AI researcher',\n",
       "   'end_time': '2023-07-11T16:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T16:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-13',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-13',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 2',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T15:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 13',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-13',\n",
       "   'name': 'How to know your true market value as and AI researcher',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T15:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 14': {'display_name': 'Interpretability and Fairness in Large Language Models',\n",
       "   'end_time': '2023-07-12T16:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T16:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-14',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-14',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockerside 2',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-12T15:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 14',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-14',\n",
       "   'name': 'Interpretability and Fairness in Large Language Models',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T15:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 15': {'display_name': 'LatinX',\n",
       "   'end_time': '2023-07-11T16:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T16:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-15',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-15',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T15:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 15',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-15',\n",
       "   'name': 'LatinX',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T15:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 3': {'display_name': 'ACL Mentorship',\n",
       "   'end_time': '2023-07-12T14:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T14:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-3',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-3',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockerside 2',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-12T14:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 3',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-3',\n",
       "   'name': 'ACL Mentorship',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T14:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 4': {'display_name': 'Low-Resource Languages',\n",
       "   'end_time': '2023-07-10T19:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-10T19:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-4',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-4',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-10T18:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 4',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-4',\n",
       "   'name': 'Low-Resource Languages',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T18:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 5': {'display_name': 'Mindfulness meditation in a time of NLP hyperactivity',\n",
       "   'end_time': '2023-07-11T14:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T14:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-5',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-5',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T13:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 5',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-5',\n",
       "   'name': 'Mindfulness meditation in a time of NLP hyperactivity',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T13:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 6': {'display_name': 'NLP for Social Good',\n",
       "   'end_time': '2023-07-11T16:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T16:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-6',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-6',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T15:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 6',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-6',\n",
       "   'name': 'NLP for Social Good',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T15:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 7': {'display_name': 'Discussion on biomedical and clinical NLP and healthcare data',\n",
       "   'end_time': '2023-07-11T19:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T19:30:00+00:00',\n",
       "     'id': 'birds-of-a-feather-7',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-7',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T18:00:00+00:00',\n",
       "     'track': 'Birds of a Feather 7',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-7',\n",
       "   'name': 'Discussion on biomedical and clinical NLP and healthcare data',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T18:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Birds of a Feather 8': {'display_name': 'Women in NLP advancing our careers',\n",
       "   'end_time': '2023-07-11T20:15:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T20:15:00+00:00',\n",
       "     'id': 'birds-of-a-feather-8',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-birds-of-a-feather-8',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 2',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T18:45:00+00:00',\n",
       "     'track': 'Birds of a Feather 8',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'birds-of-a-feather-8',\n",
       "   'name': 'Women in NLP advancing our careers',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T18:45:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'CAWL': {'display_name': 'W19 - The 1st Workshop on Computation and Written Language (CAWL)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'CAWL',\n",
       "   'name': 'W19 - The 1st Workshop on Computation and Written Language (CAWL)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'CAWL': {'anthology_venue_id': 'CAWL',\n",
       "     'booklet_id': 'workshop_19',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Kyle Gorman',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Brian Roark',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Richard Sproat',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Most work on NLP focuses on language in its canonical written form. This has often led researchers to ignore the differences between written and spoken language or, worse, to conflate the two. Instances of conflation are statements like Chinese is a logographic language\" or Persian is a right-to-left language\", variants of which can be found frequently in the ACL anthology. These statements confuse properties of the language with properties of its writing system. Ignoring differences between written and spoken language leads, among other things, to conflating different words that are spelled the same (e.g., English bass), or treating as different, words that have multiple spellings. \\\\\\\\newline text enFurthermore, methods for dealing with written language issues (e.g., various kinds of normalization or conversion) or for recognizing text input (e.g. OCR \\\\& handwriting recognition or text entry methods) are often regarded as precursors to NLP rather than as fundamental parts of the enterprise, despite the fact that most NLP methods rely centrally on representations derived from text rather than (spoken) language. This general lack of consideration of writing has led to much of the research on such topics to largely appear outside of ACL venues, in conferences or journals of neighboring fields such as speech technology (e.g., text normalization) or human-computer interaction (e.g., text entry). \\\\\\\\newline We will invite submissions on the relationship between written and spoken language, the properties of written language, the ways in which writing systems encode language, and applications specifically focused on characteristics of writing systems.',\n",
       "     'end_time': None,\n",
       "     'id': 'CAWL',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 2',\n",
       "     'session': 'CAWL',\n",
       "     'short_name': 'CAWL',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://cawl.wellformedness.com/'}}},\n",
       "  'CODI': {'display_name': 'W3 - The 4th Workshop on Computational Approaches to Discourse (CODI)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'CODI',\n",
       "   'name': 'W3 - The 4th Workshop on Computational Approaches to Discourse (CODI)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'CODI': {'anthology_venue_id': 'CODI',\n",
       "     'booklet_id': 'workshop_3',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Chlo Braud',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Christian Hardmeier',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Junyi Jessy Li',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Sharid Loiciga',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Michael Strube',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Amir Zeldes',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The last ten years have seen a dramatic improvement in the ability of NLP systems to understand and produce words and sentences. This development has created a renewed interest in discourse phenomena as researchers move towards the processing of long-form text and conversations. There is a surge of activity in discourse parsing, coherence models, text summarization, corpora for discourse level reading comprehension, and discourse related/aided representation learning, to name a few, but the problems in computational approaches to discourse are still substantial. At this juncture, we have organized three Workshops on Computational Approaches to Discourse (CODI) at EMNLP 2020, EMNLP 2021 and COLING 2022 to bring together discourse experts and upcoming researchers. These workshops have catalyzed work to improve the speed and knowledge needed to solve such problems and have served as a forum for the discussion of suitable datasets and reliable evaluation methods.',\n",
       "     'end_time': None,\n",
       "     'id': 'CODI',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 9',\n",
       "     'session': 'CODI',\n",
       "     'short_name': 'CODI',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/view/codi-2023/'}}},\n",
       "  'Clinical-NLP': {'display_name': 'W17 - The 5th Clinical Natural Language Processing Workshop (Clinical NLP)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'Clinical-NLP',\n",
       "   'name': 'W17 - The 5th Clinical Natural Language Processing Workshop (Clinical NLP)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'Clinical-NLP': {'anthology_venue_id': 'ClinicalNLP',\n",
       "     'booklet_id': 'workshop_17',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Asma Ben Abacha',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Steven Bethard',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Tristan Naumann',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Kirk Roberts',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Anna Rumshisky',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Clinical text is growing rapidly as electronic health records become pervasive. Much of the information recorded in a clinical encounter is located exclusively in provider narrative notes, which makes them indispensable for supplementing structured clinical data in order to better understand patient state and care provided. The methods and tools developed for the clinical domain have historically lagged behind the scientific advances in the general-domain NLP. Despite the substantial recent strides in clinical NLP, a substantial gap remains. The goal of this workshop is to address this gap by establishing a regular event in CL conferences that brings together researchers interested in developing state-of-the-art methods for the clinical domain. The focus is on improving NLP technology to enable clinical applications, and specifically, information extraction and modeling of narrative provider notes from electronic health records, patient encounter transcripts, and other clinical narratives.',\n",
       "     'end_time': None,\n",
       "     'id': 'Clinical-NLP',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 7 and 8',\n",
       "     'session': 'Clinical-NLP',\n",
       "     'short_name': 'Clinical-NLP',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://clinical-nlp.github.io/2023/'}}},\n",
       "  'Coffee Break 1': {'display_name': 'Coffee Break 1',\n",
       "   'end_time': '2023-07-09T15:00:00+00:00',\n",
       "   'events': {'coffee-break-1': {'chairs': [],\n",
       "     'end_time': '2023-07-09T15:00:00+00:00',\n",
       "     'id': 'coffee-break-1',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 1',\n",
       "     'start_time': '2023-07-09T14:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-1',\n",
       "   'name': 'Coffee Break 1',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T14:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 10': {'display_name': 'Coffee Break 10',\n",
       "   'end_time': '2023-07-12T15:00:00+00:00',\n",
       "   'events': {'coffee-break-10': {'chairs': [],\n",
       "     'end_time': '2023-07-12T15:00:00+00:00',\n",
       "     'id': 'coffee-break-10',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 10',\n",
       "     'start_time': '2023-07-12T14:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-10',\n",
       "   'name': 'Coffee Break 10',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T14:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 11': {'display_name': 'Coffee Break 11',\n",
       "   'end_time': '2023-07-12T18:00:00+00:00',\n",
       "   'events': {'coffee-break-11': {'chairs': [],\n",
       "     'end_time': '2023-07-12T18:00:00+00:00',\n",
       "     'id': 'coffee-break-11',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 11',\n",
       "     'start_time': '2023-07-12T16:30:00+00:00',\n",
       "     'track': 'Lunch Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-11',\n",
       "   'name': 'Coffee Break 11',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 12': {'display_name': 'Coffee Break 12',\n",
       "   'end_time': '2023-07-12T19:30:00+00:00',\n",
       "   'events': {'coffee-break-12': {'chairs': [],\n",
       "     'end_time': '2023-07-12T19:30:00+00:00',\n",
       "     'id': 'coffee-break-12',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 12',\n",
       "     'start_time': '2023-07-12T19:00:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-12',\n",
       "   'name': 'Coffee Break 12',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T19:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 2': {'display_name': 'Coffee Break 2',\n",
       "   'end_time': '2023-07-09T18:00:00+00:00',\n",
       "   'events': {'coffee-break-2': {'chairs': [],\n",
       "     'end_time': '2023-07-09T18:00:00+00:00',\n",
       "     'id': 'coffee-break-2',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 2',\n",
       "     'start_time': '2023-07-09T16:30:00+00:00',\n",
       "     'track': 'Lunch Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-2',\n",
       "   'name': 'Coffee Break 2',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 3': {'display_name': 'Coffee Break 3',\n",
       "   'end_time': '2023-07-09T20:00:00+00:00',\n",
       "   'events': {'coffee-break-3': {'chairs': [],\n",
       "     'end_time': '2023-07-09T20:00:00+00:00',\n",
       "     'id': 'coffee-break-3',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 3',\n",
       "     'start_time': '2023-07-09T19:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-3',\n",
       "   'name': 'Coffee Break 3',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T19:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 4': {'display_name': 'Coffee Break 4',\n",
       "   'end_time': '2023-07-10T15:00:00+00:00',\n",
       "   'events': {'coffee-break-4': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:00:00+00:00',\n",
       "     'id': 'coffee-break-4',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 4',\n",
       "     'start_time': '2023-07-10T14:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-4',\n",
       "   'name': 'Coffee Break 4',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T14:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 5': {'display_name': 'Coffee Break 5',\n",
       "   'end_time': '2023-07-10T18:00:00+00:00',\n",
       "   'events': {'coffee-break-5': {'chairs': [],\n",
       "     'end_time': '2023-07-10T18:00:00+00:00',\n",
       "     'id': 'coffee-break-5',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 5',\n",
       "     'start_time': '2023-07-10T16:30:00+00:00',\n",
       "     'track': 'Lunch Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-5',\n",
       "   'name': 'Coffee Break 5',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 6': {'display_name': 'Coffee Break 6',\n",
       "   'end_time': '2023-07-10T20:00:00+00:00',\n",
       "   'events': {'coffee-break-6': {'chairs': [],\n",
       "     'end_time': '2023-07-10T20:00:00+00:00',\n",
       "     'id': 'coffee-break-6',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 6',\n",
       "     'start_time': '2023-07-10T19:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-6',\n",
       "   'name': 'Coffee Break 6',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T19:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 7': {'display_name': 'Coffee Break 7',\n",
       "   'end_time': '2023-07-11T15:00:00+00:00',\n",
       "   'events': {'coffee-break-7': {'chairs': [],\n",
       "     'end_time': '2023-07-11T15:00:00+00:00',\n",
       "     'id': 'coffee-break-7',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 7',\n",
       "     'start_time': '2023-07-11T14:30:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-7',\n",
       "   'name': 'Coffee Break 7',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T14:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 8': {'display_name': 'Coffee Break 8',\n",
       "   'end_time': '2023-07-11T17:00:00+00:00',\n",
       "   'events': {'coffee-break-8': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:00:00+00:00',\n",
       "     'id': 'coffee-break-8',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 8',\n",
       "     'start_time': '2023-07-11T16:30:00+00:00',\n",
       "     'track': 'Lunch Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-8',\n",
       "   'name': 'Coffee Break 8',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'Coffee Break 9': {'display_name': 'Coffee Break 9',\n",
       "   'end_time': '2023-07-11T20:15:00+00:00',\n",
       "   'events': {'coffee-break-9': {'chairs': [],\n",
       "     'end_time': '2023-07-11T20:15:00+00:00',\n",
       "     'id': 'coffee-break-9',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'Coffee Break 9',\n",
       "     'start_time': '2023-07-11T19:45:00+00:00',\n",
       "     'track': 'Coffee Break',\n",
       "     'type': 'Breaks'}},\n",
       "   'id': 'coffee-break-9',\n",
       "   'name': 'Coffee Break 9',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T19:45:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Breaks',\n",
       "   'workshop_events': {}},\n",
       "  'D&I 1': {'display_name': 'D&I Lunch Social',\n",
       "   'end_time': '2023-07-10T17:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-10T17:30:00+00:00',\n",
       "     'id': 'diversity-and-inclusion-lunch1',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-event_diversity-and-inclusion',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 3',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-10T16:30:00+00:00',\n",
       "     'track': 'D&I 1',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'diversity-and-inclusion-lunch1',\n",
       "   'name': 'D&I Lunch Social',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'D&I 2': {'display_name': 'D&I Lunch Social',\n",
       "   'end_time': '2023-07-11T17:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:30:00+00:00',\n",
       "     'id': 'diversity-and-inclusion-lunch2',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-event_diversity-and-inclusion',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 3',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T16:30:00+00:00',\n",
       "     'track': 'D&I 2',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'diversity-and-inclusion-lunch2',\n",
       "   'name': 'D&I Lunch Social',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T16:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 1': {'display_name': 'Demo Session 1',\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'events': {'demo-session-1_-generation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'demo-session-1_-generation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D44', 'D63', 'D103', 'D95'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Generation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-1_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'demo-session-1_-large-language-models-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D74'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Large Language Models (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-1_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'demo-session-1_-speech-and-multimodality-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D96'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-1',\n",
       "   'name': 'Demo Session 1',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 2': {'display_name': 'Demo Session 2',\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'events': {'demo-session-2_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'demo-session-2_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D107'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-2_-generation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'demo-session-2_-generation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D156'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Generation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-2_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'demo-session-2_-information-extraction-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D91', 'D106'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Information Extraction (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-2_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'demo-session-2_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D133'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-2_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'demo-session-2_-speech-and-multimodality-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D85'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-2',\n",
       "   'name': 'Demo Session 2',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 3': {'display_name': 'Demo Session 3',\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'events': {'demo-session-3_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D39'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-information-extraction-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D89'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Information Extraction (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-linguistic-diversity-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-linguistic-diversity-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D24'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-machine-translation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-machine-translation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D69'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Machine Translation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-nlp-applications-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-nlp-applications-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D78', 'D113', 'D134', 'D123'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'NLP Applications (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-resources-and-evaluation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D148'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-3_-speech-and-multimodality-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'demo-session-3_-speech-and-multimodality-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D27'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-3',\n",
       "   'name': 'Demo Session 3',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 4': {'display_name': 'Demo Session 4',\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'events': {'demo-session-4_-information-extraction-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'demo-session-4_-information-extraction-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D94'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Extraction (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-4_-machine-learning-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'demo-session-4_-machine-learning-for-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D130'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-4_-machine-translation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'demo-session-4_-machine-translation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D45', 'D53'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Translation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'demo-session-4_-multilingualism-and-cross-lingual-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D66', 'D93'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-4',\n",
       "   'name': 'Demo Session 4',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 5': {'display_name': 'Demo Session 5',\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'events': {'demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'demo-session-5_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D56', 'D104', 'D110'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-5_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'demo-session-5_-large-language-models-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D77', 'D80'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Large Language Models (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-5_-question-answering-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'demo-session-5_-question-answering-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D47'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Question Answering (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-5_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'demo-session-5_-resources-and-evaluation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D90'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Resources and Evaluation (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-5',\n",
       "   'name': 'Demo Session 5',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 6': {'display_name': 'Demo Session 6',\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'events': {'demo-session-6_-generation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'demo-session-6_-generation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D31'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Generation (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-6_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'demo-session-6_-interpretability-and-analysis-of-models-for-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D71'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-6_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'demo-session-6_-large-language-models-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D141'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Large Language Models (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-6_-question-answering-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'demo-session-6_-question-answering-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D19', 'D26', 'D144'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Question Answering (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-6_-resources-and-evaluation-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'demo-session-6_-resources-and-evaluation-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D84'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-6',\n",
       "   'name': 'Demo Session 6',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Demo Session 7': {'display_name': 'Demo Session 7',\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'events': {'demo-session-7_-dialogue-and-interactive-systems-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'demo-session-7_-dialogue-and-interactive-systems-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D49'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-7_-large-language-models-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'demo-session-7_-large-language-models-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D18'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Large Language Models (demo)',\n",
       "     'type': 'Poster'},\n",
       "    'demo-session-7_-machine-learning-for-nlp-(demo)-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'demo-session-7_-machine-learning-for-nlp-(demo)-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D147'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Demo Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP (demo)',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'demo-session-7',\n",
       "   'name': 'Demo Session 7',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'DialDoc': {'display_name': 'W10 - The 3rd Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'DialDoc',\n",
       "   'name': 'W10 - The 3rd Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'DialDoc': {'anthology_venue_id': 'DialDoc',\n",
       "     'booklet_id': 'workshop_10',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Roee Aharoni',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Nouha Dziri',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Song Feng',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yongbin Li',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yu Li',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Hui Wan',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The DialDoc workshop focuses on Document-Grounded Dialogue and Conversational Question Answering. Given the vast amount of content created every day in various mediums, it is a meaningful yet challenging task not only to make such content accessible to end users via various conversational interfaces, but also to make sure the responses provided by the models are grounded and faithful with respect to the knowledge sources.',\n",
       "     'end_time': None,\n",
       "     'id': 'DialDoc',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 2',\n",
       "     'session': 'DialDoc',\n",
       "     'short_name': 'DialDoc',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://doc2dial.github.io/workshop2023/'}}},\n",
       "  'Dinner 1': {'display_name': 'Social event',\n",
       "   'end_time': '2023-07-12T02:00:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-12T02:00:00+00:00',\n",
       "     'id': 'social-event-1',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-event_social-event',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Steam Whistle Brewing Company',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-11T22:30:00+00:00',\n",
       "     'track': 'Dinner 1',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'social-event-1',\n",
       "   'name': 'Social event',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T22:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'Dinner 2': {'display_name': 'Welcome reception',\n",
       "   'end_time': '2023-07-10T01:30:00+00:00',\n",
       "   'events': {'id': {'chairs': [],\n",
       "     'end_time': '2023-07-10T01:30:00+00:00',\n",
       "     'id': 'social-event-2',\n",
       "     'link': 'https://acl.rocket.chat/channel/paper-event_welcome-reception',\n",
       "     'paper_ids': [],\n",
       "     'room': 'Metropolitan Ballroom',\n",
       "     'session': 'event_session',\n",
       "     'start_time': '2023-07-09T23:00:00+00:00',\n",
       "     'track': 'Dinner 2',\n",
       "     'type': 'Socials'}},\n",
       "   'id': 'social-event-2',\n",
       "   'name': 'Welcome reception',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T23:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Socials',\n",
       "   'workshop_events': {}},\n",
       "  'IWSLT': {'display_name': 'W4 - The 20th International Conference on Spoken Language Translation (IWSLT)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'IWSLT',\n",
       "   'name': 'W4 - The 20th International Conference on Spoken Language Translation (IWSLT)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'IWSLT': {'anthology_venue_id': 'IWSLT',\n",
       "     'booklet_id': 'workshop_4',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Marine Carpuat',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Marcello Federico',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Alex Waibel',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Jan Niehues',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Sebastian Stker',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Elizabeth Salesky',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Atul Kr. Ojha',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The International Conference on Spoken Language Translation (IWSLT) is an annual scientific conference, associated with an open evaluation campaign on spoken language translation, where both scientific papers and system descriptions are presented.',\n",
       "     'end_time': None,\n",
       "     'id': 'IWSLT',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 1',\n",
       "     'session': 'IWSLT',\n",
       "     'short_name': 'IWSLT',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://iwslt.org/2023/'}}},\n",
       "  'LAW': {'display_name': 'W12 - The 17th Workshop on Linguistic Annotation (LAW)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'LAW',\n",
       "   'name': 'W12 - The 17th Workshop on Linguistic Annotation (LAW)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'LAW': {'anthology_venue_id': 'LAW',\n",
       "     'booklet_id': 'workshop_12',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Annemarie Friedrich',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Jakob Prange',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Amir Zeldes',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ines Rehbein',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Linguistic annotation of natural language corpora is the backbone of supervised methods of statistical natural language processing. The Linguistic Annotation Workshop (LAW) is the annual workshop of the ACL Special Interest Group on Annotation (SIGANN), and it provides a forum for the presentation and discussion of innovative research on all aspects of linguistic annotation, including the creation and evaluation of annotation schemes, methods for automatic and manual annotation, use and evaluation of annotation software and frameworks, representation of linguistic data and annotations, semi-supervised human in the loop methods of annotation, crowd-sourcing approaches, and more. As in the past, the LAW will provide a forum for annotation researchers to work towards standardization, best practices, and interoperability of annotation information and software.',\n",
       "     'end_time': None,\n",
       "     'id': 'LAW',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 3',\n",
       "     'session': 'LAW',\n",
       "     'short_name': 'LAW',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sigann.github.io/LAW-XVII-2023/'}}},\n",
       "  'MATCHING': {'display_name': 'W11 - The 1st Workshop on Matching From Unstructured and Structured Data (MATCHING)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'MATCHING',\n",
       "   'name': 'W11 - The 1st Workshop on Matching From Unstructured and Structured Data (MATCHING)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'MATCHING': {'anthology_venue_id': 'MATCHING',\n",
       "     'booklet_id': 'workshop_11',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Dunia Mladeni',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Estevam Hruschka',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Marko Grobelnik',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Sajjadur Rahman',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Tom Mitchell',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Matching Entities from structured and unstructured sources is an important task in many  domains and applications such as HR and E-commerce. For example, in HR platforms/services, it is important to match resumes to job descriptions and job seekers to companies. Similarly in web platforms/services, it is important to match customers to businesses such as hotels and restaurant, among others. In such domains, it is also relevant to match textual customer reviews to customers queries, and sentences (or phrases) as answers to customer questions. Recent advances in Natural Language Processing, Natural Language Understanding, Conversational AI, Language Generation, Machine Learning, Deep Learning, Data Management, Information Extraction, Knowledge Bases/Graphs, (MultiSingle Hop/Commonsense) Inference/Reasoning, Recommendation Systems, and others, have demonstrated promising results in different Matching tasks related (but not limited) to the previously mentioned domains. We believe that there is tremendous opportunity to further exploit and explore the use of advanced NLP (and language related) techniques applied to Matching tasks. Therefore, the goal of this workshop is to bring together the research communities (from academia and industry) of these related areas, that are interested in the development and the application of novel natural-language-based approaches/models/systems to address challenges around different Matching tasks.',\n",
       "     'end_time': None,\n",
       "     'id': 'MATCHING',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 3',\n",
       "     'session': 'MATCHING',\n",
       "     'short_name': 'MATCHING',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://megagon.ai/matching-2023/'}}},\n",
       "  'NLP4ConvAI': {'display_name': 'W14 - The 5th Workshop on NLP for Conversational AI',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'NLP4ConvAI',\n",
       "   'name': 'W14 - The 5th Workshop on NLP for Conversational AI',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'NLP4ConvAI': {'anthology_venue_id': 'NLP4ConvAI',\n",
       "     'booklet_id': 'workshop_14',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Abhinav Rastogi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Georgios Spithourakis',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yun-Nung (Vivian) Chen',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Bing Liu',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yu Li',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Elnaz Nouri',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Alon Albalak',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Alexandros Papangelis',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Over the past decades, mathematicians, linguists, and computer scientists have dedicated their efforts towards empowering human-machine communication in natural language. While in recent years the emergence of virtual personal assistants such as Siri, Alexa, Google Assistant, Cortana, and ChatGPT has pushed the field forward, they may still have numerous challenges. \\\\newline Following the success of the 4th NLP for Conversational AI workshop at ACL, The 5th NLP4ConvAI will be a one-day workshop, co-located with ACL 2023 in Toronto, Canada. The goal of this workshop is to bring together researchers and practitioners to discuss impactful research problems in this area, share findings from real-world applications, and generate ideas for future research directions. \\\\newline The workshop will include keynotes, posters, panel sessions, and a shared task. In keynote talks, senior technical leaders from industry and academia will share insights on the latest developments in the field. We would like to encourage researchers and students to share their prospects and latest discoveries. There will also be a panel discussion with noted conversational AI leaders focused on the state of the field, future directions, and open problems across academia and industry.',\n",
       "     'end_time': None,\n",
       "     'id': 'NLP4ConvAI',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Harbour B',\n",
       "     'session': 'NLP4ConvAI',\n",
       "     'short_name': 'NLP4ConvAI',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/view/5thnlp4convai/'}}},\n",
       "  'NLRSE': {'display_name': 'W8 - The 1st Workshop on Natural Language Reasoning and Structured Explanations',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'NLRSE',\n",
       "   'name': 'W8 - The 1st Workshop on Natural Language Reasoning and Structured Explanations',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'NLRSE': {'anthology_venue_id': 'ACL',\n",
       "     'booklet_id': 'workshop_8',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Peter Clark',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ellie Pavlick',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Denny Zhou',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Noah Goodman',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Sarah Wiegreffe',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Felix Hill',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'With recent scaling of large pre-trained Transformer language models (LLMs), the scope of feasible NLP tasks has broadened. Significant recent work has focused on tasks that require some kind of natural language reasoning. A trajectory in question answering has led us from extraction-oriented datasets like SQuAD to multi-hop reasoning datasets like HotpotQA and StrategyQA. Although LLMs have shown remarkable performance on most NLP tasks, it is often unclear why their answers follow from what they know. To address this gap, a new class of explanation techniques has emerged which play an integral part in structuring the reasoning necessary to solve these datasets. For example, the chain-of-thought paradigm leverages explanations as vehicles for LLMs to mimic human reasoning processes. Entailment trees offer a way to ground multi-step reasoning in a collection of verifiable steps. Frameworks like SayCan bridge high-level planning in language and with low-level action trajectories. As a result, we see a confluence of methods blending explainable machine learning/NLP, classical AI (especially theorem proving), and cognitive science (how do humans structure explanations?). This workshop aims to bring together a diverse set of perspectives from these different traditions and attempt to establish common ground for how these various kinds of explanation structures can tackle a broad class of reasoning problems in natural language and beyond.',\n",
       "     'end_time': None,\n",
       "     'id': 'NLRSE',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 4',\n",
       "     'session': 'NLRSE',\n",
       "     'short_name': 'NLRSE',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://nl-reasoning-workshop.github.io/'}}},\n",
       "  'Narrative-Understanding': {'display_name': 'W21 - The 5th Workshop on Narrative Understanding (WNU)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'Narrative-Understanding',\n",
       "   'name': 'W21 - The 5th Workshop on Narrative Understanding (WNU)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'Narrative-Understanding': {'anthology_venue_id': 'wnu2023',\n",
       "     'booklet_id': 'workshop_21',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Nader Akoury',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Faeze Brahman',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Khyathi Chandu',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Snigdha Chaturvedi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Elizabeth Clark',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Mohit Iyyer',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'This is the 5th iteration of the Narrative Understanding Workshop, which brings together an interdisciplinary group of researchers from AI, ML, NLP, Computer Vision and other related fields, as well as scholars from the humanities to discuss methods to improve automatic narrative understanding capabilities. The workshop will consist of talks from invited speakers, a panel of researchers and writers, and talks and posters from accepted papers.',\n",
       "     'end_time': None,\n",
       "     'id': 'Narrative-Understanding',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 2',\n",
       "     'session': 'Narrative-Understanding',\n",
       "     'short_name': 'Narrative-Understanding',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/umass.edu/wnu2023'}}},\n",
       "  'Plenary: ACL Lifetime -- ToT': {'display_name': 'Plenary: ACL Lifetime -- ToT',\n",
       "   'end_time': '2023-07-12T21:00:00+00:00',\n",
       "   'events': {'plenary_-acl-lifetime----tot': {'chairs': [],\n",
       "     'end_time': '2023-07-12T21:00:00+00:00',\n",
       "     'id': 'plenary_-acl-lifetime----tot',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'plenary_-acl-lifetime----tot',\n",
       "     'start_time': '2023-07-12T19:30:00+00:00',\n",
       "     'track': 'Plenary: ACL Lifetime -- ToT',\n",
       "     'type': 'Plenary Sessions'}},\n",
       "   'id': 'plenary_-acl-lifetime----tot',\n",
       "   'name': 'Plenary: ACL Lifetime -- ToT',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T19:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Plenary: Best Paper Awards': {'display_name': 'Plenary: Best Paper Awards',\n",
       "   'end_time': '2023-07-10T21:30:00+00:00',\n",
       "   'events': {'plenary_-best-paper-awards': {'chairs': [],\n",
       "     'end_time': '2023-07-10T21:30:00+00:00',\n",
       "     'id': 'plenary_-best-paper-awards',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'plenary_-best-paper-awards',\n",
       "     'start_time': '2023-07-10T20:00:00+00:00',\n",
       "     'track': 'Plenary: Best Paper Awards',\n",
       "     'type': 'Plenary Sessions'}},\n",
       "   'id': 'plenary_-best-paper-awards',\n",
       "   'name': 'Plenary: Best Paper Awards',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T20:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Plenary: Business Meeting': {'display_name': 'Plenary: Business Meeting',\n",
       "   'end_time': '2023-07-11T18:10:00+00:00',\n",
       "   'events': {'plenary_-business-meeting': {'chairs': [],\n",
       "     'end_time': '2023-07-11T18:10:00+00:00',\n",
       "     'id': 'plenary_-business-meeting',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'plenary_-business-meeting',\n",
       "     'start_time': '2023-07-11T17:30:00+00:00',\n",
       "     'track': 'Plenary: Business Meeting',\n",
       "     'type': 'Plenary Sessions'}},\n",
       "   'id': 'plenary_-business-meeting',\n",
       "   'name': 'Plenary: Business Meeting',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T17:30:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Plenary: Closing Session': {'display_name': 'Plenary: Closing Session',\n",
       "   'end_time': '2023-07-12T21:30:00+00:00',\n",
       "   'events': {'plenary_-closing-session': {'chairs': [],\n",
       "     'end_time': '2023-07-12T21:30:00+00:00',\n",
       "     'id': 'plenary_-closing-session',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'plenary_-closing-session',\n",
       "     'start_time': '2023-07-12T21:00:00+00:00',\n",
       "     'track': 'Plenary: Closing Session',\n",
       "     'type': 'Plenary Sessions'}},\n",
       "   'id': 'plenary_-closing-session',\n",
       "   'name': 'Plenary: Closing Session',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T21:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 1': {'display_name': 'Poster Session 1',\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'events': {'poster-session-1_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3334', 'P5806'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1323', 'P1466', 'P3638', 'P3852'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4396', 'P4729'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3117', 'P4865', 'P2606'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P526',\n",
       "      'P609',\n",
       "      'P1309',\n",
       "      'P3032',\n",
       "      'P3951',\n",
       "      'P4196',\n",
       "      'P4406',\n",
       "      'P5707',\n",
       "      'P5824',\n",
       "      'P1476'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1160'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P676', 'P2441', 'P2511'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P819', 'P3977'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P294', 'P663', 'P4258', 'P4936', 'P4541'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P391'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P211', 'P1370', 'P5597', 'P5748', 'P5796', 'P5857'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P118', 'P1785', 'P3809'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P283', 'P713', 'P1368', 'P2339', 'P3329'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2379', 'P3672'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P57', 'P4357'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2126', 'P3357', 'P3524', 'P4577', 'P5661', 'P5272'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4058', 'P5684', 'P3782'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1101', 'P1695', 'P4522', 'P4858', 'P5640'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4335'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2160'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1133',\n",
       "      'P2068',\n",
       "      'P2164',\n",
       "      'P2201',\n",
       "      'P4026',\n",
       "      'P4521',\n",
       "      'P4745',\n",
       "      'P5840'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4202'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-1_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'poster-session-1_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P282', 'P692', 'P2659', 'P3849', 'P4296'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-1',\n",
       "   'name': 'Poster Session 1',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 2': {'display_name': 'Poster Session 2',\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'events': {'poster-session-2_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P959', 'P2226', 'P5673', 'P3441'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2205', 'P4484', 'P2056'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1286', 'P3763'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2283', 'P4056'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1969', 'P4838', 'P3087'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P759', 'P821', 'P2621', 'P4553'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4020', 'P4234', 'P5812'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P286', 'P2353', 'P3065'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P810',\n",
       "      'P1329',\n",
       "      'P1980',\n",
       "      'P2350',\n",
       "      'P2358',\n",
       "      'P2699',\n",
       "      'P3786',\n",
       "      'P4233',\n",
       "      'P4238',\n",
       "      'P5312',\n",
       "      'P5859'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P319', 'P5646'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P181', 'P599', 'P2448', 'P3081', 'P4446'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P505', 'P2985', 'P3174', 'P3241', 'P4471'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1542', 'P2291'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P530',\n",
       "      'P2701',\n",
       "      'P3061',\n",
       "      'P3116',\n",
       "      'P4090',\n",
       "      'P5099',\n",
       "      'P4462',\n",
       "      'P5713'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3413', 'P4089'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2684',\n",
       "      'P2823',\n",
       "      'P3506',\n",
       "      'P4209',\n",
       "      'P5057',\n",
       "      'P5643',\n",
       "      'P5662',\n",
       "      'P5775',\n",
       "      'P1477'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P35',\n",
       "      'P322',\n",
       "      'P1170',\n",
       "      'P1242',\n",
       "      'P2284',\n",
       "      'P3709',\n",
       "      'P4115',\n",
       "      'P4826',\n",
       "      'P5599'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5728'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P835', 'P4495'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P203'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P341'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1516', 'P1820', 'P3367', 'P4192', 'P5650'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4350'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-2_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'poster-session-2_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1328'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-2',\n",
       "   'name': 'Poster Session 2',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 3': {'display_name': 'Poster Session 3',\n",
       "   'end_time': '2023-07-11T10:30:00-04:00',\n",
       "   'events': {'poster-session-3_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2158', 'P2842', 'P3103', 'P3530'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P267', 'P2942'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3155'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3093',\n",
       "      'P3198',\n",
       "      'P3228',\n",
       "      'P3656',\n",
       "      'P3960',\n",
       "      'P4614',\n",
       "      'P4752',\n",
       "      'P4870'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P254',\n",
       "      'P3380',\n",
       "      'P3802',\n",
       "      'P3903',\n",
       "      'P4317',\n",
       "      'P4980',\n",
       "      'P5869'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4334'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P230', 'P1239', 'P1716', 'P2356', 'P3888', 'P4878'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P249', 'P1139'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P384', 'P1072', 'P2531'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P680',\n",
       "      'P1316',\n",
       "      'P1679',\n",
       "      'P1817',\n",
       "      'P3724',\n",
       "      'P5642',\n",
       "      'P5701'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P327', 'P1680'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P823', 'P1719'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1292', 'P2061', 'P2200', 'P2273'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3382'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2093', 'P2119', 'P2863', 'P4424', 'P4501', 'P5833'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P379',\n",
       "      'P550',\n",
       "      'P614',\n",
       "      'P860',\n",
       "      'P1696',\n",
       "      'P1808',\n",
       "      'P3205',\n",
       "      'P4565',\n",
       "      'P5447',\n",
       "      'P5620',\n",
       "      'P5648',\n",
       "      'P5721'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3828'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1518'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P867', 'P2713', 'P2776'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'poster-session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4198'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-3',\n",
       "   'name': 'Poster Session 3',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 4': {'display_name': 'Poster Session 4',\n",
       "   'end_time': '2023-07-11T12:30:00-04:00',\n",
       "   'events': {'poster-session-4_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4463', 'P5766'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1033',\n",
       "      'P2469',\n",
       "      'P2852',\n",
       "      'P3571',\n",
       "      'P4382',\n",
       "      'P5293',\n",
       "      'P2091'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5572'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P942', 'P2534', 'P4582', 'P5583'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P442', 'P4155', 'P4178', 'P4416', 'P5565'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P64', 'P115', 'P496'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4687', 'P5635'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P352', 'P3729'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1345', 'P3973'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P915', 'P1821', 'P2449', 'P2962', 'P3309', 'P4513'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3074', 'P5578', 'P5826'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P138', 'P4159', 'P4254', 'P4263', 'P4415', 'P5561'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2663', 'P4532'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2166', 'P5238'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P63',\n",
       "      'P339',\n",
       "      'P1099',\n",
       "      'P2401',\n",
       "      'P2521',\n",
       "      'P3273',\n",
       "      'P3487',\n",
       "      'P4326'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2307', 'P5743'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P521',\n",
       "      'P536',\n",
       "      'P1569',\n",
       "      'P2079',\n",
       "      'P3085',\n",
       "      'P3495',\n",
       "      'P3963',\n",
       "      'P4593',\n",
       "      'P5632',\n",
       "      'P5638',\n",
       "      'P5850'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2261', 'P4308'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3422', 'P5576'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4505'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P76', 'P2774', 'P4280'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3736'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-4_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'poster-session-4_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2033',\n",
       "      'P2589',\n",
       "      'P3203',\n",
       "      'P3608',\n",
       "      'P4101',\n",
       "      'P5151',\n",
       "      'P5797'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-4',\n",
       "   'name': 'Poster Session 4',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 5': {'display_name': 'Poster Session 5',\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'events': {'poster-session-5_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3769', 'P5854', 'P4260'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P766', 'P1183', 'P4289'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4136'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1276', 'P2633', 'P3250', 'P4226', 'P3167'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2412', 'P3911'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2012', 'P4792'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5682'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P467', 'P4018', 'P4694', 'P5724'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2859', 'P5106'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P522',\n",
       "      'P2901',\n",
       "      'P3810',\n",
       "      'P4662',\n",
       "      'P5637',\n",
       "      'P5760',\n",
       "      'P5855'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P451', 'P2970', 'P3518'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2616', 'P3909', 'P5651'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P576'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P468', 'P5832', 'P1886', 'P718'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3958'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-5_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'poster-session-5_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2910'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-5',\n",
       "   'name': 'Poster Session 5',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 6': {'display_name': 'Poster Session 6',\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'events': {'poster-session-6_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1555', 'P3607'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1340',\n",
       "      'P3930',\n",
       "      'P4005',\n",
       "      'P4419',\n",
       "      'P4803',\n",
       "      'P5234',\n",
       "      'P5623',\n",
       "      'P5639',\n",
       "      'P1684'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5730'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P679', 'P2209'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P392', 'P2649', 'P3191', 'P3704', 'P4481', 'P4558'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5692'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2410', 'P3959'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P600', 'P3109', 'P4162', 'P4549', 'P4814', 'P4889'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P91', 'P1207', 'P1448', 'P3426', 'P4559'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-linguistic-diversity-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-linguistic-diversity-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4821'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1867',\n",
       "      'P2122',\n",
       "      'P2129',\n",
       "      'P2755',\n",
       "      'P3129',\n",
       "      'P4482',\n",
       "      'P5688',\n",
       "      'P5816'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1150', 'P1706', 'P3135', 'P3784'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1737', 'P4835'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P404',\n",
       "      'P540',\n",
       "      'P683',\n",
       "      'P2140',\n",
       "      'P2530',\n",
       "      'P4409',\n",
       "      'P4570',\n",
       "      'P5580'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2525'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1455', 'P3302'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1205', 'P1381', 'P3799', 'P4581', 'P4891', 'P2991'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4106'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P965', 'P2740', 'P3701', 'P3962'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1132'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1275', 'P2666', 'P5668'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3992', 'P5772', 'P3699'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-6_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'poster-session-6_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3693'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-6',\n",
       "   'name': 'Poster Session 6',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Poster Session 7': {'display_name': 'Poster Session 7',\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'events': {'poster-session-7_-computational-social-science-and-cultural-analytics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-computational-social-science-and-cultural-analytics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4282', 'P5740', 'P5755', 'P5770', 'P5699'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2689', 'P2928'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P206'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-ethics-and-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-ethics-and-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P197', 'P2153', 'P2305', 'P4680'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P284', 'P312', 'P4157'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P824', 'P2841', 'P3976', 'P4215', 'P5767'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3549'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P790', 'P927', 'P2058', 'P3760', 'P5848', 'P4975'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1254', 'P4213', 'P544'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-large-language-models-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-large-language-models-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P292',\n",
       "      'P1087',\n",
       "      'P1302',\n",
       "      'P3998',\n",
       "      'P4529',\n",
       "      'P4763',\n",
       "      'P5676'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P957',\n",
       "      'P2296',\n",
       "      'P3324',\n",
       "      'P3627',\n",
       "      'P5706',\n",
       "      'P5742',\n",
       "      'P255'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1349', 'P1572', 'P1806', 'P3069', 'P4250', 'P1200'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-multilingualism-and-cross-lingual-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-multilingualism-and-cross-lingual-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1778', 'P5297', 'P5686'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-nlp-applications-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-nlp-applications-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P51',\n",
       "      'P394',\n",
       "      'P605',\n",
       "      'P955',\n",
       "      'P1388',\n",
       "      'P1479',\n",
       "      'P1917',\n",
       "      'P2330',\n",
       "      'P3759',\n",
       "      'P4007',\n",
       "      'P5677',\n",
       "      'P5771',\n",
       "      'P3539',\n",
       "      'P1012'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-question-answering-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-question-answering-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2364', 'P4105', 'P3537', 'P2550'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-resources-and-evaluation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-resources-and-evaluation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1624', 'P2717', 'P3490', 'P1879', 'P626'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1424', 'P1831', 'P2152'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P651', 'P909'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-speech-and-multimodality-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-speech-and-multimodality-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3721', 'P5600'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1162', 'P2232'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3504', 'P5566'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'},\n",
       "    'poster-session-7_-theme_-reality-check-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'poster-session-7_-theme_-reality-check-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3730'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Poster Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'poster-session-7',\n",
       "   'name': 'Poster Session 7',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'RepL4NLP': {'display_name': 'W5 - The 8th Workshop on Representation Learning for NLP (RepL4NLP)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'RepL4NLP',\n",
       "   'name': 'W5 - The 8th Workshop on Representation Learning for NLP (RepL4NLP)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'RepL4NLP': {'anthology_venue_id': 'ACL',\n",
       "     'booklet_id': 'workshop_5',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Burcu Can',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Maximilian Mozes',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Samuel Cahyawijaya',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Naomi Saphra',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Nora Kassner',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Shauli Ravfogel',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Abhilasha Ravichander',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Chen Zhao',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': \"The 8th Workshop on Representation Learning for NLP aims to continue the success of the Repl4NLP workshop series, with the 1st Workshop on Representation Learning for NLP having received about 50 submissions and over 250 attendees - the second most attended collocated event at ACL'16 after WMT. The workshop was introduced as a synthesis of several years of independent *CL workshops focusing on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP. The workshop will take place in a hybrid setting, and, as in previous years, feature interdisciplinary keynotes, paper presentations, posters, as well as a panel discussion.\",\n",
       "     'end_time': None,\n",
       "     'id': 'RepL4NLP',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Harbour B',\n",
       "     'session': 'RepL4NLP',\n",
       "     'short_name': 'RepL4NLP',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/view/repl4nlp2023'}}},\n",
       "  'SICon': {'display_name': 'W18 - The 1st Workshop on Social Influence in Conversations (SICon)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'SICon',\n",
       "   'name': 'W18 - The 1st Workshop on Social Influence in Conversations (SICon)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'SICon': {'anthology_venue_id': 'SICon',\n",
       "     'booklet_id': 'workshop_18',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Kushal Chawla',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Weiyan Shi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Maximillian Chen',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Liang Qiu',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yu Li',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'James Hale',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Alexandros Papangelis',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Gale Lucas',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Zhou Yu',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': \"Social influence is the change in an individual's thoughts, feelings, attitudes, or behaviors that results from interaction with another individual or a group. For example, a buyer uses social influence skills to engage in trade-offs and build rapport when bargaining with a seller. A therapist uses social influence skills like persuasion to motivate a patient towards physical exercise. Social influence is a core function of human communication, and such scenarios are ubiquitous in everyday life, from negotiations to argumentation to behavioral interventions. Consequently, realistic human-machine conversations must reflect these social influence dynamics, making it essential to systematically model and understand them in dialogue research. This requires perspectives not only from NLP and AI research but also from game theory, emotion, communication, and psychology. \\\\\\\\newline We are excited to host the First Workshop on Social Influence in Conversations (SICon 2023). SICon 2023 will be a one-day hybrid event, co-located with ACL 2023. It would be the first venue that uniquely fosters a dedicated discussion on social influence within NLP while involving researchers from other disciplines such as affective computing and the social sciences. SICon 2023 features keynote talks, panel discussions, poster sessions, and lightning talks for accepted papers. We hope to bring together researchers and practitioners from a wide variety of disciplines to discuss important problems related to social influence, as well as share findings and recent advances. We encourage researchers of all stages and backgrounds to share their exciting work!\",\n",
       "     'end_time': None,\n",
       "     'id': 'SICon',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Harbour A',\n",
       "     'session': 'SICon',\n",
       "     'short_name': 'SICon',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/view/sicon-2023/home'}}},\n",
       "  'SIGMORPHON': {'display_name': 'W22 - The 20th Workshop on Computational Morphology and Phonology (SIGMORPHON)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'SIGMORPHON',\n",
       "   'name': 'W22 - The 20th Workshop on Computational Morphology and Phonology (SIGMORPHON)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'SIGMORPHON': {'anthology_venue_id': 'ACL',\n",
       "     'booklet_id': 'workshop_22',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Garrett Nicolai',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Eleanor Chodroff',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'ar ltekin',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Fred Mailhot',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'SIGMORPHON aims to bring together researchers interested in applying computational techniques to problems in morphology, phonology, and phonetics. Work that addresses orthographic issues is also welcome. Papers will be on substantial, original, and unpublished research on these topics, potentially including strong work in progress.',\n",
       "     'end_time': None,\n",
       "     'id': 'SIGMORPHON',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Dockside 3',\n",
       "     'session': 'SIGMORPHON',\n",
       "     'short_name': 'SIGMORPHON',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sigmorphon.github.io/workshops/2023/'}}},\n",
       "  'SemEval': {'display_name': 'W1 - The 17th International Workshop on Semantic Evaluation (SemEval)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'SemEval',\n",
       "   'name': 'W1 - The 17th International Workshop on Semantic Evaluation (SemEval)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'SemEval': {'anthology_venue_id': 'SemEval',\n",
       "     'booklet_id': 'workshop_1',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Ritesh Kumar',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Atul Kr. Ojha',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'A. Seza Doruz',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Giovanni Da San Martino',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Harish Tayyar Madabushi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The 17th edition of SemEval features 12 TASKS on a range of topics, including tasks on idiomaticy detection and embedding, sarcasm detection, multilingual news similarity, and linking mathematical symbols to their descriptions. Several tasks are multilingual, and others ask for multimodal approaches.',\n",
       "     'end_time': None,\n",
       "     'id': 'SemEval',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Queens Quay',\n",
       "     'session': 'SemEval',\n",
       "     'short_name': 'SemEval',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://semeval.github.io/SemEval2023/'}}},\n",
       "  'Session 1': {'display_name': 'Session 1',\n",
       "   'end_time': '2023-07-10T12:30:00-04:00',\n",
       "   'events': {'session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2325',\n",
       "      'P2462',\n",
       "      'P4252',\n",
       "      'P5837',\n",
       "      'P3403',\n",
       "      'P1897',\n",
       "      'P4456',\n",
       "      'P2343',\n",
       "      'P3579'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4075', 'T4263'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'session-1_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P355',\n",
       "      'P382',\n",
       "      'P705',\n",
       "      'P773',\n",
       "      'P809',\n",
       "      'P953',\n",
       "      'P1212',\n",
       "      'P1605',\n",
       "      'P1667',\n",
       "      'P1744',\n",
       "      'P2466',\n",
       "      'P2767',\n",
       "      'P2796',\n",
       "      'P3058',\n",
       "      'P3165',\n",
       "      'P3190',\n",
       "      'P3473',\n",
       "      'P4585',\n",
       "      'P4706',\n",
       "      'P4730',\n",
       "      'P5246',\n",
       "      'P5489',\n",
       "      'P3649',\n",
       "      'P1223',\n",
       "      'P2766'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-discourse-and-pragmatics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3308', 'P4069', 'P4086', 'P4295', 'P4772', 'P2383'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-ethics-and-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-ethics-and-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3955', 'P728', 'P2421', 'P3322', 'P4517', 'P5626'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-1_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-ethics-and-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2039',\n",
       "      'P2145',\n",
       "      'P2270',\n",
       "      'P2934',\n",
       "      'P3969',\n",
       "      'P4017',\n",
       "      'P4032'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-generation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-generation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4503'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Poster'},\n",
       "    'session-1_-generation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-generation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4601',\n",
       "      'P233',\n",
       "      'P1018',\n",
       "      'P1149',\n",
       "      'P1482',\n",
       "      'P2181',\n",
       "      'P2274',\n",
       "      'P2547',\n",
       "      'P3325',\n",
       "      'P3993',\n",
       "      'P5166',\n",
       "      'P5792',\n",
       "      'P1495',\n",
       "      'P4348',\n",
       "      'P720'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-information-extraction-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P176',\n",
       "      'P501',\n",
       "      'P568',\n",
       "      'P799',\n",
       "      'P1178',\n",
       "      'P1541',\n",
       "      'P1780',\n",
       "      'P2824',\n",
       "      'P3243',\n",
       "      'P4049',\n",
       "      'P4210',\n",
       "      'P4490',\n",
       "      'P4915',\n",
       "      'P2730',\n",
       "      'P4451',\n",
       "      'P2489',\n",
       "      'P3138'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P784', 'P986', 'P1857', 'P2125', 'P1520', 'P3066'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P399',\n",
       "      'P1322',\n",
       "      'P1636',\n",
       "      'P1786',\n",
       "      'P3035',\n",
       "      'P3410',\n",
       "      'P3636',\n",
       "      'P3860',\n",
       "      'P3907',\n",
       "      'P4811',\n",
       "      'P4867',\n",
       "      'P5058',\n",
       "      'P5090',\n",
       "      'P5289',\n",
       "      'P5577',\n",
       "      'P664',\n",
       "      'P2562',\n",
       "      'P5660',\n",
       "      'P4051',\n",
       "      'P3563'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P527',\n",
       "      'P608',\n",
       "      'P726',\n",
       "      'P907',\n",
       "      'P2484',\n",
       "      'P2492',\n",
       "      'P2718',\n",
       "      'P2907',\n",
       "      'P5335',\n",
       "      'P4158',\n",
       "      'P2533',\n",
       "      'P345',\n",
       "      'P1977',\n",
       "      'P3948'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-language-grounding-to-vision,-robotics-and-beyond-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-language-grounding-to-vision,-robotics-and-beyond-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D17'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics and Beyond (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-large-language-models-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-large-language-models-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3572', 'P5693', 'P4503', 'P547', 'P4286', 'P2222'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Oral'},\n",
       "    'session-1_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-large-language-models-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P495',\n",
       "      'P584',\n",
       "      'P590',\n",
       "      'P987',\n",
       "      'P1071',\n",
       "      'P1084',\n",
       "      'P1243',\n",
       "      'P1959',\n",
       "      'P2197',\n",
       "      'P2204',\n",
       "      'P2957',\n",
       "      'P3883',\n",
       "      'P3908',\n",
       "      'P4013',\n",
       "      'P5316',\n",
       "      'P5582',\n",
       "      'P4641',\n",
       "      'P4242',\n",
       "      'P795',\n",
       "      'P1583',\n",
       "      'P1714'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4728', 'P5658'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-machine-learning-for-nlp-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-machine-learning-for-nlp-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D118'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4371', 'T4773'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'session-1_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-machine-learning-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3649',\n",
       "      'P844',\n",
       "      'P958',\n",
       "      'P1258',\n",
       "      'P1277',\n",
       "      'P1452',\n",
       "      'P2592',\n",
       "      'P2749',\n",
       "      'P3281',\n",
       "      'P3340',\n",
       "      'P4956',\n",
       "      'P5243',\n",
       "      'P5575',\n",
       "      'P5618',\n",
       "      'P5619',\n",
       "      'P5657',\n",
       "      'P5777',\n",
       "      'P2243',\n",
       "      'P2289',\n",
       "      'P3859',\n",
       "      'P1645',\n",
       "      'P1297',\n",
       "      'P5464'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-machine-translation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P807',\n",
       "      'P1059',\n",
       "      'P1449',\n",
       "      'P2893',\n",
       "      'P3017',\n",
       "      'P4124',\n",
       "      'P4584',\n",
       "      'P2059',\n",
       "      'P612',\n",
       "      'P5625',\n",
       "      'P2272',\n",
       "      'P3463'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-multilingualism-and-cross-lingual-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-multilingualism-and-cross-lingual-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3533', 'P5798', 'P3991', 'C2208', 'P693', 'P1915'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2746', 'P3047', 'P5095', 'P5822', 'P1078', 'P4065'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-nlp-applications-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-nlp-applications-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D9', 'D25'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'NLP Applications (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-nlp-applications-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-nlp-applications-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2602', 'P2874', 'P3953', 'P3680', 'P2961', 'P5836'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Oral'},\n",
       "    'session-1_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-nlp-applications-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1180',\n",
       "      'P1411',\n",
       "      'P1890',\n",
       "      'P2460',\n",
       "      'P2722',\n",
       "      'P3486',\n",
       "      'P3765',\n",
       "      'P4268',\n",
       "      'P4557',\n",
       "      'P4718',\n",
       "      'P5048',\n",
       "      'P5723',\n",
       "      'P788',\n",
       "      'P5647',\n",
       "      'P44',\n",
       "      'P299',\n",
       "      'P5785',\n",
       "      'P604',\n",
       "      'P3326'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-question-answering-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-question-answering-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5482', 'P3216', 'P3014', 'P5722', 'P1022', 'P326'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Oral'},\n",
       "    'session-1_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-question-answering-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P186',\n",
       "      'P452',\n",
       "      'P985',\n",
       "      'P1916',\n",
       "      'P2630',\n",
       "      'P3353',\n",
       "      'P3698',\n",
       "      'P4435',\n",
       "      'P4681',\n",
       "      'P4827',\n",
       "      'P5097',\n",
       "      'P5790',\n",
       "      'P872'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-resources-and-evaluation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P707',\n",
       "      'P976',\n",
       "      'P1530',\n",
       "      'P2946',\n",
       "      'P3179',\n",
       "      'P3389',\n",
       "      'P3446',\n",
       "      'P3630',\n",
       "      'P4087',\n",
       "      'P4682',\n",
       "      'P5125',\n",
       "      'P5726',\n",
       "      'P3546',\n",
       "      'P2221'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-semantics_-lexical-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-semantics_-lexical-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P487', 'P1876', 'P2112', 'P1436'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T5043'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P116',\n",
       "      'P1168',\n",
       "      'P2240',\n",
       "      'P3633',\n",
       "      'P3847',\n",
       "      'P4899',\n",
       "      'P5112',\n",
       "      'P3705',\n",
       "      'P5579',\n",
       "      'P472',\n",
       "      'P5631',\n",
       "      'P5255',\n",
       "      'P549'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4059', 'T4405', 'T4407'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Poster'},\n",
       "    'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2538', 'P3011', 'P5573', 'P3342'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-speech-and-multimodality-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-speech-and-multimodality-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D55'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-speech-and-multimodality-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P548',\n",
       "      'P1465',\n",
       "      'P3304',\n",
       "      'P5193',\n",
       "      'P5601',\n",
       "      'P2691',\n",
       "      'P3005',\n",
       "      'P1464'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-student-research-workshop-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S2', 'S6', 'S14', 'S95', 'S133'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-summarization-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-summarization-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1772', 'P2582', 'P3313', 'P3664', 'P378'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4152', 'P5140', 'P5362', 'P4613', 'P3091'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-1_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T12:30:00-04:00',\n",
       "     'id': 'session-1_-theme_-reality-check-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P140',\n",
       "      'P1040',\n",
       "      'P2406',\n",
       "      'P2429',\n",
       "      'P2461',\n",
       "      'P2772',\n",
       "      'P3180',\n",
       "      'P4423',\n",
       "      'P2876'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 1',\n",
       "     'start_time': '2023-07-10T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Virtual Poster'}},\n",
       "   'id': 'session-1',\n",
       "   'name': 'Session 1',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 2': {'display_name': 'Session 2',\n",
       "   'end_time': '2023-07-10T15:30:00-04:00',\n",
       "   'events': {'session-2_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4401'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'session-2_-language-grounding-to-vision,-robotics,-and-beyond-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-language-grounding-to-vision,-robotics,-and-beyond-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P908', 'P2123', 'P5738', 'P786', 'P3738', 'P4186'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Oral'},\n",
       "    'session-2_-machine-learning-for-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-machine-learning-for-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2565', 'P2522', 'P2349', 'P2404', 'P5570', 'P295'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-2_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4497'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'session-2_-machine-translation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-machine-translation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1126', 'T4591', 'P4709', 'P3890', 'P1185', 'P2258'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Oral'},\n",
       "    'session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1080', 'P2247', 'P3576', 'P3327', 'P2768', 'C2265'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Oral'},\n",
       "    'session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-syntax_-tagging,-chunking,-and-parsing-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5256', 'P1854', 'P3454', 'P989', 'C2281', 'P3614'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Oral'},\n",
       "    'session-2_-theme_-reality-check-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T15:30:00-04:00',\n",
       "     'id': 'session-2_-theme_-reality-check-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2506', 'P2131', 'P3843', 'P5113', 'P3227', 'P834'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 2',\n",
       "     'start_time': '2023-07-10T14:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Oral'}},\n",
       "   'id': 'session-2',\n",
       "   'name': 'Session 2',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T14:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 3': {'display_name': 'Session 3',\n",
       "   'end_time': '2023-07-11T10:15:00-04:00',\n",
       "   'events': {'session-3_-computational-social-science-and-cultural-analytics-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:15:00-04:00',\n",
       "     'id': 'session-3_-computational-social-science-and-cultural-analytics-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P293', 'P2244', 'P4188', 'P4923', 'P5071'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-dialogue-and-interactive-systems-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-dialogue-and-interactive-systems-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1148', 'P481', 'P3945', 'P133', 'P703', 'P2587'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4803'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'session-3_-industry-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-industry-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['I16', 'I128', 'I131', 'I173', 'I81', 'I119'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Industry',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-interpretability-and-analysis-of-models-for-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-interpretability-and-analysis-of-models-for-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P895', 'P1245', 'P4055', 'P2622', 'P4340', 'P3999'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-large-language-models-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-large-language-models-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3938', 'P854', 'P868', 'P1627', 'P1924', 'P2336'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-linguistic-diversity-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-linguistic-diversity-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4208', 'P3832', 'P1365', 'P377', 'P4315', 'P921'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity',\n",
       "     'type': 'Oral'},\n",
       "    'session-3_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4139'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Poster'},\n",
       "    'session-3_-student-research-workshop-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-student-research-workshop-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S24',\n",
       "      'S28',\n",
       "      'S29',\n",
       "      'S36',\n",
       "      'S48',\n",
       "      'S56',\n",
       "      'S58',\n",
       "      'S82',\n",
       "      'S113',\n",
       "      'S20'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Poster'},\n",
       "    'session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T10:30:00-04:00',\n",
       "     'id': 'session-3_-syntax_-tagging,-chunking,-and-parsing-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4637'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 3',\n",
       "     'start_time': '2023-07-11T09:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'session-3',\n",
       "   'name': 'Session 3',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 4': {'display_name': 'Session 4',\n",
       "   'end_time': '2023-07-11T11:45:00-04:00',\n",
       "   'events': {'session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1653', 'P4130', 'P4135', 'P4500', 'P4968', 'P5024'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4113'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4647',\n",
       "      'P21',\n",
       "      'P1102',\n",
       "      'P1213',\n",
       "      'P1504',\n",
       "      'P1513',\n",
       "      'P2057',\n",
       "      'P2576',\n",
       "      'P2769',\n",
       "      'P2797',\n",
       "      'P2860',\n",
       "      'P4985',\n",
       "      'P5254',\n",
       "      'P5352',\n",
       "      'P5831',\n",
       "      'P5768',\n",
       "      'P324',\n",
       "      'P3126',\n",
       "      'P3788',\n",
       "      'P1306',\n",
       "      'P5856',\n",
       "      'P896'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-discourse-and-pragmatics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-discourse-and-pragmatics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4589'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-discourse-and-pragmatics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3940', 'P4037', 'P4744', 'P4813', 'P886'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-ethics-and-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1674', 'P2024', 'P3390', 'P4914'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-generation-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-generation-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D32', 'D119'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Generation (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-generation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-generation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P432',\n",
       "      'P437',\n",
       "      'P1228',\n",
       "      'P1312',\n",
       "      'P1342',\n",
       "      'P1588',\n",
       "      'P3377',\n",
       "      'P3458',\n",
       "      'P5206',\n",
       "      'P5680',\n",
       "      'P424',\n",
       "      'P1810'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-industry-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-industry-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['I42',\n",
       "      'I148',\n",
       "      'I8',\n",
       "      'I30',\n",
       "      'I54',\n",
       "      'I60',\n",
       "      'I29',\n",
       "      'I13',\n",
       "      'I36',\n",
       "      'I46',\n",
       "      'I92',\n",
       "      'I107',\n",
       "      'I227',\n",
       "      'I15',\n",
       "      'I18',\n",
       "      'I109',\n",
       "      'I191',\n",
       "      'I43',\n",
       "      'I104',\n",
       "      'I120',\n",
       "      'I96',\n",
       "      'I141',\n",
       "      'I75',\n",
       "      'I83',\n",
       "      'I124',\n",
       "      'I78'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Industry',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-information-extraction-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-information-extraction-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3863'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-information-extraction-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P23',\n",
       "      'P471',\n",
       "      'P581',\n",
       "      'P598',\n",
       "      'P2422',\n",
       "      'P3024',\n",
       "      'P3147',\n",
       "      'P3430',\n",
       "      'P3541',\n",
       "      'P3780',\n",
       "      'P4182',\n",
       "      'P4519',\n",
       "      'P5017',\n",
       "      'P5805',\n",
       "      'P3497',\n",
       "      'P5589',\n",
       "      'P2826',\n",
       "      'P5791',\n",
       "      'P1550',\n",
       "      'P2480',\n",
       "      'P982',\n",
       "      'P2596',\n",
       "      'P3750'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4552', 'P1721'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-interpretability-and-analysis-of-models-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4019'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P650',\n",
       "      'P1195',\n",
       "      'P1509',\n",
       "      'P2127',\n",
       "      'P2866',\n",
       "      'P3512',\n",
       "      'P5065',\n",
       "      'P5653',\n",
       "      'P5666',\n",
       "      'P5744',\n",
       "      'P1284',\n",
       "      'P5667',\n",
       "      'P5382',\n",
       "      'P2700',\n",
       "      'P3939'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T11:45:00-04:00',\n",
       "     'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3022', 'P3421', 'P5784'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Oral'},\n",
       "    'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4447'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P25',\n",
       "      'P236',\n",
       "      'P1067',\n",
       "      'P1575',\n",
       "      'P2391',\n",
       "      'P2715',\n",
       "      'P3040',\n",
       "      'P3659',\n",
       "      'P4140',\n",
       "      'P4466',\n",
       "      'P5758',\n",
       "      'P4689',\n",
       "      'P1593'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-large-language-models-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-large-language-models-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P367', 'P3183', 'P2262', 'P3259', 'P3052', 'P4993'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Oral'},\n",
       "    'session-4_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-large-language-models-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P273',\n",
       "      'P645',\n",
       "      'P1278',\n",
       "      'P1402',\n",
       "      'P2006',\n",
       "      'P2086',\n",
       "      'P2388',\n",
       "      'P2536',\n",
       "      'P3356',\n",
       "      'P3655',\n",
       "      'P4524',\n",
       "      'P3229',\n",
       "      'P1880',\n",
       "      'P1834',\n",
       "      'P415',\n",
       "      'P5141'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-linguistic-diversity-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-linguistic-diversity-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5830', 'P2333', 'P751'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2018'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-machine-learning-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P187',\n",
       "      'P258',\n",
       "      'P331',\n",
       "      'P348',\n",
       "      'P620',\n",
       "      'P813',\n",
       "      'P933',\n",
       "      'P1074',\n",
       "      'P1843',\n",
       "      'P1856',\n",
       "      'P1937',\n",
       "      'P2048',\n",
       "      'P2098',\n",
       "      'P2311',\n",
       "      'P2786',\n",
       "      'P2887',\n",
       "      'P3686',\n",
       "      'P3961',\n",
       "      'P4371',\n",
       "      'P4442',\n",
       "      'P5109',\n",
       "      'P5603',\n",
       "      'P5611',\n",
       "      'P3287'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-machine-translation-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-machine-translation-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D46'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Translation (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-machine-translation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P832',\n",
       "      'P1348',\n",
       "      'P1829',\n",
       "      'P1934',\n",
       "      'P2848',\n",
       "      'P3043',\n",
       "      'P3152',\n",
       "      'P5811',\n",
       "      'P276',\n",
       "      'P1922',\n",
       "      'P3841',\n",
       "      'P157',\n",
       "      'P2439'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P245',\n",
       "      'P938',\n",
       "      'P1143',\n",
       "      'P1303',\n",
       "      'P2223',\n",
       "      'P2629',\n",
       "      'P2952',\n",
       "      'P4497',\n",
       "      'P5607',\n",
       "      'P2634'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-nlp-applications-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-nlp-applications-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D105'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'NLP Applications (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-nlp-applications-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P998',\n",
       "      'P1500',\n",
       "      'P1623',\n",
       "      'P1752',\n",
       "      'P1798',\n",
       "      'P2178',\n",
       "      'P2664',\n",
       "      'P3114',\n",
       "      'P3242',\n",
       "      'P3388',\n",
       "      'P3944',\n",
       "      'P4277',\n",
       "      'P5041',\n",
       "      'P5815',\n",
       "      'P5818',\n",
       "      'P4746',\n",
       "      'P3625',\n",
       "      'P4408',\n",
       "      'P5188',\n",
       "      'P2884',\n",
       "      'P2868',\n",
       "      'P1697'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-question-answering-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-question-answering-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D132'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Question Answering (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-question-answering-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4433',\n",
       "      'P1325',\n",
       "      'P1362',\n",
       "      'P1847',\n",
       "      'P1970',\n",
       "      'P2133',\n",
       "      'P2419',\n",
       "      'P3418',\n",
       "      'P3540',\n",
       "      'P4228',\n",
       "      'P4660',\n",
       "      'P5345',\n",
       "      'P5567',\n",
       "      'P5591',\n",
       "      'P5691',\n",
       "      'P1985',\n",
       "      'P3397',\n",
       "      'P3593'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-resources-and-evaluation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-resources-and-evaluation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P778', 'P1626', 'P2229', 'P2317', 'P2577', 'P2678'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Oral'},\n",
       "    'session-4_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-resources-and-evaluation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P300',\n",
       "      'P563',\n",
       "      'P1027',\n",
       "      'P3059',\n",
       "      'P3371',\n",
       "      'P3880',\n",
       "      'P3954',\n",
       "      'P4021',\n",
       "      'P4336',\n",
       "      'P5079',\n",
       "      'P3615'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4929'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P106',\n",
       "      'P1190',\n",
       "      'P1453',\n",
       "      'P1471',\n",
       "      'P1638',\n",
       "      'P1691',\n",
       "      'P1694',\n",
       "      'P2660',\n",
       "      'P2712',\n",
       "      'P2830',\n",
       "      'P5171',\n",
       "      'P1475',\n",
       "      'P5031',\n",
       "      'P2885',\n",
       "      'P353',\n",
       "      'P228'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-speech-and-multimodality-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P553', 'P2134', 'P3866', 'P3246'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-student-research-workshop-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:00:00-04:00',\n",
       "     'id': 'session-4_-student-research-workshop-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S52', 'S79', 'S123', 'S129'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Oral'},\n",
       "    'session-4_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-student-research-workshop-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S5', 'S70', 'S124', 'S92', 'S99', 'S144'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-summarization-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-summarization-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P328', 'P968', 'P970', 'P1728', 'P3833', 'T4769'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Oral'},\n",
       "    'session-4_-summarization-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-summarization-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P855',\n",
       "      'P2136',\n",
       "      'P2987',\n",
       "      'P4225',\n",
       "      'P4230',\n",
       "      'P4448',\n",
       "      'P4627',\n",
       "      'P85',\n",
       "      'P1191'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2993', 'P4008'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-4_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T12:30:00-04:00',\n",
       "     'id': 'session-4_-theme_-reality-check-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1236',\n",
       "      'P1291',\n",
       "      'P1391',\n",
       "      'P2637',\n",
       "      'P3188',\n",
       "      'P4619',\n",
       "      'P661',\n",
       "      'P3982'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 4',\n",
       "     'start_time': '2023-07-11T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Virtual Poster'}},\n",
       "   'id': 'session-4',\n",
       "   'name': 'Session 4',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 5': {'display_name': 'Session 5',\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'events': {'session-5_-generation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-generation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['C2217', 'T4607', 'P2055', 'P2165', 'P2192', 'P3438'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Oral'},\n",
       "    'session-5_-industry-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-industry-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['I186',\n",
       "      'I27',\n",
       "      'I41',\n",
       "      'I77',\n",
       "      'I135',\n",
       "      'I45',\n",
       "      'I90',\n",
       "      'I134',\n",
       "      'I156',\n",
       "      'I94',\n",
       "      'I188',\n",
       "      'I69',\n",
       "      'I110',\n",
       "      'I201',\n",
       "      'I140',\n",
       "      'I55',\n",
       "      'I3',\n",
       "      'I17',\n",
       "      'I146',\n",
       "      'I187',\n",
       "      'I196',\n",
       "      'I205',\n",
       "      'I222',\n",
       "      'I14',\n",
       "      'I66',\n",
       "      'I125',\n",
       "      'I171',\n",
       "      'I208',\n",
       "      'I215',\n",
       "      'I93',\n",
       "      'I100',\n",
       "      'I102',\n",
       "      'I47',\n",
       "      'I65',\n",
       "      'I197',\n",
       "      'I213',\n",
       "      'I225',\n",
       "      'I32'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Industry',\n",
       "     'type': 'Poster'},\n",
       "    'session-5_-information-extraction-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-information-extraction-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4372', 'P4328', 'P2130', 'P803', 'P677', 'P4207'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Oral'},\n",
       "    'session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:15:00-04:00',\n",
       "     'id': 'session-5_-interpretability-and-analysis-of-models-for-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3332', 'P4690', 'P178', 'P2373'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:30:00-04:00',\n",
       "     'id': 'session-5_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4519', 'P3175', 'P4116', 'P4171', 'P2186'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Oral'},\n",
       "    'session-5_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4305'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'session-5_-semantics_-lexical-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-semantics_-lexical-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2114', 'P2116', 'P2023', 'P2367', 'P3876', 'P3791'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Oral'},\n",
       "    'session-5_-student-research-workshop-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-11T17:45:00-04:00',\n",
       "     'id': 'session-5_-student-research-workshop-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S3', 'S43', 'S46', 'S66', 'S71'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 5',\n",
       "     'start_time': '2023-07-11T16:15:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'session-5',\n",
       "   'name': 'Session 5',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 6': {'display_name': 'Session 6',\n",
       "   'end_time': '2023-07-12T10:30:00-04:00',\n",
       "   'events': {'session-6_-dialogue-and-interactive-systems-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-dialogue-and-interactive-systems-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3887'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-industry-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-industry-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['I189', 'I111', 'I207', 'I112', 'I139', 'I199'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Industry',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-information-retrieval-and-text-mining-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-information-retrieval-and-text-mining-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4285'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4777'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-machine-learning-for-nlp-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-machine-learning-for-nlp-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P624', 'P3267', 'P3374', 'P3542', 'P4769'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-machine-translation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-machine-translation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1433', 'P5204', 'P2132', 'P899', 'P3101'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:15:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-machine-translation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-machine-translation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['C2147'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-nlp-applications-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-nlp-applications-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3245', 'P619', 'P1265', 'P5846', 'P3924', 'P2036'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-phonology,-morphology,-and-word-segmentation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T09:45:00-04:00',\n",
       "     'id': 'session-6_-phonology,-morphology,-and-word-segmentation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4139', 'P2635', 'P2943'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-phonology,-morphology,-and-word-segmentation-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-phonology,-morphology,-and-word-segmentation-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4243', 'T4721'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-semantics_-lexical-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-semantics_-lexical-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3791'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Poster'},\n",
       "    'session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T3895', 'T3967', 'P1940', 'P2953', 'P3197', 'C2092'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Oral'},\n",
       "    'session-6_-student-research-workshop-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T10:30:00-04:00',\n",
       "     'id': 'session-6_-student-research-workshop-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S32',\n",
       "      'S38',\n",
       "      'S40',\n",
       "      'S41',\n",
       "      'S47',\n",
       "      'S57',\n",
       "      'S64',\n",
       "      'S127',\n",
       "      'S139',\n",
       "      'S145'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 6',\n",
       "     'start_time': '2023-07-12T09:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Poster'}},\n",
       "   'id': 'session-6',\n",
       "   'name': 'Session 6',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Session 7': {'display_name': 'Session 7',\n",
       "   'end_time': '2023-07-12T12:30:00-04:00',\n",
       "   'events': {'session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-computational-social-science-and-cultural-analytics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P606',\n",
       "      'P1047',\n",
       "      'P1637',\n",
       "      'P3493',\n",
       "      'P3917',\n",
       "      'P4053',\n",
       "      'P4422',\n",
       "      'P5374',\n",
       "      'P1793',\n",
       "      'P5563',\n",
       "      'P3923',\n",
       "      'P5059'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Computational Social Science and Cultural Analytics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-dialogue-and-interactive-systems-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-dialogue-and-interactive-systems-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P642',\n",
       "      'P658',\n",
       "      'P845',\n",
       "      'P1090',\n",
       "      'P1206',\n",
       "      'P2519',\n",
       "      'P2651',\n",
       "      'P3275',\n",
       "      'P3626',\n",
       "      'P3642',\n",
       "      'P5756',\n",
       "      'P5828',\n",
       "      'P2251',\n",
       "      'P5708',\n",
       "      'P3057',\n",
       "      'P427'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Dialogue and Interactive Systems',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-discourse-and-pragmatics-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-discourse-and-pragmatics-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4419', 'T4197', 'P4079', 'P883', 'P3448', 'P792'],\n",
       "     'room': 'Pier 2&3',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-discourse-and-pragmatics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-discourse-and-pragmatics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5261'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Discourse and Pragmatics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-ethics-and-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-ethics-and-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1333',\n",
       "      'P2298',\n",
       "      'P2465',\n",
       "      'P2703',\n",
       "      'P2788',\n",
       "      'P3687',\n",
       "      'P4844',\n",
       "      'P4937',\n",
       "      'P5741',\n",
       "      'P3629',\n",
       "      'P3150',\n",
       "      'P2238',\n",
       "      'P3983'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Ethics and NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-generation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-generation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P302', 'P250', 'P5614'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:45:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-generation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-generation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P110',\n",
       "      'P456',\n",
       "      'P511',\n",
       "      'P558',\n",
       "      'P1355',\n",
       "      'P1538',\n",
       "      'P2297',\n",
       "      'P3082',\n",
       "      'P4494',\n",
       "      'P4950',\n",
       "      'P4974',\n",
       "      'P3794'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Generation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-information-extraction-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T11:45:00-04:00',\n",
       "     'id': 'session-7_-information-extraction-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P5043', 'P430', 'P1658'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-information-extraction-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-information-extraction-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P68',\n",
       "      'P411',\n",
       "      'P474',\n",
       "      'P585',\n",
       "      'P1595',\n",
       "      'P2067',\n",
       "      'P3447',\n",
       "      'P4181',\n",
       "      'P4691',\n",
       "      'P4824',\n",
       "      'P5026',\n",
       "      'P5669',\n",
       "      'P5809',\n",
       "      'P5694',\n",
       "      'P5168'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Extraction',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-information-retrieval-and-text-mining-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-information-retrieval-and-text-mining-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P971', 'P712', 'P4221', 'P1299', 'P4717', 'P923'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-information-retrieval-and-text-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-information-retrieval-and-text-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P406',\n",
       "      'P2105',\n",
       "      'P2269',\n",
       "      'P3153',\n",
       "      'P4509',\n",
       "      'P5745',\n",
       "      'P5574',\n",
       "      'P2923',\n",
       "      'P1580'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Information Retrieval and Text Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-interpretability-and-analysis-of-models-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P426',\n",
       "      'P1715',\n",
       "      'P1996',\n",
       "      'P2320',\n",
       "      'P2396',\n",
       "      'P2411',\n",
       "      'P4012',\n",
       "      'P4074',\n",
       "      'P4220',\n",
       "      'P4470',\n",
       "      'P5617',\n",
       "      'P5627',\n",
       "      'P5749',\n",
       "      'P5793',\n",
       "      'P5841',\n",
       "      'P3878'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Interpretability and Analysis of Models for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-language-grounding-to-vision,-robotics,-and-beyond-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P514', 'P582', 'P991', 'P2437', 'P2512', 'P4820', 'P2529'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Language Grounding to Vision, Robotics, and Beyond',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-large-language-models-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-large-language-models-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P518',\n",
       "      'P571',\n",
       "      'P1430',\n",
       "      'P2706',\n",
       "      'P2877',\n",
       "      'P3042',\n",
       "      'P3475',\n",
       "      'P3684',\n",
       "      'P4132',\n",
       "      'P4384',\n",
       "      'P4715',\n",
       "      'P4872',\n",
       "      'P5135',\n",
       "      'P5616',\n",
       "      'P5847',\n",
       "      'P4916',\n",
       "      'P4543',\n",
       "      'P3294'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Large Language Models',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-linguistic-diversity-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-linguistic-diversity-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D14', 'D30'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-linguistic-diversity-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-linguistic-diversity-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P4395'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Linguistic Diversity',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-linguistic-theories,-cognitive-modeling,-and-psycholinguistics-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P50', 'P657', 'P2021', 'P5698', 'P5739', 'P2891'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-machine-learning-for-nlp-(demo)-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-machine-learning-for-nlp-(demo)-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['D117'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP (demo)',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-machine-learning-for-nlp-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-machine-learning-for-nlp-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4291'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Poster'},\n",
       "    'session-7_-machine-learning-for-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-machine-learning-for-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['C2121',\n",
       "      'P129',\n",
       "      'P274',\n",
       "      'P486',\n",
       "      'P941',\n",
       "      'P1015',\n",
       "      'P1065',\n",
       "      'P1166',\n",
       "      'P1266',\n",
       "      'P1545',\n",
       "      'P1823',\n",
       "      'P1833',\n",
       "      'P1939',\n",
       "      'P2154',\n",
       "      'P2526',\n",
       "      'P2920',\n",
       "      'P5388',\n",
       "      'P5810',\n",
       "      'P1173',\n",
       "      'P1459',\n",
       "      'P882',\n",
       "      'P2609',\n",
       "      'P2080',\n",
       "      'P4536',\n",
       "      'P3457',\n",
       "      'P5624'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Learning for NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-machine-translation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-machine-translation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P34',\n",
       "      'P304',\n",
       "      'P1463',\n",
       "      'P2071',\n",
       "      'P2873',\n",
       "      'P4782',\n",
       "      'P5655',\n",
       "      'P1676',\n",
       "      'P2728',\n",
       "      'P2835',\n",
       "      'P279',\n",
       "      'P4966',\n",
       "      'P4153'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Machine Translation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-multilingualism-and-cross-lingual-nlp-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P194',\n",
       "      'P1936',\n",
       "      'P2106',\n",
       "      'P2288',\n",
       "      'P2331',\n",
       "      'P2416',\n",
       "      'P3121',\n",
       "      'P3596',\n",
       "      'P3667',\n",
       "      'P3845',\n",
       "      'P5357',\n",
       "      'P1855',\n",
       "      'P1075',\n",
       "      'P1315'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Multilingualism and Cross-Lingual NLP',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-nlp-applications-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-nlp-applications-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P541',\n",
       "      'P589',\n",
       "      'P601',\n",
       "      'P1154',\n",
       "      'P1227',\n",
       "      'P1678',\n",
       "      'P2037',\n",
       "      'P2092',\n",
       "      'P2187',\n",
       "      'P2354',\n",
       "      'P2507',\n",
       "      'P3836',\n",
       "      'P4341',\n",
       "      'P4590',\n",
       "      'P5015',\n",
       "      'P5167',\n",
       "      'P5486',\n",
       "      'P5652',\n",
       "      'P420',\n",
       "      'P4674',\n",
       "      'P52'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'NLP Applications',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-phonology,-morphology,-and-word-segmentation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1621', 'P4724'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Phonology, Morphology, and Word Segmentation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-question-answering-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-question-answering-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P477',\n",
       "      'P747',\n",
       "      'P1056',\n",
       "      'P1398',\n",
       "      'P1925',\n",
       "      'P2046',\n",
       "      'P4704',\n",
       "      'P5150',\n",
       "      'P5683',\n",
       "      'P1069',\n",
       "      'P1634',\n",
       "      'P3072',\n",
       "      'P4722',\n",
       "      'P1952'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Question Answering',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-resources-and-evaluation-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-resources-and-evaluation-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3145', 'P3209', 'P3601', 'P4526', 'P4776', 'P4908'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-resources-and-evaluation-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-resources-and-evaluation-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P433',\n",
       "      'P596',\n",
       "      'P736',\n",
       "      'P2407',\n",
       "      'P2645',\n",
       "      'P3184',\n",
       "      'P3773',\n",
       "      'P3868',\n",
       "      'P4028',\n",
       "      'P4433',\n",
       "      'P4810',\n",
       "      'P5609',\n",
       "      'P5644',\n",
       "      'P5679',\n",
       "      'P3107',\n",
       "      'P1378',\n",
       "      'P4253',\n",
       "      'P789',\n",
       "      'P3824'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Resources and Evaluation',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-semantics_-lexical-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-semantics_-lexical-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1662', 'P2095', 'P3970', 'P5670', 'P1142'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Semantics: Lexical',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4253'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Poster'},\n",
       "    'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-semantics_-sentence-level-semantics,-textual-inference,-and-other-areas-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4523', 'P910', 'P5325', 'P4387'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-sentiment-analysis,-stylistic-analysis,-and-argument-mining-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P701', 'P1670', 'P1816', 'P739'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-speech-and-multimodality-(oral)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-speech-and-multimodality-(oral)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3291', 'P652', 'P5725', 'P2971', 'P532', 'P3391'],\n",
       "     'room': 'Pier 4&5',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Oral'},\n",
       "    'session-7_-speech-and-multimodality-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-speech-and-multimodality-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P555', 'P1270', 'P2393', 'P3412', 'P4679', 'P5404'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Speech and Multimodality',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-student-research-workshop-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-student-research-workshop-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S12', 'S21', 'S25', 'S34', 'S50', 'S98', 'S122'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Poster'},\n",
       "    'session-7_-student-research-workshop-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-student-research-workshop-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['S19', 'S72', 'S94'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Student Research Workshop',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-summarization-(poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-summarization-(poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['T4475'],\n",
       "     'room': \"Frontenac Ballroom and Queen's Quay\",\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Poster'},\n",
       "    'session-7_-summarization-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-summarization-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2321',\n",
       "      'P2992',\n",
       "      'P3234',\n",
       "      'P3774',\n",
       "      'P4634',\n",
       "      'P5596',\n",
       "      'P2433'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Summarization',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-syntax_-tagging,-chunking,-and-parsing-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P2423', 'P3985', 'P202', 'P1498', 'P3482'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Syntax: Tagging, Chunking, and Parsing',\n",
       "     'type': 'Virtual Poster'},\n",
       "    'session-7_-theme_-reality-check-(virtual-poster)': {'chairs': [],\n",
       "     'end_time': '2023-07-12T12:30:00-04:00',\n",
       "     'id': 'session-7_-theme_-reality-check-(virtual-poster)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P3225',\n",
       "      'P3472',\n",
       "      'P3557',\n",
       "      'P3926',\n",
       "      'P4064',\n",
       "      'P4544',\n",
       "      'P4675',\n",
       "      'P5383',\n",
       "      'P5397',\n",
       "      'P3734',\n",
       "      'P395'],\n",
       "     'room': 'Pier 7&8',\n",
       "     'session': 'Session 7',\n",
       "     'start_time': '2023-07-12T11:00:00-04:00',\n",
       "     'track': 'Theme: Reality Check',\n",
       "     'type': 'Virtual Poster'}},\n",
       "   'id': 'session-7',\n",
       "   'name': 'Session 7',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-12T11:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'Spotlight Session': {'display_name': 'Spotlight Session',\n",
       "   'end_time': '2023-07-10T21:00:00-04:00',\n",
       "   'events': {'spotlight-session_-spotlight---metropolitan-centre-(spotlight)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T21:00:00-04:00',\n",
       "     'id': 'spotlight-session_-spotlight---metropolitan-centre-(spotlight)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1065',\n",
       "      'P1258',\n",
       "      'P1277',\n",
       "      'P1545',\n",
       "      'P187',\n",
       "      'P2048',\n",
       "      'P2243',\n",
       "      'P2289',\n",
       "      'P2311',\n",
       "      'P2526',\n",
       "      'P274',\n",
       "      'P2749',\n",
       "      'P2786',\n",
       "      'P2887',\n",
       "      'P331',\n",
       "      'P3340',\n",
       "      'P3961',\n",
       "      'P4371',\n",
       "      'P4956',\n",
       "      'P941',\n",
       "      'P1084',\n",
       "      'P1278',\n",
       "      'P1402',\n",
       "      'P1801',\n",
       "      'P1959',\n",
       "      'P2197',\n",
       "      'P2204',\n",
       "      'P2706',\n",
       "      'P2957',\n",
       "      'P3356',\n",
       "      'P3655',\n",
       "      'P3796',\n",
       "      'P4132',\n",
       "      'P4384',\n",
       "      'P4524',\n",
       "      'P4543',\n",
       "      'P5135',\n",
       "      'P571',\n",
       "      'P590',\n",
       "      'P1018',\n",
       "      'P1312',\n",
       "      'P1342',\n",
       "      'P1355',\n",
       "      'P1538',\n",
       "      'P2274',\n",
       "      'P2297',\n",
       "      'P3082',\n",
       "      'P3794',\n",
       "      'P3993',\n",
       "      'P1772',\n",
       "      'P2321',\n",
       "      'P3234',\n",
       "      'P4225',\n",
       "      'P4230',\n",
       "      'P4448',\n",
       "      'P4627',\n",
       "      'P4634',\n",
       "      'P1348',\n",
       "      'P1676',\n",
       "      'P1934',\n",
       "      'P2893',\n",
       "      'P3017',\n",
       "      'P4124',\n",
       "      'P5811',\n",
       "      'P23',\n",
       "      'P2422',\n",
       "      'P2824',\n",
       "      'P3447',\n",
       "      'P3780',\n",
       "      'P4519',\n",
       "      'P471',\n",
       "      'P5809',\n",
       "      'P1067',\n",
       "      'P2437',\n",
       "      'P25',\n",
       "      'P2718',\n",
       "      'P2907',\n",
       "      'P3659',\n",
       "      'P3948',\n",
       "      'P4158',\n",
       "      'P4466',\n",
       "      'P726',\n",
       "      'P1270',\n",
       "      'P2393',\n",
       "      'P5193',\n",
       "      'P548',\n",
       "      'P553'],\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 'Spotlight Session',\n",
       "     'start_time': '2023-07-10T19:00:00-04:00',\n",
       "     'track': 'Spotlight - Metropolitan Centre',\n",
       "     'type': 'Spotlight'},\n",
       "    'spotlight-session_-spotlight---metropolitan-east-(spotlight)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T21:00:00-04:00',\n",
       "     'id': 'spotlight-session_-spotlight---metropolitan-east-(spotlight)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1530',\n",
       "      'P300',\n",
       "      'P3059',\n",
       "      'P3371',\n",
       "      'P3615',\n",
       "      'P3630',\n",
       "      'P3868',\n",
       "      'P3880',\n",
       "      'P3954',\n",
       "      'P4021',\n",
       "      'P4028',\n",
       "      'P4087',\n",
       "      'P433',\n",
       "      'P4336',\n",
       "      'P4682',\n",
       "      'P4810',\n",
       "      'P5125',\n",
       "      'P707',\n",
       "      'P736',\n",
       "      'P1520',\n",
       "      'P1857',\n",
       "      'P2105',\n",
       "      'P2269',\n",
       "      'P3153',\n",
       "      'P406',\n",
       "      'P4509',\n",
       "      'P4552',\n",
       "      'P1798',\n",
       "      'P1890',\n",
       "      'P2092',\n",
       "      'P2187',\n",
       "      'P3242',\n",
       "      'P3765',\n",
       "      'P4277',\n",
       "      'P4341',\n",
       "      'P4674',\n",
       "      'P5015',\n",
       "      'P5188',\n",
       "      'P5486',\n",
       "      'P788',\n",
       "      'P2057',\n",
       "      'P2251',\n",
       "      'P2466',\n",
       "      'P2651',\n",
       "      'P2767',\n",
       "      'P2796',\n",
       "      'P2860',\n",
       "      'P3165',\n",
       "      'P3190',\n",
       "      'P3473',\n",
       "      'P382',\n",
       "      'P4706',\n",
       "      'P5756',\n",
       "      'P5831',\n",
       "      'P705',\n",
       "      'P1325',\n",
       "      'P1362',\n",
       "      'P1398',\n",
       "      'P1847',\n",
       "      'P2133',\n",
       "      'P2419',\n",
       "      'P4228',\n",
       "      'P4435',\n",
       "      'P452',\n",
       "      'P4681',\n",
       "      'P4722',\n",
       "      'P5150',\n",
       "      'P5567',\n",
       "      'P985',\n",
       "      'P1670',\n",
       "      'P1691',\n",
       "      'P1694',\n",
       "      'P2712',\n",
       "      'P2830',\n",
       "      'P3011',\n",
       "      'P2462',\n",
       "      'P3493',\n",
       "      'P3917',\n",
       "      'P4053',\n",
       "      'P4135',\n",
       "      'P4252',\n",
       "      'P4422',\n",
       "      'P4500',\n",
       "      'P5374',\n",
       "      'P606'],\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 'Spotlight Session',\n",
       "     'start_time': '2023-07-10T19:00:00-04:00',\n",
       "     'track': 'Spotlight - Metropolitan East',\n",
       "     'type': 'Spotlight'},\n",
       "    'spotlight-session_-spotlight---metropolitan-west-(spotlight)': {'chairs': [],\n",
       "     'end_time': '2023-07-10T21:00:00-04:00',\n",
       "     'id': 'spotlight-session_-spotlight---metropolitan-west-(spotlight)',\n",
       "     'link': None,\n",
       "     'paper_ids': ['P1621',\n",
       "      'P4724',\n",
       "      'P3985',\n",
       "      'P4152',\n",
       "      'P1142',\n",
       "      'P1436',\n",
       "      'P1662',\n",
       "      'P1876',\n",
       "      'P2095',\n",
       "      'P2112',\n",
       "      'P5670',\n",
       "      'P1168',\n",
       "      'P2240',\n",
       "      'P3633',\n",
       "      'P5325',\n",
       "      'P2021',\n",
       "      'P4728',\n",
       "      'P5739',\n",
       "      'P657',\n",
       "      'P2383',\n",
       "      'P2383',\n",
       "      'P3308',\n",
       "      'P3940',\n",
       "      'P4069',\n",
       "      'P4086',\n",
       "      'P4744',\n",
       "      'P4813',\n",
       "      'P2333',\n",
       "      'P4395',\n",
       "      'P5830',\n",
       "      'P1303',\n",
       "      'P1936',\n",
       "      'P194',\n",
       "      'P2288',\n",
       "      'P2416',\n",
       "      'P2629',\n",
       "      'P2746',\n",
       "      'P3121',\n",
       "      'P3596',\n",
       "      'P3667',\n",
       "      'P3845',\n",
       "      'P4065',\n",
       "      'P938',\n",
       "      'P1636',\n",
       "      'P1786',\n",
       "      'P1996',\n",
       "      'P2127',\n",
       "      'P2320',\n",
       "      'P2411',\n",
       "      'P3035',\n",
       "      'P3512',\n",
       "      'P3636',\n",
       "      'P3907',\n",
       "      'P399',\n",
       "      'P4012',\n",
       "      'P4051',\n",
       "      'P4074',\n",
       "      'P4811',\n",
       "      'P5289',\n",
       "      'P5577',\n",
       "      'P5627',\n",
       "      'P5841',\n",
       "      'P2024',\n",
       "      'P2039',\n",
       "      'P2238',\n",
       "      'P2298',\n",
       "      'P2465',\n",
       "      'P2788',\n",
       "      'P4937',\n",
       "      'P1040',\n",
       "      'P1291',\n",
       "      'P1391',\n",
       "      'P2406',\n",
       "      'P2429',\n",
       "      'P2461',\n",
       "      'P2772',\n",
       "      'P3225',\n",
       "      'P3557',\n",
       "      'P3926',\n",
       "      'P4423',\n",
       "      'P4544',\n",
       "      'P4619',\n",
       "      'P4675',\n",
       "      'P661',\n",
       "      'P661'],\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 'Spotlight Session',\n",
       "     'start_time': '2023-07-10T19:00:00-04:00',\n",
       "     'track': 'Spotlight - Metropolitan West',\n",
       "     'type': 'Spotlight'}},\n",
       "   'id': 'spotlight-session',\n",
       "   'name': 'Spotlight Session',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T19:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Paper Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'SustaiNLP': {'display_name': 'W6 - The 4th Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'SustaiNLP',\n",
       "   'name': 'W6 - The 4th Workshop on Simple and Efficient Natural Language Processing (SustaiNLP)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'SustaiNLP': {'anthology_venue_id': 'SustaiNLP',\n",
       "     'booklet_id': 'workshop_6',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Nafise Sadat Moosavi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Iryna Gurevych',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yufang Hou',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Gyuwan Kim',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Young Jin Kim',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Tal Schuster',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ameeta Agrawal',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The Natural Language Processing (NLP) community has, in recent years, demonstrated a notable focus on improving higher scores on standard benchmarks and taking the lead on community-wide leaderboards (e.g., GLUE, SentEval). While this aspiration has led to improvements in benchmark performance of (predominantly neural) models, it has also came at a cost, i.e., increased model complexity and the ever-growing amount of computational resources required for training and using the current state-of-the-art models. Moreover, the recent research efforts have, for the most part, failed to identify sources of empirical gains in models, often failing to empirically justify the model complexity beyond benchmark performance. \\\\newline Because of these easily observable trends, we have proposed the SustaiNLP workshop with the goal of promoting more sustainable NLP research and practices, with two main objectives: (1) encouraging development of more efficient NLP models; and (2) providing simpler architectures and empirical justification of model complexity. For both aspects, we will encourage submissions from all topical areas of NLP.',\n",
       "     'end_time': None,\n",
       "     'id': 'SustaiNLP',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Harbour C',\n",
       "     'session': 'SustaiNLP',\n",
       "     'short_name': 'SustaiNLP',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://sites.google.com/view/sustainlp2023'}}},\n",
       "  'TrustNLP': {'display_name': 'W15 - The 3rd Workshop on Trustworthy NLP (TrustNLP)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'TrustNLP',\n",
       "   'name': 'W15 - The 3rd Workshop on Trustworthy NLP (TrustNLP)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'TrustNLP': {'anthology_venue_id': 'TrustNLP',\n",
       "     'booklet_id': 'workshop_15',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Yada Pruksachatkun',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ninareh Mehrabi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Kai-Wei Chang',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Aram Galystan',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Jwala Dhamala',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Anaelia Ovalle',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Apurv Verma',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Yang Trista Cao',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Anoop Kumar',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Rahul Gupta',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.',\n",
       "     'end_time': None,\n",
       "     'id': 'TrustNLP',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 4',\n",
       "     'session': 'TrustNLP',\n",
       "     'short_name': 'TrustNLP',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://trustnlpworkshop.github.io/'}}},\n",
       "  'WASSA': {'display_name': 'W16 - The 13th Workshop on Computational Approaches to Subjectivity, Sentiment \\\\& Social Media Analysis (WASSA)',\n",
       "   'end_time': '2023-07-14T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'WASSA',\n",
       "   'name': 'W16 - The 13th Workshop on Computational Approaches to Subjectivity, Sentiment \\\\& Social Media Analysis (WASSA)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-14T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'WASSA': {'anthology_venue_id': 'WASSA',\n",
       "     'booklet_id': 'workshop_16',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Jeremy Barnes',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Orphe De Clercq',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Roman Klinger',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Valentin Barriere',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Salvatore Giorgi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Joa Sedoc',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Shabnam Tafreshi',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Iqra Ameer',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Necva Blc',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Hua Xu',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Ali Al Bataineh',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'Subjectivity and Sentiment Analysis has become a highly developed research area, ranging from binary classification of reviews to the detection of complex emotion structures between entities found in text. This field has expanded both on a practical level, finding numerous successful applications in business, as well as on a theoretical level, allowing researchers to explore more complex research questions related to affective computing. Its continuing importance is also shown by the interest it generates in other disciplines such as Economics, Sociology, Psychology, Marketing, Crisis Management \\\\& Digital Humanities. \\\\\\\\newline The aim of WASSA 2023 is to bring together researchers working on Subjectivity, Sentiment Analysis, Emotion Detection and Classification and their applications to other NLP or real-world tasks (e.g. public health messaging, fake news, media impact analysis, social media mining, computational literary studies) and researchers working on interdisciplinary aspects of affect computation from text.',\n",
       "     'end_time': None,\n",
       "     'id': 'WASSA',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Hourbour C',\n",
       "     'session': 'WASSA',\n",
       "     'short_name': 'WASSA',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://wassa-workshop.github.io/'}}},\n",
       "  'WOAH': {'display_name': 'W9 - The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'end_time': '2023-07-13T17:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'WOAH',\n",
       "   'name': 'W9 - The 7th Workshop on Online Abuse and Harms (WOAH)',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-13T09:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Workshops',\n",
       "   'workshop_events': {'WOAH': {'anthology_venue_id': 'ACL',\n",
       "     'booklet_id': 'workshop_9',\n",
       "     'chairs': [],\n",
       "     'committee': [{'first_name': None,\n",
       "       'full_name': 'Yi-Ling Chung',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Aida Mostafazadeh Davani',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Debora Nozza',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Paul Rttger',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None},\n",
       "      {'first_name': None,\n",
       "       'full_name': 'Zeerak Talat',\n",
       "       'google_scholar': None,\n",
       "       'last_name': None,\n",
       "       'middle_name': None,\n",
       "       'semantic_scholar': None}],\n",
       "     'description': 'The goal of The Workshop on Online Abuse and Harms (WOAH) is to advance research that develops, interrogates and applies computational methods for detecting, classifying and modelling online abuse.',\n",
       "     'end_time': None,\n",
       "     'id': 'WOAH',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': 'Pier 7 and 8',\n",
       "     'session': 'WOAH',\n",
       "     'short_name': 'WOAH',\n",
       "     'start_time': None,\n",
       "     'track': 'Workshop',\n",
       "     'type': 'Workshops',\n",
       "     'workshop_site_url': 'https://www.workshopononlineabuse.com/'}}},\n",
       "  'Welcome': {'display_name': 'Welcome',\n",
       "   'end_time': '2023-07-10T13:30:00+00:00',\n",
       "   'events': {'welcome': {'chairs': [],\n",
       "     'end_time': '2023-07-10T13:30:00+00:00',\n",
       "     'id': 'welcome',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'room': None,\n",
       "     'session': 'welcome',\n",
       "     'start_time': '2023-07-10T13:00:00+00:00',\n",
       "     'track': 'Welcome',\n",
       "     'type': 'Plenary Sessions'}},\n",
       "   'id': 'welcome',\n",
       "   'name': 'Welcome',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-10T13:00:00+00:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'acl-rolling-review-update-and-discussion': {'display_name': 'Plenary: ACL Rolling Review Update and Discussion',\n",
       "   'end_time': '2023-07-11T14:45:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'acl-rolling-review-update-and-discussion',\n",
       "   'name': 'Plenary: ACL Rolling Review Update and Discussion',\n",
       "   'plenary_events': {'acl-rolling-review-update-and-discussion': {'abstract': 'Mausam, Professor, IIT Delhi (ARR EIC), Jonathan K. Kummerfeld,\\nAssistant Professor, University of Sydney (ARR CTO)\\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 14:1514:45\\n\\nThis session will contain a presentation on progress in ARR over the\\npast year and provide an opportunity for community questions and\\ndiscussion.\\n\\nWe will briefly present:\\n\\nPersonnel Updates New aspects: Tracks, Senior Action Editors\\nImprovements, e.g. changes to the review - paper matching process\\nStatistics on timeliness and paper outcomes Next steps\\n\\nWith that context we will open the floor to questions.\\n',\n",
       "     'bio': '',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'acl-rolling-review-update-and-discussion',\n",
       "     'image_url': None,\n",
       "     'institution': None,\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': None,\n",
       "     'room': 'Metropolitan',\n",
       "     'session': 'acl-rolling-review-update-and-discussion',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: ACL Rolling Review Update and Discussion',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-11T14:15:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'ethics-panel': {'display_name': 'Plenary: Ethics Panel',\n",
       "   'end_time': '2023-07-11T17:45:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'ethics-panel',\n",
       "   'name': 'Plenary: Ethics Panel',\n",
       "   'plenary_events': {'ethics-panel': {'abstract': 'Karn Fort, Min-Yen Kan and Yulia Tsvetkov (ACL Ethics Committee\\nco-chairs) Committee Members: Luciana Benotti, Mark Dredze, Pascale\\nFung, Dirk Hovy, Jin-Dong Kim, Malvina Nissim\\nTuesday, July 11, 2023 - Room: Pier 4&5 - Time: 16:1517:45\\n\\nWe present our ACL Ethics Committees progress over the last few years.\\nOf core interest, we will present the results of the ACL stakeholder\\nsurvey about the role of ethics and ethics training exposure. Results\\nfrom the survey respondents indicate that ethics is of primary interest\\nto the community and that there is a mandate for the further creation\\nand dissemination of ethics related training for authors, reviewers and\\nevent organisers. We will briefly review the survey results and feature\\na lengthed question and answer session in support of extended dialogue\\nwith our community. Our session will culminate through a dialogue with\\nour sessions participants in a moderated panel that includes\\nparticipation from the entire ethics committee.\\n',\n",
       "     'bio': '',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'ethics-panel',\n",
       "     'image_url': None,\n",
       "     'institution': None,\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': None,\n",
       "     'room': 'Pier 4\\\\&5',\n",
       "     'session': 'ethics-panel',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: Ethics Panel',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-11T16:15:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models': {'display_name': 'Plenary: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models',\n",
       "   'end_time': '2023-07-12T15:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models',\n",
       "   'name': 'Plenary: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models',\n",
       "   'plenary_events': {'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models': {'abstract': 'Alison Gopnik\\nUniversity of California, Berkeley\\n\\nWednesday, July 12 - Time: 14:0015:00 EDT\\n\\nAbstract: Its natural to ask whether large language models like LaMDA or\\nGPT-3 are intelligent agents. But I argue that this is the wrong\\nquestion. Intelligence and agency are the wrong categories for\\nunderstanding them. Instead, these Al systems are what we might call\\ncultural technologies, like writing, print, libraries, internet search\\nengines or even language itself. They are new techniques for passing on\\ninformation from one group of people to another. Cultural technologies\\narent like intelligent humans, but they are essential for human\\nintelligence. Many animals can transmit some information from one\\nindividual or one generation to another, but no animal does it as much\\nas we do or accumulates as much information over time, . New\\ntechnologies that make cultural transmission easier and more effective\\nhave been among the greatest engines of human progress, but they have\\nalso led to negative as well as positive social consequences. Moreover,\\nwhile cultural technologies allow transmission of existing information\\ncultural evolution, which is central to human success, also depends on\\ninnovation, exploration and causal learning. Comparing LLMs responses\\nin prompts based on developmental psychology experiments to the\\nresponses of children may provide insight into which capacities can be\\nlearned through language and cultural transmission, and which require\\ninnovation and exploration in the physical world. I will present results\\nfrom several studies making such comparisons.\\n',\n",
       "     'bio': ' Alison Gopnik is a professor of psychology and affiliate professor\\nof philosophy at the University of California at Berkeley, and a member\\nof the Berkeley AI Research Group. She received her BA from McGill\\nUniversity and her PhD. from Oxford University. She is a leader in the\\nstudy of cognitive science and of childrens learning and development\\nand was one of the founders of the field of theory of mind, an\\noriginator of the theory of cognitive development, and the first to\\napply Bayesian probabilistic models to childrens learning. She has\\nreceived both the APS Lifetime Achievement Cattell and William James\\nAwards, the Bradford Washburn Award for Science Communication, and the\\nSRCD Lifetime Achievement Award for Basic Science in Child Development.\\nShe is an elected member of the Society of Experimental Psychologists\\nand the American Academy of Arts and Sciences and a Cognitive Science\\nSociety, American Association for the Advancement of Science, and\\nGuggenheim Fellow. She was 2022-23 President of the Association for\\nPsychological Science.\\n\\nShe is the author or coauthor of over 140 journal articles and several\\nbooks including Words, thoughts and theories MIT Press, 1997, and the\\nbestselling and critically acclaimed popular books The Scientist in the\\nCrib William Morrow, 1999, The Philosophical Baby; What childrens\\nminds tell us about love, truth and the meaning of life 2009, and The\\nGardener and the Carpenter 2016, Farrar, Strauss and Giroux, the latter\\ntwo won the Cognitive Development Society Best Book Prize in 2009 and\\n2016. Since 2013 she has written the Mind and Matter column for the Wall\\nStreet Journal and she has also written widely about cognitive science\\nand psychology for The New York Times, The Economist, The Atlantic, The\\nNew Yorker, Scientific American, The Times Literary Supplement, The New\\nYork Review of Books, New Scientist and Slate, among others. Her TED\\ntalk on her work has been viewed more than 5.2 million times. She has\\nfrequently appeared on TV, radio and podcasts including The Charlie\\nRose Show, The Colbert Report, Radio Lab and The Ezra Klein Show.\\nShe lives in Berkeley with her husband Alvy Ray Smith and has three\\nchildren and five grandchildren.\\n',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models',\n",
       "     'image_url': 'invited2.jpg',\n",
       "     'institution': 'University of California at Berkeley',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': 'Alison Gopnik',\n",
       "     'room': 'Metropolitan',\n",
       "     'session': 'large-language-models-as-cultural-technologies_-imitation-and-innovation-in-children-and-models',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: Large Language Models as Cultural Technologies: Imitation and Innovation in Children and Models',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-12T14:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'memorial': {'display_name': 'Plenary: Memorial',\n",
       "   'end_time': '2023-07-11T13:30:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'memorial',\n",
       "   'name': 'Plenary: Memorial',\n",
       "   'plenary_events': {'memorial': {'abstract': '[image]\\n\\nTuesday, July 11, 2023 - Room: Metropolitan - Time: 13:0013:30\\n\\nDragomir Radev, the A. Bartlett Giamatti Professor of Computer Science\\nat Yale University, passed away this year on Wed, March 29th. Drago\\ncontributed in substantial ways to research in NLP, to the organization\\nof the ACL and to mentoring the next generation of computational\\nlinguists. Dragos role in our ACL community spans four decades. He was\\nrecognized for his work over this period through his selection as an ACL\\nFellow in 2018 for his significant contributions to text summarization\\nand question answering, and through his receipt of the Distinguished ACL\\nService Award in 2022. In this session, speakers from different time\\nperiods of his life will discuss his contributions to the field and the\\nimpact his life had on so many of us.\\n',\n",
       "     'bio': '',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'memorial',\n",
       "     'image_url': 'drago.jpg',\n",
       "     'institution': None,\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': None,\n",
       "     'room': 'Metropolitan',\n",
       "     'session': 'memorial',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: Memorial',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-11T13:00:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'navigating-nlp-in-the-era-of-llm': {'display_name': 'Plenary: Navigating NLP in the Era of Large Language Models',\n",
       "   'end_time': '2023-07-11T14:30:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'navigating-nlp-in-the-era-of-llm',\n",
       "   'name': 'Plenary: Navigating NLP in the Era of Large Language Models',\n",
       "   'plenary_events': {'navigating-nlp-in-the-era-of-llm': {'abstract': 'Join us for a panel featuring experts Sara Hooker (Cohere), Swaroop Mishra (Google DeepMind), and Danqi Chen (Princeton), who will provide invaluable insights into navigating the tempestuous seas of NLP in the era of large language models. This discussion will guide students and early career researchers through impactful directions, progress-making strategies, offering perspectives from academia and industry.\\n\\nTuesday, July 11 - Time: 13:4514:30 EDT\\n\\nRoom: Pier 2&3',\n",
       "     'bio': '',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'navigating-nlp-in-the-era-of-llm',\n",
       "     'image_url': None,\n",
       "     'institution': None,\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': 'Danqi Chen (Princeton), Swaroop Mishra (Google DeepMind) and Sara Hooker (Cohere)',\n",
       "     'room': 'Pier 2\\\\&3',\n",
       "     'session': 'navigating-nlp-in-the-era-of-llm',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: Navigating NLP in the Era of Large Language Models',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-11T13:45:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  't1': {'display_name': 'T1: Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond',\n",
       "   'end_time': '2023-07-09T09:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't1',\n",
       "   'name': 'T1: Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'tutorial_events': {'t1': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.1',\n",
       "     'chairs': [],\n",
       "     'description': \"Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target conversational goal and capable of leading the conversation towards the goal, which is a significant step towards higher-level intelligence and artificial consciousness. It can not only largely improve user engagement and service efficiency in the conversation, but also empower the system to handle more complicated conversation tasks that involve strategical and motivational interactions. In this tutorial, we will introduce the recent advances on the design of agent's awareness of goals in a wide range of conversational systems.\",\n",
       "     'end_time': '2023-07-09T12:30:00-04:00',\n",
       "     'id': 't1',\n",
       "     'link': None,\n",
       "     'organizers': ['Minlie Huang',\n",
       "      'Tat-Seng Chua',\n",
       "      'Wenqiang Lei',\n",
       "      'Yang Deng'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-1',\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 't1',\n",
       "     'start_time': '2023-07-09T09:00:00-04:00',\n",
       "     'title': 'T1: Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.1.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  't2': {'display_name': 'T2: Complex Reasoning in Natural Language',\n",
       "   'end_time': '2023-07-09T09:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't2',\n",
       "   'name': 'T2: Complex Reasoning in Natural Language',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'tutorial_events': {'t2': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.2',\n",
       "     'chairs': [],\n",
       "     'description': 'Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc.A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area.We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.',\n",
       "     'end_time': '2023-07-09T12:30:00-04:00',\n",
       "     'id': 't2',\n",
       "     'link': None,\n",
       "     'organizers': ['Aman Madaan',\n",
       "      'Bill Yuchen Lin',\n",
       "      'Michihiro Yasunaga',\n",
       "      'Mor Geva',\n",
       "      'Tao Yu',\n",
       "      'Wenting Zhao'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-2',\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 't2',\n",
       "     'start_time': '2023-07-09T09:00:00-04:00',\n",
       "     'title': 'T2: Complex Reasoning in Natural Language',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.2.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  't3': {'display_name': 'T3: Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world',\n",
       "   'end_time': '2023-07-09T09:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't3',\n",
       "   'name': 'T3: Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'tutorial_events': {'t3': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.3',\n",
       "     'chairs': [],\n",
       "     'description': \"This tutorial will describe various aspects of scaling up language technologies to many of the world's languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models\",\n",
       "     'end_time': '2023-07-09T12:30:00-04:00',\n",
       "     'id': 't3',\n",
       "     'link': None,\n",
       "     'organizers': ['Barun Patra',\n",
       "      'Kabir Ahuja',\n",
       "      'Kalika Balia',\n",
       "      'Monojit Choudhury',\n",
       "      'Sunayana Sitaram',\n",
       "      'Vishrav Chaudhary'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-3',\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 't3',\n",
       "     'start_time': '2023-07-09T09:00:00-04:00',\n",
       "     'title': 'T3: Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.3.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  't4': {'display_name': 'T4: Generating Text from Language Models',\n",
       "   'end_time': '2023-07-09T14:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't4',\n",
       "   'name': 'T4: Generating Text from Language Models',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'tutorial_events': {'t4': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.4',\n",
       "     'chairs': [],\n",
       "     'description': 'An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms---like top-p sampling or beam search---which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.',\n",
       "     'end_time': '2023-07-09T17:30:00-04:00',\n",
       "     'id': 't4',\n",
       "     'link': None,\n",
       "     'organizers': ['Afra Amini',\n",
       "      'Clara Meister',\n",
       "      'John Hewitt',\n",
       "      'Ryan Cotterell',\n",
       "      'Tiago Pimentel'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-4',\n",
       "     'room': 'Metropolitan East',\n",
       "     'session': 't4',\n",
       "     'start_time': '2023-07-09T14:00:00-04:00',\n",
       "     'title': 'T4: Generating Text from Language Models',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.4.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  't5': {'display_name': 'T5: Indirectly Supervised Natural Language Processing',\n",
       "   'end_time': '2023-07-09T14:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't5',\n",
       "   'name': 'T5: Indirectly Supervised Natural Language Processing',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'tutorial_events': {'t5': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.5',\n",
       "     'chairs': [],\n",
       "     'description': \"This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a related task T? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T' that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotationsall having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.\",\n",
       "     'end_time': '2023-07-09T17:30:00-04:00',\n",
       "     'id': 't5',\n",
       "     'link': None,\n",
       "     'organizers': ['Ben Zhou',\n",
       "      'Dan Roth',\n",
       "      'Kai-Wei Chang',\n",
       "      'Muhao Chen',\n",
       "      'Qiang Ning',\n",
       "      'Wenpeng Yin'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-5',\n",
       "     'room': 'Metropolitan Centre',\n",
       "     'session': 't5',\n",
       "     'start_time': '2023-07-09T14:00:00-04:00',\n",
       "     'title': 'T5: Indirectly Supervised Natural Language Processing',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.5.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  't6': {'display_name': 'T6: Retrieval-based Language Models and Applications',\n",
       "   'end_time': '2023-07-09T14:00:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 't6',\n",
       "   'name': 'T6: Retrieval-based Language Models and Applications',\n",
       "   'plenary_events': {},\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'tutorial_events': {'t6': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.6',\n",
       "     'chairs': [],\n",
       "     'description': 'Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.',\n",
       "     'end_time': '2023-07-09T17:30:00-04:00',\n",
       "     'id': 't6',\n",
       "     'link': None,\n",
       "     'organizers': ['Akari Asai', 'Danqi Chen', 'Sewon Min', 'Zexuan Zhong'],\n",
       "     'paper_ids': [],\n",
       "     'rocketchat_channel': 'tutorial-6',\n",
       "     'room': 'Metropolitan West',\n",
       "     'session': 't6',\n",
       "     'start_time': '2023-07-09T14:00:00-04:00',\n",
       "     'title': 'T6: Retrieval-based Language Models and Applications',\n",
       "     'track': 'Tutorial',\n",
       "     'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.6.pdf',\n",
       "     'type': 'Tutorials'}},\n",
       "   'type': 'Tutorials',\n",
       "   'workshop_events': {}},\n",
       "  'the-future-of-computational-linguistics-in-the-llm-age': {'display_name': 'Plenary: The Future of Computational Linguistics in the LLM Age',\n",
       "   'end_time': '2023-07-11T15:45:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'the-future-of-computational-linguistics-in-the-llm-age',\n",
       "   'name': 'Plenary: The Future of Computational Linguistics in the LLM Age',\n",
       "   'plenary_events': {'the-future-of-computational-linguistics-in-the-llm-age': {'abstract': 'Chair: Iryna Gurevych\\nTechnische Universitt Darmstadt\\n\\nTuesday, July 11 - Time: 14:45-15:45\\n\\nThis is a panel discussion with:\\n\\n-   Dan Klein (UC Berkeley)\\n\\n-   Meg Mitchell (Hugging Face)\\n\\n-   Roy Schwartz (the Hebrew University of Jerusalem)\\n\\nThey will present short statements (5 to 7 min.) related to the main\\ntopic of the panel\\n\\n-   New opportunities (e.g., artificial general intelligence,\\n    responsible NLP);\\n\\n-   Technical challenges (e.g., multimodality, instruction-tuning, etc.)\\n\\n-   Real life problems & societal implications (e.g., hallucinations,\\n    biases, future job market);\\n\\n-   LLMs and the future of NLP; and\\n\\n-   Open-science vs. commercial LLMs\\n\\nFollowed by discussion with the panel and audience.\\n',\n",
       "     'bio': '',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'the-future-of-computational-linguistics-in-the-llm-age',\n",
       "     'image_url': None,\n",
       "     'institution': None,\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': 'Dan Klein (UC Berkeley), Meg Mitchell (Hugging Face), Roy Schwartz (Hebrew University of Jerusalem)',\n",
       "     'room': 'Metropolitan',\n",
       "     'session': 'the-future-of-computational-linguistics-in-the-llm-age',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: The Future of Computational Linguistics in the LLM Age',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-11T14:45:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}},\n",
       "  'two-paths-to-intelligence': {'display_name': 'Plenary: Two Paths to Intelligence',\n",
       "   'end_time': '2023-07-10T10:30:00-04:00',\n",
       "   'events': {},\n",
       "   'id': 'two-paths-to-intelligence',\n",
       "   'name': 'Plenary: Two Paths to Intelligence',\n",
       "   'plenary_events': {'two-paths-to-intelligence': {'abstract': 'Keynote: Geoffrey Hinton\\nCohere\\n\\nMonday, July 10 - Time: 09:3010:30 EDT\\n\\nAbstract: I will briefly describe the forty year history of neural net\\nlanguage models with particular attention to whether they understand\\nwhat they are saying. I will then discuss some of the main differences\\nbetween digital and biological intelligences and speculate on how the\\nbrain could implement something like transformers. I will conclude by\\naddressing the contentious issue of whether current multimodal LLMs have\\nsubjective experience.\\n',\n",
       "     'bio': ' Geoffrey Hinton received his PhD in Artificial Intelligence from\\nEdinburgh in 1978. After five years as a faculty member at\\nCarnegie-Mellon he became a fellow of the Canadian Institute for\\nAdvanced Research and moved to the University of Toronto where he is now\\nan emeritus professor. He is also the Chief Scientific Adviser at the\\nVector Institute.\\n\\nHe was one of the researchers who introduced the backpropagation\\nalgorithm and the first to use backpropagation for learning word\\nembeddings. His other contributions to neural network research include\\nBoltzmann machines, distributed representations, time-delay neural nets,\\nmixtures of experts, variational learning and deep learning. His\\nresearch group in Toronto made major breakthroughs in deep learning that\\nrevolutionized speech recognition and object classification.\\n\\nHe is a fellow of the UK Royal Society and a foreign member of the US\\nNational Academy of Engineering, the US National Academy of Sciences and\\nthe American Academy of Arts and Sciences. His awards include the David\\nE. Rumelhart prize, the IJCAI award for research excellence, the Killam\\nprize for Engineering, the Royal Society Royal Medal, the NSERC Herzberg\\nGold Medal, the IEEE James Clerk Maxwell Gold medal, the NEC C&C award,\\nthe BBVA award, the Honda Prize and the Turing Award.\\n',\n",
       "     'chairs': [],\n",
       "     'end_time': None,\n",
       "     'id': 'two-paths-to-intelligence',\n",
       "     'image_url': 'invited1.jpg',\n",
       "     'institution': 'University of Toronto (emeritus)',\n",
       "     'link': None,\n",
       "     'paper_ids': [],\n",
       "     'presenter': 'Geoffrey Hinton',\n",
       "     'room': 'Metropolitan',\n",
       "     'session': 'two-paths-to-intelligence',\n",
       "     'start_time': None,\n",
       "     'title': 'Plenary: Two Paths to Intelligence',\n",
       "     'track': 'Plenary',\n",
       "     'type': 'Plenary Sessions',\n",
       "     'video_url': None}},\n",
       "   'start_time': '2023-07-10T09:30:00-04:00',\n",
       "   'tutorial_events': {},\n",
       "   'type': 'Plenary Sessions',\n",
       "   'workshop_events': {}}},\n",
       " 'tutorials': {'t1': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.1',\n",
       "   'chairs': [],\n",
       "   'description': \"Conversational systems are envisioned to provide social support or functional service to human users via natural language interactions. Conventional conversation researches mainly focus on the responseability of the system, such as dialogue context understanding and response generation, but overlooks the design of an essential property in intelligent conversations, i.e., goal awareness. The awareness of goals means the state of not only being responsive to the users but also aware of the target conversational goal and capable of leading the conversation towards the goal, which is a significant step towards higher-level intelligence and artificial consciousness. It can not only largely improve user engagement and service efficiency in the conversation, but also empower the system to handle more complicated conversation tasks that involve strategical and motivational interactions. In this tutorial, we will introduce the recent advances on the design of agent's awareness of goals in a wide range of conversational systems.\",\n",
       "   'end_time': '2023-07-09T12:30:00-04:00',\n",
       "   'id': 't1',\n",
       "   'link': None,\n",
       "   'organizers': ['Minlie Huang',\n",
       "    'Tat-Seng Chua',\n",
       "    'Wenqiang Lei',\n",
       "    'Yang Deng'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-1',\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 't1',\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'title': 'T1: Goal Awareness for Conversational AI: Proactivity, Non-collaborativity, and Beyond',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.1.pdf',\n",
       "   'type': 'Tutorials'},\n",
       "  't2': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.2',\n",
       "   'chairs': [],\n",
       "   'description': 'Teaching machines to reason over texts has been a long-standing goal of natural language processing (NLP). To this end, researchers have designed a diverse set of complex reasoning tasks that involve compositional reasoning, knowledge retrieval, grounding, commonsense reasoning, etc.A standard choice for building systems that perform a desired type of reasoning is to fine-tune a pretrained language model (LM) on specific downstream tasks. However, recent research has demonstrated that such a straightforward approach is often brittle. For example, Elazar et al. (2021) and Branco et al. (2021) show that, on question-answering (QA) tasks, similar performance can be achieved with questions removed from the inputs. Min et al. (2019), Chen and Durrett (2019), and Tang et al. (2021) show that models trained on multi-hop QA do not generalize to answer single-hop questions. The reasoning capabilities of these models thus remain at a surface level, i.e., exploiting data patterns. Consequently, augmenting LMs with techniques that make them robust and effective becomes an active research area.We will start the tutorial by providing an overview of complex reasoning tasks where the standard application of pretrained language models fails. This tutorial then reviews recent promising directions for tackling these tasks. Specifically, we focus on the following groups of approaches that explicitly consider problem structures: (1) knowledge-augmented methods, where the knowledge is either incorporated during fine-tuning or pretraining; (2) few-shot prompting methods, which effectively guide the models to follow instructions; (3) neuro-symbolic methods, which produce explicit intermediate representations; and, (4) rationale-based methods, one of the most popular forms of the neuro-symbolic methods, which highlight subsets of input as explanations for individual model predictions.',\n",
       "   'end_time': '2023-07-09T12:30:00-04:00',\n",
       "   'id': 't2',\n",
       "   'link': None,\n",
       "   'organizers': ['Aman Madaan',\n",
       "    'Bill Yuchen Lin',\n",
       "    'Michihiro Yasunaga',\n",
       "    'Mor Geva',\n",
       "    'Tao Yu',\n",
       "    'Wenting Zhao'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-2',\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 't2',\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'title': 'T2: Complex Reasoning in Natural Language',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.2.pdf',\n",
       "   'type': 'Tutorials'},\n",
       "  't3': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.3',\n",
       "   'chairs': [],\n",
       "   'description': \"This tutorial will describe various aspects of scaling up language technologies to many of the world's languages by describing the latest research in Massively Multilingual Language Models (MMLMs). We will cover topics such as data collection, training and fine-tuning of models, Responsible AI issues such as fairness, bias and toxicity, linguistic diversity and evaluation in the context of MMLMs, specifically focusing on issues in non-English and low-resource languages. Further, we will also talk about some of the real-world challenges in deploying these models in language communities in the field. With the performance of MMLMs improving in the zero-shot setting for many languages, it is now becoming feasible to use them for building language technologies in many languages of the world, and this tutorial will provide the computational linguistics community with unique insights from the latest research in multilingual models\",\n",
       "   'end_time': '2023-07-09T12:30:00-04:00',\n",
       "   'id': 't3',\n",
       "   'link': None,\n",
       "   'organizers': ['Barun Patra',\n",
       "    'Kabir Ahuja',\n",
       "    'Kalika Balia',\n",
       "    'Monojit Choudhury',\n",
       "    'Sunayana Sitaram',\n",
       "    'Vishrav Chaudhary'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-3',\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 't3',\n",
       "   'start_time': '2023-07-09T09:00:00-04:00',\n",
       "   'title': 'T3: Everything you need to know about Multilingual LLMs: Towards fair, performant and reliable models for languages of the world',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.3.pdf',\n",
       "   'type': 'Tutorials'},\n",
       "  't4': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.4',\n",
       "   'chairs': [],\n",
       "   'description': 'An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms---like top-p sampling or beam search---which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.',\n",
       "   'end_time': '2023-07-09T17:30:00-04:00',\n",
       "   'id': 't4',\n",
       "   'link': None,\n",
       "   'organizers': ['Afra Amini',\n",
       "    'Clara Meister',\n",
       "    'John Hewitt',\n",
       "    'Ryan Cotterell',\n",
       "    'Tiago Pimentel'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-4',\n",
       "   'room': 'Metropolitan East',\n",
       "   'session': 't4',\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'title': 'T4: Generating Text from Language Models',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.4.pdf',\n",
       "   'type': 'Tutorials'},\n",
       "  't5': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.5',\n",
       "   'chairs': [],\n",
       "   'description': \"This tutorial targets researchers and practitioners who are interested in ML technologies for NLP from indirect supervision. In particular, we will present a diverse thread of indirect supervision studies that try to answer the following questions: (i) when and how can we provide supervision for a target task T, if all we have is data that corresponds to a related task T? (ii) humans do not use exhaustive supervision; they rely on occasional feedback, and learn from incidental signals from various sources; how can we effectively incorporate such supervision in machine learning? (iii) how can we leverage multi-modal supervision to help NLP? To the end, we will discuss several lines of research that address those challenges, including (i) indirect supervision from T' that handles T with outputs spanning from a moderate size to an open space, (ii) the use of sparsely occurring and incidental signals, such as partial labels, noisy labels, knowledge-based constraints, and cross-domain or cross-task annotationsall having statistical associations with the task, (iii) principled ways to measure and understand why these incidental signals can contribute to our target tasks, and (iv) indirect supervision from vision-language signals. We will conclude the tutorial by outlining directions for further investigation.\",\n",
       "   'end_time': '2023-07-09T17:30:00-04:00',\n",
       "   'id': 't5',\n",
       "   'link': None,\n",
       "   'organizers': ['Ben Zhou',\n",
       "    'Dan Roth',\n",
       "    'Kai-Wei Chang',\n",
       "    'Muhao Chen',\n",
       "    'Qiang Ning',\n",
       "    'Wenpeng Yin'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-5',\n",
       "   'room': 'Metropolitan Centre',\n",
       "   'session': 't5',\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'title': 'T5: Indirectly Supervised Natural Language Processing',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.5.pdf',\n",
       "   'type': 'Tutorials'},\n",
       "  't6': {'anthology_url': 'https://aclanthology.org/2023.acl-tutorials.6',\n",
       "   'chairs': [],\n",
       "   'description': 'Retrieval-based language models (LMs) have shown impressive performance on diverse NLP tasks. In this tutorial, we will provide a comprehensive and coherent overview of recent advances in retrieval-based LMs. We will start by providing preliminaries covering the foundation of LMs (e.g., masked LMs, autoregressive LMs) and retrieval systems (e.g., nearest-neighbor search). We will then detail recent progress in retrieval-based models, focusing on their model architectures and learning approaches. Finally, we will show how retrieval-based LMs are adapted to downstream applications, and extended to multilingual and multi-modal settings. Finally, we will use an exercise to showcase the effectiveness of retrieval-based LMs.',\n",
       "   'end_time': '2023-07-09T17:30:00-04:00',\n",
       "   'id': 't6',\n",
       "   'link': None,\n",
       "   'organizers': ['Akari Asai', 'Danqi Chen', 'Sewon Min', 'Zexuan Zhong'],\n",
       "   'paper_ids': [],\n",
       "   'rocketchat_channel': 'tutorial-6',\n",
       "   'room': 'Metropolitan West',\n",
       "   'session': 't6',\n",
       "   'start_time': '2023-07-09T14:00:00-04:00',\n",
       "   'title': 'T6: Retrieval-based Language Models and Applications',\n",
       "   'track': 'Tutorial',\n",
       "   'tutorial_pdf': 'https://aclanthology.org/2023.acl-tutorials.6.pdf',\n",
       "   'type': 'Tutorials'}},\n",
       " 'workshops': {'AmericasNLP': {'anthology_venue_id': 'ACL',\n",
       "   'booklet_id': 'workshop_20',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Manuel Mager',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Arturo Oncevay',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Enora Rice',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Abteen Ebrahimi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Shruti Rijhwani',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Alexis Palmer',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Katharina Kann',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'AmericasNLP aims to (a) encourage research on NLP, computational linguistics, corpus linguistics, and speech around the globe to work on native American languages; (b) )connect researchers and professionals from underrepresented communities and native speakers of endangered languages with the machine learning and natural language processing communities; and (c) )promote research on both neural and non-neural machine learning approaches suitable for low-resource languages.',\n",
       "   'end_time': None,\n",
       "   'id': 'AmericasNLP',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 3',\n",
       "   'session': 'AmericasNLP',\n",
       "   'short_name': 'AmericasNLP',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://turing.iimas.unam.mx/americasnlp/'},\n",
       "  'BEA': {'anthology_venue_id': 'BEA',\n",
       "   'booklet_id': 'workshop_7',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Ekaterina Kochmar',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Jill Burstein',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Andrea Horbach',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ronja Laarmann-Quante',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Nitin Madnani',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Anas Tack',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Victoria Yaneva',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Zheng Yuan',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Torsten Zesch',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The BEA Workshop is a leading venue for NLP innovation in the context of educational applications. It is one of the largest one-day workshops in the ACL community with over 100 registered attendees in the past several years. The growing interest in educational applications and a diverse community of researchers involved resulted in the creation of the Special Interest Group in Educational Applications (SIGEDU) in 2017, which currently has over 300 members.',\n",
       "   'end_time': None,\n",
       "   'id': 'BEA',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Harbour A',\n",
       "   'session': 'BEA',\n",
       "   'short_name': 'BEA',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sig-edu.org/bea/2023'},\n",
       "  'BioNLP-ST': {'anthology_venue_id': 'BioNLP',\n",
       "   'booklet_id': 'workshop_13',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Kevin Bretonnel Cohen',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Dina Demner-Fushman',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Sophia Ananiadou',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Jun-ichi Tsujii',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The BioNLP workshop associated with the ACL SIGBIOMED special interest group has established itself as the primary venue for presenting foundational research in language processing for the biological and medical domains. The workshop is running every year since 2002 and continues getting stronger. BioNLP welcomes and encourages work on languages other than English, and inclusion and diversity. BioNLP truly encompasses the breadth of the domain and brings together researchers in bio- and clinical NLP from all over the world. The workshop will continue presenting work on a broad and interesting range of topics in NLP. The interest to biomedical language has broadened significantly due to the COVID-19 pandemic and continues to grow: as access to information becomes easier and more people generate and access health-related text, it becomes clearer that only language technologies can enable and support adequate use of the biomedical text.',\n",
       "   'end_time': None,\n",
       "   'id': 'BioNLP-ST',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 2',\n",
       "   'session': 'BioNLP-ST',\n",
       "   'short_name': 'BioNLP-ST',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://aclweb.org/aclwiki/BioNLP_Workshop'},\n",
       "  'CAWL': {'anthology_venue_id': 'CAWL',\n",
       "   'booklet_id': 'workshop_19',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Kyle Gorman',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Brian Roark',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Richard Sproat',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Most work on NLP focuses on language in its canonical written form. This has often led researchers to ignore the differences between written and spoken language or, worse, to conflate the two. Instances of conflation are statements like Chinese is a logographic language\" or Persian is a right-to-left language\", variants of which can be found frequently in the ACL anthology. These statements confuse properties of the language with properties of its writing system. Ignoring differences between written and spoken language leads, among other things, to conflating different words that are spelled the same (e.g., English bass), or treating as different, words that have multiple spellings. \\\\\\\\newline text enFurthermore, methods for dealing with written language issues (e.g., various kinds of normalization or conversion) or for recognizing text input (e.g. OCR \\\\& handwriting recognition or text entry methods) are often regarded as precursors to NLP rather than as fundamental parts of the enterprise, despite the fact that most NLP methods rely centrally on representations derived from text rather than (spoken) language. This general lack of consideration of writing has led to much of the research on such topics to largely appear outside of ACL venues, in conferences or journals of neighboring fields such as speech technology (e.g., text normalization) or human-computer interaction (e.g., text entry). \\\\\\\\newline We will invite submissions on the relationship between written and spoken language, the properties of written language, the ways in which writing systems encode language, and applications specifically focused on characteristics of writing systems.',\n",
       "   'end_time': None,\n",
       "   'id': 'CAWL',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 2',\n",
       "   'session': 'CAWL',\n",
       "   'short_name': 'CAWL',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://cawl.wellformedness.com/'},\n",
       "  'CODI': {'anthology_venue_id': 'CODI',\n",
       "   'booklet_id': 'workshop_3',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Chlo Braud',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Christian Hardmeier',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Junyi Jessy Li',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Sharid Loiciga',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Michael Strube',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Amir Zeldes',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The last ten years have seen a dramatic improvement in the ability of NLP systems to understand and produce words and sentences. This development has created a renewed interest in discourse phenomena as researchers move towards the processing of long-form text and conversations. There is a surge of activity in discourse parsing, coherence models, text summarization, corpora for discourse level reading comprehension, and discourse related/aided representation learning, to name a few, but the problems in computational approaches to discourse are still substantial. At this juncture, we have organized three Workshops on Computational Approaches to Discourse (CODI) at EMNLP 2020, EMNLP 2021 and COLING 2022 to bring together discourse experts and upcoming researchers. These workshops have catalyzed work to improve the speed and knowledge needed to solve such problems and have served as a forum for the discussion of suitable datasets and reliable evaluation methods.',\n",
       "   'end_time': None,\n",
       "   'id': 'CODI',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 9',\n",
       "   'session': 'CODI',\n",
       "   'short_name': 'CODI',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/view/codi-2023/'},\n",
       "  'Clinical-NLP': {'anthology_venue_id': 'ClinicalNLP',\n",
       "   'booklet_id': 'workshop_17',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Asma Ben Abacha',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Steven Bethard',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Tristan Naumann',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Kirk Roberts',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Anna Rumshisky',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Clinical text is growing rapidly as electronic health records become pervasive. Much of the information recorded in a clinical encounter is located exclusively in provider narrative notes, which makes them indispensable for supplementing structured clinical data in order to better understand patient state and care provided. The methods and tools developed for the clinical domain have historically lagged behind the scientific advances in the general-domain NLP. Despite the substantial recent strides in clinical NLP, a substantial gap remains. The goal of this workshop is to address this gap by establishing a regular event in CL conferences that brings together researchers interested in developing state-of-the-art methods for the clinical domain. The focus is on improving NLP technology to enable clinical applications, and specifically, information extraction and modeling of narrative provider notes from electronic health records, patient encounter transcripts, and other clinical narratives.',\n",
       "   'end_time': None,\n",
       "   'id': 'Clinical-NLP',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 7 and 8',\n",
       "   'session': 'Clinical-NLP',\n",
       "   'short_name': 'Clinical-NLP',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://clinical-nlp.github.io/2023/'},\n",
       "  'DialDoc': {'anthology_venue_id': 'DialDoc',\n",
       "   'booklet_id': 'workshop_10',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Roee Aharoni',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Nouha Dziri',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Song Feng',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yongbin Li',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yu Li',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Hui Wan',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The DialDoc workshop focuses on Document-Grounded Dialogue and Conversational Question Answering. Given the vast amount of content created every day in various mediums, it is a meaningful yet challenging task not only to make such content accessible to end users via various conversational interfaces, but also to make sure the responses provided by the models are grounded and faithful with respect to the knowledge sources.',\n",
       "   'end_time': None,\n",
       "   'id': 'DialDoc',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Dockside 2',\n",
       "   'session': 'DialDoc',\n",
       "   'short_name': 'DialDoc',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://doc2dial.github.io/workshop2023/'},\n",
       "  'IWSLT': {'anthology_venue_id': 'IWSLT',\n",
       "   'booklet_id': 'workshop_4',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Marine Carpuat',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Marcello Federico',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Alex Waibel',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Jan Niehues',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Sebastian Stker',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Elizabeth Salesky',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Atul Kr. Ojha',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The International Conference on Spoken Language Translation (IWSLT) is an annual scientific conference, associated with an open evaluation campaign on spoken language translation, where both scientific papers and system descriptions are presented.',\n",
       "   'end_time': None,\n",
       "   'id': 'IWSLT',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Dockside 1',\n",
       "   'session': 'IWSLT',\n",
       "   'short_name': 'IWSLT',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://iwslt.org/2023/'},\n",
       "  'LAW': {'anthology_venue_id': 'LAW',\n",
       "   'booklet_id': 'workshop_12',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Annemarie Friedrich',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Jakob Prange',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Amir Zeldes',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ines Rehbein',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Linguistic annotation of natural language corpora is the backbone of supervised methods of statistical natural language processing. The Linguistic Annotation Workshop (LAW) is the annual workshop of the ACL Special Interest Group on Annotation (SIGANN), and it provides a forum for the presentation and discussion of innovative research on all aspects of linguistic annotation, including the creation and evaluation of annotation schemes, methods for automatic and manual annotation, use and evaluation of annotation software and frameworks, representation of linguistic data and annotations, semi-supervised human in the loop methods of annotation, crowd-sourcing approaches, and more. As in the past, the LAW will provide a forum for annotation researchers to work towards standardization, best practices, and interoperability of annotation information and software.',\n",
       "   'end_time': None,\n",
       "   'id': 'LAW',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 3',\n",
       "   'session': 'LAW',\n",
       "   'short_name': 'LAW',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sigann.github.io/LAW-XVII-2023/'},\n",
       "  'MATCHING': {'anthology_venue_id': 'MATCHING',\n",
       "   'booklet_id': 'workshop_11',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Dunia Mladeni',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Estevam Hruschka',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Marko Grobelnik',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Sajjadur Rahman',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Tom Mitchell',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Matching Entities from structured and unstructured sources is an important task in many  domains and applications such as HR and E-commerce. For example, in HR platforms/services, it is important to match resumes to job descriptions and job seekers to companies. Similarly in web platforms/services, it is important to match customers to businesses such as hotels and restaurant, among others. In such domains, it is also relevant to match textual customer reviews to customers queries, and sentences (or phrases) as answers to customer questions. Recent advances in Natural Language Processing, Natural Language Understanding, Conversational AI, Language Generation, Machine Learning, Deep Learning, Data Management, Information Extraction, Knowledge Bases/Graphs, (MultiSingle Hop/Commonsense) Inference/Reasoning, Recommendation Systems, and others, have demonstrated promising results in different Matching tasks related (but not limited) to the previously mentioned domains. We believe that there is tremendous opportunity to further exploit and explore the use of advanced NLP (and language related) techniques applied to Matching tasks. Therefore, the goal of this workshop is to bring together the research communities (from academia and industry) of these related areas, that are interested in the development and the application of novel natural-language-based approaches/models/systems to address challenges around different Matching tasks.',\n",
       "   'end_time': None,\n",
       "   'id': 'MATCHING',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Dockside 3',\n",
       "   'session': 'MATCHING',\n",
       "   'short_name': 'MATCHING',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://megagon.ai/matching-2023/'},\n",
       "  'NLP4ConvAI': {'anthology_venue_id': 'NLP4ConvAI',\n",
       "   'booklet_id': 'workshop_14',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Abhinav Rastogi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Georgios Spithourakis',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yun-Nung (Vivian) Chen',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Bing Liu',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yu Li',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Elnaz Nouri',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Alon Albalak',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Alexandros Papangelis',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Over the past decades, mathematicians, linguists, and computer scientists have dedicated their efforts towards empowering human-machine communication in natural language. While in recent years the emergence of virtual personal assistants such as Siri, Alexa, Google Assistant, Cortana, and ChatGPT has pushed the field forward, they may still have numerous challenges. \\\\newline Following the success of the 4th NLP for Conversational AI workshop at ACL, The 5th NLP4ConvAI will be a one-day workshop, co-located with ACL 2023 in Toronto, Canada. The goal of this workshop is to bring together researchers and practitioners to discuss impactful research problems in this area, share findings from real-world applications, and generate ideas for future research directions. \\\\newline The workshop will include keynotes, posters, panel sessions, and a shared task. In keynote talks, senior technical leaders from industry and academia will share insights on the latest developments in the field. We would like to encourage researchers and students to share their prospects and latest discoveries. There will also be a panel discussion with noted conversational AI leaders focused on the state of the field, future directions, and open problems across academia and industry.',\n",
       "   'end_time': None,\n",
       "   'id': 'NLP4ConvAI',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Harbour B',\n",
       "   'session': 'NLP4ConvAI',\n",
       "   'short_name': 'NLP4ConvAI',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/view/5thnlp4convai/'},\n",
       "  'NLRSE': {'anthology_venue_id': 'ACL',\n",
       "   'booklet_id': 'workshop_8',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Peter Clark',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ellie Pavlick',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Denny Zhou',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Noah Goodman',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Sarah Wiegreffe',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Felix Hill',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'With recent scaling of large pre-trained Transformer language models (LLMs), the scope of feasible NLP tasks has broadened. Significant recent work has focused on tasks that require some kind of natural language reasoning. A trajectory in question answering has led us from extraction-oriented datasets like SQuAD to multi-hop reasoning datasets like HotpotQA and StrategyQA. Although LLMs have shown remarkable performance on most NLP tasks, it is often unclear why their answers follow from what they know. To address this gap, a new class of explanation techniques has emerged which play an integral part in structuring the reasoning necessary to solve these datasets. For example, the chain-of-thought paradigm leverages explanations as vehicles for LLMs to mimic human reasoning processes. Entailment trees offer a way to ground multi-step reasoning in a collection of verifiable steps. Frameworks like SayCan bridge high-level planning in language and with low-level action trajectories. As a result, we see a confluence of methods blending explainable machine learning/NLP, classical AI (especially theorem proving), and cognitive science (how do humans structure explanations?). This workshop aims to bring together a diverse set of perspectives from these different traditions and attempt to establish common ground for how these various kinds of explanation structures can tackle a broad class of reasoning problems in natural language and beyond.',\n",
       "   'end_time': None,\n",
       "   'id': 'NLRSE',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 4',\n",
       "   'session': 'NLRSE',\n",
       "   'short_name': 'NLRSE',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://nl-reasoning-workshop.github.io/'},\n",
       "  'Narrative-Understanding': {'anthology_venue_id': 'wnu2023',\n",
       "   'booklet_id': 'workshop_21',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Nader Akoury',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Faeze Brahman',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Khyathi Chandu',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Snigdha Chaturvedi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Elizabeth Clark',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Mohit Iyyer',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'This is the 5th iteration of the Narrative Understanding Workshop, which brings together an interdisciplinary group of researchers from AI, ML, NLP, Computer Vision and other related fields, as well as scholars from the humanities to discuss methods to improve automatic narrative understanding capabilities. The workshop will consist of talks from invited speakers, a panel of researchers and writers, and talks and posters from accepted papers.',\n",
       "   'end_time': None,\n",
       "   'id': 'Narrative-Understanding',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Dockside 2',\n",
       "   'session': 'Narrative-Understanding',\n",
       "   'short_name': 'Narrative-Understanding',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/umass.edu/wnu2023'},\n",
       "  'RepL4NLP': {'anthology_venue_id': 'ACL',\n",
       "   'booklet_id': 'workshop_5',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Burcu Can',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Maximilian Mozes',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Samuel Cahyawijaya',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Naomi Saphra',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Nora Kassner',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Shauli Ravfogel',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Abhilasha Ravichander',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Chen Zhao',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': \"The 8th Workshop on Representation Learning for NLP aims to continue the success of the Repl4NLP workshop series, with the 1st Workshop on Representation Learning for NLP having received about 50 submissions and over 250 attendees - the second most attended collocated event at ACL'16 after WMT. The workshop was introduced as a synthesis of several years of independent *CL workshops focusing on vector space models of meaning, compositionality, and the application of deep neural networks and spectral methods to NLP. It provides a forum for discussing recent advances on these topics, as well as future research directions in linguistically motivated vector-based models in NLP. The workshop will take place in a hybrid setting, and, as in previous years, feature interdisciplinary keynotes, paper presentations, posters, as well as a panel discussion.\",\n",
       "   'end_time': None,\n",
       "   'id': 'RepL4NLP',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Harbour B',\n",
       "   'session': 'RepL4NLP',\n",
       "   'short_name': 'RepL4NLP',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/view/repl4nlp2023'},\n",
       "  'SICon': {'anthology_venue_id': 'SICon',\n",
       "   'booklet_id': 'workshop_18',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Kushal Chawla',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Weiyan Shi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Maximillian Chen',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Liang Qiu',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yu Li',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'James Hale',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Alexandros Papangelis',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Gale Lucas',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Zhou Yu',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': \"Social influence is the change in an individual's thoughts, feelings, attitudes, or behaviors that results from interaction with another individual or a group. For example, a buyer uses social influence skills to engage in trade-offs and build rapport when bargaining with a seller. A therapist uses social influence skills like persuasion to motivate a patient towards physical exercise. Social influence is a core function of human communication, and such scenarios are ubiquitous in everyday life, from negotiations to argumentation to behavioral interventions. Consequently, realistic human-machine conversations must reflect these social influence dynamics, making it essential to systematically model and understand them in dialogue research. This requires perspectives not only from NLP and AI research but also from game theory, emotion, communication, and psychology. \\\\\\\\newline We are excited to host the First Workshop on Social Influence in Conversations (SICon 2023). SICon 2023 will be a one-day hybrid event, co-located with ACL 2023. It would be the first venue that uniquely fosters a dedicated discussion on social influence within NLP while involving researchers from other disciplines such as affective computing and the social sciences. SICon 2023 features keynote talks, panel discussions, poster sessions, and lightning talks for accepted papers. We hope to bring together researchers and practitioners from a wide variety of disciplines to discuss important problems related to social influence, as well as share findings and recent advances. We encourage researchers of all stages and backgrounds to share their exciting work!\",\n",
       "   'end_time': None,\n",
       "   'id': 'SICon',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Harbour A',\n",
       "   'session': 'SICon',\n",
       "   'short_name': 'SICon',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/view/sicon-2023/home'},\n",
       "  'SIGMORPHON': {'anthology_venue_id': 'ACL',\n",
       "   'booklet_id': 'workshop_22',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Garrett Nicolai',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Eleanor Chodroff',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'ar ltekin',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Fred Mailhot',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'SIGMORPHON aims to bring together researchers interested in applying computational techniques to problems in morphology, phonology, and phonetics. Work that addresses orthographic issues is also welcome. Papers will be on substantial, original, and unpublished research on these topics, potentially including strong work in progress.',\n",
       "   'end_time': None,\n",
       "   'id': 'SIGMORPHON',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Dockside 3',\n",
       "   'session': 'SIGMORPHON',\n",
       "   'short_name': 'SIGMORPHON',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sigmorphon.github.io/workshops/2023/'},\n",
       "  'SemEval': {'anthology_venue_id': 'SemEval',\n",
       "   'booklet_id': 'workshop_1',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Ritesh Kumar',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Atul Kr. Ojha',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'A. Seza Doruz',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Giovanni Da San Martino',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Harish Tayyar Madabushi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The 17th edition of SemEval features 12 TASKS on a range of topics, including tasks on idiomaticy detection and embedding, sarcasm detection, multilingual news similarity, and linking mathematical symbols to their descriptions. Several tasks are multilingual, and others ask for multimodal approaches.',\n",
       "   'end_time': None,\n",
       "   'id': 'SemEval',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Queens Quay',\n",
       "   'session': 'SemEval',\n",
       "   'short_name': 'SemEval',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://semeval.github.io/SemEval2023/'},\n",
       "  'SustaiNLP': {'anthology_venue_id': 'SustaiNLP',\n",
       "   'booklet_id': 'workshop_6',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Nafise Sadat Moosavi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Iryna Gurevych',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yufang Hou',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Gyuwan Kim',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Young Jin Kim',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Tal Schuster',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ameeta Agrawal',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The Natural Language Processing (NLP) community has, in recent years, demonstrated a notable focus on improving higher scores on standard benchmarks and taking the lead on community-wide leaderboards (e.g., GLUE, SentEval). While this aspiration has led to improvements in benchmark performance of (predominantly neural) models, it has also came at a cost, i.e., increased model complexity and the ever-growing amount of computational resources required for training and using the current state-of-the-art models. Moreover, the recent research efforts have, for the most part, failed to identify sources of empirical gains in models, often failing to empirically justify the model complexity beyond benchmark performance. \\\\newline Because of these easily observable trends, we have proposed the SustaiNLP workshop with the goal of promoting more sustainable NLP research and practices, with two main objectives: (1) encouraging development of more efficient NLP models; and (2) providing simpler architectures and empirical justification of model complexity. For both aspects, we will encourage submissions from all topical areas of NLP.',\n",
       "   'end_time': None,\n",
       "   'id': 'SustaiNLP',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Harbour C',\n",
       "   'session': 'SustaiNLP',\n",
       "   'short_name': 'SustaiNLP',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://sites.google.com/view/sustainlp2023'},\n",
       "  'TrustNLP': {'anthology_venue_id': 'TrustNLP',\n",
       "   'booklet_id': 'workshop_15',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Yada Pruksachatkun',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ninareh Mehrabi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Kai-Wei Chang',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Aram Galystan',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Jwala Dhamala',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Anaelia Ovalle',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Apurv Verma',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Yang Trista Cao',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Anoop Kumar',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Rahul Gupta',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.',\n",
       "   'end_time': None,\n",
       "   'id': 'TrustNLP',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 4',\n",
       "   'session': 'TrustNLP',\n",
       "   'short_name': 'TrustNLP',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://trustnlpworkshop.github.io/'},\n",
       "  'WASSA': {'anthology_venue_id': 'WASSA',\n",
       "   'booklet_id': 'workshop_16',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Jeremy Barnes',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Orphe De Clercq',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Roman Klinger',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Valentin Barriere',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Salvatore Giorgi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Joa Sedoc',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Shabnam Tafreshi',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Iqra Ameer',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Necva Blc',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Hua Xu',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Ali Al Bataineh',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'Subjectivity and Sentiment Analysis has become a highly developed research area, ranging from binary classification of reviews to the detection of complex emotion structures between entities found in text. This field has expanded both on a practical level, finding numerous successful applications in business, as well as on a theoretical level, allowing researchers to explore more complex research questions related to affective computing. Its continuing importance is also shown by the interest it generates in other disciplines such as Economics, Sociology, Psychology, Marketing, Crisis Management \\\\& Digital Humanities. \\\\\\\\newline The aim of WASSA 2023 is to bring together researchers working on Subjectivity, Sentiment Analysis, Emotion Detection and Classification and their applications to other NLP or real-world tasks (e.g. public health messaging, fake news, media impact analysis, social media mining, computational literary studies) and researchers working on interdisciplinary aspects of affect computation from text.',\n",
       "   'end_time': None,\n",
       "   'id': 'WASSA',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Hourbour C',\n",
       "   'session': 'WASSA',\n",
       "   'short_name': 'WASSA',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://wassa-workshop.github.io/'},\n",
       "  'WOAH': {'anthology_venue_id': 'ACL',\n",
       "   'booklet_id': 'workshop_9',\n",
       "   'chairs': [],\n",
       "   'committee': [{'first_name': None,\n",
       "     'full_name': 'Yi-Ling Chung',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Aida Mostafazadeh Davani',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Debora Nozza',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Paul Rttger',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None},\n",
       "    {'first_name': None,\n",
       "     'full_name': 'Zeerak Talat',\n",
       "     'google_scholar': None,\n",
       "     'last_name': None,\n",
       "     'middle_name': None,\n",
       "     'semantic_scholar': None}],\n",
       "   'description': 'The goal of The Workshop on Online Abuse and Harms (WOAH) is to advance research that develops, interrogates and applies computational methods for detecting, classifying and modelling online abuse.',\n",
       "   'end_time': None,\n",
       "   'id': 'WOAH',\n",
       "   'link': None,\n",
       "   'paper_ids': [],\n",
       "   'room': 'Pier 7 and 8',\n",
       "   'session': 'WOAH',\n",
       "   'short_name': 'WOAH',\n",
       "   'start_time': None,\n",
       "   'track': 'Workshop',\n",
       "   'type': 'Workshops',\n",
       "   'workshop_site_url': 'https://www.workshopononlineabuse.com/'}}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/acl-org/acl-2023-miniconf/main/data/acl_2023/data/conference.json\"\n",
    "data = requests.get(url).json()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec5f38f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Computational Social Science and Cultural Analytics',\n",
       " 'Dialogue and Interactive Systems',\n",
       " 'Discourse and Pragmatics',\n",
       " 'Ethics and NLP',\n",
       " 'Generation',\n",
       " 'Industry',\n",
       " 'Information Extraction',\n",
       " 'Information Retrieval and Text Mining',\n",
       " 'Interpretability and Analysis of Models for NLP',\n",
       " 'Language Grounding to Vision, Robotics, and Beyond',\n",
       " 'Large Language Models',\n",
       " 'Linguistic Diversity',\n",
       " 'Linguistic Theories, Cognitive Modeling, and Psycholinguistics',\n",
       " 'Machine Learning for NLP',\n",
       " 'Machine Translation',\n",
       " 'Multilingualism and Cross-Lingual NLP',\n",
       " 'NLP Applications',\n",
       " 'Phonology, Morphology, and Word Segmentation',\n",
       " 'Question Answering',\n",
       " 'Resources and Evaluation',\n",
       " 'Semantics: Lexical',\n",
       " 'Semantics: Sentence-level Semantics, Textual Inference, and Other Areas',\n",
       " 'Sentiment Analysis, Stylistic Analysis, and Argument Mining',\n",
       " 'Speech and Multimodality',\n",
       " 'Summarization',\n",
       " 'Syntax: Tagging, Chunking, and Parsing',\n",
       " 'Theme: Reality Check'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "tracks = set()\n",
    "main_conference_categories = {'Main-Oral', 'CL', 'TACL', 'Industry', 'Main-Poster'}\n",
    "for key, value in data['papers'].items():\n",
    "    if value['category'] not in main_conference_categories:\n",
    "        continue\n",
    "        \n",
    "    track = re.sub(r'( \\(demo\\))?', '', value['track'])\n",
    "    tracks.add(track)\n",
    "    \n",
    "    \n",
    "tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f07a833d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>area</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2092</td>\n",
       "      <td>Semantics: Sentence-level Semantics, Textual I...</td>\n",
       "      <td>Curing the SICK and other NLI maladies</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2121</td>\n",
       "      <td>Machine Learning for NLP</td>\n",
       "      <td>Certified Robustness to Text Adversarial Attac...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2147</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>Onception: Active Learning with Expert Advice ...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2208</td>\n",
       "      <td>Multilingualism and Cross-Lingual NLP</td>\n",
       "      <td>Data-driven Cross-lingual Syntax: An Agreement...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C2217</td>\n",
       "      <td>Generation</td>\n",
       "      <td>Neural Data-to-Text Generation Based on Small ...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>T4773</td>\n",
       "      <td>Machine Learning for NLP</td>\n",
       "      <td>Rank-Aware Negative Training for Semi-Supervis...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>T4777</td>\n",
       "      <td>Linguistic Theories, Cognitive Modeling, and P...</td>\n",
       "      <td>Transparency Helps Reveal When Language Models...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>T4803</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>Design Choices for Crowdsourcing Implicit Disc...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>T4929</td>\n",
       "      <td>Semantics: Sentence-level Semantics, Textual I...</td>\n",
       "      <td>Time-and-Space-Efficient Weighted Deduction</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>T5043</td>\n",
       "      <td>Semantics: Sentence-level Semantics, Textual I...</td>\n",
       "      <td>Collective Human Opinions in Semantic Textual ...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               area  \\\n",
       "0     C2092  Semantics: Sentence-level Semantics, Textual I...   \n",
       "1     C2121                           Machine Learning for NLP   \n",
       "2     C2147                                Machine Translation   \n",
       "3     C2208              Multilingualism and Cross-Lingual NLP   \n",
       "4     C2217                                         Generation   \n",
       "...     ...                                                ...   \n",
       "1196  T4773                           Machine Learning for NLP   \n",
       "1197  T4777  Linguistic Theories, Cognitive Modeling, and P...   \n",
       "1198  T4803                           Discourse and Pragmatics   \n",
       "1199  T4929  Semantics: Sentence-level Semantics, Textual I...   \n",
       "1200  T5043  Semantics: Sentence-level Semantics, Textual I...   \n",
       "\n",
       "                                                  title source  year   doi  \\\n",
       "0                Curing the SICK and other NLI maladies    ACL  2023  None   \n",
       "1     Certified Robustness to Text Adversarial Attac...    ACL  2023  None   \n",
       "2     Onception: Active Learning with Expert Advice ...    ACL  2023  None   \n",
       "3     Data-driven Cross-lingual Syntax: An Agreement...    ACL  2023  None   \n",
       "4     Neural Data-to-Text Generation Based on Small ...    ACL  2023  None   \n",
       "...                                                 ...    ...   ...   ...   \n",
       "1196  Rank-Aware Negative Training for Semi-Supervis...    ACL  2023  None   \n",
       "1197  Transparency Helps Reveal When Language Models...    ACL  2023  None   \n",
       "1198  Design Choices for Crowdsourcing Implicit Disc...    ACL  2023  None   \n",
       "1199        Time-and-Space-Efficient Weighted Deduction    ACL  2023  None   \n",
       "1200  Collective Human Opinions in Semantic Textual ...    ACL  2023  None   \n",
       "\n",
       "     abstract  \n",
       "0        None  \n",
       "1        None  \n",
       "2        None  \n",
       "3        None  \n",
       "4        None  \n",
       "...       ...  \n",
       "1196     None  \n",
       "1197     None  \n",
       "1198     None  \n",
       "1199     None  \n",
       "1200     None  \n",
       "\n",
       "[1201 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = []\n",
    "for key, value in data['papers'].items():\n",
    "    if value['category'] not in main_conference_categories:\n",
    "        continue\n",
    "    track = re.sub(r'( \\(demo\\))?', '', value['track'])\n",
    "    title = re.sub(r'(\\[.*] )?', '', value['title'])\n",
    "    paper_id = value['id']\n",
    "    source = 'ACL'\n",
    "    year = 2023\n",
    "    doi = None\n",
    "    abstract = value['abstract']\n",
    "    if len(abstract) == 0:\n",
    "        abstract = None\n",
    "    else:\n",
    "        abstract = abstract.replace('\\n', ' ')\n",
    "    paper = {\n",
    "        'id': paper_id,\n",
    "        'area': track,\n",
    "        'title': title,\n",
    "        'source': source,\n",
    "        'year': year,\n",
    "        'doi': doi,\n",
    "        'abstract': abstract,\n",
    "\n",
    "    }\n",
    "    papers.append(paper)\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f45686-b644-4e1b-9453-5b1835ce13aa",
   "metadata": {},
   "source": [
    "# Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "170fa58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.1004</td>\n",
       "      <td>AnswerFact: Fact Checking in Product Question ...</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1004</td>\n",
       "      <td>Product-related question answering platforms n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.1006</td>\n",
       "      <td>Knowledge-Grounded Dialogue Generation with Pr...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1006</td>\n",
       "      <td>We study knowledge-grounded dialogue generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.1009</td>\n",
       "      <td>BiST: Bi-directional Spatio-Temporal Reasoning...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1009</td>\n",
       "      <td>Video-grounded dialogues are very challenging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.1010</td>\n",
       "      <td>A Knowledge-Aware Sequence-to-Tree Network for...</td>\n",
       "      <td>NLP Applications</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1010</td>\n",
       "      <td>With the advancements in natural language proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.1011</td>\n",
       "      <td>Knowledge Association with Hyperbolic Knowledg...</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1011</td>\n",
       "      <td>Capturing associations for knowledge graphs (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9255 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0     main.1004  AnswerFact: Fact Checking in Product Question ...   \n",
       "1     main.1006  Knowledge-Grounded Dialogue Generation with Pr...   \n",
       "2     main.1009  BiST: Bi-directional Spatio-Temporal Reasoning...   \n",
       "3     main.1010  A Knowledge-Aware Sequence-to-Tree Network for...   \n",
       "4     main.1011  Knowledge Association with Hyperbolic Knowledg...   \n",
       "...         ...                                                ...   \n",
       "9250        889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9251       2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9252        384                          Visual Story Post-Editing   \n",
       "9253       1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9254       2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area source  year  \\\n",
       "0                                    Question Answering  EMNLP  2020   \n",
       "1                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "2                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "3                                      NLP Applications  EMNLP  2020   \n",
       "4                                Information Extraction  EMNLP  2020   \n",
       "...                                                 ...    ...   ...   \n",
       "9250  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9251  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9252  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9253  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9254  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "\n",
       "                                   doi  \\\n",
       "0     10.18653/v1/2020.emnlp-main.1004   \n",
       "1     10.18653/v1/2020.emnlp-main.1006   \n",
       "2     10.18653/v1/2020.emnlp-main.1009   \n",
       "3     10.18653/v1/2020.emnlp-main.1010   \n",
       "4     10.18653/v1/2020.emnlp-main.1011   \n",
       "...                                ...   \n",
       "9250                               NaN   \n",
       "9251                               NaN   \n",
       "9252                               NaN   \n",
       "9253                               NaN   \n",
       "9254                               NaN   \n",
       "\n",
       "                                               abstract  \n",
       "0     Product-related question answering platforms n...  \n",
       "1     We study knowledge-grounded dialogue generatio...  \n",
       "2     Video-grounded dialogues are very challenging ...  \n",
       "3     With the advancements in natural language proc...  \n",
       "4     Capturing associations for knowledge graphs (K...  \n",
       "...                                                 ...  \n",
       "9250                                                NaN  \n",
       "9251                                                NaN  \n",
       "9252                                                NaN  \n",
       "9253                                                NaN  \n",
       "9254                                                NaN  \n",
       "\n",
       "[9255 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_df = pd.read_csv('../data/cl_papers.csv', index_col=0)\n",
    "previous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee377101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.1004</td>\n",
       "      <td>AnswerFact: Fact Checking in Product Question ...</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1004</td>\n",
       "      <td>Product-related question answering platforms n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.1006</td>\n",
       "      <td>Knowledge-Grounded Dialogue Generation with Pr...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1006</td>\n",
       "      <td>We study knowledge-grounded dialogue generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.1009</td>\n",
       "      <td>BiST: Bi-directional Spatio-Temporal Reasoning...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1009</td>\n",
       "      <td>Video-grounded dialogues are very challenging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.1010</td>\n",
       "      <td>A Knowledge-Aware Sequence-to-Tree Network for...</td>\n",
       "      <td>NLP Applications</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1010</td>\n",
       "      <td>With the advancements in natural language proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.1011</td>\n",
       "      <td>Knowledge Association with Hyperbolic Knowledg...</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1011</td>\n",
       "      <td>Capturing associations for knowledge graphs (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>889</td>\n",
       "      <td>Multimodal Transformer for Unaligned Multimoda...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>2155</td>\n",
       "      <td>Show, Describe and Conclude: On Exploiting the...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>384</td>\n",
       "      <td>Visual Story Post-Editing</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>1891</td>\n",
       "      <td>Multimodal Abstractive Summarization for How2 ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>2118</td>\n",
       "      <td>Learning to Relate from Captions and Bounding ...</td>\n",
       "      <td>Vision, Robotics, Multimodal, Grounding and Sp...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8054 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0     main.1004  AnswerFact: Fact Checking in Product Question ...   \n",
       "1     main.1006  Knowledge-Grounded Dialogue Generation with Pr...   \n",
       "2     main.1009  BiST: Bi-directional Spatio-Temporal Reasoning...   \n",
       "3     main.1010  A Knowledge-Aware Sequence-to-Tree Network for...   \n",
       "4     main.1011  Knowledge Association with Hyperbolic Knowledg...   \n",
       "...         ...                                                ...   \n",
       "9250        889  Multimodal Transformer for Unaligned Multimoda...   \n",
       "9251       2155  Show, Describe and Conclude: On Exploiting the...   \n",
       "9252        384                          Visual Story Post-Editing   \n",
       "9253       1891  Multimodal Abstractive Summarization for How2 ...   \n",
       "9254       2118  Learning to Relate from Captions and Bounding ...   \n",
       "\n",
       "                                                   area source  year  \\\n",
       "0                                    Question Answering  EMNLP  2020   \n",
       "1                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "2                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "3                                      NLP Applications  EMNLP  2020   \n",
       "4                                Information Extraction  EMNLP  2020   \n",
       "...                                                 ...    ...   ...   \n",
       "9250  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9251  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9252  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9253  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "9254  Vision, Robotics, Multimodal, Grounding and Sp...    ACL  2019   \n",
       "\n",
       "                                   doi  \\\n",
       "0     10.18653/v1/2020.emnlp-main.1004   \n",
       "1     10.18653/v1/2020.emnlp-main.1006   \n",
       "2     10.18653/v1/2020.emnlp-main.1009   \n",
       "3     10.18653/v1/2020.emnlp-main.1010   \n",
       "4     10.18653/v1/2020.emnlp-main.1011   \n",
       "...                                ...   \n",
       "9250                               NaN   \n",
       "9251                               NaN   \n",
       "9252                               NaN   \n",
       "9253                               NaN   \n",
       "9254                               NaN   \n",
       "\n",
       "                                               abstract  \n",
       "0     Product-related question answering platforms n...  \n",
       "1     We study knowledge-grounded dialogue generatio...  \n",
       "2     Video-grounded dialogues are very challenging ...  \n",
       "3     With the advancements in natural language proc...  \n",
       "4     Capturing associations for knowledge graphs (K...  \n",
       "...                                                 ...  \n",
       "9250                                                NaN  \n",
       "9251                                                NaN  \n",
       "9252                                                NaN  \n",
       "9253                                                NaN  \n",
       "9254                                                NaN  \n",
       "\n",
       "[8054 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll delete previous rows from the conference\n",
    "previous_df = previous_df[~((previous_df['source'] == 'ACL') & (previous_df['year'] == 2023))]\n",
    "previous_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "424d2acf-8310-4742-895d-861050171039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>area</th>\n",
       "      <th>source</th>\n",
       "      <th>year</th>\n",
       "      <th>doi</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main.1004</td>\n",
       "      <td>AnswerFact: Fact Checking in Product Question ...</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1004</td>\n",
       "      <td>Product-related question answering platforms n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main.1006</td>\n",
       "      <td>Knowledge-Grounded Dialogue Generation with Pr...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1006</td>\n",
       "      <td>We study knowledge-grounded dialogue generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>main.1009</td>\n",
       "      <td>BiST: Bi-directional Spatio-Temporal Reasoning...</td>\n",
       "      <td>Dialog and Interactive Systems</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1009</td>\n",
       "      <td>Video-grounded dialogues are very challenging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>main.1010</td>\n",
       "      <td>A Knowledge-Aware Sequence-to-Tree Network for...</td>\n",
       "      <td>NLP Applications</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1010</td>\n",
       "      <td>With the advancements in natural language proc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>main.1011</td>\n",
       "      <td>Knowledge Association with Hyperbolic Knowledg...</td>\n",
       "      <td>Information Extraction</td>\n",
       "      <td>EMNLP</td>\n",
       "      <td>2020</td>\n",
       "      <td>10.18653/v1/2020.emnlp-main.1011</td>\n",
       "      <td>Capturing associations for knowledge graphs (K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9250</th>\n",
       "      <td>T4773</td>\n",
       "      <td>Rank-Aware Negative Training for Semi-Supervis...</td>\n",
       "      <td>Machine Learning for NLP</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9251</th>\n",
       "      <td>T4777</td>\n",
       "      <td>Transparency Helps Reveal When Language Models...</td>\n",
       "      <td>Linguistic Theories, Cognitive Modeling, and P...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9252</th>\n",
       "      <td>T4803</td>\n",
       "      <td>Design Choices for Crowdsourcing Implicit Disc...</td>\n",
       "      <td>Discourse and Pragmatics</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>T4929</td>\n",
       "      <td>Time-and-Space-Efficient Weighted Deduction</td>\n",
       "      <td>Semantics: Sentence-level Semantics, Textual I...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>T5043</td>\n",
       "      <td>Collective Human Opinions in Semantic Textual ...</td>\n",
       "      <td>Semantics: Sentence-level Semantics, Textual I...</td>\n",
       "      <td>ACL</td>\n",
       "      <td>2023</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9255 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                              title  \\\n",
       "0     main.1004  AnswerFact: Fact Checking in Product Question ...   \n",
       "1     main.1006  Knowledge-Grounded Dialogue Generation with Pr...   \n",
       "2     main.1009  BiST: Bi-directional Spatio-Temporal Reasoning...   \n",
       "3     main.1010  A Knowledge-Aware Sequence-to-Tree Network for...   \n",
       "4     main.1011  Knowledge Association with Hyperbolic Knowledg...   \n",
       "...         ...                                                ...   \n",
       "9250      T4773  Rank-Aware Negative Training for Semi-Supervis...   \n",
       "9251      T4777  Transparency Helps Reveal When Language Models...   \n",
       "9252      T4803  Design Choices for Crowdsourcing Implicit Disc...   \n",
       "9253      T4929        Time-and-Space-Efficient Weighted Deduction   \n",
       "9254      T5043  Collective Human Opinions in Semantic Textual ...   \n",
       "\n",
       "                                                   area source  year  \\\n",
       "0                                    Question Answering  EMNLP  2020   \n",
       "1                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "2                        Dialog and Interactive Systems  EMNLP  2020   \n",
       "3                                      NLP Applications  EMNLP  2020   \n",
       "4                                Information Extraction  EMNLP  2020   \n",
       "...                                                 ...    ...   ...   \n",
       "9250                           Machine Learning for NLP    ACL  2023   \n",
       "9251  Linguistic Theories, Cognitive Modeling, and P...    ACL  2023   \n",
       "9252                           Discourse and Pragmatics    ACL  2023   \n",
       "9253  Semantics: Sentence-level Semantics, Textual I...    ACL  2023   \n",
       "9254  Semantics: Sentence-level Semantics, Textual I...    ACL  2023   \n",
       "\n",
       "                                   doi  \\\n",
       "0     10.18653/v1/2020.emnlp-main.1004   \n",
       "1     10.18653/v1/2020.emnlp-main.1006   \n",
       "2     10.18653/v1/2020.emnlp-main.1009   \n",
       "3     10.18653/v1/2020.emnlp-main.1010   \n",
       "4     10.18653/v1/2020.emnlp-main.1011   \n",
       "...                                ...   \n",
       "9250                              None   \n",
       "9251                              None   \n",
       "9252                              None   \n",
       "9253                              None   \n",
       "9254                              None   \n",
       "\n",
       "                                               abstract  \n",
       "0     Product-related question answering platforms n...  \n",
       "1     We study knowledge-grounded dialogue generatio...  \n",
       "2     Video-grounded dialogues are very challenging ...  \n",
       "3     With the advancements in natural language proc...  \n",
       "4     Capturing associations for knowledge graphs (K...  \n",
       "...                                                 ...  \n",
       "9250                                               None  \n",
       "9251                                               None  \n",
       "9252                                               None  \n",
       "9253                                               None  \n",
       "9254                                               None  \n",
       "\n",
       "[9255 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df = pd.concat([previous_df, df], ignore_index=True)\n",
    "updated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e6a8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv('../data/cl_papers.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
